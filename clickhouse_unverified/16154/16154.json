{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 16154,
  "instance_id": "ClickHouse__ClickHouse-16154",
  "issue_numbers": [
    "6636"
  ],
  "base_commit": "e853af9b4f05391c544feb90c8e72ee4ead91478",
  "patch": "diff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 8f303e3fb486..f71dd20a1b52 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -153,6 +153,7 @@ class IColumn;\n     \\\n     M(DistributedProductMode, distributed_product_mode, DistributedProductMode::DENY, \"How are distributed subqueries performed inside IN or JOIN sections?\", IMPORTANT) \\\n     \\\n+    M(UInt64, max_concurrent_queries_for_all_users, 0, \"The maximum number of concurrent requests for all users.\", 0) \\\n     M(UInt64, max_concurrent_queries_for_user, 0, \"The maximum number of concurrent requests per user.\", 0) \\\n     \\\n     M(Bool, insert_deduplicate, true, \"For INSERT queries in the replicated table, specifies that deduplication of insertings blocks should be performed\", 0) \\\ndiff --git a/src/Interpreters/ProcessList.cpp b/src/Interpreters/ProcessList.cpp\nindex 2203e91e9135..dc021210d9d5 100644\n--- a/src/Interpreters/ProcessList.cpp\n+++ b/src/Interpreters/ProcessList.cpp\n@@ -89,6 +89,34 @@ ProcessList::EntryPtr ProcessList::insert(const String & query_, const IAST * as\n                 throw Exception(\"Too many simultaneous queries. Maximum: \" + toString(max_size), ErrorCodes::TOO_MANY_SIMULTANEOUS_QUERIES);\n         }\n \n+        {\n+            /**\n+             * `max_size` check above is controlled by `max_concurrent_queries` server setting and is a \"hard\" limit for how many\n+             * queries the server can process concurrently. It is configured at startup. When the server is overloaded with queries and the\n+             * hard limit is reached it is impossible to connect to the server to run queries for investigation.\n+             *\n+             * With `max_concurrent_queries_for_all_users` it is possible to configure an additional, runtime configurable, limit for query concurrency.\n+             * Usually it should be configured just once for `default_profile` which is inherited by all users. DBAs can override\n+             * this setting when connecting to ClickHouse, or it can be configured for a DBA profile to have a value greater than that of\n+             * the default profile (or 0 for unlimited).\n+             *\n+             * One example is to set `max_size=X`, `max_concurrent_queries_for_all_users=X-10` for default profile,\n+             * and `max_concurrent_queries_for_all_users=0` for DBAs or accounts that are vital for ClickHouse operations (like metrics\n+             * exporters).\n+             *\n+             * Another creative example is to configure `max_concurrent_queries_for_all_users=50` for \"analyst\" profiles running adhoc queries\n+             * and `max_concurrent_queries_for_all_users=100` for \"customer facing\" services. This way \"analyst\" queries will be rejected\n+             * once is already processing 50+ concurrent queries (including analysts or any other users).\n+             */\n+\n+            if (!is_unlimited_query && settings.max_concurrent_queries_for_all_users\n+                && processes.size() >= settings.max_concurrent_queries_for_all_users)\n+                throw Exception(\n+                    \"Too many simultaneous queries for all users. Current: \" + toString(processes.size())\n+                    + \", maximum: \" + settings.max_concurrent_queries_for_all_users.toString(),\n+                    ErrorCodes::TOO_MANY_SIMULTANEOUS_QUERIES);\n+        }\n+\n         /** Why we use current user?\n           * Because initial one is passed by client and credentials for it is not verified,\n           *  and using initial_user for limits will be insecure.\n",
  "test_patch": "diff --git a/tests/integration/test_concurrent_queries_for_all_users_restriction/__init__.py b/tests/integration/test_concurrent_queries_for_all_users_restriction/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_concurrent_queries_for_all_users_restriction/configs/user_restrictions.xml b/tests/integration/test_concurrent_queries_for_all_users_restriction/configs/user_restrictions.xml\nnew file mode 100644\nindex 000000000000..87790f2536c3\n--- /dev/null\n+++ b/tests/integration/test_concurrent_queries_for_all_users_restriction/configs/user_restrictions.xml\n@@ -0,0 +1,38 @@\n+<yandex>\n+    <profiles>\n+        <default>\n+            <max_memory_usage>10000000000</max_memory_usage>\n+            <use_uncompressed_cache>0</use_uncompressed_cache>\n+            <load_balancing>random</load_balancing>\n+            <max_concurrent_queries_for_all_users>2</max_concurrent_queries_for_all_users>\n+        </default>\n+        <someuser>\n+            <max_memory_usage>10000000000</max_memory_usage>\n+            <use_uncompressed_cache>0</use_uncompressed_cache>\n+            <load_balancing>random</load_balancing>\n+        </someuser>\n+    </profiles>\n+    <users>\n+        <default>\n+            <password></password>\n+            <networks incl=\"networks\" replace=\"replace\">\n+                <ip>::/0</ip>\n+            </networks>\n+            <profile>default</profile>\n+            <quota>default</quota>\n+        </default>\n+        <someuser>\n+            <password></password>\n+            <networks incl=\"networks\" replace=\"replace\">\n+                <ip>::/0</ip>\n+            </networks>\n+            <profile>someuser</profile>\n+            <quota>default</quota>\n+        </someuser>\n+    </users>\n+\n+    <quotas>\n+        <default>\n+        </default>\n+    </quotas>\n+</yandex>\ndiff --git a/tests/integration/test_concurrent_queries_for_all_users_restriction/test.py b/tests/integration/test_concurrent_queries_for_all_users_restriction/test.py\nnew file mode 100644\nindex 000000000000..ac6e87cdee58\n--- /dev/null\n+++ b/tests/integration/test_concurrent_queries_for_all_users_restriction/test.py\n@@ -0,0 +1,41 @@\n+import time\n+from multiprocessing.dummy import Pool\n+\n+import pytest\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node1 = cluster.add_instance('node1', user_configs=['configs/user_restrictions.xml'])\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+        node1.query(\"create table nums (number UInt64) ENGINE = MergeTree() order by tuple()\")\n+        node1.query(\"insert into nums values (0), (1)\")\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_exception_message(started_cluster):\n+    assert node1.query(\"select number from nums order by number\") == \"0\\n1\\n\"\n+\n+    def node_busy(_):\n+        for i in range(10):\n+            node1.query(\"select sleep(2)\", user='someuser', ignore_error=True)\n+\n+    busy_pool = Pool(3)\n+    busy_pool.map_async(node_busy, range(3))\n+    time.sleep(1)  # wait a little until polling starts\n+\n+    with pytest.raises(Exception) as exc_info:\n+        for i in range(3):\n+            assert node1.query(\"select number from remote('node1', 'default', 'nums')\", user='default') == \"0\\n1\\n\"\n+    exc_info.match(\"Too many simultaneous queries for all users\")\n+\n+    for i in range(3):\n+        assert node1.query(\"select number from remote('node1', 'default', 'nums')\", user='default',\n+                           settings={'max_concurrent_queries_for_all_users': 0}) == \"0\\n1\\n\"\n",
  "problem_statement": "Add \"max_simultaneous_queries_for_all_users\" settings.\n**Use case**\r\nTo set the limit on total number of queries. That limit can be overriden. For example, it can be set to 99 for all users and dba can set it to 100 for itself to run queries even when the server is overloaded.\r\n\r\n**Describe alternatives you've considered**\r\nThere is global `max_connections` and `max_concurrent_queries` parameters in config. When the server is overloaded, it's impossible to connect and run queries for investigation.\n",
  "hints_text": "Maybe instead add a option to skip max_concurrent_queries check? Use cases include not just dba, but also metrics exporters (prometheus exporter) that need to query system tables.\r\n\r\nAt the moment if the limit is hit you lose visibility into ClickHouse metrics which is not ideal.\nOr, make this part of resource pools feature, which ideally would be hierarchical. One pool for critical queries with say limit of 10 concurrent requests. Another pool for production workload with say limit of 100, with nested pools to split up this production workload between services if necessary.\n> Maybe instead add a option to skip max_concurrent_queries check?\r\n\r\nThe user will be able to set this limit to zero, that means unlimited. Also the user can set it to some high enough value like 1000 while still limiting other users to 100.\r\n\r\n> Or, make this part of resource pools feature, which ideally would be hierarchical.\r\n\r\nThis task has more narrow scope and should be as simple as possible.",
  "created_at": "2020-10-19T14:53:29Z"
}