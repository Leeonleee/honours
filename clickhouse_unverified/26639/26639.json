{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 26639,
  "instance_id": "ClickHouse__ClickHouse-26639",
  "issue_numbers": [
    "26914",
    "25447"
  ],
  "base_commit": "8a843ae15f275694bf925a04ca8795c9a65e85f6",
  "patch": "diff --git a/src/AggregateFunctions/UniqVariadicHash.h b/src/AggregateFunctions/UniqVariadicHash.h\nindex b3607a632852..94f54a7a0590 100644\n--- a/src/AggregateFunctions/UniqVariadicHash.h\n+++ b/src/AggregateFunctions/UniqVariadicHash.h\n@@ -5,6 +5,7 @@\n #include <Common/SipHash.h>\n #include <Common/assert_cast.h>\n #include <Columns/ColumnTuple.h>\n+#include <DataTypes/IDataType.h>\n \n \n namespace DB\ndiff --git a/src/Columns/ColumnTuple.h b/src/Columns/ColumnTuple.h\nindex 3f5422c77195..177ff6c412a9 100644\n--- a/src/Columns/ColumnTuple.h\n+++ b/src/Columns/ColumnTuple.h\n@@ -1,6 +1,6 @@\n #pragma once\n \n-#include <Core/Block.h>\n+#include <Columns/IColumn.h>\n \n \n namespace DB\ndiff --git a/src/Core/Block.cpp b/src/Core/Block.cpp\nindex 96667862e417..ddfd62c2efbe 100644\n--- a/src/Core/Block.cpp\n+++ b/src/Core/Block.cpp\n@@ -22,6 +22,85 @@ namespace ErrorCodes\n     extern const int POSITION_OUT_OF_BOUND;\n     extern const int NOT_FOUND_COLUMN_IN_BLOCK;\n     extern const int SIZES_OF_COLUMNS_DOESNT_MATCH;\n+    extern const int AMBIGUOUS_COLUMN_NAME;\n+}\n+\n+template <typename ReturnType>\n+static ReturnType onError(const std::string & message [[maybe_unused]], int code [[maybe_unused]])\n+{\n+    if constexpr (std::is_same_v<ReturnType, void>)\n+        throw Exception(message, code);\n+    else\n+        return false;\n+};\n+\n+\n+template <typename ReturnType>\n+static ReturnType checkColumnStructure(const ColumnWithTypeAndName & actual, const ColumnWithTypeAndName & expected,\n+    const std::string & context_description, bool allow_remove_constants, int code)\n+{\n+    if (actual.name != expected.name)\n+        return onError<ReturnType>(\"Block structure mismatch in \" + context_description + \" stream: different names of columns:\\n\"\n+            + actual.dumpStructure() + \"\\n\" + expected.dumpStructure(), code);\n+\n+    if (!actual.type->equals(*expected.type))\n+        return onError<ReturnType>(\"Block structure mismatch in \" + context_description + \" stream: different types:\\n\"\n+            + actual.dumpStructure() + \"\\n\" + expected.dumpStructure(), code);\n+\n+    if (!actual.column || !expected.column)\n+        return ReturnType(true);\n+\n+    const IColumn * actual_column = actual.column.get();\n+\n+    /// If we allow to remove constants, and expected column is not const, then unwrap actual constant column.\n+    if (allow_remove_constants && !isColumnConst(*expected.column))\n+    {\n+        if (const auto * column_const = typeid_cast<const ColumnConst *>(actual_column))\n+            actual_column = &column_const->getDataColumn();\n+    }\n+\n+    if (actual_column->getName() != expected.column->getName())\n+        return onError<ReturnType>(\"Block structure mismatch in \" + context_description + \" stream: different columns:\\n\"\n+            + actual.dumpStructure() + \"\\n\" + expected.dumpStructure(), code);\n+\n+    if (isColumnConst(*actual.column) && isColumnConst(*expected.column))\n+    {\n+        Field actual_value = assert_cast<const ColumnConst &>(*actual.column).getField();\n+        Field expected_value = assert_cast<const ColumnConst &>(*expected.column).getField();\n+\n+        if (actual_value != expected_value)\n+            return onError<ReturnType>(\"Block structure mismatch in \" + context_description + \" stream: different values of constants, actual: \"\n+                + applyVisitor(FieldVisitorToString(), actual_value) + \", expected: \" + applyVisitor(FieldVisitorToString(), expected_value),\n+                code);\n+    }\n+\n+    return ReturnType(true);\n+}\n+\n+\n+template <typename ReturnType>\n+static ReturnType checkBlockStructure(const Block & lhs, const Block & rhs, const std::string & context_description, bool allow_remove_constants)\n+{\n+    size_t columns = rhs.columns();\n+    if (lhs.columns() != columns)\n+        return onError<ReturnType>(\"Block structure mismatch in \" + context_description + \" stream: different number of columns:\\n\"\n+            + lhs.dumpStructure() + \"\\n\" + rhs.dumpStructure(), ErrorCodes::LOGICAL_ERROR);\n+\n+    for (size_t i = 0; i < columns; ++i)\n+    {\n+        const auto & actual = lhs.getByPosition(i);\n+        const auto & expected = rhs.getByPosition(i);\n+\n+        if constexpr (std::is_same_v<ReturnType, bool>)\n+        {\n+            if (!checkColumnStructure<ReturnType>(actual, expected, context_description, allow_remove_constants, ErrorCodes::LOGICAL_ERROR))\n+                return false;\n+        }\n+        else\n+            checkColumnStructure<ReturnType>(actual, expected, context_description, allow_remove_constants, ErrorCodes::LOGICAL_ERROR);\n+    }\n+\n+    return ReturnType(true);\n }\n \n \n@@ -57,24 +136,41 @@ void Block::insert(size_t position, ColumnWithTypeAndName elem)\n         throw Exception(\"Position out of bound in Block::insert(), max position = \"\n         + toString(data.size()), ErrorCodes::POSITION_OUT_OF_BOUND);\n \n+    if (elem.name.empty())\n+        throw Exception(\"Column name in Block cannot be empty\", ErrorCodes::AMBIGUOUS_COLUMN_NAME);\n+\n     for (auto & name_pos : index_by_name)\n         if (name_pos.second >= position)\n             ++name_pos.second;\n \n-    index_by_name.emplace(elem.name, position);\n+    auto [it, inserted] = index_by_name.emplace(elem.name, position);\n+    if (!inserted)\n+        checkColumnStructure<void>(data[it->second], elem,\n+            \"(columns with identical name must have identical structure)\", true, ErrorCodes::AMBIGUOUS_COLUMN_NAME);\n+\n     data.emplace(data.begin() + position, std::move(elem));\n }\n \n \n void Block::insert(ColumnWithTypeAndName elem)\n {\n-    index_by_name.emplace(elem.name, data.size());\n+    if (elem.name.empty())\n+        throw Exception(\"Column name in Block cannot be empty\", ErrorCodes::AMBIGUOUS_COLUMN_NAME);\n+\n+    auto [it, inserted] = index_by_name.emplace(elem.name, data.size());\n+    if (!inserted)\n+        checkColumnStructure<void>(data[it->second], elem,\n+            \"(columns with identical name must have identical structure)\", true, ErrorCodes::AMBIGUOUS_COLUMN_NAME);\n+\n     data.emplace_back(std::move(elem));\n }\n \n \n void Block::insertUnique(ColumnWithTypeAndName elem)\n {\n+    if (elem.name.empty())\n+        throw Exception(\"Column name in Block cannot be empty\", ErrorCodes::AMBIGUOUS_COLUMN_NAME);\n+\n     if (index_by_name.end() == index_by_name.find(elem.name))\n         insert(std::move(elem));\n }\n@@ -487,67 +583,6 @@ DataTypes Block::getDataTypes() const\n }\n \n \n-template <typename ReturnType>\n-static ReturnType checkBlockStructure(const Block & lhs, const Block & rhs, const std::string & context_description, bool allow_remove_constants)\n-{\n-    auto on_error = [](const std::string & message [[maybe_unused]], int code [[maybe_unused]])\n-    {\n-        if constexpr (std::is_same_v<ReturnType, void>)\n-            throw Exception(message, code);\n-        else\n-            return false;\n-    };\n-\n-    size_t columns = rhs.columns();\n-    if (lhs.columns() != columns)\n-        return on_error(\"Block structure mismatch in \" + context_description + \" stream: different number of columns:\\n\"\n-            + lhs.dumpStructure() + \"\\n\" + rhs.dumpStructure(), ErrorCodes::LOGICAL_ERROR);\n-\n-    for (size_t i = 0; i < columns; ++i)\n-    {\n-        const auto & expected = rhs.getByPosition(i);\n-        const auto & actual = lhs.getByPosition(i);\n-\n-        if (actual.name != expected.name)\n-            return on_error(\"Block structure mismatch in \" + context_description + \" stream: different names of columns:\\n\"\n-                + lhs.dumpStructure() + \"\\n\" + rhs.dumpStructure(), ErrorCodes::LOGICAL_ERROR);\n-\n-        if (!actual.type->equals(*expected.type))\n-            return on_error(\"Block structure mismatch in \" + context_description + \" stream: different types:\\n\"\n-                + lhs.dumpStructure() + \"\\n\" + rhs.dumpStructure(), ErrorCodes::LOGICAL_ERROR);\n-\n-        if (!actual.column || !expected.column)\n-            continue;\n-\n-        const IColumn * actual_column = actual.column.get();\n-\n-        /// If we allow to remove constants, and expected column is not const, then unwrap actual constant column.\n-        if (allow_remove_constants && !isColumnConst(*expected.column))\n-        {\n-            if (const auto * column_const = typeid_cast<const ColumnConst *>(actual_column))\n-                actual_column = &column_const->getDataColumn();\n-        }\n-\n-        if (actual_column->getName() != expected.column->getName())\n-            return on_error(\"Block structure mismatch in \" + context_description + \" stream: different columns:\\n\"\n-                + lhs.dumpStructure() + \"\\n\" + rhs.dumpStructure(), ErrorCodes::LOGICAL_ERROR);\n-\n-        if (isColumnConst(*actual.column) && isColumnConst(*expected.column))\n-        {\n-            Field actual_value = assert_cast<const ColumnConst &>(*actual.column).getField();\n-            Field expected_value = assert_cast<const ColumnConst &>(*expected.column).getField();\n-\n-            if (actual_value != expected_value)\n-                return on_error(\"Block structure mismatch in \" + context_description + \" stream: different values of constants, actual: \"\n-                    + applyVisitor(FieldVisitorToString(), actual_value) + \", expected: \" + applyVisitor(FieldVisitorToString(), expected_value),\n-                    ErrorCodes::LOGICAL_ERROR);\n-        }\n-    }\n-\n-    return ReturnType(true);\n-}\n-\n-\n bool blocksHaveEqualStructure(const Block & lhs, const Block & rhs)\n {\n     return checkBlockStructure<bool>(lhs, rhs, {}, false);\ndiff --git a/src/Functions/in.cpp b/src/Functions/in.cpp\nindex 7cd9f64004dd..db4194308fc8 100644\n--- a/src/Functions/in.cpp\n+++ b/src/Functions/in.cpp\n@@ -102,7 +102,7 @@ class FunctionIn : public IFunction\n             throw Exception(\"Second argument for function '\" + getName() + \"' must be Set; found \" + column_set_ptr->getName(),\n                 ErrorCodes::ILLEGAL_COLUMN);\n \n-        DB::Block columns_of_key_columns;\n+        ColumnsWithTypeAndName columns_of_key_columns;\n \n         /// First argument may be a tuple or a single column.\n         const ColumnWithTypeAndName & left_arg = arguments[0];\n@@ -125,16 +125,16 @@ class FunctionIn : public IFunction\n             const DataTypes & tuple_types = type_tuple->getElements();\n             size_t tuple_size = tuple_columns.size();\n             for (size_t i = 0; i < tuple_size; ++i)\n-                columns_of_key_columns.insert({ tuple_columns[i], tuple_types[i], \"\" });\n+                columns_of_key_columns.emplace_back(tuple_columns[i], tuple_types[i], \"_\" + toString(i));\n         }\n         else\n-            columns_of_key_columns.insert(left_arg);\n+            columns_of_key_columns.emplace_back(left_arg);\n \n         /// Replace single LowCardinality column to it's dictionary if possible.\n         ColumnPtr lc_indexes = nullptr;\n-        if (columns_of_key_columns.columns() == 1)\n+        if (columns_of_key_columns.size() == 1)\n         {\n-            auto & arg = columns_of_key_columns.safeGetByPosition(0);\n+            auto & arg = columns_of_key_columns.at(0);\n             const auto * col = arg.column.get();\n             if (const auto * const_col = typeid_cast<const ColumnConst *>(col))\n                 col = &const_col->getDataColumn();\ndiff --git a/src/Interpreters/ActionsVisitor.cpp b/src/Interpreters/ActionsVisitor.cpp\nindex 61e484ff6f19..9a27043160f6 100644\n--- a/src/Interpreters/ActionsVisitor.cpp\n+++ b/src/Interpreters/ActionsVisitor.cpp\n@@ -374,8 +374,8 @@ SetPtr makeExplicitSet(\n \n     SetPtr set\n         = std::make_shared<Set>(size_limits, create_ordered_set, context->getSettingsRef().transform_null_in);\n-    set->setHeader(block.cloneEmpty());\n-    set->insertFromBlock(block);\n+    set->setHeader(block.cloneEmpty().getColumnsWithTypeAndName());\n+    set->insertFromBlock(block.getColumnsWithTypeAndName());\n     set->finishInsert();\n \n     prepared_sets[set_key] = set;\ndiff --git a/src/Interpreters/ExecuteScalarSubqueriesVisitor.cpp b/src/Interpreters/ExecuteScalarSubqueriesVisitor.cpp\nindex f46cbdd24652..2b858512b988 100644\n--- a/src/Interpreters/ExecuteScalarSubqueriesVisitor.cpp\n+++ b/src/Interpreters/ExecuteScalarSubqueriesVisitor.cpp\n@@ -80,9 +80,13 @@ void ExecuteScalarSubqueriesMatcher::visit(const ASTSubquery & subquery, ASTPtr\n \n     Block scalar;\n     if (data.getContext()->hasQueryContext() && data.getContext()->getQueryContext()->hasScalar(scalar_query_hash_str))\n+    {\n         scalar = data.getContext()->getQueryContext()->getScalar(scalar_query_hash_str);\n+    }\n     else if (data.scalars.count(scalar_query_hash_str))\n+    {\n         scalar = data.scalars[scalar_query_hash_str];\n+    }\n     else\n     {\n         auto subquery_context = Context::createCopy(data.getContext());\n@@ -149,7 +153,8 @@ void ExecuteScalarSubqueriesMatcher::visit(const ASTSubquery & subquery, ASTPtr\n                 throw Exception(\"Scalar subquery returned more than one row\", ErrorCodes::INCORRECT_RESULT_OF_SCALAR_SUBQUERY);\n \n             Block tmp_block;\n-            while (tmp_block.rows() == 0 && executor.pull(tmp_block));\n+            while (tmp_block.rows() == 0 && executor.pull(tmp_block))\n+                ;\n \n             if (tmp_block.rows() != 0)\n                 throw Exception(\"Scalar subquery returned more than one row\", ErrorCodes::INCORRECT_RESULT_OF_SCALAR_SUBQUERY);\n@@ -173,10 +178,10 @@ void ExecuteScalarSubqueriesMatcher::visit(const ASTSubquery & subquery, ASTPtr\n         }\n         else\n         {\n-            ColumnWithTypeAndName ctn;\n-            ctn.type = std::make_shared<DataTypeTuple>(block.getDataTypes());\n-            ctn.column = ColumnTuple::create(block.getColumns());\n-            scalar.insert(ctn);\n+            scalar.insert({\n+                ColumnTuple::create(block.getColumns()),\n+                std::make_shared<DataTypeTuple>(block.getDataTypes()),\n+                \"tuple\"});\n         }\n     }\n \ndiff --git a/src/Interpreters/ExpressionActions.cpp b/src/Interpreters/ExpressionActions.cpp\nindex ef5c1f8e48f8..1dd4f7628f8b 100644\n--- a/src/Interpreters/ExpressionActions.cpp\n+++ b/src/Interpreters/ExpressionActions.cpp\n@@ -800,12 +800,15 @@ ExpressionActionsChain::JoinStep::JoinStep(\n     : Step({})\n     , analyzed_join(std::move(analyzed_join_))\n     , join(std::move(join_))\n-    , result_columns(std::move(required_columns_))\n {\n-    for (const auto & column : result_columns)\n+    for (const auto & column : required_columns_)\n         required_columns.emplace_back(column.name, column.type);\n \n-    analyzed_join->addJoinedColumnsAndCorrectTypes(result_columns);\n+    NamesAndTypesList result_names_and_types = required_columns;\n+    analyzed_join->addJoinedColumnsAndCorrectTypes(result_names_and_types);\n+    for (const auto & [name, type] : result_names_and_types)\n+        /// `column` is `nullptr` because we don't care on constness here, it may be changed in join\n+        result_columns.emplace_back(nullptr, type, name);\n }\n \n void ExpressionActionsChain::JoinStep::finalize(const NameSet & required_output_)\ndiff --git a/src/Interpreters/ExpressionAnalyzer.cpp b/src/Interpreters/ExpressionAnalyzer.cpp\nindex 77598e69c00a..c8a5ed6c56af 100644\n--- a/src/Interpreters/ExpressionAnalyzer.cpp\n+++ b/src/Interpreters/ExpressionAnalyzer.cpp\n@@ -216,7 +216,7 @@ void ExpressionAnalyzer::analyzeAggregation()\n         if (join)\n         {\n             getRootActionsNoMakeSet(analyzedJoin().leftKeysList(), true, temp_actions, false);\n-            auto sample_columns = temp_actions->getResultColumns();\n+            auto sample_columns = temp_actions->getNamesAndTypesList();\n             analyzedJoin().addJoinedColumnsAndCorrectTypes(sample_columns);\n             temp_actions = std::make_shared<ActionsDAG>(sample_columns);\n         }\n@@ -337,7 +337,7 @@ void ExpressionAnalyzer::tryMakeSetForIndexFromSubquery(const ASTPtr & subquery_\n     PullingAsyncPipelineExecutor executor(io.pipeline);\n \n     SetPtr set = std::make_shared<Set>(settings.size_limits_for_set, true, getContext()->getSettingsRef().transform_null_in);\n-    set->setHeader(executor.getHeader());\n+    set->setHeader(executor.getHeader().getColumnsWithTypeAndName());\n \n     Block block;\n     while (executor.pull(block))\n@@ -346,7 +346,7 @@ void ExpressionAnalyzer::tryMakeSetForIndexFromSubquery(const ASTPtr & subquery_\n             continue;\n \n         /// If the limits have been exceeded, give up and let the default subquery processing actions take place.\n-        if (!set->insertFromBlock(block))\n+        if (!set->insertFromBlock(block.getColumnsWithTypeAndName()))\n             return;\n     }\n \n@@ -1206,7 +1206,7 @@ void SelectQueryExpressionAnalyzer::appendSelect(ExpressionActionsChain & chain,\n }\n \n ActionsDAGPtr SelectQueryExpressionAnalyzer::appendOrderBy(ExpressionActionsChain & chain, bool only_types, bool optimize_read_in_order,\n-                                                  ManyExpressionActions & order_by_elements_actions)\n+                                                           ManyExpressionActions & order_by_elements_actions)\n {\n     const auto * select_query = getSelectQuery();\n \ndiff --git a/src/Interpreters/Set.cpp b/src/Interpreters/Set.cpp\nindex ff502b499cd7..5ab59ba3f079 100644\n--- a/src/Interpreters/Set.cpp\n+++ b/src/Interpreters/Set.cpp\n@@ -99,14 +99,14 @@ void NO_INLINE Set::insertFromBlockImplCase(\n }\n \n \n-void Set::setHeader(const Block & header)\n+void Set::setHeader(const ColumnsWithTypeAndName & header)\n {\n     std::unique_lock lock(rwlock);\n \n     if (!data.empty())\n         return;\n \n-    keys_size = header.columns();\n+    keys_size = header.size();\n     ColumnRawPtrs key_columns;\n     key_columns.reserve(keys_size);\n     data_types.reserve(keys_size);\n@@ -118,10 +118,10 @@ void Set::setHeader(const Block & header)\n     /// Remember the columns we will work with\n     for (size_t i = 0; i < keys_size; ++i)\n     {\n-        materialized_columns.emplace_back(header.safeGetByPosition(i).column->convertToFullColumnIfConst());\n+        materialized_columns.emplace_back(header.at(i).column->convertToFullColumnIfConst());\n         key_columns.emplace_back(materialized_columns.back().get());\n-        data_types.emplace_back(header.safeGetByPosition(i).type);\n-        set_elements_types.emplace_back(header.safeGetByPosition(i).type);\n+        data_types.emplace_back(header.at(i).type);\n+        set_elements_types.emplace_back(header.at(i).type);\n \n         /// Convert low cardinality column to full.\n         if (const auto * low_cardinality_type = typeid_cast<const DataTypeLowCardinality *>(data_types.back().get()))\n@@ -161,7 +161,7 @@ void Set::setHeader(const Block & header)\n }\n \n \n-bool Set::insertFromBlock(const Block & block)\n+bool Set::insertFromBlock(const ColumnsWithTypeAndName & columns)\n {\n     std::unique_lock lock(rwlock);\n \n@@ -177,11 +177,11 @@ bool Set::insertFromBlock(const Block & block)\n     /// Remember the columns we will work with\n     for (size_t i = 0; i < keys_size; ++i)\n     {\n-        materialized_columns.emplace_back(block.safeGetByPosition(i).column->convertToFullColumnIfConst()->convertToFullColumnIfLowCardinality());\n+        materialized_columns.emplace_back(columns.at(i).column->convertToFullColumnIfConst()->convertToFullColumnIfLowCardinality());\n         key_columns.emplace_back(materialized_columns.back().get());\n     }\n \n-    size_t rows = block.rows();\n+    size_t rows = columns.at(0).column->size();\n \n     /// We will insert to the Set only keys, where all components are not NULL.\n     ConstNullMapPtr null_map{};\n@@ -192,7 +192,7 @@ bool Set::insertFromBlock(const Block & block)\n     /// Filter to extract distinct values from the block.\n     ColumnUInt8::MutablePtr filter;\n     if (fill_set_elements)\n-        filter = ColumnUInt8::create(block.rows());\n+        filter = ColumnUInt8::create(rows);\n \n     switch (data.type)\n     {\n@@ -224,16 +224,16 @@ bool Set::insertFromBlock(const Block & block)\n }\n \n \n-ColumnPtr Set::execute(const Block & block, bool negative) const\n+ColumnPtr Set::execute(const ColumnsWithTypeAndName & columns, bool negative) const\n {\n-    size_t num_key_columns = block.columns();\n+    size_t num_key_columns = columns.size();\n \n     if (0 == num_key_columns)\n         throw Exception(\"Logical error: no columns passed to Set::execute method.\", ErrorCodes::LOGICAL_ERROR);\n \n     auto res = ColumnUInt8::create();\n     ColumnUInt8::Container & vec_res = res->getData();\n-    vec_res.resize(block.safeGetByPosition(0).column->size());\n+    vec_res.resize(columns.at(0).column->size());\n \n     if (vec_res.empty())\n         return res;\n@@ -264,7 +264,7 @@ ColumnPtr Set::execute(const Block & block, bool negative) const\n     {\n         ColumnPtr result;\n \n-        const auto & column_before_cast = block.safeGetByPosition(i);\n+        const auto & column_before_cast = columns.at(i);\n         ColumnWithTypeAndName column_to_cast\n             = {column_before_cast.column->convertToFullColumnIfConst(), column_before_cast.type, column_before_cast.name};\n \n@@ -428,7 +428,7 @@ MergeTreeSetIndex::MergeTreeSetIndex(const Columns & set_elements, std::vector<K\n     SortDescription sort_description;\n     for (size_t i = 0; i < tuple_size; ++i)\n     {\n-        block_to_sort.insert({ ordered_set[i], nullptr, \"\" });\n+        block_to_sort.insert({ ordered_set[i], nullptr, \"_\" + toString(i) });\n         sort_description.emplace_back(i, 1, 1);\n     }\n \ndiff --git a/src/Interpreters/Set.h b/src/Interpreters/Set.h\nindex 9bf6630b8448..727a2c144a19 100644\n--- a/src/Interpreters/Set.h\n+++ b/src/Interpreters/Set.h\n@@ -42,10 +42,10 @@ class Set\n     /** Create a Set from stream.\n       * Call setHeader, then call insertFromBlock for each block.\n       */\n-    void setHeader(const Block & header);\n+    void setHeader(const ColumnsWithTypeAndName & header);\n \n     /// Returns false, if some limit was exceeded and no need to insert more data.\n-    bool insertFromBlock(const Block & block);\n+    bool insertFromBlock(const ColumnsWithTypeAndName & columns);\n     /// Call after all blocks were inserted. To get the information that set is already created.\n     void finishInsert() { is_created = true; }\n \n@@ -54,7 +54,7 @@ class Set\n     /** For columns of 'block', check belonging of corresponding rows to the set.\n       * Return UInt8 column with the result.\n       */\n-    ColumnPtr execute(const Block & block, bool negative) const;\n+    ColumnPtr execute(const ColumnsWithTypeAndName & columns, bool negative) const;\n \n     bool empty() const;\n     size_t getTotalRowCount() const;\ndiff --git a/src/Interpreters/TableJoin.cpp b/src/Interpreters/TableJoin.cpp\nindex 20e8f6b18b4a..86c84d9c8c9c 100644\n--- a/src/Interpreters/TableJoin.cpp\n+++ b/src/Interpreters/TableJoin.cpp\n@@ -231,20 +231,7 @@ void TableJoin::addJoinedColumn(const NameAndTypePair & joined_column)\n \n void TableJoin::addJoinedColumnsAndCorrectTypes(NamesAndTypesList & names_and_types, bool correct_nullability) const\n {\n-    ColumnsWithTypeAndName columns;\n-    for (auto & pair : names_and_types)\n-        columns.emplace_back(nullptr, std::move(pair.type), std::move(pair.name));\n-    names_and_types.clear();\n-\n-    addJoinedColumnsAndCorrectTypes(columns, correct_nullability);\n-\n-    for (auto & col : columns)\n-        names_and_types.emplace_back(std::move(col.name), std::move(col.type));\n-}\n-\n-void TableJoin::addJoinedColumnsAndCorrectTypes(ColumnsWithTypeAndName & columns, bool correct_nullability) const\n-{\n-    for (auto & col : columns)\n+    for (auto & col : names_and_types)\n     {\n         if (hasUsing())\n         {\n@@ -252,17 +239,12 @@ void TableJoin::addJoinedColumnsAndCorrectTypes(ColumnsWithTypeAndName & columns\n                 col.type = it->second;\n         }\n         if (correct_nullability && leftBecomeNullable(col.type))\n-        {\n-            /// No need to nullify constants\n-            bool is_column_const = col.column && isColumnConst(*col.column);\n-            if (!is_column_const)\n-                col.type = JoinCommon::convertTypeToNullable(col.type);\n-        }\n+            col.type = JoinCommon::convertTypeToNullable(col.type);\n     }\n \n     /// Types in columns_added_by_join already converted and set nullable if needed\n     for (const auto & col : columns_added_by_join)\n-        columns.emplace_back(nullptr, col.type, col.name);\n+        names_and_types.emplace_back(col.name, col.type);\n }\n \n bool TableJoin::sameStrictnessAndKind(ASTTableJoin::Strictness strictness_, ASTTableJoin::Kind kind_) const\ndiff --git a/src/Interpreters/TableJoin.h b/src/Interpreters/TableJoin.h\nindex 4c8c16028f59..4fe9565666fd 100644\n--- a/src/Interpreters/TableJoin.h\n+++ b/src/Interpreters/TableJoin.h\n@@ -191,7 +191,6 @@ class TableJoin\n     void addJoinedColumn(const NameAndTypePair & joined_column);\n \n     void addJoinedColumnsAndCorrectTypes(NamesAndTypesList & names_and_types, bool correct_nullability = true) const;\n-    void addJoinedColumnsAndCorrectTypes(ColumnsWithTypeAndName & columns, bool correct_nullability = true) const;\n \n     /// Calculates common supertypes for corresponding join key columns.\n     bool inferJoinKeyCommonType(const NamesAndTypesList & left, const NamesAndTypesList & right);\ndiff --git a/src/Interpreters/evaluateConstantExpression.cpp b/src/Interpreters/evaluateConstantExpression.cpp\nindex e46f644e8364..c05118b7c6a3 100644\n--- a/src/Interpreters/evaluateConstantExpression.cpp\n+++ b/src/Interpreters/evaluateConstantExpression.cpp\n@@ -18,6 +18,7 @@\n #include <Interpreters/FunctionNameNormalizer.h>\n #include <Interpreters/ReplaceQueryParameterVisitor.h>\n #include <Poco/Util/AbstractConfiguration.h>\n+#include <unordered_map>\n \n namespace DB\n {\n@@ -121,6 +122,7 @@ std::tuple<bool, ASTPtr> evaluateDatabaseNameForMergeEngine(const ASTPtr & node,\n     return std::tuple{false, ast};\n }\n \n+\n namespace\n {\n     using Conjunction = ColumnsWithTypeAndName;\n@@ -213,7 +215,7 @@ namespace\n \n             Disjunction result;\n \n-            auto add_dnf = [&](const auto &dnf)\n+            auto add_dnf = [&](const auto & dnf)\n             {\n                 if (dnf.size() > limit)\n                 {\n@@ -338,6 +340,7 @@ std::optional<Blocks> evaluateExpressionOverConstantCondition(const ASTPtr & nod\n \n     if (const auto * fn = node->as<ASTFunction>())\n     {\n+        std::unordered_map<std::string, bool> always_false_map;\n         const auto dnf = analyzeFunction(fn, target_expr, limit);\n \n         if (dnf.empty() || !limit)\n@@ -368,7 +371,41 @@ std::optional<Blocks> evaluateExpressionOverConstantCondition(const ASTPtr & nod\n \n         for (const auto & conjunct : dnf)\n         {\n-            Block block(conjunct);\n+            Block block;\n+\n+            for (const auto & elem : conjunct)\n+            {\n+                if (!block.has(elem.name))\n+                {\n+                    block.insert(elem);\n+                }\n+                else\n+                {\n+                    /// Conjunction of condition on column equality to distinct values can never be satisfied.\n+\n+                    const ColumnWithTypeAndName & prev = block.getByName(elem.name);\n+\n+                    if (isColumnConst(*prev.column) && isColumnConst(*elem.column))\n+                    {\n+                        Field prev_value = assert_cast<const ColumnConst &>(*prev.column).getField();\n+                        Field curr_value = assert_cast<const ColumnConst &>(*elem.column).getField();\n+\n+                        if (!always_false_map.count(elem.name))\n+                        {\n+                            always_false_map[elem.name] = prev_value != curr_value;\n+                        }\n+                        else\n+                        {\n+                            auto & always_false = always_false_map[elem.name];\n+                            /// If at least one of conjunct is not always false, we should preserve this.\n+                            if (always_false)\n+                            {\n+                                always_false = prev_value != curr_value;\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n \n             // Block should contain all required columns from `target_expr`\n             if (!has_required_columns(block))\n@@ -393,6 +430,11 @@ std::optional<Blocks> evaluateExpressionOverConstantCondition(const ASTPtr & nod\n                 return {};\n             }\n         }\n+\n+        bool any_always_false = std::any_of(always_false_map.begin(), always_false_map.end(), [](const auto & v) { return v.second; });\n+        if (any_always_false)\n+            return Blocks{};\n+\n     }\n     else if (const auto * literal = node->as<ASTLiteral>())\n     {\ndiff --git a/src/Processors/Transforms/CreatingSetsTransform.cpp b/src/Processors/Transforms/CreatingSetsTransform.cpp\nindex 6f69765ee238..d9b383030d31 100644\n--- a/src/Processors/Transforms/CreatingSetsTransform.cpp\n+++ b/src/Processors/Transforms/CreatingSetsTransform.cpp\n@@ -84,7 +84,7 @@ void CreatingSetsTransform::init()\n     is_initialized = true;\n \n     if (subquery.set)\n-        subquery.set->setHeader(getInputPort().getHeader());\n+        subquery.set->setHeader(getInputPort().getHeader().getColumnsWithTypeAndName());\n \n     watch.restart();\n     startSubquery();\n@@ -97,7 +97,7 @@ void CreatingSetsTransform::consume(Chunk chunk)\n \n     if (!done_with_set)\n     {\n-        if (!subquery.set->insertFromBlock(block))\n+        if (!subquery.set->insertFromBlock(block.getColumnsWithTypeAndName()))\n             done_with_set = true;\n     }\n \ndiff --git a/src/Storages/StorageDistributed.h b/src/Storages/StorageDistributed.h\nindex f8b16dec7be3..b6a26467a3fd 100644\n--- a/src/Storages/StorageDistributed.h\n+++ b/src/Storages/StorageDistributed.h\n@@ -174,8 +174,9 @@ class StorageDistributed final : public shared_ptr_helper<StorageDistributed>, p\n     /// - optimize_skip_unused_shards\n     /// - force_optimize_skip_unused_shards\n     ClusterPtr getOptimizedCluster(ContextPtr, const StorageMetadataPtr & metadata_snapshot, const ASTPtr & query_ptr) const;\n-    ClusterPtr\n-    skipUnusedShards(ClusterPtr cluster, const ASTPtr & query_ptr, const StorageMetadataPtr & metadata_snapshot, ContextPtr context) const;\n+\n+    ClusterPtr skipUnusedShards(\n+        ClusterPtr cluster, const ASTPtr & query_ptr, const StorageMetadataPtr & metadata_snapshot, ContextPtr context) const;\n \n     /// This method returns optimal query processing stage.\n     ///\ndiff --git a/src/Storages/StorageMerge.cpp b/src/Storages/StorageMerge.cpp\nindex 243294351f33..ed203db1a354 100644\n--- a/src/Storages/StorageMerge.cpp\n+++ b/src/Storages/StorageMerge.cpp\n@@ -435,11 +435,17 @@ Pipe StorageMerge::createSources(\n     if (!pipe.empty())\n     {\n         if (concat_streams && pipe.numOutputPorts() > 1)\n+        {\n             // It's possible to have many tables read from merge, resize(1) might open too many files at the same time.\n             // Using concat instead.\n             pipe.addTransform(std::make_shared<ConcatProcessor>(pipe.getHeader(), pipe.numOutputPorts()));\n+        }\n+\n+        /// Add virtual columns if we don't already have them.\n+\n+        Block pipe_header = pipe.getHeader();\n \n-        if (has_database_virtual_column)\n+        if (has_database_virtual_column && !pipe_header.has(\"_database\"))\n         {\n             ColumnWithTypeAndName column;\n             column.name = \"_database\";\n@@ -457,7 +463,7 @@ Pipe StorageMerge::createSources(\n             });\n         }\n \n-        if (has_table_virtual_column)\n+        if (has_table_virtual_column && !pipe_header.has(\"_table\"))\n         {\n             ColumnWithTypeAndName column;\n             column.name = \"_table\";\ndiff --git a/src/Storages/StorageSet.cpp b/src/Storages/StorageSet.cpp\nindex 67fd89f5098d..c16b60af45fb 100644\n--- a/src/Storages/StorageSet.cpp\n+++ b/src/Storages/StorageSet.cpp\n@@ -146,13 +146,13 @@ StorageSet::StorageSet(\n \n     Block header = getInMemoryMetadataPtr()->getSampleBlock();\n     header = header.sortColumns();\n-    set->setHeader(header);\n+    set->setHeader(header.getColumnsWithTypeAndName());\n \n     restore();\n }\n \n \n-void StorageSet::insertBlock(const Block & block) { set->insertFromBlock(block); }\n+void StorageSet::insertBlock(const Block & block) { set->insertFromBlock(block.getColumnsWithTypeAndName()); }\n void StorageSet::finishInsert() { set->finishInsert(); }\n \n size_t StorageSet::getSize() const { return set->getTotalRowCount(); }\n@@ -170,7 +170,7 @@ void StorageSet::truncate(const ASTPtr &, const StorageMetadataPtr & metadata_sn\n \n     increment = 0;\n     set = std::make_shared<Set>(SizeLimits(), false, true);\n-    set->setHeader(header);\n+    set->setHeader(header.getColumnsWithTypeAndName());\n }\n \n \ndiff --git a/src/Storages/System/StorageSystemZooKeeper.cpp b/src/Storages/System/StorageSystemZooKeeper.cpp\nindex d19aef47616a..cba10548852b 100644\n--- a/src/Storages/System/StorageSystemZooKeeper.cpp\n+++ b/src/Storages/System/StorageSystemZooKeeper.cpp\n@@ -97,12 +97,12 @@ static bool extractPathImpl(const IAST & elem, Paths & res, ContextPtr context)\n             auto stream = interpreter_subquery->execute().getInputStream();\n             SizeLimits limites(context->getSettingsRef().max_rows_in_set, context->getSettingsRef().max_bytes_in_set, OverflowMode::THROW);\n             Set set(limites, true, context->getSettingsRef().transform_null_in);\n-            set.setHeader(stream->getHeader());\n+            set.setHeader(stream->getHeader().getColumnsWithTypeAndName());\n \n             stream->readPrefix();\n             while (Block block = stream->read())\n             {\n-                set.insertFromBlock(block);\n+                set.insertFromBlock(block.getColumnsWithTypeAndName());\n             }\n             set.finishInsert();\n             stream->readSuffix();\n",
  "test_patch": "diff --git a/src/Storages/tests/gtest_storage_log.cpp b/src/Storages/tests/gtest_storage_log.cpp\nindex 16902eafc98c..b3ceef7e6972 100644\n--- a/src/Storages/tests/gtest_storage_log.cpp\n+++ b/src/Storages/tests/gtest_storage_log.cpp\n@@ -128,6 +128,7 @@ std::string readData(DB::StoragePtr & table, const DB::ContextPtr context)\n     {\n         ColumnWithTypeAndName col;\n         col.type = std::make_shared<DataTypeUInt64>();\n+        col.name = \"a\";\n         sample.insert(std::move(col));\n     }\n \ndiff --git a/tests/integration/test_dictionaries_postgresql/configs/dictionaries/postgres_dict.xml b/tests/integration/test_dictionaries_postgresql/configs/dictionaries/postgres_dict.xml\nindex 4ee07d0972a3..734da0cff707 100644\n--- a/tests/integration/test_dictionaries_postgresql/configs/dictionaries/postgres_dict.xml\n+++ b/tests/integration/test_dictionaries_postgresql/configs/dictionaries/postgres_dict.xml\n@@ -19,10 +19,10 @@\n       <structure>\n          <id>\n             <name>id</name>\n-            <type>UInt32</type>\n+            <!--<type>UInt32</type>-->\n          </id>\n          <attribute>\n-            <name>id</name>\n+            <name>key</name>\n             <type>UInt32</type>\n             <null_value></null_value>\n          </attribute>\n@@ -65,10 +65,10 @@\n       <structure>\n          <id>\n             <name>id</name>\n-            <type>UInt32</type>\n+            <!--<type>UInt32</type>-->\n          </id>\n          <attribute>\n-            <name>id</name>\n+            <name>key</name>\n             <type>UInt32</type>\n             <null_value></null_value>\n          </attribute>\ndiff --git a/tests/integration/test_dictionaries_postgresql/test.py b/tests/integration/test_dictionaries_postgresql/test.py\nindex 6eb4a04ed2c8..58a503bd571b 100644\n--- a/tests/integration/test_dictionaries_postgresql/test.py\n+++ b/tests/integration/test_dictionaries_postgresql/test.py\n@@ -13,11 +13,11 @@\n \n postgres_dict_table_template = \"\"\"\n     CREATE TABLE IF NOT EXISTS {} (\n-    id Integer NOT NULL, value Integer NOT NULL, PRIMARY KEY (id))\n+    id Integer NOT NULL, key Integer NOT NULL, value Integer NOT NULL, PRIMARY KEY (id))\n     \"\"\"\n click_dict_table_template = \"\"\"\n     CREATE TABLE IF NOT EXISTS `test`.`dict_table_{}` (\n-        `id` UInt64, `value` UInt32\n+        `key` UInt32, `value` UInt32\n     ) ENGINE = Dictionary({})\n     \"\"\"\n \n@@ -43,7 +43,7 @@ def create_and_fill_postgres_table(cursor, table_name, port, host):\n     create_postgres_table(cursor, table_name)\n     # Fill postgres table using clickhouse postgres table function and check\n     table_func = '''postgresql('{}:{}', 'clickhouse', '{}', 'postgres', 'mysecretpassword')'''.format(host, port, table_name)\n-    node1.query('''INSERT INTO TABLE FUNCTION {} SELECT number, number from numbers(10000)\n+    node1.query('''INSERT INTO TABLE FUNCTION {} SELECT number, number, number from numbers(10000)\n             '''.format(table_func, table_name))\n     result = node1.query(\"SELECT count() FROM {}\".format(table_func))\n     assert result.rstrip() == '10000'\n@@ -82,7 +82,7 @@ def test_load_dictionaries(started_cluster):\n \n     node1.query(\"SYSTEM RELOAD DICTIONARY {}\".format(dict_name))\n     assert node1.query(\"SELECT count() FROM `test`.`dict_table_{}`\".format(table_name)).rstrip() == '10000'\n-    assert node1.query(\"SELECT dictGetUInt32('{}', 'id', toUInt64(0))\".format(dict_name)) == '0\\n'\n+    assert node1.query(\"SELECT dictGetUInt32('{}', 'key', toUInt64(0))\".format(dict_name)) == '0\\n'\n     assert node1.query(\"SELECT dictGetUInt32('{}', 'value', toUInt64(9999))\".format(dict_name)) == '9999\\n'\n \n     cursor.execute(\"DROP TABLE IF EXISTS {}\".format(table_name))\n@@ -252,11 +252,11 @@ def test_dictionary_with_replicas(started_cluster):\n     create_postgres_table(cursor1, 'test1')\n     create_postgres_table(cursor2, 'test1')\n \n-    cursor1.execute('INSERT INTO test1 select i, i from generate_series(0, 99) as t(i);');\n-    cursor2.execute('INSERT INTO test1 select i, i from generate_series(100, 199) as t(i);');\n+    cursor1.execute('INSERT INTO test1 select i, i, i from generate_series(0, 99) as t(i);')\n+    cursor2.execute('INSERT INTO test1 select i, i, i from generate_series(100, 199) as t(i);')\n \n     create_dict('test1', 1)\n-    result = node1.query(\"SELECT * FROM `test`.`dict_table_test1` ORDER BY id\")\n+    result = node1.query(\"SELECT * FROM `test`.`dict_table_test1` ORDER BY key\")\n \n     # priority 0 - non running port\n     assert node1.contains_in_log('PostgreSQLConnectionPool: Connection error*')\ndiff --git a/tests/integration/test_odbc_interaction/configs/dictionaries/postgres_odbc_hashed_dictionary.xml b/tests/integration/test_odbc_interaction/configs/dictionaries/postgres_odbc_hashed_dictionary.xml\nindex 6aad3ad9917a..a65360b0e26a 100644\n--- a/tests/integration/test_odbc_interaction/configs/dictionaries/postgres_odbc_hashed_dictionary.xml\n+++ b/tests/integration/test_odbc_interaction/configs/dictionaries/postgres_odbc_hashed_dictionary.xml\n@@ -18,7 +18,7 @@\n \n        <structure>\n            <id>\n-               <name>column1</name>\n+               <name>id</name>\n            </id>\n \n            <attribute>\ndiff --git a/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_cached_dictionary.xml b/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_cached_dictionary.xml\nindex 45f3966ee8af..3a505b79304f 100644\n--- a/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_cached_dictionary.xml\n+++ b/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_cached_dictionary.xml\n@@ -20,7 +20,7 @@\n \n        <structure>\n            <id>\n-               <name>X</name>\n+               <name>id</name>\n            </id>\n \n            <attribute>\ndiff --git a/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_hashed_dictionary.xml b/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_hashed_dictionary.xml\nindex 18a14b896bda..5b53818cf13c 100644\n--- a/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_hashed_dictionary.xml\n+++ b/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_hashed_dictionary.xml\n@@ -20,7 +20,7 @@\n \n        <structure>\n            <id>\n-               <name>X</name>\n+               <name>id</name>\n            </id>\n \n            <attribute>\ndiff --git a/tests/integration/test_odbc_interaction/test.py b/tests/integration/test_odbc_interaction/test.py\nindex 39a283448f5e..4d2f70ad08cd 100644\n--- a/tests/integration/test_odbc_interaction/test.py\n+++ b/tests/integration/test_odbc_interaction/test.py\n@@ -99,19 +99,19 @@ def started_cluster():\n \n         logging.debug(f\"sqlite data received: {sqlite_db}\")\n         node1.exec_in_container(\n-            [\"sqlite3\", sqlite_db, \"CREATE TABLE t1(x INTEGER PRIMARY KEY ASC, y, z);\"],\n+            [\"sqlite3\", sqlite_db, \"CREATE TABLE t1(id INTEGER PRIMARY KEY ASC, x INTEGER, y, z);\"],\n             privileged=True, user='root')\n         node1.exec_in_container(\n-            [\"sqlite3\", sqlite_db, \"CREATE TABLE t2(X INTEGER PRIMARY KEY ASC, Y, Z);\"],\n+            [\"sqlite3\", sqlite_db, \"CREATE TABLE t2(id INTEGER PRIMARY KEY ASC, X INTEGER, Y, Z);\"],\n             privileged=True, user='root')\n         node1.exec_in_container(\n-            [\"sqlite3\", sqlite_db, \"CREATE TABLE t3(X INTEGER PRIMARY KEY ASC, Y, Z);\"],\n+            [\"sqlite3\", sqlite_db, \"CREATE TABLE t3(id INTEGER PRIMARY KEY ASC, X INTEGER, Y, Z);\"],\n             privileged=True, user='root')\n         node1.exec_in_container(\n-            [\"sqlite3\", sqlite_db, \"CREATE TABLE t4(X INTEGER PRIMARY KEY ASC, Y, Z);\"],\n+            [\"sqlite3\", sqlite_db, \"CREATE TABLE t4(id INTEGER PRIMARY KEY ASC, X INTEGER, Y, Z);\"],\n             privileged=True, user='root')\n         node1.exec_in_container(\n-            [\"sqlite3\", sqlite_db, \"CREATE TABLE tf1(x INTEGER PRIMARY KEY ASC, y, z);\"],\n+            [\"sqlite3\", sqlite_db, \"CREATE TABLE tf1(id INTEGER PRIMARY KEY ASC, x INTEGER, y, z);\"],\n             privileged=True, user='root')\n         logging.debug(\"sqlite tables created\")\n         mysql_conn = get_mysql_conn()\n@@ -128,7 +128,7 @@ def started_cluster():\n \n         cursor = postgres_conn.cursor()\n         cursor.execute(\n-            \"create table if not exists clickhouse.test_table (column1 int primary key, column2 varchar(40) not null)\")\n+            \"create table if not exists clickhouse.test_table (id int primary key, column1 int not null, column2 varchar(40) not null)\")\n \n         yield cluster\n \n@@ -210,9 +210,9 @@ def test_sqlite_simple_select_function_works(started_cluster):\n     sqlite_setup = node1.odbc_drivers[\"SQLite3\"]\n     sqlite_db = sqlite_setup[\"Database\"]\n \n-    node1.exec_in_container([\"sqlite3\", sqlite_db, \"INSERT INTO t1 values(1, 2, 3);\"],\n+    node1.exec_in_container([\"sqlite3\", sqlite_db, \"INSERT INTO t1 values(1, 1, 2, 3);\"],\n                             privileged=True, user='root')\n-    assert node1.query(\"select * from odbc('DSN={}', '{}')\".format(sqlite_setup[\"DSN\"], 't1')) == \"1\\t2\\t3\\n\"\n+    assert node1.query(\"select * from odbc('DSN={}', '{}')\".format(sqlite_setup[\"DSN\"], 't1')) == \"1\\t1\\t2\\t3\\n\"\n \n     assert node1.query(\"select y from odbc('DSN={}', '{}')\".format(sqlite_setup[\"DSN\"], 't1')) == \"2\\n\"\n     assert node1.query(\"select z from odbc('DSN={}', '{}')\".format(sqlite_setup[\"DSN\"], 't1')) == \"3\\n\"\n@@ -228,10 +228,10 @@ def test_sqlite_table_function(started_cluster):\n     sqlite_setup = node1.odbc_drivers[\"SQLite3\"]\n     sqlite_db = sqlite_setup[\"Database\"]\n \n-    node1.exec_in_container([\"sqlite3\", sqlite_db, \"INSERT INTO tf1 values(1, 2, 3);\"],\n+    node1.exec_in_container([\"sqlite3\", sqlite_db, \"INSERT INTO tf1 values(1, 1, 2, 3);\"],\n                             privileged=True, user='root')\n     node1.query(\"create table odbc_tf as odbc('DSN={}', '{}')\".format(sqlite_setup[\"DSN\"], 'tf1'))\n-    assert node1.query(\"select * from odbc_tf\") == \"1\\t2\\t3\\n\"\n+    assert node1.query(\"select * from odbc_tf\") == \"1\\t1\\t2\\t3\\n\"\n \n     assert node1.query(\"select y from odbc_tf\") == \"2\\n\"\n     assert node1.query(\"select z from odbc_tf\") == \"3\\n\"\n@@ -246,7 +246,7 @@ def test_sqlite_simple_select_storage_works(started_cluster):\n     sqlite_setup = node1.odbc_drivers[\"SQLite3\"]\n     sqlite_db = sqlite_setup[\"Database\"]\n \n-    node1.exec_in_container([\"sqlite3\", sqlite_db, \"INSERT INTO t4 values(1, 2, 3);\"],\n+    node1.exec_in_container([\"sqlite3\", sqlite_db, \"INSERT INTO t4 values(1, 1, 2, 3);\"],\n                             privileged=True, user='root')\n     node1.query(\"create table SqliteODBC (x Int32, y String, z String) engine = ODBC('DSN={}', '', 't4')\".format(\n         sqlite_setup[\"DSN\"]))\n@@ -264,7 +264,7 @@ def test_sqlite_odbc_hashed_dictionary(started_cluster):\n     skip_test_msan(node1)\n \n     sqlite_db = node1.odbc_drivers[\"SQLite3\"][\"Database\"]\n-    node1.exec_in_container([\"sqlite3\", sqlite_db, \"INSERT INTO t2 values(1, 2, 3);\"],\n+    node1.exec_in_container([\"sqlite3\", sqlite_db, \"INSERT INTO t2 values(1, 1, 2, 3);\"],\n                             privileged=True, user='root')\n \n     node1.query(\"SYSTEM RELOAD DICTIONARY sqlite3_odbc_hashed\")\n@@ -282,7 +282,7 @@ def test_sqlite_odbc_hashed_dictionary(started_cluster):\n         logging.debug(\"Waiting dictionary to update for the second time\")\n         time.sleep(0.1)\n \n-    node1.exec_in_container([\"sqlite3\", sqlite_db, \"INSERT INTO t2 values(200, 2, 7);\"],\n+    node1.exec_in_container([\"sqlite3\", sqlite_db, \"INSERT INTO t2 values(200, 200, 2, 7);\"],\n                             privileged=True, user='root')\n \n     # No reload because of invalidate query\n@@ -299,7 +299,7 @@ def test_sqlite_odbc_hashed_dictionary(started_cluster):\n     assert_eq_with_retry(node1, \"select dictGetUInt8('sqlite3_odbc_hashed', 'Z', toUInt64(1))\", \"3\")\n     assert_eq_with_retry(node1, \"select dictGetUInt8('sqlite3_odbc_hashed', 'Z', toUInt64(200))\", \"1\") # still default\n \n-    node1.exec_in_container([\"sqlite3\", sqlite_db, \"REPLACE INTO t2 values(1, 2, 5);\"],\n+    node1.exec_in_container([\"sqlite3\", sqlite_db, \"REPLACE INTO t2 values(1, 1, 2, 5);\"],\n                             privileged=True, user='root')\n \n     assert_eq_with_retry(node1, \"select dictGetUInt8('sqlite3_odbc_hashed', 'Z', toUInt64(1))\", \"5\")\n@@ -310,7 +310,7 @@ def test_sqlite_odbc_cached_dictionary(started_cluster):\n     skip_test_msan(node1)\n \n     sqlite_db = node1.odbc_drivers[\"SQLite3\"][\"Database\"]\n-    node1.exec_in_container([\"sqlite3\", sqlite_db, \"INSERT INTO t3 values(1, 2, 3);\"],\n+    node1.exec_in_container([\"sqlite3\", sqlite_db, \"INSERT INTO t3 values(1, 1, 2, 3);\"],\n                             privileged=True, user='root')\n \n     assert node1.query(\"select dictGetUInt8('sqlite3_odbc_cached', 'Z', toUInt64(1))\") == \"3\\n\"\n@@ -319,12 +319,12 @@ def test_sqlite_odbc_cached_dictionary(started_cluster):\n     node1.exec_in_container([\"chmod\", \"a+rw\", \"/tmp\"], privileged=True, user='root')\n     node1.exec_in_container([\"chmod\", \"a+rw\", sqlite_db], privileged=True, user='root')\n \n-    node1.query(\"insert into table function odbc('DSN={};ReadOnly=0', '', 't3') values (200, 2, 7)\".format(\n+    node1.query(\"insert into table function odbc('DSN={};ReadOnly=0', '', 't3') values (200, 200, 2, 7)\".format(\n         node1.odbc_drivers[\"SQLite3\"][\"DSN\"]))\n \n     assert node1.query(\"select dictGetUInt8('sqlite3_odbc_cached', 'Z', toUInt64(200))\") == \"7\\n\"  # new value\n \n-    node1.exec_in_container([\"sqlite3\", sqlite_db, \"REPLACE INTO t3 values(1, 2, 12);\"],\n+    node1.exec_in_container([\"sqlite3\", sqlite_db, \"REPLACE INTO t3 values(1, 1, 2, 12);\"],\n                             privileged=True, user='root')\n \n     assert_eq_with_retry(node1, \"select dictGetUInt8('sqlite3_odbc_cached', 'Z', toUInt64(1))\", \"12\")\n@@ -336,7 +336,7 @@ def test_postgres_odbc_hashed_dictionary_with_schema(started_cluster):\n     conn = get_postgres_conn(started_cluster)\n     cursor = conn.cursor()\n     cursor.execute(\"truncate table clickhouse.test_table\")\n-    cursor.execute(\"insert into clickhouse.test_table values(1, 'hello'),(2, 'world')\")\n+    cursor.execute(\"insert into clickhouse.test_table values(1, 1, 'hello'),(2, 2, 'world')\")\n     node1.query(\"SYSTEM RELOAD DICTIONARY postgres_odbc_hashed\")\n     assert_eq_with_retry(node1, \"select dictGetString('postgres_odbc_hashed', 'column2', toUInt64(1))\", \"hello\")\n     assert_eq_with_retry(node1, \"select dictGetString('postgres_odbc_hashed', 'column2', toUInt64(2))\", \"world\")\n@@ -348,7 +348,7 @@ def test_postgres_odbc_hashed_dictionary_no_tty_pipe_overflow(started_cluster):\n     conn = get_postgres_conn(started_cluster)\n     cursor = conn.cursor()\n     cursor.execute(\"truncate table clickhouse.test_table\")\n-    cursor.execute(\"insert into clickhouse.test_table values(3, 'xxx')\")\n+    cursor.execute(\"insert into clickhouse.test_table values(3, 3, 'xxx')\")\n     for i in range(100):\n         try:\n             node1.query(\"system reload dictionary postgres_odbc_hashed\", timeout=15)\n@@ -369,13 +369,13 @@ def test_postgres_insert(started_cluster):\n     # reconstruction of connection string.\n \n     node1.query(\n-        \"create table pg_insert (column1 UInt8, column2 String) engine=ODBC('DSN=postgresql_odbc;Servername=postgre-sql.local', 'clickhouse', 'test_table')\")\n-    node1.query(\"insert into pg_insert values (1, 'hello'), (2, 'world')\")\n-    assert node1.query(\"select * from pg_insert\") == '1\\thello\\n2\\tworld\\n'\n-    node1.query(\"insert into table function odbc('DSN=postgresql_odbc', 'clickhouse', 'test_table') format CSV 3,test\")\n+        \"create table pg_insert (id UInt64, column1 UInt8, column2 String) engine=ODBC('DSN=postgresql_odbc;Servername=postgre-sql.local', 'clickhouse', 'test_table')\")\n+    node1.query(\"insert into pg_insert values (1, 1, 'hello'), (2, 2, 'world')\")\n+    assert node1.query(\"select * from pg_insert\") == '1\\t1\\thello\\n2\\t2\\tworld\\n'\n+    node1.query(\"insert into table function odbc('DSN=postgresql_odbc', 'clickhouse', 'test_table') format CSV 3,3,test\")\n     node1.query(\n         \"insert into table function odbc('DSN=postgresql_odbc;Servername=postgre-sql.local', 'clickhouse', 'test_table')\" \\\n-        \" select number, 's' || toString(number) from numbers (4, 7)\")\n+        \" select number, number, 's' || toString(number) from numbers (4, 7)\")\n     assert node1.query(\"select sum(column1), count(column1) from pg_insert\") == \"55\\t10\\n\"\n     assert node1.query(\n         \"select sum(n), count(n) from (select (*,).1 as n from (select * from odbc('DSN=postgresql_odbc', 'clickhouse', 'test_table')))\") == \"55\\t10\\n\"\n@@ -426,19 +426,19 @@ def test_odbc_postgres_date_data_type(started_cluster):\n \n     conn = get_postgres_conn(started_cluster);\n     cursor = conn.cursor()\n-    cursor.execute(\"CREATE TABLE IF NOT EXISTS clickhouse.test_date (column1 integer, column2 date)\")\n+    cursor.execute(\"CREATE TABLE IF NOT EXISTS clickhouse.test_date (id integer, column1 integer, column2 date)\")\n \n-    cursor.execute(\"INSERT INTO clickhouse.test_date VALUES (1, '2020-12-01')\")\n-    cursor.execute(\"INSERT INTO clickhouse.test_date VALUES (2, '2020-12-02')\")\n-    cursor.execute(\"INSERT INTO clickhouse.test_date VALUES (3, '2020-12-03')\")\n+    cursor.execute(\"INSERT INTO clickhouse.test_date VALUES (1, 1, '2020-12-01')\")\n+    cursor.execute(\"INSERT INTO clickhouse.test_date VALUES (2, 2, '2020-12-02')\")\n+    cursor.execute(\"INSERT INTO clickhouse.test_date VALUES (3, 3, '2020-12-03')\")\n     conn.commit()\n \n     node1.query(\n         '''\n-        CREATE TABLE test_date (column1 UInt64, column2 Date)\n+        CREATE TABLE test_date (id UInt64, column1 UInt64, column2 Date)\n         ENGINE=ODBC('DSN=postgresql_odbc; Servername=postgre-sql.local', 'clickhouse', 'test_date')''')\n \n-    expected = '1\\t2020-12-01\\n2\\t2020-12-02\\n3\\t2020-12-03\\n'\n+    expected = '1\\t1\\t2020-12-01\\n2\\t2\\t2020-12-02\\n3\\t3\\t2020-12-03\\n'\n     result = node1.query('SELECT * FROM test_date');\n     assert(result == expected)\n     cursor.execute(\"DROP TABLE IF EXISTS clickhouse.test_date\")\ndiff --git a/tests/queries/0_stateless/00818_alias_bug_4110.reference b/tests/queries/0_stateless/00818_alias_bug_4110.reference\nindex e6013d269c23..210fc67db665 100644\n--- a/tests/queries/0_stateless/00818_alias_bug_4110.reference\n+++ b/tests/queries/0_stateless/00818_alias_bug_4110.reference\n@@ -4,7 +4,6 @@\n 11\t12\n 12\t11\n 10\t10\n-10\t11\t11\n 12\t11\n 10\t12\n 11\t12\ndiff --git a/tests/queries/0_stateless/00818_alias_bug_4110.sql b/tests/queries/0_stateless/00818_alias_bug_4110.sql\nindex 7b2fd5d38648..df7e70cb2759 100644\n--- a/tests/queries/0_stateless/00818_alias_bug_4110.sql\n+++ b/tests/queries/0_stateless/00818_alias_bug_4110.sql\n@@ -5,7 +5,7 @@ select s.a + 1 as b, s.a + 2 as a from (select 10 as a) s;\n select s.a + 2 as b, s.a + 1 as a from (select 10 as a) s;\n \n select a, a as a from (select 10 as a);\n-select s.a, a, a + 1 as a from (select 10 as a) as s;\n+select s.a, a, a + 1 as a from (select 10 as a) as s; -- { serverError 352 }\n select s.a + 2 as b, b - 1 as a from (select 10 as a) s;\n select s.a as a, s.a + 2 as b from (select 10 as a) s;\n select s.a + 1 as a, s.a + 2 as b from (select 10 as a) s;\ndiff --git a/tests/queries/0_stateless/01101_literal_column_clash.reference b/tests/queries/0_stateless/01101_literal_column_clash.reference\nindex b89f59abb182..8f76d98575c7 100644\n--- a/tests/queries/0_stateless/01101_literal_column_clash.reference\n+++ b/tests/queries/0_stateless/01101_literal_column_clash.reference\n@@ -3,7 +3,6 @@\n 7\t0\n 7\t1\n xyzabc\t2\n-1\t3\n 1\t2\t0\t0\n 1\t0\t0\t3\n \\N\t1\t2\t\\N\t0\ndiff --git a/tests/queries/0_stateless/01101_literal_column_clash.sql b/tests/queries/0_stateless/01101_literal_column_clash.sql\nindex 4a6064141ea6..b9645e3609ec 100644\n--- a/tests/queries/0_stateless/01101_literal_column_clash.sql\n+++ b/tests/queries/0_stateless/01101_literal_column_clash.sql\n@@ -7,7 +7,7 @@ join (select '1' as sid) as t2 on t2.sid = cast(t1.iid as String);\n select cast(7 as String), * from (select 3 \"'String'\");\n select cast(7 as String), * from (select number \"'String'\" FROM numbers(2));\n SELECT concat('xyz', 'abc'), * FROM (SELECT 2 AS \"'xyz'\");\n-with 3 as \"1\" select 1, \"1\";\n+with 3 as \"1\" select 1, \"1\"; -- { serverError 352 }\n \n -- https://github.com/ClickHouse/ClickHouse/issues/9953\n select 1, * from (select 2 x) a left join (select 1, 3 y) b on y = x;\ndiff --git a/tests/queries/0_stateless/01236_graphite_mt.sql b/tests/queries/0_stateless/01236_graphite_mt.sql\nindex ccf7c066e757..a6dd4b8b6fbc 100644\n--- a/tests/queries/0_stateless/01236_graphite_mt.sql\n+++ b/tests/queries/0_stateless/01236_graphite_mt.sql\n@@ -13,7 +13,7 @@ WITH dates AS\n                today - INTERVAL 3 day as older_date\n     )\n     -- Newer than 2 days are kept in windows of 600 seconds\n-    select 1, 'sum_1', today - number * 60 - 30, number, 1, number from dates, numbers(300) union all\n+    select 1 AS key, 'sum_1' AS s, today - number * 60 - 30, number, 1, number from dates, numbers(300) union all\n     select 2, 'sum_1', today - number * 60 - 30, number, 1, number from dates, numbers(300) union all\n     select 1, 'sum_2', today - number * 60 - 30, number, 1, number from dates, numbers(300) union all\n     select 2, 'sum_2', today - number * 60 - 30, number, 1, number from dates, numbers(300) union all\n@@ -23,7 +23,7 @@ WITH dates AS\n     select 2, 'max_2', today - number * 60 - 30, number, 1, number from dates, numbers(300) union all\n \n     -- Older than 2 days use 6000 second windows\n-    select 1, 'sum_1', older_date - number * 60 - 30, number, 1, number from dates, numbers(1200) union all\n+    select 1 AS key, 'sum_1' AS s, older_date - number * 60 - 30, number, 1, number from dates, numbers(1200) union all\n     select 2, 'sum_1', older_date - number * 60 - 30, number, 1, number from dates, numbers(1200) union all\n     select 1, 'sum_2', older_date - number * 60 - 30, number, 1, number from dates, numbers(1200) union all\n     select 2, 'sum_2', older_date - number * 60 - 30, number, 1, number from dates, numbers(1200) union all\ndiff --git a/tests/queries/0_stateless/01950_aliases_bad_cast.reference b/tests/queries/0_stateless/01950_aliases_bad_cast.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/01950_aliases_bad_cast.sql b/tests/queries/0_stateless/01950_aliases_bad_cast.sql\nnew file mode 100644\nindex 000000000000..bdd2339f855d\n--- /dev/null\n+++ b/tests/queries/0_stateless/01950_aliases_bad_cast.sql\n@@ -0,0 +1,2 @@\n+SELECT 1, * FROM (SELECT NULL AS `1`); -- { serverError 352 }\n+SELECT '7', 'xyz', * FROM (SELECT NULL AS `'xyz'`); -- { serverError 352 }\ndiff --git a/tests/queries/0_stateless/01950_kill_large_group_by_query.reference b/tests/queries/0_stateless/01950_kill_large_group_by_query.reference\nindex 1602d6587ad8..f1df26588973 100644\n--- a/tests/queries/0_stateless/01950_kill_large_group_by_query.reference\n+++ b/tests/queries/0_stateless/01950_kill_large_group_by_query.reference\n@@ -1,2 +1,2 @@\n-finished\ttest_01948_tcp_default\tdefault\tSELECT * FROM\\n    (\\n        SELECT a.name as n\\n        FROM\\n        (\\n            SELECT \\'Name\\' as name, number FROM system.numbers LIMIT 2000000\\n        ) AS a,\\n        (\\n            SELECT \\'Name\\' as name, number FROM system.numbers LIMIT 2000000\\n        ) as b\\n        GROUP BY n\\n    )\\n    LIMIT 20\\n    FORMAT Null\n-finished\ttest_01948_http_default\tdefault\tSELECT * FROM\\n    (\\n        SELECT a.name as n\\n        FROM\\n        (\\n            SELECT \\'Name\\' as name, number FROM system.numbers LIMIT 2000000\\n        ) AS a,\\n        (\\n            SELECT \\'Name\\' as name, number FROM system.numbers LIMIT 2000000\\n        ) as b\\n        GROUP BY n\\n    )\\n    LIMIT 20\\n    FORMAT Null\n+finished\ttest_01948_tcp_default\tdefault\tSELECT * FROM\\n    (\\n        SELECT a.name as n\\n        FROM\\n        (\\n            SELECT \\'Name\\' as name, number FROM system.numbers LIMIT 2000000\\n        ) AS a,\\n        (\\n            SELECT \\'Name\\' as name2, number FROM system.numbers LIMIT 2000000\\n        ) as b\\n        GROUP BY n\\n    )\\n    LIMIT 20\\n    FORMAT Null\n+finished\ttest_01948_http_default\tdefault\tSELECT * FROM\\n    (\\n        SELECT a.name as n\\n        FROM\\n        (\\n            SELECT \\'Name\\' as name, number FROM system.numbers LIMIT 2000000\\n        ) AS a,\\n        (\\n            SELECT \\'Name\\' as name2, number FROM system.numbers LIMIT 2000000\\n        ) as b\\n        GROUP BY n\\n    )\\n    LIMIT 20\\n    FORMAT Null\ndiff --git a/tests/queries/0_stateless/01950_kill_large_group_by_query.sh b/tests/queries/0_stateless/01950_kill_large_group_by_query.sh\nindex 465b923187e2..0b369c7257e0 100755\n--- a/tests/queries/0_stateless/01950_kill_large_group_by_query.sh\n+++ b/tests/queries/0_stateless/01950_kill_large_group_by_query.sh\n@@ -23,7 +23,7 @@ $CLICKHOUSE_CLIENT --max_execution_time 10 --query_id \"test_01948_tcp_$CLICKHOUS\n             SELECT 'Name' as name, number FROM system.numbers LIMIT 2000000\n         ) AS a,\n         (\n-            SELECT 'Name' as name, number FROM system.numbers LIMIT 2000000\n+            SELECT 'Name' as name2, number FROM system.numbers LIMIT 2000000\n         ) as b\n         GROUP BY n\n     )\n@@ -44,7 +44,7 @@ ${CLICKHOUSE_CURL_COMMAND} -q --max-time 10 -sS \"$CLICKHOUSE_URL&query_id=test_0\n             SELECT 'Name' as name, number FROM system.numbers LIMIT 2000000\n         ) AS a,\n         (\n-            SELECT 'Name' as name, number FROM system.numbers LIMIT 2000000\n+            SELECT 'Name' as name2, number FROM system.numbers LIMIT 2000000\n         ) as b\n         GROUP BY n\n     )\n",
  "problem_statement": "Bad cast from type DB::ColumnVector<unsigned int> to DB::ColumnVector<char8_t>\n```\r\nSELECT\r\n    ignore(ignore(ignore(ignore(NULL), '0.1'), 1.1920928955078125e-7, 'xyz', NULL)),\r\n    'xyz',\r\n    *\r\nFROM\r\n(\r\n    SELECT NULL AS `'xyz'`\r\n)\r\n\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 21.9.1 revision 54449.\r\n\r\n\u250c\u2500ignore(ignore(ignore(ignore(NULL), '0.1'), 1.1920928955078125e-7, 'xyz', NULL))\u2500\u252c\u2500'xyz'\u2500\u252c\u2500'xyz'\u2500\u2510\r\n\u2502                                                                               0 \u2502 xyz   \u2502 \u1d3a\u1d41\u1d38\u1d38  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nLogical error: 'Bad cast from type DB::ColumnVector<unsigned int> to DB::ColumnVector<char8_t>'.\r\nAborted (core dumped)\r\n```\r\nhttps://clickhouse-test-reports.s3.yandex.net/26716/f35e6eee19b57a89e1105b5fa2ccefb583c02f9f/fuzzer_ubsan/report.html#fail1\r\n\nBad cast from ColumnString to ColumnNullable\n**Describe the bug**\r\nhttps://clickhouse-test-reports.s3.yandex.net/25409/8d4ccb40de5a956cc412df58698176c3e9c0488c/fuzzer_debug/report.html#fail1\r\n\r\n**How to reproduce**\r\n```\r\nSELECT '7', 'xyz', * FROM (SELECT NULL AS `'xyz'`)\r\n```\r\n\r\n```\r\n../src/Common/assert_cast.h:50:12: runtime error: downcast of address 0x7ff1cc2e6ee0 which does not point to an object of type 'const DB::ColumnNullable'\r\n0x7ff1cc2e6ee0: note: object is of type 'DB::ColumnString'\r\n 00 00 00 00  50 3b 35 0a 00 00 00 00  03 00 00 00 00 00 00 00  80 3b 0d c4 f4 7f 00 00  88 3b 0d c4\r\n              ^~~~~~~~~~~~~~~~~~~~~~~\r\n              vptr for 'DB::ColumnString'\r\n```\n",
  "hints_text": "\nIt is reproducing very easily.\n```\r\nSELECT 1, * FROM (SELECT NULL AS `1`)\r\n```\r\n\r\nWe should simply forbid these aliases.",
  "created_at": "2021-07-21T01:44:40Z",
  "modified_files": [
    "src/AggregateFunctions/UniqVariadicHash.h",
    "src/Columns/ColumnTuple.h",
    "src/Core/Block.cpp",
    "src/Functions/in.cpp",
    "src/Interpreters/ActionsVisitor.cpp",
    "src/Interpreters/ExecuteScalarSubqueriesVisitor.cpp",
    "src/Interpreters/ExpressionActions.cpp",
    "src/Interpreters/ExpressionAnalyzer.cpp",
    "src/Interpreters/Set.cpp",
    "src/Interpreters/Set.h",
    "src/Interpreters/TableJoin.cpp",
    "src/Interpreters/TableJoin.h",
    "src/Interpreters/evaluateConstantExpression.cpp",
    "src/Processors/Transforms/CreatingSetsTransform.cpp",
    "src/Storages/StorageDistributed.h",
    "src/Storages/StorageMerge.cpp",
    "src/Storages/StorageSet.cpp",
    "src/Storages/System/StorageSystemZooKeeper.cpp"
  ],
  "modified_test_files": [
    "src/Storages/tests/gtest_storage_log.cpp",
    "tests/integration/test_dictionaries_postgresql/configs/dictionaries/postgres_dict.xml",
    "tests/integration/test_dictionaries_postgresql/test.py",
    "tests/integration/test_odbc_interaction/configs/dictionaries/postgres_odbc_hashed_dictionary.xml",
    "tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_cached_dictionary.xml",
    "tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_hashed_dictionary.xml",
    "tests/integration/test_odbc_interaction/test.py",
    "tests/queries/0_stateless/00818_alias_bug_4110.reference",
    "tests/queries/0_stateless/00818_alias_bug_4110.sql",
    "tests/queries/0_stateless/01101_literal_column_clash.reference",
    "tests/queries/0_stateless/01101_literal_column_clash.sql",
    "tests/queries/0_stateless/01236_graphite_mt.sql",
    "b/tests/queries/0_stateless/01950_aliases_bad_cast.sql",
    "tests/queries/0_stateless/01950_kill_large_group_by_query.reference",
    "tests/queries/0_stateless/01950_kill_large_group_by_query.sh"
  ]
}