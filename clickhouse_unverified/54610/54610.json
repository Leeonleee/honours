{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 54610,
  "instance_id": "ClickHouse__ClickHouse-54610",
  "issue_numbers": [
    "54606"
  ],
  "base_commit": "aaa7025b1a927c47174ec2e21e54cce9daa9ae17",
  "patch": "diff --git a/src/Storages/MergeTree/MergeTreeSelectAlgorithms.cpp b/src/Storages/MergeTree/MergeTreeSelectAlgorithms.cpp\nindex 8bc4377cffb1..bf97d269dc68 100644\n--- a/src/Storages/MergeTree/MergeTreeSelectAlgorithms.cpp\n+++ b/src/Storages/MergeTree/MergeTreeSelectAlgorithms.cpp\n@@ -1,5 +1,4 @@\n #include <Storages/MergeTree/MergeTreeSelectAlgorithms.h>\n-#include <Storages/MergeTree/IMergeTreeReadPool.h>\n \n namespace DB\n {\n@@ -9,57 +8,29 @@ namespace ErrorCodes\n     extern const int LOGICAL_ERROR;\n }\n \n-MergeTreeThreadSelectAlgorithm::TaskResult MergeTreeThreadSelectAlgorithm::getNewTask(IMergeTreeReadPool & pool, MergeTreeReadTask * previous_task)\n-{\n-    TaskResult res;\n-    res.first = pool.getTask(thread_idx, previous_task);\n-    res.second = !!res.first;\n-    return res;\n-}\n-\n-MergeTreeReadTask::BlockAndProgress MergeTreeThreadSelectAlgorithm::readFromTask(MergeTreeReadTask * task, const MergeTreeReadTask::BlockSizeParams & params)\n-{\n-    if (!task)\n-        return {};\n-\n-    return task->read(params);\n-}\n-\n-IMergeTreeSelectAlgorithm::TaskResult MergeTreeInOrderSelectAlgorithm::getNewTask(IMergeTreeReadPool & pool, MergeTreeReadTask * previous_task)\n+MergeTreeReadTaskPtr MergeTreeInOrderSelectAlgorithm::getNewTask(IMergeTreeReadPool & pool, MergeTreeReadTask * previous_task)\n {\n     if (!pool.preservesOrderOfRanges())\n         throw Exception(ErrorCodes::LOGICAL_ERROR,\n             \"MergeTreeInOrderSelectAlgorithm requires read pool that preserves order of ranges, got: {}\", pool.getName());\n \n-    TaskResult res;\n-    res.first = pool.getTask(part_idx, previous_task);\n-    res.second = !!res.first;\n-    return res;\n+    return pool.getTask(part_idx, previous_task);\n }\n \n-MergeTreeReadTask::BlockAndProgress MergeTreeInOrderSelectAlgorithm::readFromTask(MergeTreeReadTask * task, const BlockSizeParams & params)\n-{\n-    if (!task)\n-        return {};\n-\n-    return task->read(params);\n-}\n-\n-IMergeTreeSelectAlgorithm::TaskResult MergeTreeInReverseOrderSelectAlgorithm::getNewTask(IMergeTreeReadPool & pool, MergeTreeReadTask * previous_task)\n+MergeTreeReadTaskPtr MergeTreeInReverseOrderSelectAlgorithm::getNewTask(IMergeTreeReadPool & pool, MergeTreeReadTask * previous_task)\n {\n     if (!pool.preservesOrderOfRanges())\n         throw Exception(ErrorCodes::LOGICAL_ERROR,\n             \"MergeTreeInReverseOrderSelectAlgorithm requires read pool that preserves order of ranges, got: {}\", pool.getName());\n \n-    TaskResult res;\n-    res.first = pool.getTask(part_idx, previous_task);\n-    /// We may have some chunks to return in buffer.\n-    /// Set continue_reading to true but actually don't create a new task.\n-    res.second = !!res.first || !chunks.empty();\n-    return res;\n+    if (!chunks.empty())\n+        throw Exception(ErrorCodes::LOGICAL_ERROR,\n+            \"Cannot get new task for reading in reverse order because there are {} buffered chunks\", chunks.size());\n+\n+    return pool.getTask(part_idx, previous_task);\n }\n \n-MergeTreeReadTask::BlockAndProgress MergeTreeInReverseOrderSelectAlgorithm::readFromTask(MergeTreeReadTask * task, const BlockSizeParams & params)\n+MergeTreeReadTask::BlockAndProgress MergeTreeInReverseOrderSelectAlgorithm::readFromTask(MergeTreeReadTask & task, const BlockSizeParams & params)\n {\n     MergeTreeReadTask::BlockAndProgress res;\n \n@@ -70,11 +41,8 @@ MergeTreeReadTask::BlockAndProgress MergeTreeInReverseOrderSelectAlgorithm::read\n         return res;\n     }\n \n-    if (!task)\n-        return {};\n-\n-    while (!task->isFinished())\n-        chunks.push_back(task->read(params));\n+    while (!task.isFinished())\n+        chunks.push_back(task.read(params));\n \n     if (chunks.empty())\n         return {};\ndiff --git a/src/Storages/MergeTree/MergeTreeSelectAlgorithms.h b/src/Storages/MergeTree/MergeTreeSelectAlgorithms.h\nindex a6254a90687b..afc8032bb990 100644\n--- a/src/Storages/MergeTree/MergeTreeSelectAlgorithms.h\n+++ b/src/Storages/MergeTree/MergeTreeSelectAlgorithms.h\n@@ -1,6 +1,7 @@\n #pragma once\n \n #include <Storages/MergeTree/MergeTreeReadTask.h>\n+#include <Storages/MergeTree/IMergeTreeReadPool.h>\n #include <boost/core/noncopyable.hpp>\n \n namespace DB\n@@ -11,15 +12,16 @@ class IMergeTreeReadPool;\n class IMergeTreeSelectAlgorithm : private boost::noncopyable\n {\n public:\n-    /// The pair of {task, continue_reading}.\n-    using TaskResult = std::pair<MergeTreeReadTaskPtr, bool>;\n     using BlockSizeParams = MergeTreeReadTask::BlockSizeParams;\n+    using BlockAndProgress = MergeTreeReadTask::BlockAndProgress;\n \n     virtual ~IMergeTreeSelectAlgorithm() = default;\n \n     virtual String getName() const = 0;\n-    virtual TaskResult getNewTask(IMergeTreeReadPool & pool, MergeTreeReadTask * previous_task) = 0;\n-    virtual MergeTreeReadTask::BlockAndProgress readFromTask(MergeTreeReadTask * task, const BlockSizeParams & params) = 0;\n+    virtual bool needNewTask(const MergeTreeReadTask & task) const = 0;\n+\n+    virtual MergeTreeReadTaskPtr getNewTask(IMergeTreeReadPool & pool, MergeTreeReadTask * previous_task) = 0;\n+    virtual BlockAndProgress readFromTask(MergeTreeReadTask & task, const BlockSizeParams & params) = 0;\n };\n \n using MergeTreeSelectAlgorithmPtr = std::unique_ptr<IMergeTreeSelectAlgorithm>;\n@@ -28,9 +30,12 @@ class MergeTreeThreadSelectAlgorithm : public IMergeTreeSelectAlgorithm\n {\n public:\n     explicit MergeTreeThreadSelectAlgorithm(size_t thread_idx_) : thread_idx(thread_idx_) {}\n+\n     String getName() const override { return \"Thread\"; }\n-    TaskResult getNewTask(IMergeTreeReadPool & pool, MergeTreeReadTask * previous_task) override;\n-    MergeTreeReadTask::BlockAndProgress readFromTask(MergeTreeReadTask * task, const BlockSizeParams & params) override;\n+    bool needNewTask(const MergeTreeReadTask & task) const override { return task.isFinished(); }\n+\n+    MergeTreeReadTaskPtr getNewTask(IMergeTreeReadPool & pool, MergeTreeReadTask * previous_task) override { return pool.getTask(thread_idx, previous_task); }\n+    BlockAndProgress readFromTask(MergeTreeReadTask & task, const BlockSizeParams & params) override { return task.read(params); }\n \n private:\n     const size_t thread_idx;\n@@ -40,9 +45,12 @@ class MergeTreeInOrderSelectAlgorithm : public IMergeTreeSelectAlgorithm\n {\n public:\n     explicit MergeTreeInOrderSelectAlgorithm(size_t part_idx_) : part_idx(part_idx_) {}\n+\n     String getName() const override { return \"InOrder\"; }\n-    TaskResult getNewTask(IMergeTreeReadPool & pool, MergeTreeReadTask * previous_task) override;\n-    MergeTreeReadTask::BlockAndProgress readFromTask(MergeTreeReadTask * task, const BlockSizeParams & params) override;\n+    bool needNewTask(const MergeTreeReadTask & task) const override { return task.isFinished(); }\n+\n+    MergeTreeReadTaskPtr getNewTask(IMergeTreeReadPool & pool, MergeTreeReadTask * previous_task) override;\n+    MergeTreeReadTask::BlockAndProgress readFromTask(MergeTreeReadTask & task, const BlockSizeParams & params) override { return task.read(params); }\n \n private:\n     const size_t part_idx;\n@@ -52,13 +60,16 @@ class MergeTreeInReverseOrderSelectAlgorithm : public IMergeTreeSelectAlgorithm\n {\n public:\n     explicit MergeTreeInReverseOrderSelectAlgorithm(size_t part_idx_) : part_idx(part_idx_) {}\n+\n     String getName() const override { return \"InReverseOrder\"; }\n-    TaskResult getNewTask(IMergeTreeReadPool & pool, MergeTreeReadTask * previous_task) override;\n-    MergeTreeReadTask::BlockAndProgress readFromTask(MergeTreeReadTask * task, const BlockSizeParams & params) override;\n+    bool needNewTask(const MergeTreeReadTask & task) const override { return chunks.empty() && task.isFinished(); }\n+\n+    MergeTreeReadTaskPtr getNewTask(IMergeTreeReadPool & pool, MergeTreeReadTask * previous_task) override;\n+    BlockAndProgress readFromTask(MergeTreeReadTask & task, const BlockSizeParams & params) override;\n \n private:\n     const size_t part_idx;\n-    std::vector<MergeTreeReadTask::BlockAndProgress> chunks;\n+    std::vector<BlockAndProgress> chunks;\n };\n \n }\ndiff --git a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\nindex 975fad1ab6b5..95fcde23f8e4 100644\n--- a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\n@@ -139,11 +139,10 @@ ChunkAndProgress MergeTreeSelectProcessor::read()\n     {\n         try\n         {\n-            bool continue_reading = true;\n-            if (!task || task->isFinished())\n-                std::tie(task, continue_reading) = algorithm->getNewTask(*pool, task.get());\n+            if (!task || algorithm->needNewTask(*task))\n+                task = algorithm->getNewTask(*pool, task.get());\n \n-            if (!continue_reading)\n+            if (!task)\n                 break;\n         }\n         catch (const Exception & e)\n@@ -153,10 +152,10 @@ ChunkAndProgress MergeTreeSelectProcessor::read()\n             throw;\n         }\n \n-        if (task && !task->getMainRangeReader().isInitialized())\n+        if (!task->getMainRangeReader().isInitialized())\n             initializeRangeReaders();\n \n-        auto res = algorithm->readFromTask(task.get(), block_size_params);\n+        auto res = algorithm->readFromTask(*task, block_size_params);\n \n         if (res.row_count)\n         {\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02883_read_in_reverse_order_virtual_column.reference b/tests/queries/0_stateless/02883_read_in_reverse_order_virtual_column.reference\nnew file mode 100644\nindex 000000000000..f77195f1f318\n--- /dev/null\n+++ b/tests/queries/0_stateless/02883_read_in_reverse_order_virtual_column.reference\n@@ -0,0 +1,1 @@\n+198401_1_1_0\ndiff --git a/tests/queries/0_stateless/02883_read_in_reverse_order_virtual_column.sql b/tests/queries/0_stateless/02883_read_in_reverse_order_virtual_column.sql\nnew file mode 100644\nindex 000000000000..76821c8797d4\n--- /dev/null\n+++ b/tests/queries/0_stateless/02883_read_in_reverse_order_virtual_column.sql\n@@ -0,0 +1,10 @@\n+DROP TABLE IF EXISTS t_reverse_order_virt_col;\n+\n+CREATE TABLE t_reverse_order_virt_col (`order_0` Decimal(76, 53), `p_time` Date)\n+ENGINE = MergeTree PARTITION BY toYYYYMM(p_time)\n+ORDER BY order_0;\n+\n+INSERT INTO t_reverse_order_virt_col SELECT number, '1984-01-01' FROM numbers(1000000);\n+SELECT DISTINCT _part FROM (SELECT _part FROM t_reverse_order_virt_col ORDER BY order_0 DESC);\n+\n+DROP TABLE IF EXISTS t_reverse_order_virt_col;\n",
  "problem_statement": "ASTFuzzer: \"Cannot insert virtual columns to non-empty chunk without specified task\"\nhttps://s3.amazonaws.com/clickhouse-test-reports/54370/10c368f40a2d5c2d602a30880b21230af5a44640/fuzzer_astfuzzerasan/report.html\r\n\r\nStacktrace:\r\n```\r\n2023.09.13 13:29:27.151464 [ 518 ] {8533990c-3544-4818-9ada-142783208bac} <Fatal> : Logical error: 'Cannot insert virtual columns to non-empty chunk without specified task.'.\r\n2023.09.13 13:29:27.151971 [ 794 ] {} <Fatal> BaseDaemon: ########## Short fault info ############\r\n2023.09.13 13:29:27.152010 [ 794 ] {} <Fatal> BaseDaemon: (version 23.9.1.1, build id: 16D0E0F7BBAE61F891423A4C9E565FE6EDF79108, git hash: 3b8c4cbc58c9fdd696fd37dd17593fd0c029bc45) (from thread 518) Received signal 6\r\n2023.09.13 13:29:27.152048 [ 794 ] {} <Fatal> BaseDaemon: Signal description: Aborted\r\n2023.09.13 13:29:27.152070 [ 794 ] {} <Fatal> BaseDaemon: \r\n2023.09.13 13:29:27.152099 [ 794 ] {} <Fatal> BaseDaemon: Stack trace: 0x00007fc551daaa7c 0x00007fc551d56476 0x00007fc551d3c7f3 0x0000557e372abb28 0x0000557e372acb66 0x0000557e263a3449 0x0000557e4e1608f0 0x0000557e4e16244e 0x0000557e4f9fbc18 0x0000557e4eca170b 0x0000557e4ece8b4e 0x0000557e4eccad25 0x0000557e4eccdb02 0x0000557e37590d3c 0x0000557e3759b1a0 0x0000557e37587832 0x0000557e37595b0d 0x00007fc551da8b43 0x00007fc551e3aa00\r\n2023.09.13 13:29:27.152134 [ 794 ] {} <Fatal> BaseDaemon: ########################################\r\n2023.09.13 13:29:27.152198 [ 794 ] {} <Fatal> BaseDaemon: (version 23.9.1.1, build id: 16D0E0F7BBAE61F891423A4C9E565FE6EDF79108, git hash: 3b8c4cbc58c9fdd696fd37dd17593fd0c029bc45) (from thread 518) (query_id: 8533990c-3544-4818-9ada-142783208bac) (query: SELECT toUInt32(toUInt32(NULL, toUInt32(NULL, '0.0000001023'), NULL * 1.0001, toUInt32('255', NULL * '2147483647') * 257, NULL * NULL) * NULL, NULL * '-92233720368547758.07', toUInt32(NULL * -2147483647, NULL, '-0.2147483649', NULL * '0.00000001'), NULL), '255', _part_offset FROM t_1__fuzz_36 ORDER BY order_0 DESC NULLS LAST LIMIT 1048575) Received signal Aborted (6)\r\n2023.09.13 13:29:27.152249 [ 794 ] {} <Fatal> BaseDaemon: \r\n2023.09.13 13:29:27.152295 [ 794 ] {} <Fatal> BaseDaemon: Stack trace: 0x00007fc551daaa7c 0x00007fc551d56476 0x00007fc551d3c7f3 0x0000557e372abb28 0x0000557e372acb66 0x0000557e263a3449 0x0000557e4e1608f0 0x0000557e4e16244e 0x0000557e4f9fbc18 0x0000557e4eca170b 0x0000557e4ece8b4e 0x0000557e4eccad25 0x0000557e4eccdb02 0x0000557e37590d3c 0x0000557e3759b1a0 0x0000557e37587832 0x0000557e37595b0d 0x00007fc551da8b43 0x00007fc551e3aa00\r\n2023.09.13 13:29:27.152353 [ 794 ] {} <Fatal> BaseDaemon: 3. ? @ 0x00007fc551daaa7c in ?\r\n2023.09.13 13:29:27.152402 [ 794 ] {} <Fatal> BaseDaemon: 4. ? @ 0x00007fc551d56476 in ?\r\n2023.09.13 13:29:27.152472 [ 794 ] {} <Fatal> BaseDaemon: 5. ? @ 0x00007fc551d3c7f3 in ?\r\n2023.09.13 13:29:27.243634 [ 794 ] {} <Fatal> BaseDaemon: 6. ./build_docker/./src/Common/Exception.cpp:43: DB::abortOnFailedAssertion(String const&) @ 0x000000001d71eb28 in /workspace/clickhouse\r\n2023.09.13 13:29:27.332896 [ 794 ] {} <Fatal> BaseDaemon: 7.1. inlined from ./build_docker/./src/Common/Exception.cpp:68: DB::handle_error_code(String const&, int, bool, std::vector<void*, std::allocator<void*>> const&)\r\n2023.09.13 13:29:27.332974 [ 794 ] {} <Fatal> BaseDaemon: 7. ./build_docker/./src/Common/Exception.cpp:102: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000001d71fb66 in /workspace/clickhouse\r\n2023.09.13 13:29:31.494811 [ 794 ] {} <Fatal> BaseDaemon: 8. DB::Exception::Exception<char const (&) [73]>(int, char const (&) [73]) @ 0x000000000c816449 in /workspace/clickhouse\r\n2023.09.13 13:29:31.626650 [ 794 ] {} <Fatal> BaseDaemon: 9. ./build_docker/./src/Storages/MergeTree/MergeTreeSelectProcessor.cpp:299: DB::injectPartConstVirtualColumns(unsigned long, DB::Block&, DB::MergeTreeReadTask*, std::shared_ptr<DB::IDataType const> const&, std::vector<String, std::allocator<String>> const&) @ 0x00000000345d38f0 in /workspace/clickhouse\r\n2023.09.13 13:29:31.737427 [ 794 ] {} <Fatal> BaseDaemon: 10. ./build_docker/./src/Storages/MergeTree/MergeTreeSelectProcessor.cpp:0: DB::MergeTreeSelectProcessor::read() @ 0x00000000345d544e in /workspace/clickhouse\r\n2023.09.13 13:29:31.790476 [ 794 ] {} <Fatal> BaseDaemon: 11.1. inlined from ./build_docker/./src/Storages/MergeTree/MergeTreeSource.cpp:181: DB::MergeTreeSource::processReadResult(DB::ChunkAndProgress)\r\n2023.09.13 13:29:31.790534 [ 794 ] {} <Fatal> BaseDaemon: 11. ./build_docker/./src/Storages/MergeTree/MergeTreeSource.cpp:226: DB::MergeTreeSource::tryGenerate() @ 0x0000000035e6ec18 in /workspace/clickhouse\r\n2023.09.13 13:29:31.834571 [ 794 ] {} <Fatal> BaseDaemon: 12. ./build_docker/./src/Processors/ISource.cpp:0: DB::ISource::work() @ 0x000000003511470b in /workspace/clickhouse\r\n2023.09.13 13:29:31.848868 [ 794 ] {} <Fatal> BaseDaemon: 13.1. inlined from ./build_docker/./src/Processors/Executors/ExecutionThreadContext.cpp:0: DB::executeJob(DB::ExecutingGraph::Node*, DB::ReadProgressCallback*)\r\n2023.09.13 13:29:31.848915 [ 794 ] {} <Fatal> BaseDaemon: 13. ./build_docker/./src/Processors/Executors/ExecutionThreadContext.cpp:95: DB::ExecutionThreadContext::executeTask() @ 0x000000003515bb4e in /workspace/clickhouse\r\n2023.09.13 13:29:31.902702 [ 794 ] {} <Fatal> BaseDaemon: 14. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:273: DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x000000003513dd25 in /workspace/clickhouse\r\n2023.09.13 13:29:31.977273 [ 794 ] {} <Fatal> BaseDaemon: 15.1. inlined from ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:371: operator()\r\n2023.09.13 13:29:31.977360 [ 794 ] {} <Fatal> BaseDaemon: 15.2. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/invoke.h:394: decltype(std::declval<DB::PipelineExecutor::spawnThreads()::$_0&>()()) std::__invoke[abi:v15000]<DB::PipelineExecutor::spawnThreads()::$_0&>(DB::PipelineExecutor::spawnThreads()::$_0&)\r\n2023.09.13 13:29:31.977402 [ 794 ] {} <Fatal> BaseDaemon: 15.3. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/invoke.h:479: void std::__invoke_void_return_wrapper<void, true>::__call<DB::PipelineExecutor::spawnThreads()::$_0&>(DB::PipelineExecutor::spawnThreads()::$_0&)\r\n2023.09.13 13:29:31.977439 [ 794 ] {} <Fatal> BaseDaemon: 15.4. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/function.h:235: std::__function::__default_alloc_func<DB::PipelineExecutor::spawnThreads()::$_0, void ()>::operator()[abi:v15000]()\r\n2023.09.13 13:29:31.977465 [ 794 ] {} <Fatal> BaseDaemon: 15. ./build_docker/./contrib/llvm-project/libcxx/include/__functional/function.h:716: void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<DB::PipelineExecutor::spawnThreads()::$_0, void ()>>(std::__function::__policy_storage const*) @ 0x0000000035140b02 in /workspace/clickhouse\r\n2023.09.13 13:29:32.040870 [ 794 ] {} <Fatal> BaseDaemon: 16.1. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:809: bool wide::integer<128ul, unsigned int>::_impl::operator_eq<wide::integer<128ul, unsigned int>>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.09.13 13:29:32.040954 [ 794 ] {} <Fatal> BaseDaemon: 16.2. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:1482: bool wide::operator==<128ul, unsigned int, 128ul, unsigned int>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.09.13 13:29:32.040990 [ 794 ] {} <Fatal> BaseDaemon: 16.3. inlined from ./build_docker/./base/base/../base/strong_typedef.h:42: StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>::operator==(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&) const\r\n2023.09.13 13:29:32.041025 [ 794 ] {} <Fatal> BaseDaemon: 16.4. inlined from ./build_docker/./src/Common/OpenTelemetryTraceContext.h:65: DB::OpenTelemetry::Span::isTraceEnabled() const\r\n2023.09.13 13:29:32.041054 [ 794 ] {} <Fatal> BaseDaemon: 16. ./build_docker/./src/Common/ThreadPool.cpp:428: ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::worker(std::__list_iterator<ThreadFromGlobalPoolImpl<false>, void*>) @ 0x000000001da03d3c in /workspace/clickhouse\r\n2023.09.13 13:29:32.088201 [ 794 ] {} <Fatal> BaseDaemon: 17. ./build_docker/./src/Common/ThreadPool.h:243: ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'()::operator()() @ 0x000000001da0e1a0 in /workspace/clickhouse\r\n2023.09.13 13:29:32.143112 [ 794 ] {} <Fatal> BaseDaemon: 18.1. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:809: bool wide::integer<128ul, unsigned int>::_impl::operator_eq<wide::integer<128ul, unsigned int>>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.09.13 13:29:32.143181 [ 794 ] {} <Fatal> BaseDaemon: 18.2. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:1482: bool wide::operator==<128ul, unsigned int, 128ul, unsigned int>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.09.13 13:29:32.143220 [ 794 ] {} <Fatal> BaseDaemon: 18.3. inlined from ./build_docker/./base/base/../base/strong_typedef.h:42: StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>::operator==(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&) const\r\n2023.09.13 13:29:32.143258 [ 794 ] {} <Fatal> BaseDaemon: 18.4. inlined from ./build_docker/./src/Common/OpenTelemetryTraceContext.h:65: DB::OpenTelemetry::Span::isTraceEnabled() const\r\n2023.09.13 13:29:32.143287 [ 794 ] {} <Fatal> BaseDaemon: 18. ./build_docker/./src/Common/ThreadPool.cpp:428: ThreadPoolImpl<std::thread>::worker(std::__list_iterator<std::thread, void*>) @ 0x000000001d9fa832 in /workspace/clickhouse\r\n2023.09.13 13:29:32.214987 [ 794 ] {} <Fatal> BaseDaemon: 19.1. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:302: std::unique_ptr<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>, std::default_delete<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>>::reset[abi:v15000](std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>*)\r\n2023.09.13 13:29:32.215058 [ 794 ] {} <Fatal> BaseDaemon: 19.2. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__memory/unique_ptr.h:259: ~unique_ptr\r\n2023.09.13 13:29:32.215091 [ 794 ] {} <Fatal> BaseDaemon: 19. ./build_docker/./contrib/llvm-project/libcxx/include/thread:297: void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x000000001da08b0d in /workspace/clickhouse\r\n2023.09.13 13:29:32.215135 [ 794 ] {} <Fatal> BaseDaemon: 20. ? @ 0x00007fc551da8b43 in ?\r\n2023.09.13 13:29:32.215161 [ 794 ] {} <Fatal> BaseDaemon: 21. ? @ 0x00007fc551e3aa00 in ?\r\n2023.09.13 13:29:32.215196 [ 794 ] {} <Fatal> BaseDaemon: Integrity check of the executable skipped because the reference checksum could not be read.\r\n2023.09.13 13:29:37.228074 [ 794 ] {} <Fatal> BaseDaemon: This ClickHouse version is not official and should be upgraded to the official build.\r\n2023.09.13 13:29:37.228225 [ 794 ] {} <Fatal> BaseDaemon: Changed settings: receive_timeout = 10., receive_data_timeout_ms = 10000, allow_suspicious_low_cardinality_types = true, log_queries = true, table_function_remote_max_addresses = 200, max_execution_time = 10., max_memory_usage = 10000000000, send_logs_level = 'fatal', allow_introspection_functions = true\r\n2023.09.13 13:30:03.623389 [ 170 ] {} <Fatal> Application: Child process was terminated by signal 6.\r\n```\r\n\r\nHow to reproduce:\r\n```sql\r\nCREATE TABLE t_random_1 (`ordinary_1` UInt32) ENGINE = GenerateRandom(1, 5, 3);\r\nCREATE TABLE t_1__fuzz_36 (`order_0` Decimal(76, 53), `ordinary_1` UInt32, `p_time` Date, `computed` ALIAS concat('computed_', CAST(p_time, 'String')), `granule` MATERIALIZED CAST(order_0 / 8192, 'UInt64') % 3, INDEX index_granule granule TYPE minmax GRANULARITY 1) ENGINE = MergeTree PARTITION BY toYYYYMM(p_time) ORDER BY order_0 SETTINGS index_granularity = 8192, index_granularity_bytes = '10Mi';\r\nINSERT INTO t_1__fuzz_36 SELECT rowNumberInAllBlocks(), *, '1984-01-01' FROM t_random_1 LIMIT 1000000;\r\nSELECT toUInt32(toUInt32(NULL, toUInt32(NULL, '0.0000001023'), NULL * 1.0001, toUInt32('255', NULL * '2147483647') * 257, NULL * NULL) * NULL, NULL * '-92233720368547758.07', toUInt32(NULL * -2147483647, NULL, '-0.2147483649', NULL * '0.00000001'), NULL), '255', _part_offset FROM t_1__fuzz_36 ORDER BY order_0 DESC NULLS LAST LIMIT 1048575;\r\n```\n",
  "hints_text": "Changing type of column `order_0` to `UInt64` fixes the problem\nA simpler query to reproduce:\r\n```sql\r\nCREATE TABLE t_1__fuzz_36 (`order_0` Decimal(76, 53), `p_time` Date) ENGINE = MergeTree PARTITION BY toYYYYMM(p_time) ORDER BY order_0;\r\nINSERT INTO t_1__fuzz_36 SELECT number, '1984-01-01' FROM numbers(1000000);\r\nSELECT _part_offset FROM t_1__fuzz_36 ORDER BY order_0 DESC\r\n```",
  "created_at": "2023-09-13T21:12:20Z"
}