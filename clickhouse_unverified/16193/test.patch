diff --git a/docker/test/fasttest/run.sh b/docker/test/fasttest/run.sh
index 1c5f62a9e467..c9c8cb1382d9 100755
--- a/docker/test/fasttest/run.sh
+++ b/docker/test/fasttest/run.sh
@@ -326,7 +326,7 @@ function run_tests
         # Look at DistributedFilesToInsert, so cannot run in parallel.
         01460_DistributedFilesToInsert
 
-        01541_max_memory_usage_for_user
+        01541_max_memory_usage_for_user_long
 
         # Require python libraries like scipy, pandas and numpy
         01322_ttest_scipy
diff --git a/docker/test/stateful/run.sh b/docker/test/stateful/run.sh
index f2fcefd604f1..7779f0e9dc20 100755
--- a/docker/test/stateful/run.sh
+++ b/docker/test/stateful/run.sh
@@ -60,4 +60,8 @@ fi
 # more idiologically correct.
 read -ra ADDITIONAL_OPTIONS <<< "${ADDITIONAL_OPTIONS:-}"
 
+if [[ -n "$USE_DATABASE_REPLICATED" ]] && [[ "$USE_DATABASE_REPLICATED" -eq 1 ]]; then
+    ADDITIONAL_OPTIONS+=('--replicated-database')
+fi
+
 clickhouse-test --testname --shard --zookeeper --no-stateless --hung-check --print-time "$SKIP_LIST_OPT" "${ADDITIONAL_OPTIONS[@]}" "$SKIP_TESTS_OPTION" 2>&1 | ts '%Y-%m-%d %H:%M:%S' | tee test_output/test_result.txt
diff --git a/docker/test/stateless/run.sh b/docker/test/stateless/run.sh
index 575be721a544..d078f3739fdf 100755
--- a/docker/test/stateless/run.sh
+++ b/docker/test/stateless/run.sh
@@ -57,6 +57,10 @@ function run_tests()
         ADDITIONAL_OPTIONS+=('4')
     fi
 
+    if [[ -n "$USE_DATABASE_REPLICATED" ]] && [[ "$USE_DATABASE_REPLICATED" -eq 1 ]]; then
+        ADDITIONAL_OPTIONS+=('--replicated-database')
+    fi
+
     clickhouse-test --testname --shard --zookeeper --hung-check --print-time \
             --test-runs "$NUM_TRIES" \
             "$SKIP_LIST_OPT" "${ADDITIONAL_OPTIONS[@]}" 2>&1 \
diff --git a/docker/test/stress/stress b/docker/test/stress/stress
index d2ec86b4421d..841556cf090d 100755
--- a/docker/test/stress/stress
+++ b/docker/test/stress/stress
@@ -23,12 +23,15 @@ def get_options(i):
     if 0 < i:
         options += " --order=random"
 
-    if i % 2 == 1:
+    if i % 3 == 1:
         options += " --db-engine=Ordinary"
 
+    if i % 3 == 2:
+        options += ''' --db-engine="Replicated('/test/db/test_{}', 's1', 'r1')"'''.format(i)
+
     # If database name is not specified, new database is created for each functional test.
     # Run some threads with one database for all tests.
-    if i % 3 == 1:
+    if i % 2 == 1:
         options += " --database=test_{}".format(i)
 
     if i == 13:
diff --git a/tests/ci/ci_config.json b/tests/ci/ci_config.json
index 44b35d61601d..0e4673192854 100644
--- a/tests/ci/ci_config.json
+++ b/tests/ci/ci_config.json
@@ -261,6 +261,18 @@
                 "with_coverage": false
             }
         },
+        "Functional stateful tests (release, DatabaseReplicated)": {
+            "required_build_properties": {
+                "compiler": "clang-11",
+                "package_type": "deb",
+                "build_type": "relwithdebuginfo",
+                "sanitizer": "none",
+                "bundled": "bundled",
+                "splitted": "unsplitted",
+                "clang-tidy": "disable",
+                "with_coverage": false
+            }
+        },
         "Functional stateless tests (address)": {
             "required_build_properties": {
                 "compiler": "clang-11",
@@ -381,6 +393,18 @@
                 "with_coverage": false
             }
         },
+        "Functional stateless tests (release, DatabaseReplicated)": {
+            "required_build_properties": {
+                "compiler": "clang-11",
+                "package_type": "deb",
+                "build_type": "relwithdebuginfo",
+                "sanitizer": "none",
+                "bundled": "bundled",
+                "splitted": "unsplitted",
+                "clang-tidy": "disable",
+                "with_coverage": false
+            }
+        },
         "Stress test (address)": {
             "required_build_properties": {
                 "compiler": "clang-11",
diff --git a/tests/clickhouse-test b/tests/clickhouse-test
index 74f5f07eb9dd..64a93416c41b 100755
--- a/tests/clickhouse-test
+++ b/tests/clickhouse-test
@@ -105,7 +105,9 @@ def remove_control_characters(s):
     s = re.sub(r"[\x00-\x08\x0b\x0e-\x1f\x7f]", "", s)
     return s
 
-def get_db_engine(args):
+def get_db_engine(args, database_name):
+    if args.replicated_database:
+        return " ENGINE=Replicated('/test/clickhouse/db/{}', 's1', 'r1')".format(database_name)
     if args.db_engine:
         return " ENGINE=" + args.db_engine
     return ""   # Will use default engine
@@ -128,7 +130,7 @@ def run_single_test(args, ext, server_logs_level, client_options, case_file, std
 
         clickhouse_proc_create = Popen(shlex.split(args.client), stdin=PIPE, stdout=PIPE, stderr=PIPE, universal_newlines=True)
         try:
-            clickhouse_proc_create.communicate(("CREATE DATABASE " + database + get_db_engine(args)), timeout=args.timeout)
+            clickhouse_proc_create.communicate(("CREATE DATABASE " + database + get_db_engine(args, database)), timeout=args.timeout)
         except TimeoutExpired:
             total_time = (datetime.now() - start_time).total_seconds()
             return clickhouse_proc_create, "", "Timeout creating database {} before test".format(database), total_time
@@ -161,7 +163,12 @@ def run_single_test(args, ext, server_logs_level, client_options, case_file, std
     while (datetime.now() - start_time).total_seconds() < args.timeout and proc.poll() is None:
         sleep(0.01)
 
-    if not args.database:
+    need_drop_database = not args.database
+    if need_drop_database and args.no_drop_if_fail:
+        maybe_passed = (proc.returncode == 0) and (proc.stderr is None) and (proc.stdout is None or 'Exception' not in proc.stdout)
+        need_drop_database = not maybe_passed
+
+    if need_drop_database:
         clickhouse_proc_create = Popen(shlex.split(args.client), stdin=PIPE, stdout=PIPE, stderr=PIPE, universal_newlines=True)
         seconds_left = max(args.timeout - (datetime.now() - start_time).total_seconds(), 10)
         try:
@@ -182,7 +189,8 @@ def run_single_test(args, ext, server_logs_level, client_options, case_file, std
 
     # Normalize randomized database names in stdout, stderr files.
     os.system("LC_ALL=C sed -i -e 's/{test_db}/default/g' {file}".format(test_db=database, file=stdout_file))
-    os.system("LC_ALL=C sed -i -e 's/{test_db}/default/g' {file}".format(test_db=database, file=stderr_file))
+    if not args.show_db_name:
+        os.system("LC_ALL=C sed -i -e 's/{test_db}/default/g' {file}".format(test_db=database, file=stderr_file))
 
     stdout = open(stdout_file, 'rb').read() if os.path.exists(stdout_file) else b''
     stdout = str(stdout, errors='replace', encoding='utf-8')
@@ -526,6 +534,8 @@ class BuildFlags():
     RELEASE = 'release-build'
     DATABASE_ORDINARY = 'database-ordinary'
     POLYMORPHIC_PARTS = 'polymorphic-parts'
+    ANTLR = 'antlr'
+    DATABASE_REPLICATED = 'database-replicated'
 
 
 def collect_build_flags(client):
@@ -607,7 +617,9 @@ def main(args):
 
     build_flags = collect_build_flags(args.client)
     if args.antlr:
-        build_flags.append('antlr')
+        build_flags.append(BuildFlags.ANTLR)
+    if args.replicated_database:
+        build_flags.append(BuildFlags.DATABASE_REPLICATED)
 
     if args.use_skip_list:
         tests_to_skip_from_list = collect_tests_to_skip(args.skip_list_path, build_flags)
@@ -660,10 +672,10 @@ def main(args):
 
     if args.database and args.database != "test":
         clickhouse_proc_create = Popen(shlex.split(args.client), stdin=PIPE, stdout=PIPE, stderr=PIPE, universal_newlines=True)
-        clickhouse_proc_create.communicate(("CREATE DATABASE IF NOT EXISTS " + args.database + get_db_engine(args)))
+        clickhouse_proc_create.communicate(("CREATE DATABASE IF NOT EXISTS " + args.database + get_db_engine(args, args.database)))
 
     clickhouse_proc_create = Popen(shlex.split(args.client), stdin=PIPE, stdout=PIPE, stderr=PIPE, universal_newlines=True)
-    clickhouse_proc_create.communicate(("CREATE DATABASE IF NOT EXISTS test" + get_db_engine(args)))
+    clickhouse_proc_create.communicate(("CREATE DATABASE IF NOT EXISTS test" + get_db_engine(args, 'test')))
 
     def is_test_from_dir(suite_dir, case):
         case_file = os.path.join(suite_dir, case)
@@ -907,6 +919,8 @@ if __name__ == '__main__':
     parser.add_argument('--hung-check', action='store_true', default=False)
     parser.add_argument('--force-color', action='store_true', default=False)
     parser.add_argument('--database', help='Database for tests (random name test_XXXXXX by default)')
+    parser.add_argument('--no-drop-if-fail', action='store_true', help='Do not drop database for test if test has failed')
+    parser.add_argument('--show-db-name', action='store_true', help='Do not replace random database name with "default"')
     parser.add_argument('--parallel', default='1/1', help='One parallel test run number/total')
     parser.add_argument('-j', '--jobs', default=1, nargs='?', type=int, help='Run all tests in parallel')
     parser.add_argument('--test-runs', default=1, nargs='?', type=int, help='Run each test many times (useful for e.g. flaky check)')
@@ -915,6 +929,7 @@ if __name__ == '__main__':
     parser.add_argument('--skip-list-path', help="Path to skip-list file")
     parser.add_argument('--use-skip-list', action='store_true', default=False, help="Use skip list to skip tests if found")
     parser.add_argument('--db-engine', help='Database engine name')
+    parser.add_argument('--replicated-database', action='store_true', default=False, help='Run tests with Replicated database engine')
 
     parser.add_argument('--antlr', action='store_true', default=False, dest='antlr', help='Use new ANTLR parser in tests')
     parser.add_argument('--no-stateless', action='store_true', help='Disable all stateless tests')
diff --git a/tests/config/install.sh b/tests/config/install.sh
index 9965e1fb1ad4..de6ba2a7a091 100755
--- a/tests/config/install.sh
+++ b/tests/config/install.sh
@@ -61,5 +61,8 @@ fi
 if [[ -n "$USE_DATABASE_ORDINARY" ]] && [[ "$USE_DATABASE_ORDINARY" -eq 1 ]]; then
     ln -sf $SRC_PATH/users.d/database_ordinary.xml $DEST_SERVER_PATH/users.d/
 fi
+if [[ -n "$USE_DATABASE_REPLICATED" ]] && [[ "$USE_DATABASE_REPLICATED" -eq 1 ]]; then
+    ln -sf $SRC_PATH/users.d/database_replicated.xml $DEST_SERVER_PATH/users.d/
+fi
 
 ln -sf $SRC_PATH/client_config.xml $DEST_CLIENT_PATH/config.xml
diff --git a/tests/config/users.d/database_replicated.xml b/tests/config/users.d/database_replicated.xml
new file mode 100644
index 000000000000..23801d001548
--- /dev/null
+++ b/tests/config/users.d/database_replicated.xml
@@ -0,0 +1,10 @@
+<yandex>
+    <profiles>
+        <default>
+            <allow_experimental_database_replicated>1</allow_experimental_database_replicated>
+            <database_replicated_ddl_output>0</database_replicated_ddl_output>
+            <database_replicated_initial_query_timeout_sec>30</database_replicated_initial_query_timeout_sec>
+            <distributed_ddl_task_timeout>30</distributed_ddl_task_timeout>
+        </default>
+    </profiles>
+</yandex>
diff --git a/tests/integration/helpers/test_tools.py b/tests/integration/helpers/test_tools.py
index bbab12e55d47..5fedadd3380b 100644
--- a/tests/integration/helpers/test_tools.py
+++ b/tests/integration/helpers/test_tools.py
@@ -47,20 +47,20 @@ def toMat(contents):
 
 
 def assert_eq_with_retry(instance, query, expectation, retry_count=20, sleep_time=0.5, stdin=None, timeout=None,
-                         settings=None, user=None, ignore_error=False):
+                         settings=None, user=None, ignore_error=False, get_result=lambda x: x):
     expectation_tsv = TSV(expectation)
     for i in range(retry_count):
         try:
-            if TSV(instance.query(query, user=user, stdin=stdin, timeout=timeout, settings=settings,
-                                  ignore_error=ignore_error)) == expectation_tsv:
+            if TSV(get_result(instance.query(query, user=user, stdin=stdin, timeout=timeout, settings=settings,
+                                  ignore_error=ignore_error))) == expectation_tsv:
                 break
             time.sleep(sleep_time)
         except Exception as ex:
             print(("assert_eq_with_retry retry {} exception {}".format(i + 1, ex)))
             time.sleep(sleep_time)
     else:
-        val = TSV(instance.query(query, user=user, stdin=stdin, timeout=timeout, settings=settings,
-                                 ignore_error=ignore_error))
+        val = TSV(get_result(instance.query(query, user=user, stdin=stdin, timeout=timeout, settings=settings,
+                                 ignore_error=ignore_error)))
         if expectation_tsv != val:
             raise AssertionError("'{}' != '{}'
{}".format(expectation_tsv, val, '
'.join(
                 expectation_tsv.diff(val, n1="expectation", n2="query"))))
diff --git a/tests/integration/test_replicated_database/__init__.py b/tests/integration/test_replicated_database/__init__.py
new file mode 100644
index 000000000000..e69de29bb2d1
diff --git a/tests/integration/test_replicated_database/configs/config.xml b/tests/integration/test_replicated_database/configs/config.xml
new file mode 100644
index 000000000000..ebceee3aa5c8
--- /dev/null
+++ b/tests/integration/test_replicated_database/configs/config.xml
@@ -0,0 +1,34 @@
+<yandex>
+    <database_atomic_delay_before_drop_table_sec>10</database_atomic_delay_before_drop_table_sec>
+
+    <remote_servers>
+        <cluster>
+            <shard>
+                <internal_replication>true</internal_replication>
+                <replica>
+                    <host>main_node</host>
+                    <port>9000</port>
+                </replica>
+                <replica>
+                    <host>dummy_node</host>
+                    <port>9000</port>
+                </replica>
+                <replica>
+                    <host>competing_node</host>
+                    <port>9000</port>
+                </replica>
+            </shard>
+            <shard>
+                <internal_replication>true</internal_replication>
+                <replica>
+                    <host>snapshotting_node</host>
+                    <port>9000</port>
+                </replica>
+                <replica>
+                    <host>snapshot_recovering_node</host>
+                    <port>9000</port>
+                </replica>
+            </shard>
+        </cluster>
+    </remote_servers>
+</yandex>
diff --git a/tests/integration/test_replicated_database/configs/settings.xml b/tests/integration/test_replicated_database/configs/settings.xml
new file mode 100644
index 000000000000..e0f7e8691e66
--- /dev/null
+++ b/tests/integration/test_replicated_database/configs/settings.xml
@@ -0,0 +1,12 @@
+<yandex>
+    <profiles>
+        <default>
+            <allow_experimental_database_replicated>1</allow_experimental_database_replicated>
+        </default>
+    </profiles>
+    <users>
+        <default>
+            <profile>default</profile>
+        </default>
+    </users>
+</yandex>
diff --git a/tests/integration/test_replicated_database/test.py b/tests/integration/test_replicated_database/test.py
new file mode 100644
index 000000000000..99e7d6077f82
--- /dev/null
+++ b/tests/integration/test_replicated_database/test.py
@@ -0,0 +1,278 @@
+import time
+import re
+import pytest
+
+from helpers.cluster import ClickHouseCluster
+from helpers.test_tools import assert_eq_with_retry, assert_logs_contain
+from helpers.network import PartitionManager
+
+cluster = ClickHouseCluster(__file__)
+
+main_node = cluster.add_instance('main_node', main_configs=['configs/config.xml'], user_configs=['configs/settings.xml'], with_zookeeper=True, stay_alive=True, macros={"shard": 1, "replica": 1})
+dummy_node = cluster.add_instance('dummy_node', main_configs=['configs/config.xml'], user_configs=['configs/settings.xml'], with_zookeeper=True, stay_alive=True, macros={"shard": 1, "replica": 2})
+competing_node = cluster.add_instance('competing_node', main_configs=['configs/config.xml'], user_configs=['configs/settings.xml'], with_zookeeper=True, macros={"shard": 1, "replica": 3})
+snapshotting_node = cluster.add_instance('snapshotting_node', main_configs=['configs/config.xml'], user_configs=['configs/settings.xml'], with_zookeeper=True, macros={"shard": 2, "replica": 1})
+snapshot_recovering_node = cluster.add_instance('snapshot_recovering_node', main_configs=['configs/config.xml'], user_configs=['configs/settings.xml'], with_zookeeper=True, macros={"shard": 2, "replica": 2})
+
+all_nodes = [main_node, dummy_node, competing_node, snapshotting_node, snapshot_recovering_node]
+
+uuid_regex = re.compile("[0-9a-f]{8}\-[0-9a-f]{4}\-[0-9a-f]{4}\-[0-9a-f]{4}\-[0-9a-f]{12}")
+def assert_create_query(nodes, table_name, expected):
+    replace_uuid = lambda x: re.sub(uuid_regex, "uuid", x)
+    query = "show create table {}".format(table_name)
+    for node in nodes:
+        assert_eq_with_retry(node, query, expected, get_result=replace_uuid)
+
+@pytest.fixture(scope="module")
+def started_cluster():
+    try:
+        cluster.start()
+        main_node.query("CREATE DATABASE testdb ENGINE = Replicated('/clickhouse/databases/test1', 'shard1', 'replica1');")
+        dummy_node.query("CREATE DATABASE testdb ENGINE = Replicated('/clickhouse/databases/test1', 'shard1', 'replica2');")
+        yield cluster
+
+    finally:
+        cluster.shutdown()
+
+def test_create_replicated_table(started_cluster):
+    assert "Old syntax is not allowed" in \
+           main_node.query_and_get_error("CREATE TABLE testdb.replicated_table (d Date, k UInt64, i32 Int32) ENGINE=ReplicatedMergeTree('/test/tmp', 'r', d, k, 8192);")
+
+    main_node.query("CREATE TABLE testdb.replicated_table (d Date, k UInt64, i32 Int32) ENGINE=ReplicatedMergeTree ORDER BY k PARTITION BY toYYYYMM(d);")
+
+    expected = "CREATE TABLE testdb.replicated_table\
(\
    `d` Date,\
    `k` UInt64,\
    `i32` Int32\
)\
" \
+               "ENGINE = ReplicatedMergeTree(\\'/clickhouse/tables/uuid/{shard}\\', \\'{replica}\\')\
" \
+               "PARTITION BY toYYYYMM(d)\
ORDER BY k\
SETTINGS index_granularity = 8192"
+    assert_create_query([main_node, dummy_node], "testdb.replicated_table", expected)
+    # assert without replacing uuid
+    assert main_node.query("show create testdb.replicated_table") == dummy_node.query("show create testdb.replicated_table")
+
+@pytest.mark.parametrize("engine", ['MergeTree', 'ReplicatedMergeTree'])
+def test_simple_alter_table(started_cluster, engine):
+    # test_simple_alter_table
+    name  = "testdb.alter_test_{}".format(engine)
+    main_node.query("CREATE TABLE {} "
+                    "(CounterID UInt32, StartDate Date, UserID UInt32, VisitID UInt32, NestedColumn Nested(A UInt8, S String), ToDrop UInt32) "
+                    "ENGINE = {} PARTITION BY StartDate ORDER BY (CounterID, StartDate, intHash32(UserID), VisitID);".format(name, engine))
+    main_node.query("ALTER TABLE {} ADD COLUMN Added0 UInt32;".format(name))
+    main_node.query("ALTER TABLE {} ADD COLUMN Added2 UInt32;".format(name))
+    main_node.query("ALTER TABLE {} ADD COLUMN Added1 UInt32 AFTER Added0;".format(name))
+    main_node.query("ALTER TABLE {} ADD COLUMN AddedNested1 Nested(A UInt32, B UInt64) AFTER Added2;".format(name))
+    main_node.query("ALTER TABLE {} ADD COLUMN AddedNested1.C Array(String) AFTER AddedNested1.B;".format(name))
+    main_node.query("ALTER TABLE {} ADD COLUMN AddedNested2 Nested(A UInt32, B UInt64) AFTER AddedNested1;".format(name))
+
+    full_engine = engine if not "Replicated" in engine else engine + "(\\'/clickhouse/tables/uuid/{shard}\\', \\'{replica}\\')"
+    expected = "CREATE TABLE {}\
(\
    `CounterID` UInt32,\
    `StartDate` Date,\
    `UserID` UInt32,\
" \
+               "    `VisitID` UInt32,\
    `NestedColumn.A` Array(UInt8),\
    `NestedColumn.S` Array(String),\
" \
+               "    `ToDrop` UInt32,\
    `Added0` UInt32,\
    `Added1` UInt32,\
    `Added2` UInt32,\
" \
+               "    `AddedNested1.A` Array(UInt32),\
    `AddedNested1.B` Array(UInt64),\
    `AddedNested1.C` Array(String),\
" \
+               "    `AddedNested2.A` Array(UInt32),\
    `AddedNested2.B` Array(UInt64)\
)\
" \
+               "ENGINE = {}\
PARTITION BY StartDate\
ORDER BY (CounterID, StartDate, intHash32(UserID), VisitID)\
" \
+               "SETTINGS index_granularity = 8192".format(name, full_engine)
+
+    assert_create_query([main_node, dummy_node], name, expected)
+
+    # test_create_replica_after_delay
+    competing_node.query("CREATE DATABASE IF NOT EXISTS testdb ENGINE = Replicated('/clickhouse/databases/test1', 'shard1', 'replica3');")
+
+    name  = "testdb.alter_test_{}".format(engine)
+    main_node.query("ALTER TABLE {} ADD COLUMN Added3 UInt32;".format(name))
+    main_node.query("ALTER TABLE {} DROP COLUMN AddedNested1;".format(name))
+    main_node.query("ALTER TABLE {} RENAME COLUMN Added1 TO AddedNested1;".format(name))
+
+    full_engine = engine if not "Replicated" in engine else engine + "(\\'/clickhouse/tables/uuid/{shard}\\', \\'{replica}\\')"
+    expected = "CREATE TABLE {}\
(\
    `CounterID` UInt32,\
    `StartDate` Date,\
    `UserID` UInt32,\
" \
+               "    `VisitID` UInt32,\
    `NestedColumn.A` Array(UInt8),\
    `NestedColumn.S` Array(String),\
" \
+               "    `ToDrop` UInt32,\
    `Added0` UInt32,\
    `AddedNested1` UInt32,\
    `Added2` UInt32,\
" \
+               "    `AddedNested2.A` Array(UInt32),\
    `AddedNested2.B` Array(UInt64),\
    `Added3` UInt32\
)\
" \
+               "ENGINE = {}\
PARTITION BY StartDate\
ORDER BY (CounterID, StartDate, intHash32(UserID), VisitID)\
" \
+               "SETTINGS index_granularity = 8192".format(name, full_engine)
+
+    assert_create_query([main_node, dummy_node, competing_node], name, expected)
+
+
+def test_alters_from_different_replicas(started_cluster):
+    # test_alters_from_different_replicas
+    competing_node.query("CREATE DATABASE IF NOT EXISTS testdb ENGINE = Replicated('/clickhouse/databases/test1', 'shard1', 'replica3');")
+
+    main_node.query("CREATE TABLE testdb.concurrent_test "
+                    "(CounterID UInt32, StartDate Date, UserID UInt32, VisitID UInt32, NestedColumn Nested(A UInt8, S String), ToDrop UInt32) "
+                    "ENGINE = MergeTree(StartDate, intHash32(UserID), (CounterID, StartDate, intHash32(UserID), VisitID), 8192);")
+
+    main_node.query("CREATE TABLE testdb.dist AS testdb.concurrent_test ENGINE = Distributed(cluster, testdb, concurrent_test, CounterID)")
+
+    dummy_node.stop_clickhouse(kill=True)
+
+    settings = {"distributed_ddl_task_timeout": 10}
+    assert "There are 1 unfinished hosts (0 of them are currently active)" in \
+        competing_node.query_and_get_error("ALTER TABLE testdb.concurrent_test ADD COLUMN Added0 UInt32;", settings=settings)
+    dummy_node.start_clickhouse()
+    main_node.query("ALTER TABLE testdb.concurrent_test ADD COLUMN Added2 UInt32;")
+    competing_node.query("ALTER TABLE testdb.concurrent_test ADD COLUMN Added1 UInt32 AFTER Added0;")
+    main_node.query("ALTER TABLE testdb.concurrent_test ADD COLUMN AddedNested1 Nested(A UInt32, B UInt64) AFTER Added2;")
+    competing_node.query("ALTER TABLE testdb.concurrent_test ADD COLUMN AddedNested1.C Array(String) AFTER AddedNested1.B;")
+    main_node.query("ALTER TABLE testdb.concurrent_test ADD COLUMN AddedNested2 Nested(A UInt32, B UInt64) AFTER AddedNested1;")
+
+    expected = "CREATE TABLE testdb.concurrent_test\
(\
    `CounterID` UInt32,\
    `StartDate` Date,\
    `UserID` UInt32,\
" \
+               "    `VisitID` UInt32,\
    `NestedColumn.A` Array(UInt8),\
    `NestedColumn.S` Array(String),\
    `ToDrop` UInt32,\
" \
+               "    `Added0` UInt32,\
    `Added1` UInt32,\
    `Added2` UInt32,\
    `AddedNested1.A` Array(UInt32),\
" \
+               "    `AddedNested1.B` Array(UInt64),\
    `AddedNested1.C` Array(String),\
    `AddedNested2.A` Array(UInt32),\
" \
+               "    `AddedNested2.B` Array(UInt64)\
)\
" \
+               "ENGINE = MergeTree(StartDate, intHash32(UserID), (CounterID, StartDate, intHash32(UserID), VisitID), 8192)"
+
+    assert_create_query([main_node, competing_node], "testdb.concurrent_test", expected)
+
+    # test_create_replica_after_delay
+    main_node.query("DROP TABLE testdb.concurrent_test")
+    main_node.query("CREATE TABLE testdb.concurrent_test "
+                    "(CounterID UInt32, StartDate Date, UserID UInt32, VisitID UInt32, NestedColumn Nested(A UInt8, S String), ToDrop UInt32) "
+                    "ENGINE = ReplicatedMergeTree ORDER BY CounterID;")
+
+    expected = "CREATE TABLE testdb.concurrent_test\
(\
    `CounterID` UInt32,\
    `StartDate` Date,\
    `UserID` UInt32,\
" \
+               "    `VisitID` UInt32,\
    `NestedColumn.A` Array(UInt8),\
    `NestedColumn.S` Array(String),\
    `ToDrop` UInt32\
)\
" \
+               "ENGINE = ReplicatedMergeTree(\\'/clickhouse/tables/uuid/{shard}\\', \\'{replica}\\')\
ORDER BY CounterID\
SETTINGS index_granularity = 8192"
+
+    assert_create_query([main_node, competing_node], "testdb.concurrent_test", expected)
+
+    main_node.query("INSERT INTO testdb.dist (CounterID, StartDate, UserID) SELECT number, addDays(toDate('2020-02-02'), number), intHash32(number) FROM numbers(10)")
+
+    # test_replica_restart
+    main_node.restart_clickhouse()
+
+    expected = "CREATE TABLE testdb.concurrent_test\
(\
    `CounterID` UInt32,\
    `StartDate` Date,\
    `UserID` UInt32,\
" \
+               "    `VisitID` UInt32,\
    `NestedColumn.A` Array(UInt8),\
    `NestedColumn.S` Array(String),\
    `ToDrop` UInt32\
)\
" \
+               "ENGINE = ReplicatedMergeTree(\\'/clickhouse/tables/uuid/{shard}\\', \\'{replica}\\')\
ORDER BY CounterID\
SETTINGS index_granularity = 8192"
+
+
+    # test_snapshot_and_snapshot_recover
+    snapshotting_node.query("CREATE DATABASE testdb ENGINE = Replicated('/clickhouse/databases/test1', 'shard2', 'replica1');")
+    snapshot_recovering_node.query("CREATE DATABASE testdb ENGINE = Replicated('/clickhouse/databases/test1', 'shard2', 'replica2');")
+    assert_create_query(all_nodes, "testdb.concurrent_test", expected)
+
+    main_node.query("SYSTEM FLUSH DISTRIBUTED testdb.dist")
+    main_node.query("ALTER TABLE testdb.concurrent_test UPDATE StartDate = addYears(StartDate, 1) WHERE 1")
+    res = main_node.query("ALTER TABLE testdb.concurrent_test DELETE WHERE UserID % 2")
+    assert "shard1|replica1" in res and "shard1|replica2" in res and "shard1|replica3" in res
+    assert "shard2|replica1" in res and "shard2|replica2" in res
+
+    expected = "1\t1\tmain_node
" \
+               "1\t2\tdummy_node
" \
+               "1\t3\tcompeting_node
" \
+               "2\t1\tsnapshotting_node
" \
+               "2\t2\tsnapshot_recovering_node
"
+    assert main_node.query("SELECT shard_num, replica_num, host_name FROM system.clusters WHERE cluster='testdb'") == expected
+
+    # test_drop_and_create_replica
+    main_node.query("DROP DATABASE testdb SYNC")
+    main_node.query("CREATE DATABASE testdb ENGINE = Replicated('/clickhouse/databases/test1', 'shard1', 'replica1');")
+
+    expected = "CREATE TABLE testdb.concurrent_test\
(\
    `CounterID` UInt32,\
    `StartDate` Date,\
    `UserID` UInt32,\
" \
+               "    `VisitID` UInt32,\
    `NestedColumn.A` Array(UInt8),\
    `NestedColumn.S` Array(String),\
    `ToDrop` UInt32\
)\
" \
+               "ENGINE = ReplicatedMergeTree(\\'/clickhouse/tables/uuid/{shard}\\', \\'{replica}\\')\
ORDER BY CounterID\
SETTINGS index_granularity = 8192"
+
+    assert_create_query([main_node, competing_node], "testdb.concurrent_test", expected)
+    assert_create_query(all_nodes, "testdb.concurrent_test", expected)
+
+    for node in all_nodes:
+        node.query("SYSTEM SYNC REPLICA testdb.concurrent_test")
+
+    expected = "0\t2021-02-02\t4249604106
" \
+               "1\t2021-02-03\t1343103100
" \
+               "4\t2021-02-06\t3902320246
" \
+               "7\t2021-02-09\t3844986530
" \
+               "9\t2021-02-11\t1241149650
"
+
+    assert_eq_with_retry(dummy_node, "SELECT CounterID, StartDate, UserID FROM testdb.dist ORDER BY CounterID", expected)
+
+def test_recover_staled_replica(started_cluster):
+    main_node.query("CREATE DATABASE recover ENGINE = Replicated('/clickhouse/databases/recover', 'shard1', 'replica1');")
+    started_cluster.get_kazoo_client('zoo1').set('/clickhouse/databases/recover/logs_to_keep', b'10')
+    dummy_node.query("CREATE DATABASE recover ENGINE = Replicated('/clickhouse/databases/recover', 'shard1', 'replica2');")
+
+    settings = {"distributed_ddl_task_timeout": 0}
+    main_node.query("CREATE TABLE recover.t1 (n int) ENGINE=Memory", settings=settings)
+    dummy_node.query("CREATE TABLE recover.t2 (s String) ENGINE=Memory", settings=settings)
+    main_node.query("CREATE TABLE recover.mt1 (n int) ENGINE=MergeTree order by n", settings=settings)
+    dummy_node.query("CREATE TABLE recover.mt2 (n int) ENGINE=MergeTree order by n", settings=settings)
+    main_node.query("CREATE TABLE recover.rmt1 (n int) ENGINE=ReplicatedMergeTree order by n", settings=settings)
+    dummy_node.query("CREATE TABLE recover.rmt2 (n int) ENGINE=ReplicatedMergeTree order by n", settings=settings)
+    main_node.query("CREATE TABLE recover.rmt3 (n int) ENGINE=ReplicatedMergeTree order by n", settings=settings)
+    dummy_node.query("CREATE TABLE recover.rmt5 (n int) ENGINE=ReplicatedMergeTree order by n", settings=settings)
+    main_node.query("CREATE DICTIONARY recover.d1 (n int DEFAULT 0, m int DEFAULT 1) PRIMARY KEY n SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 USER 'default' TABLE 'rmt1' PASSWORD '' DB 'recover')) LIFETIME(MIN 1 MAX 10) LAYOUT(FLAT())")
+    dummy_node.query("CREATE DICTIONARY recover.d2 (n int DEFAULT 0, m int DEFAULT 1) PRIMARY KEY n SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 USER 'default' TABLE 'rmt2' PASSWORD '' DB 'recover')) LIFETIME(MIN 1 MAX 10) LAYOUT(FLAT())")
+
+    for table in ['t1', 't2', 'mt1', 'mt2', 'rmt1', 'rmt2', 'rmt3', 'rmt5']:
+        main_node.query("INSERT INTO recover.{} VALUES (42)".format(table))
+    for table in ['t1', 't2', 'mt1', 'mt2']:
+        dummy_node.query("INSERT INTO recover.{} VALUES (42)".format(table))
+    for table in ['rmt1', 'rmt2', 'rmt3', 'rmt5']:
+        main_node.query("SYSTEM SYNC REPLICA recover.{}".format(table))
+
+    with PartitionManager() as pm:
+        pm.drop_instance_zk_connections(dummy_node)
+        dummy_node.query_and_get_error("RENAME TABLE recover.t1 TO recover.m1")
+        main_node.query("RENAME TABLE recover.t1 TO recover.m1", settings=settings)
+        main_node.query("ALTER TABLE recover.mt1  ADD COLUMN m int", settings=settings)
+        main_node.query("ALTER TABLE recover.rmt1 ADD COLUMN m int", settings=settings)
+        main_node.query("RENAME TABLE recover.rmt3 TO recover.rmt4", settings=settings)
+        main_node.query("DROP TABLE recover.rmt5", settings=settings)
+        main_node.query("DROP DICTIONARY recover.d2", settings=settings)
+        main_node.query("CREATE DICTIONARY recover.d2 (n int DEFAULT 0, m int DEFAULT 1) PRIMARY KEY n SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 USER 'default' TABLE 'rmt1' PASSWORD '' DB 'recover')) LIFETIME(MIN 1 MAX 10) LAYOUT(FLAT());", settings=settings)
+
+        main_node.query("CREATE TABLE recover.tmp AS recover.m1", settings=settings)
+        main_node.query("DROP TABLE recover.tmp", settings=settings)
+        main_node.query("CREATE TABLE recover.tmp AS recover.m1", settings=settings)
+        main_node.query("DROP TABLE recover.tmp", settings=settings)
+        main_node.query("CREATE TABLE recover.tmp AS recover.m1", settings=settings)
+        main_node.query("DROP TABLE recover.tmp", settings=settings)
+        main_node.query("CREATE TABLE recover.tmp AS recover.m1", settings=settings)
+
+    assert main_node.query("SELECT name FROM system.tables WHERE database='recover' ORDER BY name") == "d1
d2
m1
mt1
mt2
rmt1
rmt2
rmt4
t2
tmp
"
+    query = "SELECT name, uuid, create_table_query FROM system.tables WHERE database='recover' ORDER BY name"
+    expected = main_node.query(query)
+    assert_eq_with_retry(dummy_node, query, expected)
+
+    for table in ['m1', 't2', 'mt1', 'mt2', 'rmt1', 'rmt2', 'rmt4', 'd1', 'd2']:
+        assert main_node.query("SELECT (*,).1 FROM recover.{}".format(table)) == "42
"
+    for table in ['t2', 'rmt1', 'rmt2', 'rmt4', 'd1', 'd2', 'mt2']:
+        assert dummy_node.query("SELECT (*,).1 FROM recover.{}".format(table)) == "42
"
+    for table in ['m1', 'mt1']:
+        assert dummy_node.query("SELECT count() FROM recover.{}".format(table)) == "0
"
+
+    assert dummy_node.query("SELECT count() FROM system.tables WHERE database='recover_broken_tables'") == "2
"
+    table = dummy_node.query("SHOW TABLES FROM recover_broken_tables LIKE 'mt1_26_%'").strip()
+    assert dummy_node.query("SELECT (*,).1 FROM recover_broken_tables.{}".format(table)) == "42
"
+    table = dummy_node.query("SHOW TABLES FROM recover_broken_tables LIKE 'rmt5_26_%'").strip()
+    assert dummy_node.query("SELECT (*,).1 FROM recover_broken_tables.{}".format(table)) == "42
"
+
+    expected = "Cleaned 4 outdated objects: dropped 1 dictionaries and 1 tables, moved 2 tables"
+    assert_logs_contain(dummy_node, expected)
+
+    dummy_node.query("DROP TABLE recover.tmp")
+    assert_eq_with_retry(main_node, "SELECT count() FROM system.tables WHERE database='recover' AND name='tmp'", "0
")
+
+def test_startup_without_zk(started_cluster):
+    main_node.query("DROP DATABASE IF EXISTS testdb SYNC")
+    main_node.query("DROP DATABASE IF EXISTS recover SYNC")
+    with PartitionManager() as pm:
+        pm.drop_instance_zk_connections(main_node)
+        err = main_node.query_and_get_error("CREATE DATABASE startup ENGINE = Replicated('/clickhouse/databases/startup', 'shard1', 'replica1');")
+        assert "ZooKeeper" in err
+    main_node.query("CREATE DATABASE startup ENGINE = Replicated('/clickhouse/databases/startup', 'shard1', 'replica1');")
+    #main_node.query("CREATE TABLE startup.rmt (n int) ENGINE=ReplicatedMergeTree order by n")
+    main_node.query("CREATE TABLE startup.rmt (n int) ENGINE=MergeTree order by n")
+    main_node.query("INSERT INTO startup.rmt VALUES (42)")
+    with PartitionManager() as pm:
+        pm.drop_instance_zk_connections(main_node)
+        main_node.restart_clickhouse(stop_start_wait_sec=30)
+        assert main_node.query("SELECT (*,).1 FROM startup.rmt") == "42
"
+
+    for _ in range(10):
+        try:
+            main_node.query("CREATE TABLE startup.m (n int) ENGINE=Memory")
+            break
+        except:
+            time.sleep(1)
+
+    main_node.query("EXCHANGE TABLES startup.rmt AND startup.m")
+    assert main_node.query("SELECT (*,).1 FROM startup.m") == "42
"
diff --git a/tests/queries/0_stateless/01018_ddl_dictionaries_concurrent_requrests.sh b/tests/queries/0_stateless/01018_ddl_dictionaries_concurrent_requrests.sh
index bc13e44934a8..025fe51e2a9c 100755
--- a/tests/queries/0_stateless/01018_ddl_dictionaries_concurrent_requrests.sh
+++ b/tests/queries/0_stateless/01018_ddl_dictionaries_concurrent_requrests.sh
@@ -113,8 +113,8 @@ timeout $TIMEOUT bash -c thread7 2> /dev/null &
 wait
 $CLICKHOUSE_CLIENT -q "SELECT 'Still alive'"
 
-$CLICKHOUSE_CLIENT -q "ATTACH DICTIONARY database_for_dict.dict1"
-$CLICKHOUSE_CLIENT -q "ATTACH DICTIONARY database_for_dict.dict2"
+$CLICKHOUSE_CLIENT -q "ATTACH DICTIONARY IF NOT EXISTS database_for_dict.dict1"
+$CLICKHOUSE_CLIENT -q "ATTACH DICTIONARY IF NOT EXISTS database_for_dict.dict2"
 
 $CLICKHOUSE_CLIENT -n -q "
     DROP TABLE table_for_dict1;
diff --git a/tests/queries/0_stateless/01238_http_memory_tracking.sh b/tests/queries/0_stateless/01238_http_memory_tracking.sh
index 90a7611c7c79..8c900e4c2088 100755
--- a/tests/queries/0_stateless/01238_http_memory_tracking.sh
+++ b/tests/queries/0_stateless/01238_http_memory_tracking.sh
@@ -18,3 +18,6 @@ yes 'SELECT 1' 2>/dev/null | {
 } | grep -x -c 1
 
 wait
+
+# Reset max_memory_usage_for_user, so it will not affect other tests
+${CLICKHOUSE_CLIENT} --max_memory_usage_for_user=0 -q "SELECT 1 FORMAT Null"
diff --git a/tests/queries/0_stateless/01281_group_by_limit_memory_tracking.sh b/tests/queries/0_stateless/01281_group_by_limit_memory_tracking.sh
index 9909d9b566d7..4667c76cb607 100755
--- a/tests/queries/0_stateless/01281_group_by_limit_memory_tracking.sh
+++ b/tests/queries/0_stateless/01281_group_by_limit_memory_tracking.sh
@@ -42,3 +42,6 @@ execute_group_by
 # if memory accounting will be incorrect, the second query will be failed with MEMORY_LIMIT_EXCEEDED
 execute_group_by
 wait
+
+# Reset max_memory_usage_for_user, so it will not affect other tests
+${CLICKHOUSE_CLIENT} --max_memory_usage_for_user=0 -q "SELECT 1 FORMAT Null"
diff --git a/tests/queries/0_stateless/01541_max_memory_usage_for_user.reference b/tests/queries/0_stateless/01541_max_memory_usage_for_user_long.reference
similarity index 100%
rename from tests/queries/0_stateless/01541_max_memory_usage_for_user.reference
rename to tests/queries/0_stateless/01541_max_memory_usage_for_user_long.reference
diff --git a/tests/queries/0_stateless/01541_max_memory_usage_for_user.sh b/tests/queries/0_stateless/01541_max_memory_usage_for_user_long.sh
similarity index 94%
rename from tests/queries/0_stateless/01541_max_memory_usage_for_user.sh
rename to tests/queries/0_stateless/01541_max_memory_usage_for_user_long.sh
index c81bd1a6ce4b..32877bfd0fe6 100755
--- a/tests/queries/0_stateless/01541_max_memory_usage_for_user.sh
+++ b/tests/queries/0_stateless/01541_max_memory_usage_for_user_long.sh
@@ -66,4 +66,7 @@ echo 'OK'
 
 ${CLICKHOUSE_CLIENT} --query "DROP USER test_01541";
 
+# Reset max_memory_usage_for_user, so it will not affect other tests
+${CLICKHOUSE_CLIENT} --max_memory_usage_for_user=0 -q "SELECT 1 FORMAT Null"
+
 exit 0
diff --git a/tests/queries/skip_list.json b/tests/queries/skip_list.json
index fdb845b7e723..77c4d4870822 100644
--- a/tests/queries/skip_list.json
+++ b/tests/queries/skip_list.json
@@ -102,6 +102,158 @@
         "00510_materizlized_view_and_deduplication_zookeeper",
         "00738_lock_for_inner_table"
     ],
+    "database-replicated": [
+        /// Tests with DETACH TABLE (it's not allowed)
+        /// and tests with SET (session and query settings are not supported)
+        "memory_tracking",
+        "memory_usage",
+        "live_view",
+        "00825_protobuf_format_map",
+        "00152_insert_different_granularity",
+        "01715_background_checker_blather_zookeeper",
+        "01714_alter_drop_version",
+        "01114_materialize_clear_index_compact_parts",
+        "00814_replicated_minimalistic_part_header_zookeeper",
+        "01188_attach_table_from_pat",
+        "01415_sticking_mutations",
+        "01130_in_memory_parts",
+        "01110_dictionary_layout_without_arguments",
+        "01018_ddl_dictionaries_create",
+        "01018_ddl_dictionaries_select",
+        "01414_freeze_does_not_prevent_alters",
+        "01018_ddl_dictionaries_bad_queries",
+        "01686_rocksdb",
+        "01550_mutation_subquery",
+        "01070_mutations_with_dependencies",
+        "01070_materialize_ttl",
+        "01055_compact_parts",
+        "01017_mutations_with_nondeterministic_functions_zookeeper",
+        "00926_adaptive_index_granularity_pk",
+        "00910_zookeeper_test_alter_compression_codecs",
+        "00908_bloom_filter_index",
+        "00616_final_single_part",
+        "00446_clear_column_in_partition_zookeeper",
+        "01533_multiple_nested",
+        "01213_alter_rename_column_zookeeper",
+        "01575_disable_detach_table_of_dictionary",
+        "01457_create_as_table_function_structure",
+        "01415_inconsistent_merge_tree_settings",
+        "01413_allow_non_metadata_alters",
+        "01378_alter_rename_with_ttl_zookeeper",
+        "01349_mutation_datetime_key",
+        "01325_freeze_mutation_stuck",
+        "01272_suspicious_codecs",
+        "01181_db_atomic_drop_on_cluster",
+        "00957_delta_diff_bug",
+        "00910_zookeeper_custom_compression_codecs_replicated",
+        "00899_long_attach_memory_limit",
+        "00804_test_custom_compression_codes_log_storages",
+        "00804_test_alter_compression_codecs",
+        "00804_test_delta_codec_no_type_alter",
+        "00804_test_custom_compression_codecs",
+        "00753_alter_attach",
+        "00715_fetch_merged_or_mutated_part_zookeeper",
+        "00688_low_cardinality_serialization",
+        "01575_disable_detach_table_of_dictionary",
+        "00738_lock_for_inner_table",
+        "01666_blns",
+        "01652_ignore_and_low_cardinality",
+        "01651_map_functions",
+        "01650_fetch_patition_with_macro_in_zk_path",
+        "01648_mutations_and_escaping",
+        "01640_marks_corruption_regression",
+        "01622_byte_size",
+        "01611_string_to_low_cardinality_key_alter",
+        "01602_show_create_view",
+        "01600_log_queries_with_extensive_info",
+        "01560_ttl_remove_empty_parts",
+        "01554_bloom_filter_index_big_integer_uuid",
+        "01550_type_map_formats_input",
+        "01550_type_map_formats",
+        "01550_create_map_type",
+        "01532_primary_key_without_order_by_zookeeper",
+        "01511_alter_version_versioned_collapsing_merge_tree_zookeeper",
+        "01509_parallel_quorum_insert_no_replicas",
+        "01504_compression_multiple_streams",
+        "01494_storage_join_persistency",
+        "01493_storage_set_persistency",
+        "01493_alter_remove_properties_zookeeper",
+        "01475_read_subcolumns_storages",
+        "01475_read_subcolumns",
+        "01451_replicated_detach_drop_part",
+        "01451_detach_drop_part",
+        "01440_big_int_exotic_casts",
+        "01430_modify_sample_by_zookeeper",
+        "01417_freeze_partition_verbose_zookeeper",
+        "01417_freeze_partition_verbose",
+        "01396_inactive_replica_cleanup_nodes_zookeeper",
+        "01375_compact_parts_codecs",
+        "01357_version_collapsing_attach_detach_zookeeper",
+        "01355_alter_column_with_order",
+        "01291_geo_types",
+        "01270_optimize_skip_unused_shards_low_cardinality",
+        "01182_materialized_view_different_structure",
+        "01150_ddl_guard_rwr",
+        "01148_zookeeper_path_macros_unfolding",
+        "01135_default_and_alter_zookeeper",
+        "01130_in_memory_parts_partitons",
+        "01127_month_partitioning_consistency_select",
+        "01114_database_atomic",
+        "01083_expressions_in_engine_arguments",
+        "01073_attach_if_not_exists",
+        "01072_optimize_skip_unused_shards_const_expr_eval",
+        "01071_prohibition_secondary_index_with_old_format_merge_tree",
+        "01062_alter_on_mutataion_zookeeper",
+        "01060_shutdown_table_after_detach",
+        "01056_create_table_as",
+        "01035_avg",
+        "01021_only_tuple_columns",
+        "01019_alter_materialized_view_query",
+        "01019_alter_materialized_view_consistent",
+        "01019_alter_materialized_view_atomic",
+        "01015_attach_part",
+        "00989_parallel_parts_loading",
+        "00980_zookeeper_merge_tree_alter_settings",
+        "00980_merge_alter_settings",
+        "00955_test_final_mark",
+        "00933_reserved_word",
+        "00926_zookeeper_adaptive_index_granularity_replicated_merge_tree",
+        "00926_adaptive_index_granularity_replacing_merge_tree",
+        "00926_adaptive_index_granularity_merge_tree",
+        "00925_zookeeper_empty_replicated_merge_tree_optimize_final",
+        "00800_low_cardinality_distinct_numeric",
+        "00754_alter_modify_order_by_replicated_zookeeper",
+        "00751_low_cardinality_nullable_group_by",
+        "00751_default_databasename_for_view",
+        "00719_parallel_ddl_table",
+        "00718_low_cardinaliry_alter",
+        "00717_low_cardinaliry_distributed_group_by",
+        "00688_low_cardinality_syntax",
+        "00688_low_cardinality_nullable_cast",
+        "00688_low_cardinality_in",
+        "00652_replicated_mutations_zookeeper",
+        "00634_rename_view",
+        "00626_replace_partition_from_table",
+        "00625_arrays_in_nested",
+        "00623_replicated_truncate_table_zookeeper",
+        "00619_union_highlite",
+        "00599_create_view_with_subquery",
+        "00571_non_exist_database_when_create_materializ_view",
+        "00553_buff_exists_materlized_column",
+        "00516_deduplication_after_drop_partition_zookeeper",
+        "00508_materialized_view_to",
+        "00446_clear_column_in_partition_concurrent_zookeeper",
+        "00423_storage_log_single_thread",
+        "00311_array_primary_key",
+        "00236_replicated_drop_on_non_leader_zookeeper",
+        "00226_zookeeper_deduplication_and_unexpected_parts",
+        "00215_primary_key_order_zookeeper",
+        "00180_attach_materialized_view",
+        "00121_drop_column_zookeeper",
+        "00116_storage_set",
+        "00083_create_merge_tree_zookeeper",
+        "00062_replicated_merge_tree_alter_zookeeper"
+    ],
     "polymorphic-parts": [
         "01508_partition_pruning_long", /// bug, shoud be fixed
         "01482_move_to_prewhere_and_cast" /// bug, shoud be fixed
@@ -158,6 +310,7 @@
         "01015_attach_part",
         "01015_database_bad_tables",
         "01017_uniqCombined_memory_usage",
+        "01018_ddl_dictionaries_concurrent_requrests",  /// Cannot parse ATTACH DICTIONARY IF NOT EXISTS
         "01019_alter_materialized_view_atomic",
         "01019_alter_materialized_view_consistent",
         "01019_alter_materialized_view_query",
@@ -287,7 +440,7 @@
         "01530_drop_database_atomic_sync",
         "01532_execute_merges_on_single_replica",
         "01532_primary_key_without_order_by_zookeeper",
-        "01541_max_memory_usage_for_user",
+        "01541_max_memory_usage_for_user_long",
         "01551_mergetree_read_in_order_spread",
         "01552_dict_fixedstring",
         "01554_bloom_filter_index_big_integer_uuid",
@@ -564,7 +717,7 @@
         "01527_clickhouse_local_optimize",
         "01527_dist_sharding_key_dictGet_reload",
         "01530_drop_database_atomic_sync",
-        "01541_max_memory_usage_for_user",
+        "01541_max_memory_usage_for_user_long",
         "01542_dictionary_load_exception_race",
         "01575_disable_detach_table_of_dictionary",
         "01593_concurrent_alter_mutations_kill",
