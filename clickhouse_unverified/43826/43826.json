{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 43826,
  "instance_id": "ClickHouse__ClickHouse-43826",
  "issue_numbers": [
    "43800"
  ],
  "base_commit": "bca18298a14bbc311342d757878aea631f16f8df",
  "patch": "diff --git a/src/Storages/FileLog/ReadBufferFromFileLog.h b/src/Storages/FileLog/ReadBufferFromFileLog.h\nindex d581ead951e0..5991fe29b70e 100644\n--- a/src/Storages/FileLog/ReadBufferFromFileLog.h\n+++ b/src/Storages/FileLog/ReadBufferFromFileLog.h\n@@ -7,11 +7,6 @@\n #include <fstream>\n #include <mutex>\n \n-namespace Poco\n-{\n-    class Logger;\n-}\n-\n namespace DB\n {\n class ReadBufferFromFileLog : public ReadBuffer\ndiff --git a/src/Storages/FileLog/StorageFileLog.cpp b/src/Storages/FileLog/StorageFileLog.cpp\nindex 0f4563b6f35d..64b82eb4000b 100644\n--- a/src/Storages/FileLog/StorageFileLog.cpp\n+++ b/src/Storages/FileLog/StorageFileLog.cpp\n@@ -1,6 +1,7 @@\n #include <DataTypes/DataTypeLowCardinality.h>\n #include <DataTypes/DataTypeString.h>\n #include <DataTypes/DataTypesNumber.h>\n+#include <Disks/StoragePolicy.h>\n #include <IO/ReadBufferFromFile.h>\n #include <IO/ReadHelpers.h>\n #include <IO/WriteBufferFromFile.h>\n@@ -17,11 +18,11 @@\n #include <Storages/StorageFactory.h>\n #include <Storages/StorageMaterializedView.h>\n #include <Storages/checkAndGetLiteralArgument.h>\n-#include <Common/logger_useful.h>\n #include <Common/Exception.h>\n #include <Common/Macros.h>\n #include <Common/filesystemHelpers.h>\n #include <Common/getNumberOfPhysicalCPUCores.h>\n+#include <Common/logger_useful.h>\n \n #include <sys/stat.h>\n \n@@ -37,7 +38,6 @@ namespace ErrorCodes\n     extern const int CANNOT_READ_ALL_DATA;\n     extern const int LOGICAL_ERROR;\n     extern const int TABLE_METADATA_ALREADY_EXISTS;\n-    extern const int DIRECTORY_DOESNT_EXIST;\n     extern const int CANNOT_SELECT;\n     extern const int QUERY_NOT_ALLOWED;\n }\n@@ -64,6 +64,7 @@ StorageFileLog::StorageFileLog(\n     , metadata_base_path(std::filesystem::path(metadata_base_path_) / \"metadata\")\n     , format_name(format_name_)\n     , log(&Poco::Logger::get(\"StorageFileLog (\" + table_id_.table_name + \")\"))\n+    , disk(getContext()->getStoragePolicy(\"default\")->getDisks().at(0))\n     , milliseconds_to_wait(filelog_settings->poll_directory_watch_events_backoff_init.totalMilliseconds())\n {\n     StorageInMemoryMetadata storage_metadata;\n@@ -75,21 +76,14 @@ StorageFileLog::StorageFileLog(\n     {\n         if (!attach)\n         {\n-            std::error_code ec;\n-            std::filesystem::create_directories(metadata_base_path, ec);\n-\n-            if (ec)\n+            if (disk->exists(metadata_base_path))\n             {\n-                if (ec == std::make_error_code(std::errc::file_exists))\n-                {\n-                    throw Exception(ErrorCodes::TABLE_METADATA_ALREADY_EXISTS,\n-                        \"Metadata files already exist by path: {}, remove them manually if it is intended\",\n-                        metadata_base_path);\n-                }\n-                else\n-                    throw Exception(ErrorCodes::DIRECTORY_DOESNT_EXIST,\n-                        \"Could not create directory {}, reason: {}\", metadata_base_path, ec.message());\n+                throw Exception(\n+                    ErrorCodes::TABLE_METADATA_ALREADY_EXISTS,\n+                    \"Metadata files already exist by path: {}, remove them manually if it is intended\",\n+                    metadata_base_path);\n             }\n+            disk->createDirectories(metadata_base_path);\n         }\n \n         loadMetaFiles(attach);\n@@ -117,19 +111,8 @@ void StorageFileLog::loadMetaFiles(bool attach)\n     /// Attach table\n     if (attach)\n     {\n-        const auto & storage = getStorageID();\n-\n-        auto metadata_path_exist = std::filesystem::exists(metadata_base_path);\n-        auto previous_path = std::filesystem::path(getContext()->getPath()) / \".filelog_storage_metadata\" / storage.getDatabaseName() / storage.getTableName();\n-\n-        /// For compatibility with the previous path version.\n-        if (std::filesystem::exists(previous_path) && !metadata_path_exist)\n-        {\n-            std::filesystem::copy(previous_path, metadata_base_path, std::filesystem::copy_options::recursive);\n-            std::filesystem::remove_all(previous_path);\n-        }\n         /// Meta file may lost, log and create directory\n-        else if (!metadata_path_exist)\n+        if (!disk->exists(metadata_base_path))\n         {\n             /// Create metadata_base_path directory when store meta data\n             LOG_ERROR(log, \"Metadata files of table {} are lost.\", getStorageID().getTableName());\n@@ -189,7 +172,7 @@ void StorageFileLog::loadFiles()\n             /// data file have been renamed, need update meta file's name\n             if (it->second.file_name != file)\n             {\n-                std::filesystem::rename(getFullMetaPath(it->second.file_name), getFullMetaPath(file));\n+                disk->replaceFile(getFullMetaPath(it->second.file_name), getFullMetaPath(file));\n                 it->second.file_name = file;\n             }\n         }\n@@ -217,7 +200,7 @@ void StorageFileLog::loadFiles()\n                 valid_metas.emplace(inode, meta);\n             /// Delete meta file from filesystem\n             else\n-                std::filesystem::remove(getFullMetaPath(meta.file_name));\n+                disk->removeFileIfExists(getFullMetaPath(meta.file_name));\n         }\n         file_infos.meta_by_inode.swap(valid_metas);\n     }\n@@ -228,70 +211,71 @@ void StorageFileLog::serialize() const\n     for (const auto & [inode, meta] : file_infos.meta_by_inode)\n     {\n         auto full_name = getFullMetaPath(meta.file_name);\n-        if (!std::filesystem::exists(full_name))\n+        if (!disk->exists(full_name))\n         {\n-            FS::createFile(full_name);\n+            disk->createFile(full_name);\n         }\n         else\n         {\n             checkOffsetIsValid(full_name, meta.last_writen_position);\n         }\n-        WriteBufferFromFile out(full_name);\n-        writeIntText(inode, out);\n-        writeChar('\\n', out);\n-        writeIntText(meta.last_writen_position, out);\n+        auto out = disk->writeFile(full_name);\n+        writeIntText(inode, *out);\n+        writeChar('\\n', *out);\n+        writeIntText(meta.last_writen_position, *out);\n     }\n }\n \n void StorageFileLog::serialize(UInt64 inode, const FileMeta & file_meta) const\n {\n     auto full_name = getFullMetaPath(file_meta.file_name);\n-    if (!std::filesystem::exists(full_name))\n+    if (!disk->exists(full_name))\n     {\n-        FS::createFile(full_name);\n+        disk->createFile(full_name);\n     }\n     else\n     {\n         checkOffsetIsValid(full_name, file_meta.last_writen_position);\n     }\n-    WriteBufferFromFile out(full_name);\n-    writeIntText(inode, out);\n-    writeChar('\\n', out);\n-    writeIntText(file_meta.last_writen_position, out);\n+    auto out = disk->writeFile(full_name);\n+    writeIntText(inode, *out);\n+    writeChar('\\n', *out);\n+    writeIntText(file_meta.last_writen_position, *out);\n }\n \n void StorageFileLog::deserialize()\n {\n-    if (!std::filesystem::exists(metadata_base_path))\n+    if (!disk->exists(metadata_base_path))\n         return;\n     /// In case of single file (not a watched directory),\n     /// iterated directory always has one file inside.\n-    for (const auto & dir_entry : std::filesystem::directory_iterator{metadata_base_path})\n+    for (const auto dir_iter = disk->iterateDirectory(metadata_base_path); dir_iter->isValid(); dir_iter->next())\n     {\n-        if (!dir_entry.is_regular_file())\n+        auto full_name = getFullMetaPath(dir_iter->name());\n+        if (!disk->isFile(full_name))\n         {\n             throw Exception(\n                 ErrorCodes::BAD_FILE_TYPE,\n                 \"The file {} under {} is not a regular file when deserializing meta files\",\n-                dir_entry.path().c_str(),\n+                dir_iter->name(),\n                 metadata_base_path);\n         }\n \n-        ReadBufferFromFile in(dir_entry.path().c_str());\n+        auto in = disk->readFile(full_name);\n         FileMeta meta;\n         UInt64 inode, last_written_pos;\n \n-        if (!tryReadIntText(inode, in))\n+        if (!tryReadIntText(inode, *in))\n         {\n-            throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Read meta file {} failed\", dir_entry.path().c_str());\n+            throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Read meta file {} failed\", dir_iter->path());\n         }\n-        assertChar('\\n', in);\n-        if (!tryReadIntText(last_written_pos, in))\n+        assertChar('\\n', *in);\n+        if (!tryReadIntText(last_written_pos, *in))\n         {\n-            throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Read meta file {} failed\", dir_entry.path().c_str());\n+            throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Read meta file {} failed\", dir_iter->path());\n         }\n \n-        meta.file_name = dir_entry.path().filename();\n+        meta.file_name = dir_iter->name();\n         meta.last_writen_position = last_written_pos;\n \n         file_infos.meta_by_inode.emplace(inode, meta);\n@@ -506,17 +490,17 @@ void StorageFileLog::storeMetas(size_t start, size_t end)\n     }\n }\n \n-void StorageFileLog::checkOffsetIsValid(const String & full_name, UInt64 offset)\n+void StorageFileLog::checkOffsetIsValid(const String & full_name, UInt64 offset) const\n {\n-    ReadBufferFromFile in(full_name);\n+    auto in = disk->readFile(full_name);\n     UInt64 _, last_written_pos;\n \n-    if (!tryReadIntText(_, in))\n+    if (!tryReadIntText(_, *in))\n     {\n         throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Read meta file {} failed\", full_name);\n     }\n-    assertChar('\\n', in);\n-    if (!tryReadIntText(last_written_pos, in))\n+    assertChar('\\n', *in);\n+    if (!tryReadIntText(last_written_pos, *in))\n     {\n         throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Read meta file {} failed\", full_name);\n     }\ndiff --git a/src/Storages/FileLog/StorageFileLog.h b/src/Storages/FileLog/StorageFileLog.h\nindex 56f2d40ef5a2..9737c31acb63 100644\n--- a/src/Storages/FileLog/StorageFileLog.h\n+++ b/src/Storages/FileLog/StorageFileLog.h\n@@ -1,5 +1,7 @@\n #pragma once\n \n+#include <Disks/IDisk.h>\n+\n #include <Storages/FileLog/Buffer_fwd.h>\n #include <Storages/FileLog/FileLogDirectoryWatcher.h>\n #include <Storages/FileLog/FileLogSettings.h>\n@@ -147,6 +149,8 @@ class StorageFileLog final : public IStorage, WithContext\n     const String format_name;\n     Poco::Logger * log;\n \n+    DiskPtr disk;\n+\n     uint64_t milliseconds_to_wait;\n \n     /// In order to avoid data race, using a naive trick to forbid execute two select\n@@ -198,7 +202,7 @@ class StorageFileLog final : public IStorage, WithContext\n     void serialize(UInt64 inode, const FileMeta & file_meta) const;\n \n     void deserialize();\n-    static void checkOffsetIsValid(const String & full_name, UInt64 offset);\n+    void checkOffsetIsValid(const String & full_name, UInt64 offset) const;\n };\n \n }\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02022_storage_filelog_one_file.sh b/tests/queries/0_stateless/02022_storage_filelog_one_file.sh\nindex 2f47001eda97..2f43423e13e5 100755\n--- a/tests/queries/0_stateless/02022_storage_filelog_one_file.sh\n+++ b/tests/queries/0_stateless/02022_storage_filelog_one_file.sh\n@@ -20,23 +20,25 @@ done\n ${CLICKHOUSE_CLIENT} --query \"drop table if exists file_log;\"\n ${CLICKHOUSE_CLIENT} --query \"create table file_log(k UInt8, v UInt8) engine=FileLog('${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}.txt', 'CSV');\"\n \n-${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k settings stream_like_engine_allow_direct_select=1;\"\n \n for i in {100..120}\n do\n \techo $i, $i >> ${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}.txt\n done\n \n-${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k settings stream_like_engine_allow_direct_select=1;\"\n \n # touch does not change file content, no event\n touch ${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}.txt\n-${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k settings stream_like_engine_allow_direct_select=1;\"\n \n ${CLICKHOUSE_CLIENT} --query \"detach table file_log;\"\n ${CLICKHOUSE_CLIENT} --query \"attach table file_log;\"\n \n # should no records return\n-${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k settings stream_like_engine_allow_direct_select=1;\"\n+\n+${CLICKHOUSE_CLIENT} --query \"drop table file_log;\"\n \n rm -rf ${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}.txt\ndiff --git a/tests/queries/0_stateless/02023_storage_filelog.sh b/tests/queries/0_stateless/02023_storage_filelog.sh\nindex 7480e378d8b6..71ed5ba54714 100755\n--- a/tests/queries/0_stateless/02023_storage_filelog.sh\n+++ b/tests/queries/0_stateless/02023_storage_filelog.sh\n@@ -23,11 +23,11 @@ done\n ${CLICKHOUSE_CLIENT} --query \"drop table if exists file_log;\"\n ${CLICKHOUSE_CLIENT} --query \"create table file_log(k UInt8, v UInt8) engine=FileLog('${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/', 'CSV');\"\n \n-${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k settings stream_like_engine_allow_direct_select=1;\"\n \n cp ${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/a.txt ${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/b.txt\n \n-${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k settings stream_like_engine_allow_direct_select=1;\"\n \n for i in {100..120}\n do\n@@ -44,7 +44,7 @@ mv ${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/b.txt ${user_files_path}/${\n \n rm ${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/d.txt\n \n-${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k settings stream_like_engine_allow_direct_select=1;\"\n \n ${CLICKHOUSE_CLIENT} --query \"detach table file_log;\"\n cp ${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/e.txt ${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/f.txt\n@@ -60,18 +60,18 @@ do\n done\n ${CLICKHOUSE_CLIENT} --query \"attach table file_log;\"\n \n-${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k settings stream_like_engine_allow_direct_select=1;\"\n \n ${CLICKHOUSE_CLIENT} --query \"detach table file_log;\"\n ${CLICKHOUSE_CLIENT} --query \"attach table file_log;\"\n \n # should no records return\n-${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\"\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k settings stream_like_engine_allow_direct_select=1;\"\n \n truncate ${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/a.txt --size 0\n \n # exception happend\n-${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\" 2>&1 | grep -q \"Code: 33\" && echo 'OK' || echo 'FAIL'\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k settings stream_like_engine_allow_direct_select=1;\" 2>&1 | grep -q \"Code: 33\" && echo 'OK' || echo 'FAIL'\n \n ${CLICKHOUSE_CLIENT} --query \"drop table file_log;\"\n \ndiff --git a/tests/queries/0_stateless/02025_storage_filelog_virtual_col.sh b/tests/queries/0_stateless/02025_storage_filelog_virtual_col.sh\nindex f0faafe55d5c..e4041b2d7557 100755\n--- a/tests/queries/0_stateless/02025_storage_filelog_virtual_col.sh\n+++ b/tests/queries/0_stateless/02025_storage_filelog_virtual_col.sh\n@@ -24,11 +24,11 @@ done\n ${CLICKHOUSE_CLIENT} --query \"drop table if exists file_log;\"\n ${CLICKHOUSE_CLIENT} --query \"create table file_log(k UInt8, v UInt8) engine=FileLog('${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/', 'CSV');\"\n \n-${CLICKHOUSE_CLIENT} --query \"select *, _filename, _offset from file_log order by  _filename, _offset;\"\n+${CLICKHOUSE_CLIENT} --query \"select *, _filename, _offset from file_log order by  _filename, _offset settings stream_like_engine_allow_direct_select=1;\"\n \n cp ${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/a.txt ${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/b.txt\n \n-${CLICKHOUSE_CLIENT} --query \"select *, _filename, _offset from file_log order by  _filename, _offset;\"\n+${CLICKHOUSE_CLIENT} --query \"select *, _filename, _offset from file_log order by  _filename, _offset settings stream_like_engine_allow_direct_select=1;\"\n \n for i in {100..120}\n do\n@@ -44,18 +44,18 @@ cp ${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/a.txt ${user_files_path}/${\n \n rm ${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/d.txt\n \n-${CLICKHOUSE_CLIENT} --query \"select *, _filename, _offset from file_log order by  _filename, _offset;\"\n+${CLICKHOUSE_CLIENT} --query \"select *, _filename, _offset from file_log order by  _filename, _offset settings stream_like_engine_allow_direct_select=1;\"\n \n ${CLICKHOUSE_CLIENT} --query \"detach table file_log;\"\n ${CLICKHOUSE_CLIENT} --query \"attach table file_log;\"\n \n # should no records return\n-${CLICKHOUSE_CLIENT} --query \"select *, _filename, _offset from file_log order by  _filename, _offset;\"\n+${CLICKHOUSE_CLIENT} --query \"select *, _filename, _offset from file_log order by  _filename, _offset settings stream_like_engine_allow_direct_select=1;\"\n \n truncate ${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/a.txt --size 0\n \n # exception happend\n-${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k;\" 2>&1 | grep -q \"Code: 33\" && echo 'OK' || echo 'FAIL'\n+${CLICKHOUSE_CLIENT} --query \"select * from file_log order by k settings stream_like_engine_allow_direct_select=1;\" 2>&1 | grep -q \"Code: 33\" && echo 'OK' || echo 'FAIL'\n \n ${CLICKHOUSE_CLIENT} --query \"drop table file_log;\"\n \ndiff --git a/tests/queries/0_stateless/02026_storage_filelog_largefile.sh b/tests/queries/0_stateless/02026_storage_filelog_largefile.sh\nindex c28d20c9e8af..41a9d82949c7 100755\n--- a/tests/queries/0_stateless/02026_storage_filelog_largefile.sh\n+++ b/tests/queries/0_stateless/02026_storage_filelog_largefile.sh\n@@ -26,14 +26,14 @@ done\n ${CLICKHOUSE_CLIENT} --query \"drop table if exists file_log;\"\n ${CLICKHOUSE_CLIENT} --query \"create table file_log(k UInt32, v UInt32) engine=FileLog('${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/', 'CSV');\"\n \n-${CLICKHOUSE_CLIENT} --query \"select count() from file_log \"\n+${CLICKHOUSE_CLIENT} --query \"select count() from file_log settings stream_like_engine_allow_direct_select=1;\"\n \n for i in {11..20}\n do\n \t${CLICKHOUSE_CLIENT} --query \"insert into function file('${user_files_path}/${CLICKHOUSE_TEST_UNIQUE_NAME}/test$i.csv', 'CSV', 'k UInt32, v UInt32') select number, number from numbers(10000);\"\n done\n \n-${CLICKHOUSE_CLIENT} --query \"select count() from file_log \"\n+${CLICKHOUSE_CLIENT} --query \"select count() from file_log settings stream_like_engine_allow_direct_select=1;\"\n \n ${CLICKHOUSE_CLIENT} --query \"drop table file_log;\"\n \n",
  "problem_statement": "BC check: `StorageFileLog`: Read meta file failed\n```\r\n2022.11.28 23:39:57.139031 [ 295613 ] {} <Error> DB::StorageFileLog::StorageFileLog(const DB::StorageID &, DB::ContextPtr, const DB::ColumnsDescription &, const DB::String &, const DB::String &, const DB::String &, std::unique_ptr<FileLogSettings>, const DB::String &, bool): Code: 33. DB::Exception: Read meta file data/test_13/file_log/metadata/test17.csv failed. (CANNOT_READ_ALL_DATA), Stack trace (when copying this message, always include the lines below):\r\n```\r\nhttps://s3.amazonaws.com/clickhouse-test-reports/43675/0342e6b2361b0f3c78af2ad7a5238d63c69add16/stress_test__asan_.html\r\n\r\ncc: @ucasfl, @kssenii \r\n\n",
  "hints_text": "Currently, all metadata related operation is based on local filesystem, but now we store metadata on `data` directory, so if the table use non-local disk to store data like S3, the metadata related operation will failed.",
  "created_at": "2022-11-30T10:44:54Z",
  "modified_files": [
    "src/Storages/FileLog/ReadBufferFromFileLog.h",
    "src/Storages/FileLog/StorageFileLog.cpp",
    "src/Storages/FileLog/StorageFileLog.h"
  ],
  "modified_test_files": [
    "tests/queries/0_stateless/02022_storage_filelog_one_file.sh",
    "tests/queries/0_stateless/02023_storage_filelog.sh",
    "tests/queries/0_stateless/02025_storage_filelog_virtual_col.sh",
    "tests/queries/0_stateless/02026_storage_filelog_largefile.sh"
  ]
}