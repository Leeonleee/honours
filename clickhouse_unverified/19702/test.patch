diff --git a/tests/integration/test_broken_part_during_merge/__init__.py b/tests/integration/test_broken_part_during_merge/__init__.py
new file mode 100644
index 000000000000..e5a0d9b4834e
--- /dev/null
+++ b/tests/integration/test_broken_part_during_merge/__init__.py
@@ -0,0 +1,1 @@
+#!/usr/bin/env python3
diff --git a/tests/integration/test_broken_part_during_merge/test.py b/tests/integration/test_broken_part_during_merge/test.py
new file mode 100644
index 000000000000..33719166f4ac
--- /dev/null
+++ b/tests/integration/test_broken_part_during_merge/test.py
@@ -0,0 +1,61 @@
+import pytest
+
+from helpers.cluster import ClickHouseCluster
+from multiprocessing.dummy import Pool
+from helpers.network import PartitionManager
+import time
+
+cluster = ClickHouseCluster(__file__)
+
+node1 = cluster.add_instance('node1', with_zookeeper=True)
+
+
+@pytest.fixture(scope="module")
+def started_cluster():
+    try:
+        cluster.start()
+
+        node1.query('''
+            CREATE TABLE replicated_mt(date Date, id UInt32, value Int32)
+            ENGINE = ReplicatedMergeTree('/clickhouse/tables/replicated_mt', '{replica}') ORDER BY id;
+                '''.format(replica=node1.name))
+
+        yield cluster
+
+    finally:
+        cluster.shutdown()
+
+def corrupt_data_part_on_disk(node, table, part_name):
+    part_path = node.query(
+        "SELECT path FROM system.parts WHERE table = '{}' and name = '{}'".format(table, part_name)).strip()
+    node.exec_in_container(['bash', '-c',
+                            'cd {p} && ls *.bin | head -n 1 | xargs -I{{}} sh -c \'echo "1" >> $1\' -- {{}}'.format(
+                                p=part_path)], privileged=True)
+
+
+def test_merge_and_part_corruption(started_cluster):
+    node1.query("SYSTEM STOP REPLICATION QUEUES replicated_mt")
+    for i in range(4):
+        node1.query("INSERT INTO replicated_mt SELECT toDate('2019-10-01'), number, number * number FROM numbers ({f}, 100000)".format(f=i*100000))
+
+    assert node1.query("SELECT COUNT() FROM system.parts WHERE table='replicated_mt' AND active=1") == "4
"
+
+    # Need to corrupt "border part" (left or right). If we will corrupt something in the middle
+    # clickhouse will not consider merge as broken, because we have parts with the same min and max
+    # block numbers.
+    corrupt_data_part_on_disk(node1, 'replicated_mt', 'all_3_3_0')
+
+    with Pool(1) as p:
+        def optimize_with_delay(x):
+            node1.query("OPTIMIZE TABLE replicated_mt FINAL", timeout=30)
+
+        # corrupt part after merge already assigned, but not started
+        res_opt = p.apply_async(optimize_with_delay, (1,))
+        node1.query("CHECK TABLE replicated_mt", settings={"check_query_single_value_result": 0})
+        # start merge
+        node1.query("SYSTEM START REPLICATION QUEUES replicated_mt")
+        res_opt.get()
+
+        # will hung if checked bug not fixed
+        node1.query("ALTER TABLE replicated_mt UPDATE value = 7 WHERE 1", settings={"mutations_sync": 2}, timeout=30)
+        assert node1.query("SELECT sum(value) FROM replicated_mt") == "2100000
"
diff --git a/tests/integration/test_check_table/test.py b/tests/integration/test_check_table/test.py
index 916b2ead7f73..d204f6c58104 100644
--- a/tests/integration/test_check_table/test.py
+++ b/tests/integration/test_check_table/test.py
@@ -13,18 +13,6 @@ def started_cluster():
     try:
         cluster.start()
 
-        for node in [node1, node2]:
-            node.query('''
-            CREATE TABLE replicated_mt(date Date, id UInt32, value Int32)
-            ENGINE = ReplicatedMergeTree('/clickhouse/tables/replicated_mt', '{replica}') PARTITION BY toYYYYMM(date) ORDER BY id;
-                '''.format(replica=node.name))
-
-        node1.query('''
-            CREATE TABLE non_replicated_mt(date Date, id UInt32, value Int32)
-            ENGINE = MergeTree() PARTITION BY toYYYYMM(date) ORDER BY id
-            SETTINGS min_bytes_for_wide_part=0;
-        ''')
-
         yield cluster
 
     finally:
@@ -54,6 +42,14 @@ def remove_part_from_disk(node, table, part_name):
 
 
 def test_check_normal_table_corruption(started_cluster):
+    node1.query("DROP TABLE IF EXISTS non_replicated_mt")
+
+    node1.query('''
+        CREATE TABLE non_replicated_mt(date Date, id UInt32, value Int32)
+        ENGINE = MergeTree() PARTITION BY toYYYYMM(date) ORDER BY id
+        SETTINGS min_bytes_for_wide_part=0;
+    ''')
+
     node1.query("INSERT INTO non_replicated_mt VALUES (toDate('2019-02-01'), 1, 10), (toDate('2019-02-01'), 2, 12)")
     assert node1.query("CHECK TABLE non_replicated_mt PARTITION 201902",
                        settings={"check_query_single_value_result": 0}) == "201902_1_1_0\t1\t
"
@@ -94,8 +90,14 @@ def test_check_normal_table_corruption(started_cluster):
 
 
 def test_check_replicated_table_simple(started_cluster):
-    node1.query("TRUNCATE TABLE replicated_mt")
-    node2.query("SYSTEM SYNC REPLICA replicated_mt")
+    for node in [node1, node2]:
+        node.query("DROP TABLE IF EXISTS replicated_mt")
+
+        node.query('''
+        CREATE TABLE replicated_mt(date Date, id UInt32, value Int32)
+        ENGINE = ReplicatedMergeTree('/clickhouse/tables/replicated_mt', '{replica}') PARTITION BY toYYYYMM(date) ORDER BY id;
+            '''.format(replica=node.name))
+
     node1.query("INSERT INTO replicated_mt VALUES (toDate('2019-02-01'), 1, 10), (toDate('2019-02-01'), 2, 12)")
     node2.query("SYSTEM SYNC REPLICA replicated_mt")
 
@@ -119,34 +121,40 @@ def test_check_replicated_table_simple(started_cluster):
 
 
 def test_check_replicated_table_corruption(started_cluster):
-    node1.query("TRUNCATE TABLE replicated_mt")
-    node2.query("SYSTEM SYNC REPLICA replicated_mt")
-    node1.query("INSERT INTO replicated_mt VALUES (toDate('2019-02-01'), 1, 10), (toDate('2019-02-01'), 2, 12)")
-    node1.query("INSERT INTO replicated_mt VALUES (toDate('2019-01-02'), 3, 10), (toDate('2019-01-02'), 4, 12)")
-    node2.query("SYSTEM SYNC REPLICA replicated_mt")
+    for node in [node1, node2]:
+        node.query("DROP TABLE IF EXISTS replicated_mt_1")
 
-    assert node1.query("SELECT count() from replicated_mt") == "4
"
-    assert node2.query("SELECT count() from replicated_mt") == "4
"
+        node.query('''
+        CREATE TABLE replicated_mt_1(date Date, id UInt32, value Int32)
+        ENGINE = ReplicatedMergeTree('/clickhouse/tables/replicated_mt_1', '{replica}') PARTITION BY toYYYYMM(date) ORDER BY id;
+            '''.format(replica=node.name))
+
+    node1.query("INSERT INTO replicated_mt_1 VALUES (toDate('2019-02-01'), 1, 10), (toDate('2019-02-01'), 2, 12)")
+    node1.query("INSERT INTO replicated_mt_1 VALUES (toDate('2019-01-02'), 3, 10), (toDate('2019-01-02'), 4, 12)")
+    node2.query("SYSTEM SYNC REPLICA replicated_mt_1")
+
+    assert node1.query("SELECT count() from replicated_mt_1") == "4
"
+    assert node2.query("SELECT count() from replicated_mt_1") == "4
"
 
     part_name = node1.query(
-        "SELECT name from system.parts where table = 'replicated_mt' and partition_id = '201901' and active = 1").strip()
+        "SELECT name from system.parts where table = 'replicated_mt_1' and partition_id = '201901' and active = 1").strip()
 
-    corrupt_data_part_on_disk(node1, "replicated_mt", part_name)
-    assert node1.query("CHECK TABLE replicated_mt PARTITION 201901", settings={
-        "check_query_single_value_result": 0}) == "{p}\t0\tPart {p} looks broken. Removing it and queueing a fetch.
".format(
+    corrupt_data_part_on_disk(node1, "replicated_mt_1", part_name)
+    assert node1.query("CHECK TABLE replicated_mt_1 PARTITION 201901", settings={
+        "check_query_single_value_result": 0}) == "{p}\t0\tPart {p} looks broken. Removing it and will try to fetch.
".format(
         p=part_name)
 
-    node1.query("SYSTEM SYNC REPLICA replicated_mt")
-    assert node1.query("CHECK TABLE replicated_mt PARTITION 201901",
+    node1.query("SYSTEM SYNC REPLICA replicated_mt_1")
+    assert node1.query("CHECK TABLE replicated_mt_1 PARTITION 201901",
                        settings={"check_query_single_value_result": 0}) == "{}\t1\t
".format(part_name)
-    assert node1.query("SELECT count() from replicated_mt") == "4
"
+    assert node1.query("SELECT count() from replicated_mt_1") == "4
"
 
-    remove_part_from_disk(node2, "replicated_mt", part_name)
-    assert node2.query("CHECK TABLE replicated_mt PARTITION 201901", settings={
-        "check_query_single_value_result": 0}) == "{p}\t0\tPart {p} looks broken. Removing it and queueing a fetch.
".format(
+    remove_part_from_disk(node2, "replicated_mt_1", part_name)
+    assert node2.query("CHECK TABLE replicated_mt_1 PARTITION 201901", settings={
+        "check_query_single_value_result": 0}) == "{p}\t0\tPart {p} looks broken. Removing it and will try to fetch.
".format(
         p=part_name)
 
-    node1.query("SYSTEM SYNC REPLICA replicated_mt")
-    assert node1.query("CHECK TABLE replicated_mt PARTITION 201901",
+    node1.query("SYSTEM SYNC REPLICA replicated_mt_1")
+    assert node1.query("CHECK TABLE replicated_mt_1 PARTITION 201901",
                        settings={"check_query_single_value_result": 0}) == "{}\t1\t
".format(part_name)
-    assert node1.query("SELECT count() from replicated_mt") == "4
"
+    assert node1.query("SELECT count() from replicated_mt_1") == "4
"
diff --git a/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill.sh b/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill.sh
index 7f111538a063..6ae103bdf6e3 100755
--- a/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill.sh
+++ b/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill.sh
@@ -28,7 +28,7 @@ function kill_mutation_thread
         # find any mutation and kill it
         mutation_id=$($CLICKHOUSE_CLIENT --query "SELECT mutation_id FROM system.mutations WHERE is_done=0 and database='${CLICKHOUSE_DATABASE}' and table='concurrent_mutate_kill' LIMIT 1")
         if [ ! -z "$mutation_id" ]; then
-            $CLICKHOUSE_CLIENT --query "KILL MUTATION WHERE mutation_id='$mutation_id'" 1> /dev/null
+            $CLICKHOUSE_CLIENT --query "KILL MUTATION WHERE mutation_id='$mutation_id' and table='concurrent_mutate_kill' and database='${CLICKHOUSE_DATABASE}'" 1> /dev/null
             sleep 1
         fi
     done
diff --git a/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill_many_replicas.sh b/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill_many_replicas.sh
index 60e2adb4204d..bfa68328c065 100755
--- a/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill_many_replicas.sh
+++ b/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill_many_replicas.sh
@@ -40,7 +40,7 @@ function kill_mutation_thread
         # find any mutation and kill it
         mutation_id=$($CLICKHOUSE_CLIENT --query "SELECT mutation_id FROM system.mutations WHERE is_done = 0 and table like 'concurrent_kill_%' and database='${CLICKHOUSE_DATABASE}' LIMIT 1")
         if [ ! -z "$mutation_id" ]; then
-            $CLICKHOUSE_CLIENT --query "KILL MUTATION WHERE mutation_id='$mutation_id'" 1> /dev/null
+            $CLICKHOUSE_CLIENT --query "KILL MUTATION WHERE mutation_id='$mutation_id' and table like 'concurrent_kill_%' and database='${CLICKHOUSE_DATABASE}'" 1> /dev/null
             sleep 1
         fi
     done
diff --git a/tests/queries/skip_list.json b/tests/queries/skip_list.json
index 96da47ee9c2c..b25b96b6af98 100644
--- a/tests/queries/skip_list.json
+++ b/tests/queries/skip_list.json
@@ -557,6 +557,8 @@
         "01541_max_memory_usage_for_user",
         "01542_dictionary_load_exception_race",
         "01575_disable_detach_table_of_dictionary",
+        "01593_concurrent_alter_mutations_kill",
+        "01593_concurrent_alter_mutations_kill_many_replicas",
         "01600_count_of_parts_metrics", // tests global system metrics
         "01600_detach_permanently",
         "01600_log_queries_with_extensive_info",
