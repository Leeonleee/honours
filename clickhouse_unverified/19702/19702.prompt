You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
CH forgot about several parts
20.9.7.
During a merge a part was detached by ReplicatedMergeTreePartCheckThread as broken.
CH wasn't able to recover the part because the table only has one replica (later I attached the part back successfully though).
Because of that the NEW (not created yet) part got marked as 'lost forever' (unexpected behaviour), which lead to another unexpected behaviour when we needed to drop a column from the table.

The details:

A merge was scheduled:
```
2021.01.19 22:51:20.987159 [ 9711 ] {} <Trace> db.table: Created log entry /clickhouse/tables/14/table/log/log-0000198617 for merge 20210119_254_293_2
2021.01.19 22:51:20.988217 [ 9709 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000198617 - log-0000198617
2021.01.19 22:51:20.994938 [ 9709 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Pulled 1 entries to queue.
2021.01.19 22:51:20.995006 [ 9730 ] {} <Trace> db.table: Executing log entry to merge parts 20210119_254_261_1, 20210119_262_267_1, 20210119_268_273_1, 20210119_274_279_1, 20210119_280_285_1, 20210119_286_291_1, 20210119_292_292_0, 20210119_293_293_0 to 20210119_254_293_2
2021.01.19 22:51:20.995057 [ 9730 ] {} <Debug> DiskLocal: Reserving 8.71 GiB on disk `default`, having unreserved 5.82 TiB.
2021.01.19 22:51:20.995081 [ 9730 ] {} <Debug> db.table (MergerMutator): Merging 8 parts: from 20210119_254_261_1 to 20210119_293_293_0 into Wide
2021.01.19 22:51:20.995464 [ 9730 ] {} <Debug> db.table (MergerMutator): Selected MergeAlgorithm: Vertical
2021.01.19 22:51:21.011655 [ 9730 ] {} <Trace> MergeTreeSequentialSource: Reading 1069 marks from part 20210119_254_261_1, total 8752897 rows starting from the beginning of the part
```
It was reading data when another thread started checking a part:
```
2021.01.19 22:52:15.724000 [ 9730 ] {} <Trace> MergeTreeSequentialSource: Reading 1069 marks from part 20210119_254_261_1, total 8752897 rows starting from the beginning of the part, column floor

2021.01.19 22:52:16.211682 [ 9758 ] {} <Warning> db.table (ReplicatedMergeTreePartCheckThread): Checking part 20210119_254_261_1
2021.01.19 22:52:16.212705 [ 9758 ] {} <Warning> db.table (ReplicatedMergeTreePartCheckThread): Checking data of part 20210119_254_261_1.
```
The merging thread 9730 failed:
```
2021.01.19 22:52:16.406541 [ 9730 ] {} <Error> db.table: DB::StorageReplicatedMergeTree::queueTask()::<lambda(DB::StorageReplicatedMergeTree::LogEntryPtr&)>: Code: 40, e.displayText() = DB::Exception: Checksum doesn't match: corrupted data. Reference: aedfece83e71a6af9ab401e8c70fe7bd. Actual: 6db5d49e651b5b6ad63b9244e50db5ca. Size of compressed block: 11405: (while reading column floor_price): (while reading from part /var/lib/clickhouse/data/db/table/20210119_254_261_1/ from mark 938 with max_rows_to_read = 8192): While executing MergeTreeSequentialSource: Cannot fetch required block. Stream PipelineExecuting, part 0, Stack trace (when copying this message, always include the lines below)
```
The checking thread 9758 detached the part:
```
2021.01.19 22:52:17.780119 [ 9758 ] {} <Error> db.table (ReplicatedMergeTreePartCheckThread): Part 20210119_254_261_1 looks broken. Removing it and queueing a fetch.
2021.01.19 22:52:17.789749 [ 9758 ] {} <Information> db.table: Renaming 20210119_254_261_1 to broken20210119_254_261_1 and forgiving it.
2021.01.19 22:52:17.790296 [ 9758 ] {} <Trace> db.table (ReplicatedMergeTreePartCheckThread): Execution took 1578 ms.
```
Another thread started the merge again, but one part was missing, so the thread marked as lost the NEW part, 20210119_254_293_2 (as well as the detached part)
```
2021.01.19 22:52:27.473582 [ 9727 ] {} <Trace> db.table: Executing log entry to merge parts 20210119_254_261_1, 20210119_262_267_1, 20210119_268_273_1, 20210119_274_279_1, 20210119_280_285_1, 20210119_286_291_1, 20210119_292_292_0, 20210119_293_293_0 to 20210119_254_293_2
2021.01.19 22:52:27.473605 [ 9727 ] {} <Debug> db.table: Don't have all parts for merge 20210119_254_293_2; will try to fetch it instead
2021.01.19 22:52:27.474010 [ 9740 ] {} <Warning> db.table (ReplicatedMergeTreePartCheckThread): Checking part 20210119_254_293_2
2021.01.19 22:52:27.474032 [ 9727 ] {} <Information> db.table: DB::Exception: No active replica has part 20210119_254_293_2 or covering part
2021.01.19 22:52:27.474408 [ 9740 ] {} <Warning> db.table (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part covering 20210119_254_293_2.
2021.01.19 22:52:27.475311 [ 9740 ] {} <Error> db.table (ReplicatedMergeTreePartCheckThread): No replica has part covering 20210119_254_293_2 and a merge is impossible: we didn't find a smaller part with the same max block.
2021.01.19 22:52:27.481798 [ 9740 ] {} <Error> db.table (ReplicatedMergeTreePartCheckThread): Part 20210119_254_293_2 is lost forever.

2021.01.19 22:52:27.794690 [ 9740 ] {} <Warning> db.table (ReplicatedMergeTreePartCheckThread): Checking part 20210119_254_261_1
2021.01.19 22:52:27.794700 [ 9720 ] {} <Information> db.table: DB::Exception: No active replica has part 20210119_254_261_1 or covering part
2021.01.19 22:52:27.795130 [ 9740 ] {} <Warning> db.table (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part covering 20210119_254_261_1.
2021.01.19 22:52:27.796030 [ 9740 ] {} <Error> db.table (ReplicatedMergeTreePartCheckThread): No replica has part covering 20210119_254_261_1 and a merge is impossible: we didn't find smaller parts with either the same min block or the same max block.
2021.01.19 22:52:27.803382 [ 9740 ] {} <Error> db.table (ReplicatedMergeTreePartCheckThread): Part 20210119_254_261_1 is lost forever.
```
A few days later a mutation skipped the parts that were supposed to be merged with the 'broken' part (20210119_262_267_1, 20210119_268_273_1, 20210119_274_279_1, 20210119_280_285_1, 20210119_286_291_1, 20210119_292_292_0, 20210119_293_293_0):
```
2021.01.25 09:12:38.455858 [ 31330 ] {1c262294-4fe9-4e4b-acec-ca3c42a1b7ec} <Debug> executeQuery: (from [::1]:48234) alter table db.table drop column bba7dt
2021.01.25 09:12:39.874691 [ 9734 ] {} <Information> db.table (ReplicatedMergeTreeQueue): Loading 1 mutation entries: 0000000013 - 0000000013
2021.01.25 09:12:39.891032 [ 9734 ] {} <Trace> db.table (ReplicatedMergeTreeQueue): Adding mutation 0000000013 for partition 20210119 for all block numbers less than 480
...
2021.01.25 09:13:15.696975 [ 9734 ] {} <Trace> db.table: Created log entry for mutation 20210119_0_216_3_480
2021.01.25 09:13:15.734053 [ 9713 ] {} <Trace> db.table: Created log entry for mutation 20210119_217_253_2_480
!!!! skipped parts should have been here
2021.01.25 09:13:15.749085 [ 9711 ] {} <Trace> db.table: Created log entry for mutation 20210119_294_328_2_480
2021.01.25 09:13:15.760000 [ 9742 ] {} <Trace> db.table: Created log entry for mutation 20210119_329_363_2_480
...
2021.01.25 09:13:24.553450 [ 9740 ] {} <Trace> db.table: Created log entry for mutation 20210125_19_19_0_20
```
After that the mutation got stuck with parts_to_do=8.
Here is the list of parts in ZooKeeper at the time:
```
name                   | ctime              
-----------------------+--------------------
20210119_0_216_3_480   | 2021-01-25 09:13:16
20210119_217_253_2_480 | 2021-01-25 09:13:16
20210119_262_267_1     | 2021-01-19 17:50:13
20210119_268_273_1     | 2021-01-19 18:06:43
20210119_274_279_1     | 2021-01-19 18:17:58
20210119_280_285_1     | 2021-01-19 18:33:07
20210119_286_291_1     | 2021-01-19 18:50:02
20210119_292_292_0     | 2021-01-19 18:50:29
20210119_293_293_0     | 2021-01-19 18:51:15
20210119_294_328_2_480 | 2021-01-25 09:13:16
20210119_329_363_2_480 | 2021-01-25 09:13:16
```
After a few hours I restarted the table:
```
2021.01.25 14:28:15.850433 [ 1378 ] {2839b9af-2fa8-42bb-ab06-d806fb18644e} <Debug> executeQuery: (from [::1]:45158) system restart replica db.table
```
As a result the ALTER got interrupted.
```
2021.01.25 14:28:16.649570 [ 31330 ] {1c262294-4fe9-4e4b-acec-ca3c42a1b7ec} <Trace> db.table (ReplicatedMergeTreeRestartingThread): Restarting thread finished
2021.01.25 14:28:16.649593 [ 31330 ] {1c262294-4fe9-4e4b-acec-ca3c42a1b7ec} <Trace> db.table (ReplicatedMergeTreeRestartingThread): Waiting for threads to finish
2021.01.25 14:28:16.649598 [ 31330 ] {1c262294-4fe9-4e4b-acec-ca3c42a1b7ec} <Trace> db.table (ReplicatedMergeTreeRestartingThread): Threads finished
2021.01.25 14:28:16.710937 [ 31330 ] {1c262294-4fe9-4e4b-acec-ca3c42a1b7ec} <Error> executeQuery: Code: 341, e.displayText() = DB::Exception: Mutation is not finished because table shutdown was called. It will be done after table restart. (version 20.9.7.11 (off
icial build)) (from [::1]:48234) (in query: alter table db.table drop column bba7dt
```
The mutation found 7 parts. MergerMutator found them as well. They were merged then mutated.
```
2021.01.25 14:28:28.230124 [ 9734 ] {} <Trace> db.table (ReplicatedMergeTreeQueue): Mutation 0000000013 is not done yet because 7 parts to mutate suddenly appeared.
2021.01.25 14:28:28.233083 [ 9709 ] {} <Debug> db.table (MergerMutator): Selected 7 parts from 20210119_262_267_1 to 20210119_293_293_0
2021.01.25 14:28:28.239167 [ 9709 ] {} <Trace> db.table: Created log entry /clickhouse/tables/14/table/log/log-0000202318 for merge 20210119_262_293_2
2021.01.25 14:28:28.239294 [ 9757 ] {} <Trace> db.table (ReplicatedMergeTreeQueue): Mutation 0000000013 is not done yet because 7 parts to mutate suddenly appeared.
2021.01.25 14:28:28.241162 [ 9758 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000202318 - log-0000202318
2021.01.25 14:28:28.246171 [ 9758 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Pulled 1 entries to queue.
...
2021.01.25 14:39:17.698423 [ 9729 ] {} <Trace> db.table: Renaming temporary part tmp_merge_20210119_262_293_2 to 20210119_262_293_2.
2021.01.25 14:39:17.698466 [ 9729 ] {} <Trace> db.table (MergerMutator): Merged 7 parts: from 20210119_262_267_1 to 20210119_293_293_0
2021.01.25 14:39:17.715249 [ 9711 ] {} <Trace> db.table: Created log entry for mutation 20210119_262_293_2_480
2021.01.25 14:39:17.716625 [ 9732 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000202319 - log-0000202319
2021.01.25 14:39:17.720744 [ 9732 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Pulled 1 entries to queue.
2021.01.25 14:39:17.720815 [ 9725 ] {} <Trace> db.table: Executing log entry to mutate part 20210119_262_293_2 to 20210119_262_293_2_480
2021.01.25 14:39:17.720839 [ 9725 ] {} <Debug> DiskLocal: Reserving 6.83 GiB on disk `default`, having unreserved 5.88 TiB.
2021.01.25 14:39:17.721298 [ 9725 ] {} <Trace> db.table (MergerMutator): Mutating part 20210119_262_293_2 to mutation version 480
2021.01.25 14:39:17.738997 [ 9725 ] {} <Trace> db.table: Renaming temporary part tmp_mut_20210119_262_293_2_480 to 20210119_262_293_2_480.
2021.01.25 14:39:17.744894 [ 9740 ] {} <Trace> db.table (ReplicatedMergeTreeQueue): Will check if mutation 0000000013 is done
2021.01.25 14:39:17.744931 [ 9740 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Trying to finalize 1 mutations
2021.01.25 14:39:17.758855 [ 9740 ] {} <Trace> db.table (ReplicatedMergeTreeQueue): Mutation 0000000013 is done
2021.01.25 14:39:17.758878 [ 9740 ] {} <Trace> db.table (ReplicatedMergeTreeQueue): Finishing data alter with version 53 for entry 0000000013
```
Happy end :)
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
