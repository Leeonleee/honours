{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 19702,
  "instance_id": "ClickHouse__ClickHouse-19702",
  "issue_numbers": [
    "19593"
  ],
  "base_commit": "7249845f624203e599780bf5d808a1bc7d17c7ec",
  "patch": "diff --git a/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.cpp b/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.cpp\nindex 22cb5ed6e9c2..b2a144ca7486 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.cpp\n@@ -74,20 +74,9 @@ size_t ReplicatedMergeTreePartCheckThread::size() const\n }\n \n \n-void ReplicatedMergeTreePartCheckThread::searchForMissingPart(const String & part_name)\n+ReplicatedMergeTreePartCheckThread::MissingPartSearchResult ReplicatedMergeTreePartCheckThread::searchForMissingPartOnOtherReplicas(const String & part_name)\n {\n     auto zookeeper = storage.getZooKeeper();\n-    String part_path = storage.replica_path + \"/parts/\" + part_name;\n-\n-    /// If the part is in ZooKeeper, remove it from there and add the task to download it to the queue.\n-    if (zookeeper->exists(part_path))\n-    {\n-        LOG_WARNING(log, \"Part {} exists in ZooKeeper but not locally. Removing from ZooKeeper and queueing a fetch.\", part_name);\n-        ProfileEvents::increment(ProfileEvents::ReplicatedPartChecksFailed);\n-\n-        storage.removePartAndEnqueueFetch(part_name);\n-        return;\n-    }\n \n     /// If the part is not in ZooKeeper, we'll check if it's at least somewhere.\n     auto part_info = MergeTreePartInfo::fromPartName(part_name, storage.format_version);\n@@ -115,7 +104,7 @@ void ReplicatedMergeTreePartCheckThread::searchForMissingPart(const String & par\n         *   and don't delete the queue entry when in doubt.\n         */\n \n-    LOG_WARNING(log, \"Checking if anyone has a part covering {}.\", part_name);\n+    LOG_WARNING(log, \"Checking if anyone has a part {} or covering part.\", part_name);\n \n     bool found_part_with_the_same_min_block = false;\n     bool found_part_with_the_same_max_block = false;\n@@ -123,15 +112,27 @@ void ReplicatedMergeTreePartCheckThread::searchForMissingPart(const String & par\n     Strings replicas = zookeeper->getChildren(storage.zookeeper_path + \"/replicas\");\n     for (const String & replica : replicas)\n     {\n-        Strings parts = zookeeper->getChildren(storage.zookeeper_path + \"/replicas/\" + replica + \"/parts\");\n+        String replica_path = storage.zookeeper_path + \"/replicas/\" + replica;\n+\n+        Strings parts = zookeeper->getChildren(replica_path + \"/parts\");\n         for (const String & part_on_replica : parts)\n         {\n             auto part_on_replica_info = MergeTreePartInfo::fromPartName(part_on_replica, storage.format_version);\n \n+            if (part_info == part_on_replica_info)\n+            {\n+                /// Found missing part at ourself. If we are here then something wrong with this part, so skipping.\n+                if (replica_path == storage.replica_path)\n+                    continue;\n+\n+                LOG_WARNING(log, \"Found the missing part {} at {} on {}\", part_name, part_on_replica, replica);\n+                return MissingPartSearchResult::FoundAndNeedFetch;\n+            }\n+\n             if (part_on_replica_info.contains(part_info))\n             {\n                 LOG_WARNING(log, \"Found part {} on {} that covers the missing part {}\", part_on_replica, replica, part_name);\n-                return;\n+                return MissingPartSearchResult::FoundAndDontNeedFetch;\n             }\n \n             if (part_info.contains(part_on_replica_info))\n@@ -144,7 +145,7 @@ void ReplicatedMergeTreePartCheckThread::searchForMissingPart(const String & par\n                 if (found_part_with_the_same_min_block && found_part_with_the_same_max_block)\n                 {\n                     LOG_WARNING(log, \"Found parts with the same min block and with the same max block as the missing part {}. Hoping that it will eventually appear as a result of a merge.\", part_name);\n-                    return;\n+                    return MissingPartSearchResult::FoundAndDontNeedFetch;\n                 }\n             }\n         }\n@@ -160,28 +161,61 @@ void ReplicatedMergeTreePartCheckThread::searchForMissingPart(const String & par\n         not_found_msg = \"smaller parts with either the same min block or the same max block.\";\n     LOG_ERROR(log, \"No replica has part covering {} and a merge is impossible: we didn't find {}\", part_name, not_found_msg);\n \n-    ProfileEvents::increment(ProfileEvents::ReplicatedPartChecksFailed);\n+    return MissingPartSearchResult::LostForever;\n+}\n+\n+void ReplicatedMergeTreePartCheckThread::searchForMissingPartAndFetchIfPossible(const String & part_name, bool exists_in_zookeeper)\n+{\n+    auto zookeeper = storage.getZooKeeper();\n+    auto missing_part_search_result = searchForMissingPartOnOtherReplicas(part_name);\n \n-    /// Is it in the replication queue? If there is - delete, because the task can not be processed.\n-    if (!storage.queue.remove(zookeeper, part_name))\n+    /// If the part is in ZooKeeper, remove it from there and add the task to download it to the queue.\n+    if (exists_in_zookeeper)\n     {\n-        /// The part was not in our queue. Why did it happen?\n-        LOG_ERROR(log, \"Missing part {} is not in our queue.\", part_name);\n-        return;\n+        /// If part found on some other replica\n+        if (missing_part_search_result == MissingPartSearchResult::FoundAndNeedFetch)\n+        {\n+            LOG_WARNING(log, \"Part {} exists in ZooKeeper but not locally and found on other replica. Removing from ZooKeeper and queueing a fetch.\", part_name);\n+            storage.removePartAndEnqueueFetch(part_name);\n+        }\n+        else /// If we have covering part on other replica or part is lost forever we don't need to fetch anything\n+        {\n+            LOG_WARNING(log, \"Part {} exists in ZooKeeper but not locally and not found on other replica. Removing it from ZooKeeper.\", part_name);\n+            storage.removePartFromZooKeeper(part_name);\n+        }\n     }\n \n-    /** This situation is possible if on all the replicas where the part was, it deteriorated.\n-        * For example, a replica that has just written it has power turned off and the data has not been written from cache to disk.\n-        */\n-    LOG_ERROR(log, \"Part {} is lost forever.\", part_name);\n-    ProfileEvents::increment(ProfileEvents::ReplicatedDataLoss);\n-}\n+    ProfileEvents::increment(ProfileEvents::ReplicatedPartChecksFailed);\n \n+    if (missing_part_search_result == MissingPartSearchResult::LostForever)\n+    {\n+        /// Is it in the replication queue? If there is - delete, because the task can not be processed.\n+        if (!storage.queue.remove(zookeeper, part_name))\n+        {\n+            /// The part was not in our queue.\n+            LOG_WARNING(log, \"Missing part {} is not in our queue, this can happen rarely.\", part_name);\n+        }\n \n-CheckResult ReplicatedMergeTreePartCheckThread::checkPart(const String & part_name)\n+        /** This situation is possible if on all the replicas where the part was, it deteriorated.\n+            * For example, a replica that has just written it has power turned off and the data has not been written from cache to disk.\n+            */\n+        LOG_ERROR(log, \"Part {} is lost forever.\", part_name);\n+        ProfileEvents::increment(ProfileEvents::ReplicatedDataLoss);\n+    }\n+}\n+\n+std::pair<bool, MergeTreeDataPartPtr> ReplicatedMergeTreePartCheckThread::findLocalPart(const String & part_name)\n {\n-    LOG_WARNING(log, \"Checking part {}\", part_name);\n-    ProfileEvents::increment(ProfileEvents::ReplicatedPartChecks);\n+    auto zookeeper = storage.getZooKeeper();\n+    String part_path = storage.replica_path + \"/parts/\" + part_name;\n+\n+    /// It's important to check zookeeper first and after that check local storage,\n+    /// because our checks of local storage and zookeeper are not consistent.\n+    /// If part exists in zookeeper and doesn't exists in local storage definitely require\n+    /// to fetch this part. But if we check local storage first and than check zookeeper\n+    /// some background process can successfully commit part between this checks (both to the local stoarge and zookeeper),\n+    /// but checker thread will remove part from zookeeper and queue fetch.\n+    bool exists_in_zookeeper = zookeeper->exists(part_path);\n \n     /// If the part is still in the PreCommitted -> Committed transition, it is not lost\n     /// and there is no need to go searching for it on other replicas. To definitely find the needed part\n@@ -190,17 +224,27 @@ CheckResult ReplicatedMergeTreePartCheckThread::checkPart(const String & part_na\n     if (!part)\n         part = storage.getActiveContainingPart(part_name);\n \n+    return std::make_pair(exists_in_zookeeper, part);\n+}\n+\n+CheckResult ReplicatedMergeTreePartCheckThread::checkPart(const String & part_name)\n+{\n+    LOG_WARNING(log, \"Checking part {}\", part_name);\n+    ProfileEvents::increment(ProfileEvents::ReplicatedPartChecks);\n+\n+    auto [exists_in_zookeeper, part] = findLocalPart(part_name);\n+\n     /// We do not have this or a covering part.\n     if (!part)\n     {\n-        searchForMissingPart(part_name);\n+        searchForMissingPartAndFetchIfPossible(part_name, exists_in_zookeeper);\n         return {part_name, false, \"Part is missing, will search for it\"};\n     }\n+\n     /// We have this part, and it's active. We will check whether we need this part and whether it has the right data.\n-    else if (part->name == part_name)\n+    if (part->name == part_name)\n     {\n         auto zookeeper = storage.getZooKeeper();\n-\n         auto table_lock = storage.lockForShare(RWLockImpl::NO_QUERY, storage.getSettings()->lock_acquire_timeout_for_background_operations);\n \n         auto local_part_header = ReplicatedMergeTreePartHeader::fromColumnsAndChecksums(\n@@ -254,11 +298,11 @@ CheckResult ReplicatedMergeTreePartCheckThread::checkPart(const String & part_na\n \n                 tryLogCurrentException(log, __PRETTY_FUNCTION__);\n \n-                String message = \"Part \" + part_name + \" looks broken. Removing it and queueing a fetch.\";\n+                String message = \"Part \" + part_name + \" looks broken. Removing it and will try to fetch.\";\n                 LOG_ERROR(log, message);\n-                ProfileEvents::increment(ProfileEvents::ReplicatedPartChecksFailed);\n \n-                storage.removePartAndEnqueueFetch(part_name);\n+                /// Part is broken, let's try to find it and fetch.\n+                searchForMissingPartAndFetchIfPossible(part_name, exists_in_zookeeper);\n \n                 /// Delete part locally.\n                 storage.forgetPartAndMoveToDetached(part, \"broken\");\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.h b/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.h\nindex 4239d7a8051e..8257898fe3f1 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.h\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreePartCheckThread.h\n@@ -12,6 +12,7 @@\n #include <common/logger_useful.h>\n #include <Core/BackgroundSchedulePool.h>\n #include <Storages/CheckResults.h>\n+#include <Storages/MergeTree/IMergeTreeDataPart.h>\n \n namespace DB\n {\n@@ -73,7 +74,26 @@ class ReplicatedMergeTreePartCheckThread\n private:\n     void run();\n \n-    void searchForMissingPart(const String & part_name);\n+    /// Search for missing part and queue fetch if possible. Otherwise\n+    /// remove part from zookeeper and queue.\n+    void searchForMissingPartAndFetchIfPossible(const String & part_name, bool exists_in_zookeeper);\n+\n+    std::pair<bool, MergeTreeDataPartPtr> findLocalPart(const String & part_name);\n+\n+    enum MissingPartSearchResult\n+    {\n+        /// We found this part on other replica, let's fetch it.\n+        FoundAndNeedFetch,\n+        /// We found covering part or source part with same min and max block number\n+        /// don't need to fetch because we should do it during normal queue processing.\n+        FoundAndDontNeedFetch,\n+        /// Covering part not found anywhere and exact part_name doesn't found on other\n+        /// replicas.\n+        LostForever,\n+    };\n+\n+    /// Search for missing part on other replicas or covering part on all replicas (including our replica).\n+    MissingPartSearchResult searchForMissingPartOnOtherReplicas(const String & part_name);\n \n     StorageReplicatedMergeTree & storage;\n     String log_name;\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\nindex ab1254acc5f7..26a916d2356d 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\n@@ -420,13 +420,26 @@ bool ReplicatedMergeTreeQueue::remove(zkutil::ZooKeeperPtr zookeeper, const Stri\n     {\n         std::unique_lock lock(state_mutex);\n \n-        virtual_parts.remove(part_name);\n+        bool removed = virtual_parts.remove(part_name);\n \n         for (Queue::iterator it = queue.begin(); it != queue.end();)\n         {\n             if ((*it)->new_part_name == part_name)\n             {\n                 found = *it;\n+                if (removed)\n+                {\n+                    /// Preserve invariant `virtual_parts` = `current_parts` + `queue`.\n+                    /// We remove new_part from virtual parts and add all source parts\n+                    /// which present in current_parts.\n+                    for (const auto & source_part : found->source_parts)\n+                    {\n+                        auto part_in_current_parts = current_parts.getContainingPart(source_part);\n+                        if (part_in_current_parts == source_part)\n+                            virtual_parts.add(source_part);\n+                    }\n+                }\n+\n                 updateStateOnQueueEntryRemoval(\n                     found, /* is_successful = */ false,\n                     min_unprocessed_insert_time_changed, max_processed_insert_time_changed, lock);\n@@ -1010,7 +1023,7 @@ bool ReplicatedMergeTreeQueue::isNotCoveredByFuturePartsImpl(const String & log_\n     /// NOTE The above is redundant, but left for a more convenient message in the log.\n     auto result_part = MergeTreePartInfo::fromPartName(new_part_name, format_version);\n \n-    /// It can slow down when the size of `future_parts` is large. But it can not be large, since `BackgroundProcessingPool` is limited.\n+    /// It can slow down when the size of `future_parts` is large. But it can not be large, since background pool is limited.\n     for (const auto & future_part_elem : future_parts)\n     {\n         auto future_part = MergeTreePartInfo::fromPartName(future_part_elem.first, format_version);\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 4eb7d7ebccde..a6fd50c4c8e2 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -3013,6 +3013,21 @@ void StorageReplicatedMergeTree::removePartFromZooKeeper(const String & part_nam\n     ops.emplace_back(zkutil::makeRemoveRequest(part_path, -1));\n }\n \n+void StorageReplicatedMergeTree::removePartFromZooKeeper(const String & part_name)\n+{\n+    auto zookeeper = getZooKeeper();\n+    String part_path = replica_path + \"/parts/\" + part_name;\n+    Coordination::Stat stat;\n+\n+    /// Part doesn't exist, nothing to remove\n+    if (!zookeeper->exists(part_path, &stat))\n+        return;\n+\n+    Coordination::Requests ops;\n+\n+    removePartFromZooKeeper(part_name, ops, stat.numChildren > 0);\n+    zookeeper->multi(ops);\n+}\n \n void StorageReplicatedMergeTree::removePartAndEnqueueFetch(const String & part_name)\n {\ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex cf36cf82fc98..6db05294b635 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -381,6 +381,9 @@ class StorageReplicatedMergeTree final : public ext::shared_ptr_helper<StorageRe\n     /// Set has_children to true for \"old-style\" parts (those with /columns and /checksums child znodes).\n     void removePartFromZooKeeper(const String & part_name, Coordination::Requests & ops, bool has_children);\n \n+    /// Just removes part from ZooKeeper using previous method\n+    void removePartFromZooKeeper(const String & part_name);\n+\n     /// Quickly removes big set of parts from ZooKeeper (using async multi queries)\n     void removePartsFromZooKeeper(zkutil::ZooKeeperPtr & zookeeper, const Strings & part_names,\n                                   NameSet * parts_should_be_retried = nullptr);\n",
  "test_patch": "diff --git a/tests/integration/test_broken_part_during_merge/__init__.py b/tests/integration/test_broken_part_during_merge/__init__.py\nnew file mode 100644\nindex 000000000000..e5a0d9b4834e\n--- /dev/null\n+++ b/tests/integration/test_broken_part_during_merge/__init__.py\n@@ -0,0 +1,1 @@\n+#!/usr/bin/env python3\ndiff --git a/tests/integration/test_broken_part_during_merge/test.py b/tests/integration/test_broken_part_during_merge/test.py\nnew file mode 100644\nindex 000000000000..33719166f4ac\n--- /dev/null\n+++ b/tests/integration/test_broken_part_during_merge/test.py\n@@ -0,0 +1,61 @@\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+from multiprocessing.dummy import Pool\n+from helpers.network import PartitionManager\n+import time\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node1 = cluster.add_instance('node1', with_zookeeper=True)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+\n+        node1.query('''\n+            CREATE TABLE replicated_mt(date Date, id UInt32, value Int32)\n+            ENGINE = ReplicatedMergeTree('/clickhouse/tables/replicated_mt', '{replica}') ORDER BY id;\n+                '''.format(replica=node1.name))\n+\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+def corrupt_data_part_on_disk(node, table, part_name):\n+    part_path = node.query(\n+        \"SELECT path FROM system.parts WHERE table = '{}' and name = '{}'\".format(table, part_name)).strip()\n+    node.exec_in_container(['bash', '-c',\n+                            'cd {p} && ls *.bin | head -n 1 | xargs -I{{}} sh -c \\'echo \"1\" >> $1\\' -- {{}}'.format(\n+                                p=part_path)], privileged=True)\n+\n+\n+def test_merge_and_part_corruption(started_cluster):\n+    node1.query(\"SYSTEM STOP REPLICATION QUEUES replicated_mt\")\n+    for i in range(4):\n+        node1.query(\"INSERT INTO replicated_mt SELECT toDate('2019-10-01'), number, number * number FROM numbers ({f}, 100000)\".format(f=i*100000))\n+\n+    assert node1.query(\"SELECT COUNT() FROM system.parts WHERE table='replicated_mt' AND active=1\") == \"4\\n\"\n+\n+    # Need to corrupt \"border part\" (left or right). If we will corrupt something in the middle\n+    # clickhouse will not consider merge as broken, because we have parts with the same min and max\n+    # block numbers.\n+    corrupt_data_part_on_disk(node1, 'replicated_mt', 'all_3_3_0')\n+\n+    with Pool(1) as p:\n+        def optimize_with_delay(x):\n+            node1.query(\"OPTIMIZE TABLE replicated_mt FINAL\", timeout=30)\n+\n+        # corrupt part after merge already assigned, but not started\n+        res_opt = p.apply_async(optimize_with_delay, (1,))\n+        node1.query(\"CHECK TABLE replicated_mt\", settings={\"check_query_single_value_result\": 0})\n+        # start merge\n+        node1.query(\"SYSTEM START REPLICATION QUEUES replicated_mt\")\n+        res_opt.get()\n+\n+        # will hung if checked bug not fixed\n+        node1.query(\"ALTER TABLE replicated_mt UPDATE value = 7 WHERE 1\", settings={\"mutations_sync\": 2}, timeout=30)\n+        assert node1.query(\"SELECT sum(value) FROM replicated_mt\") == \"2100000\\n\"\ndiff --git a/tests/integration/test_check_table/test.py b/tests/integration/test_check_table/test.py\nindex 916b2ead7f73..d204f6c58104 100644\n--- a/tests/integration/test_check_table/test.py\n+++ b/tests/integration/test_check_table/test.py\n@@ -13,18 +13,6 @@ def started_cluster():\n     try:\n         cluster.start()\n \n-        for node in [node1, node2]:\n-            node.query('''\n-            CREATE TABLE replicated_mt(date Date, id UInt32, value Int32)\n-            ENGINE = ReplicatedMergeTree('/clickhouse/tables/replicated_mt', '{replica}') PARTITION BY toYYYYMM(date) ORDER BY id;\n-                '''.format(replica=node.name))\n-\n-        node1.query('''\n-            CREATE TABLE non_replicated_mt(date Date, id UInt32, value Int32)\n-            ENGINE = MergeTree() PARTITION BY toYYYYMM(date) ORDER BY id\n-            SETTINGS min_bytes_for_wide_part=0;\n-        ''')\n-\n         yield cluster\n \n     finally:\n@@ -54,6 +42,14 @@ def remove_part_from_disk(node, table, part_name):\n \n \n def test_check_normal_table_corruption(started_cluster):\n+    node1.query(\"DROP TABLE IF EXISTS non_replicated_mt\")\n+\n+    node1.query('''\n+        CREATE TABLE non_replicated_mt(date Date, id UInt32, value Int32)\n+        ENGINE = MergeTree() PARTITION BY toYYYYMM(date) ORDER BY id\n+        SETTINGS min_bytes_for_wide_part=0;\n+    ''')\n+\n     node1.query(\"INSERT INTO non_replicated_mt VALUES (toDate('2019-02-01'), 1, 10), (toDate('2019-02-01'), 2, 12)\")\n     assert node1.query(\"CHECK TABLE non_replicated_mt PARTITION 201902\",\n                        settings={\"check_query_single_value_result\": 0}) == \"201902_1_1_0\\t1\\t\\n\"\n@@ -94,8 +90,14 @@ def test_check_normal_table_corruption(started_cluster):\n \n \n def test_check_replicated_table_simple(started_cluster):\n-    node1.query(\"TRUNCATE TABLE replicated_mt\")\n-    node2.query(\"SYSTEM SYNC REPLICA replicated_mt\")\n+    for node in [node1, node2]:\n+        node.query(\"DROP TABLE IF EXISTS replicated_mt\")\n+\n+        node.query('''\n+        CREATE TABLE replicated_mt(date Date, id UInt32, value Int32)\n+        ENGINE = ReplicatedMergeTree('/clickhouse/tables/replicated_mt', '{replica}') PARTITION BY toYYYYMM(date) ORDER BY id;\n+            '''.format(replica=node.name))\n+\n     node1.query(\"INSERT INTO replicated_mt VALUES (toDate('2019-02-01'), 1, 10), (toDate('2019-02-01'), 2, 12)\")\n     node2.query(\"SYSTEM SYNC REPLICA replicated_mt\")\n \n@@ -119,34 +121,40 @@ def test_check_replicated_table_simple(started_cluster):\n \n \n def test_check_replicated_table_corruption(started_cluster):\n-    node1.query(\"TRUNCATE TABLE replicated_mt\")\n-    node2.query(\"SYSTEM SYNC REPLICA replicated_mt\")\n-    node1.query(\"INSERT INTO replicated_mt VALUES (toDate('2019-02-01'), 1, 10), (toDate('2019-02-01'), 2, 12)\")\n-    node1.query(\"INSERT INTO replicated_mt VALUES (toDate('2019-01-02'), 3, 10), (toDate('2019-01-02'), 4, 12)\")\n-    node2.query(\"SYSTEM SYNC REPLICA replicated_mt\")\n+    for node in [node1, node2]:\n+        node.query(\"DROP TABLE IF EXISTS replicated_mt_1\")\n \n-    assert node1.query(\"SELECT count() from replicated_mt\") == \"4\\n\"\n-    assert node2.query(\"SELECT count() from replicated_mt\") == \"4\\n\"\n+        node.query('''\n+        CREATE TABLE replicated_mt_1(date Date, id UInt32, value Int32)\n+        ENGINE = ReplicatedMergeTree('/clickhouse/tables/replicated_mt_1', '{replica}') PARTITION BY toYYYYMM(date) ORDER BY id;\n+            '''.format(replica=node.name))\n+\n+    node1.query(\"INSERT INTO replicated_mt_1 VALUES (toDate('2019-02-01'), 1, 10), (toDate('2019-02-01'), 2, 12)\")\n+    node1.query(\"INSERT INTO replicated_mt_1 VALUES (toDate('2019-01-02'), 3, 10), (toDate('2019-01-02'), 4, 12)\")\n+    node2.query(\"SYSTEM SYNC REPLICA replicated_mt_1\")\n+\n+    assert node1.query(\"SELECT count() from replicated_mt_1\") == \"4\\n\"\n+    assert node2.query(\"SELECT count() from replicated_mt_1\") == \"4\\n\"\n \n     part_name = node1.query(\n-        \"SELECT name from system.parts where table = 'replicated_mt' and partition_id = '201901' and active = 1\").strip()\n+        \"SELECT name from system.parts where table = 'replicated_mt_1' and partition_id = '201901' and active = 1\").strip()\n \n-    corrupt_data_part_on_disk(node1, \"replicated_mt\", part_name)\n-    assert node1.query(\"CHECK TABLE replicated_mt PARTITION 201901\", settings={\n-        \"check_query_single_value_result\": 0}) == \"{p}\\t0\\tPart {p} looks broken. Removing it and queueing a fetch.\\n\".format(\n+    corrupt_data_part_on_disk(node1, \"replicated_mt_1\", part_name)\n+    assert node1.query(\"CHECK TABLE replicated_mt_1 PARTITION 201901\", settings={\n+        \"check_query_single_value_result\": 0}) == \"{p}\\t0\\tPart {p} looks broken. Removing it and will try to fetch.\\n\".format(\n         p=part_name)\n \n-    node1.query(\"SYSTEM SYNC REPLICA replicated_mt\")\n-    assert node1.query(\"CHECK TABLE replicated_mt PARTITION 201901\",\n+    node1.query(\"SYSTEM SYNC REPLICA replicated_mt_1\")\n+    assert node1.query(\"CHECK TABLE replicated_mt_1 PARTITION 201901\",\n                        settings={\"check_query_single_value_result\": 0}) == \"{}\\t1\\t\\n\".format(part_name)\n-    assert node1.query(\"SELECT count() from replicated_mt\") == \"4\\n\"\n+    assert node1.query(\"SELECT count() from replicated_mt_1\") == \"4\\n\"\n \n-    remove_part_from_disk(node2, \"replicated_mt\", part_name)\n-    assert node2.query(\"CHECK TABLE replicated_mt PARTITION 201901\", settings={\n-        \"check_query_single_value_result\": 0}) == \"{p}\\t0\\tPart {p} looks broken. Removing it and queueing a fetch.\\n\".format(\n+    remove_part_from_disk(node2, \"replicated_mt_1\", part_name)\n+    assert node2.query(\"CHECK TABLE replicated_mt_1 PARTITION 201901\", settings={\n+        \"check_query_single_value_result\": 0}) == \"{p}\\t0\\tPart {p} looks broken. Removing it and will try to fetch.\\n\".format(\n         p=part_name)\n \n-    node1.query(\"SYSTEM SYNC REPLICA replicated_mt\")\n-    assert node1.query(\"CHECK TABLE replicated_mt PARTITION 201901\",\n+    node1.query(\"SYSTEM SYNC REPLICA replicated_mt_1\")\n+    assert node1.query(\"CHECK TABLE replicated_mt_1 PARTITION 201901\",\n                        settings={\"check_query_single_value_result\": 0}) == \"{}\\t1\\t\\n\".format(part_name)\n-    assert node1.query(\"SELECT count() from replicated_mt\") == \"4\\n\"\n+    assert node1.query(\"SELECT count() from replicated_mt_1\") == \"4\\n\"\ndiff --git a/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill.sh b/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill.sh\nindex 7f111538a063..6ae103bdf6e3 100755\n--- a/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill.sh\n+++ b/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill.sh\n@@ -28,7 +28,7 @@ function kill_mutation_thread\n         # find any mutation and kill it\n         mutation_id=$($CLICKHOUSE_CLIENT --query \"SELECT mutation_id FROM system.mutations WHERE is_done=0 and database='${CLICKHOUSE_DATABASE}' and table='concurrent_mutate_kill' LIMIT 1\")\n         if [ ! -z \"$mutation_id\" ]; then\n-            $CLICKHOUSE_CLIENT --query \"KILL MUTATION WHERE mutation_id='$mutation_id'\" 1> /dev/null\n+            $CLICKHOUSE_CLIENT --query \"KILL MUTATION WHERE mutation_id='$mutation_id' and table='concurrent_mutate_kill' and database='${CLICKHOUSE_DATABASE}'\" 1> /dev/null\n             sleep 1\n         fi\n     done\ndiff --git a/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill_many_replicas.sh b/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill_many_replicas.sh\nindex 60e2adb4204d..bfa68328c065 100755\n--- a/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill_many_replicas.sh\n+++ b/tests/queries/0_stateless/01593_concurrent_alter_mutations_kill_many_replicas.sh\n@@ -40,7 +40,7 @@ function kill_mutation_thread\n         # find any mutation and kill it\n         mutation_id=$($CLICKHOUSE_CLIENT --query \"SELECT mutation_id FROM system.mutations WHERE is_done = 0 and table like 'concurrent_kill_%' and database='${CLICKHOUSE_DATABASE}' LIMIT 1\")\n         if [ ! -z \"$mutation_id\" ]; then\n-            $CLICKHOUSE_CLIENT --query \"KILL MUTATION WHERE mutation_id='$mutation_id'\" 1> /dev/null\n+            $CLICKHOUSE_CLIENT --query \"KILL MUTATION WHERE mutation_id='$mutation_id' and table like 'concurrent_kill_%' and database='${CLICKHOUSE_DATABASE}'\" 1> /dev/null\n             sleep 1\n         fi\n     done\ndiff --git a/tests/queries/skip_list.json b/tests/queries/skip_list.json\nindex 96da47ee9c2c..b25b96b6af98 100644\n--- a/tests/queries/skip_list.json\n+++ b/tests/queries/skip_list.json\n@@ -557,6 +557,8 @@\n         \"01541_max_memory_usage_for_user\",\n         \"01542_dictionary_load_exception_race\",\n         \"01575_disable_detach_table_of_dictionary\",\n+        \"01593_concurrent_alter_mutations_kill\",\n+        \"01593_concurrent_alter_mutations_kill_many_replicas\",\n         \"01600_count_of_parts_metrics\", // tests global system metrics\n         \"01600_detach_permanently\",\n         \"01600_log_queries_with_extensive_info\",\n",
  "problem_statement": "CH forgot about several parts\n20.9.7.\r\nDuring a merge a part was detached by ReplicatedMergeTreePartCheckThread as broken.\r\nCH wasn't able to recover the part because the table only has one replica (later I attached the part back successfully though).\r\nBecause of that the NEW (not created yet) part got marked as 'lost forever' (unexpected behaviour), which lead to another unexpected behaviour when we needed to drop a column from the table.\r\n\r\nThe details:\r\n\r\nA merge was scheduled:\r\n```\r\n2021.01.19 22:51:20.987159 [ 9711 ] {} <Trace> db.table: Created log entry /clickhouse/tables/14/table/log/log-0000198617 for merge 20210119_254_293_2\r\n2021.01.19 22:51:20.988217 [ 9709 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000198617 - log-0000198617\r\n2021.01.19 22:51:20.994938 [ 9709 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Pulled 1 entries to queue.\r\n2021.01.19 22:51:20.995006 [ 9730 ] {} <Trace> db.table: Executing log entry to merge parts 20210119_254_261_1, 20210119_262_267_1, 20210119_268_273_1, 20210119_274_279_1, 20210119_280_285_1, 20210119_286_291_1, 20210119_292_292_0, 20210119_293_293_0 to 20210119_254_293_2\r\n2021.01.19 22:51:20.995057 [ 9730 ] {} <Debug> DiskLocal: Reserving 8.71 GiB on disk `default`, having unreserved 5.82 TiB.\r\n2021.01.19 22:51:20.995081 [ 9730 ] {} <Debug> db.table (MergerMutator): Merging 8 parts: from 20210119_254_261_1 to 20210119_293_293_0 into Wide\r\n2021.01.19 22:51:20.995464 [ 9730 ] {} <Debug> db.table (MergerMutator): Selected MergeAlgorithm: Vertical\r\n2021.01.19 22:51:21.011655 [ 9730 ] {} <Trace> MergeTreeSequentialSource: Reading 1069 marks from part 20210119_254_261_1, total 8752897 rows starting from the beginning of the part\r\n```\r\nIt was reading data when another thread started checking a part:\r\n```\r\n2021.01.19 22:52:15.724000 [ 9730 ] {} <Trace> MergeTreeSequentialSource: Reading 1069 marks from part 20210119_254_261_1, total 8752897 rows starting from the beginning of the part, column floor\r\n\r\n2021.01.19 22:52:16.211682 [ 9758 ] {} <Warning> db.table (ReplicatedMergeTreePartCheckThread): Checking part 20210119_254_261_1\r\n2021.01.19 22:52:16.212705 [ 9758 ] {} <Warning> db.table (ReplicatedMergeTreePartCheckThread): Checking data of part 20210119_254_261_1.\r\n```\r\nThe merging thread 9730 failed:\r\n```\r\n2021.01.19 22:52:16.406541 [ 9730 ] {} <Error> db.table: DB::StorageReplicatedMergeTree::queueTask()::<lambda(DB::StorageReplicatedMergeTree::LogEntryPtr&)>: Code: 40, e.displayText() = DB::Exception: Checksum doesn't match: corrupted data. Reference: aedfece83e71a6af9ab401e8c70fe7bd. Actual: 6db5d49e651b5b6ad63b9244e50db5ca. Size of compressed block: 11405: (while reading column floor_price): (while reading from part /var/lib/clickhouse/data/db/table/20210119_254_261_1/ from mark 938 with max_rows_to_read = 8192): While executing MergeTreeSequentialSource: Cannot fetch required block. Stream PipelineExecuting, part 0, Stack trace (when copying this message, always include the lines below)\r\n```\r\nThe checking thread 9758 detached the part:\r\n```\r\n2021.01.19 22:52:17.780119 [ 9758 ] {} <Error> db.table (ReplicatedMergeTreePartCheckThread): Part 20210119_254_261_1 looks broken. Removing it and queueing a fetch.\r\n2021.01.19 22:52:17.789749 [ 9758 ] {} <Information> db.table: Renaming 20210119_254_261_1 to broken20210119_254_261_1 and forgiving it.\r\n2021.01.19 22:52:17.790296 [ 9758 ] {} <Trace> db.table (ReplicatedMergeTreePartCheckThread): Execution took 1578 ms.\r\n```\r\nAnother thread started the merge again, but one part was missing, so the thread marked as lost the NEW part, 20210119_254_293_2 (as well as the detached part)\r\n```\r\n2021.01.19 22:52:27.473582 [ 9727 ] {} <Trace> db.table: Executing log entry to merge parts 20210119_254_261_1, 20210119_262_267_1, 20210119_268_273_1, 20210119_274_279_1, 20210119_280_285_1, 20210119_286_291_1, 20210119_292_292_0, 20210119_293_293_0 to 20210119_254_293_2\r\n2021.01.19 22:52:27.473605 [ 9727 ] {} <Debug> db.table: Don't have all parts for merge 20210119_254_293_2; will try to fetch it instead\r\n2021.01.19 22:52:27.474010 [ 9740 ] {} <Warning> db.table (ReplicatedMergeTreePartCheckThread): Checking part 20210119_254_293_2\r\n2021.01.19 22:52:27.474032 [ 9727 ] {} <Information> db.table: DB::Exception: No active replica has part 20210119_254_293_2 or covering part\r\n2021.01.19 22:52:27.474408 [ 9740 ] {} <Warning> db.table (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part covering 20210119_254_293_2.\r\n2021.01.19 22:52:27.475311 [ 9740 ] {} <Error> db.table (ReplicatedMergeTreePartCheckThread): No replica has part covering 20210119_254_293_2 and a merge is impossible: we didn't find a smaller part with the same max block.\r\n2021.01.19 22:52:27.481798 [ 9740 ] {} <Error> db.table (ReplicatedMergeTreePartCheckThread): Part 20210119_254_293_2 is lost forever.\r\n\r\n2021.01.19 22:52:27.794690 [ 9740 ] {} <Warning> db.table (ReplicatedMergeTreePartCheckThread): Checking part 20210119_254_261_1\r\n2021.01.19 22:52:27.794700 [ 9720 ] {} <Information> db.table: DB::Exception: No active replica has part 20210119_254_261_1 or covering part\r\n2021.01.19 22:52:27.795130 [ 9740 ] {} <Warning> db.table (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part covering 20210119_254_261_1.\r\n2021.01.19 22:52:27.796030 [ 9740 ] {} <Error> db.table (ReplicatedMergeTreePartCheckThread): No replica has part covering 20210119_254_261_1 and a merge is impossible: we didn't find smaller parts with either the same min block or the same max block.\r\n2021.01.19 22:52:27.803382 [ 9740 ] {} <Error> db.table (ReplicatedMergeTreePartCheckThread): Part 20210119_254_261_1 is lost forever.\r\n```\r\nA few days later a mutation skipped the parts that were supposed to be merged with the 'broken' part (20210119_262_267_1, 20210119_268_273_1, 20210119_274_279_1, 20210119_280_285_1, 20210119_286_291_1, 20210119_292_292_0, 20210119_293_293_0):\r\n```\r\n2021.01.25 09:12:38.455858 [ 31330 ] {1c262294-4fe9-4e4b-acec-ca3c42a1b7ec} <Debug> executeQuery: (from [::1]:48234) alter table db.table drop column bba7dt\r\n2021.01.25 09:12:39.874691 [ 9734 ] {} <Information> db.table (ReplicatedMergeTreeQueue): Loading 1 mutation entries: 0000000013 - 0000000013\r\n2021.01.25 09:12:39.891032 [ 9734 ] {} <Trace> db.table (ReplicatedMergeTreeQueue): Adding mutation 0000000013 for partition 20210119 for all block numbers less than 480\r\n...\r\n2021.01.25 09:13:15.696975 [ 9734 ] {} <Trace> db.table: Created log entry for mutation 20210119_0_216_3_480\r\n2021.01.25 09:13:15.734053 [ 9713 ] {} <Trace> db.table: Created log entry for mutation 20210119_217_253_2_480\r\n!!!! skipped parts should have been here\r\n2021.01.25 09:13:15.749085 [ 9711 ] {} <Trace> db.table: Created log entry for mutation 20210119_294_328_2_480\r\n2021.01.25 09:13:15.760000 [ 9742 ] {} <Trace> db.table: Created log entry for mutation 20210119_329_363_2_480\r\n...\r\n2021.01.25 09:13:24.553450 [ 9740 ] {} <Trace> db.table: Created log entry for mutation 20210125_19_19_0_20\r\n```\r\nAfter that the mutation got stuck with parts_to_do=8.\r\nHere is the list of parts in ZooKeeper at the time:\r\n```\r\nname                   | ctime              \r\n-----------------------+--------------------\r\n20210119_0_216_3_480   | 2021-01-25 09:13:16\r\n20210119_217_253_2_480 | 2021-01-25 09:13:16\r\n20210119_262_267_1     | 2021-01-19 17:50:13\r\n20210119_268_273_1     | 2021-01-19 18:06:43\r\n20210119_274_279_1     | 2021-01-19 18:17:58\r\n20210119_280_285_1     | 2021-01-19 18:33:07\r\n20210119_286_291_1     | 2021-01-19 18:50:02\r\n20210119_292_292_0     | 2021-01-19 18:50:29\r\n20210119_293_293_0     | 2021-01-19 18:51:15\r\n20210119_294_328_2_480 | 2021-01-25 09:13:16\r\n20210119_329_363_2_480 | 2021-01-25 09:13:16\r\n```\r\nAfter a few hours I restarted the table:\r\n```\r\n2021.01.25 14:28:15.850433 [ 1378 ] {2839b9af-2fa8-42bb-ab06-d806fb18644e} <Debug> executeQuery: (from [::1]:45158) system restart replica db.table\r\n```\r\nAs a result the ALTER got interrupted.\r\n```\r\n2021.01.25 14:28:16.649570 [ 31330 ] {1c262294-4fe9-4e4b-acec-ca3c42a1b7ec} <Trace> db.table (ReplicatedMergeTreeRestartingThread): Restarting thread finished\r\n2021.01.25 14:28:16.649593 [ 31330 ] {1c262294-4fe9-4e4b-acec-ca3c42a1b7ec} <Trace> db.table (ReplicatedMergeTreeRestartingThread): Waiting for threads to finish\r\n2021.01.25 14:28:16.649598 [ 31330 ] {1c262294-4fe9-4e4b-acec-ca3c42a1b7ec} <Trace> db.table (ReplicatedMergeTreeRestartingThread): Threads finished\r\n2021.01.25 14:28:16.710937 [ 31330 ] {1c262294-4fe9-4e4b-acec-ca3c42a1b7ec} <Error> executeQuery: Code: 341, e.displayText() = DB::Exception: Mutation is not finished because table shutdown was called. It will be done after table restart. (version 20.9.7.11 (off\r\nicial build)) (from [::1]:48234) (in query: alter table db.table drop column bba7dt\r\n```\r\nThe mutation found 7 parts. MergerMutator found them as well. They were merged then mutated.\r\n```\r\n2021.01.25 14:28:28.230124 [ 9734 ] {} <Trace> db.table (ReplicatedMergeTreeQueue): Mutation 0000000013 is not done yet because 7 parts to mutate suddenly appeared.\r\n2021.01.25 14:28:28.233083 [ 9709 ] {} <Debug> db.table (MergerMutator): Selected 7 parts from 20210119_262_267_1 to 20210119_293_293_0\r\n2021.01.25 14:28:28.239167 [ 9709 ] {} <Trace> db.table: Created log entry /clickhouse/tables/14/table/log/log-0000202318 for merge 20210119_262_293_2\r\n2021.01.25 14:28:28.239294 [ 9757 ] {} <Trace> db.table (ReplicatedMergeTreeQueue): Mutation 0000000013 is not done yet because 7 parts to mutate suddenly appeared.\r\n2021.01.25 14:28:28.241162 [ 9758 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000202318 - log-0000202318\r\n2021.01.25 14:28:28.246171 [ 9758 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Pulled 1 entries to queue.\r\n...\r\n2021.01.25 14:39:17.698423 [ 9729 ] {} <Trace> db.table: Renaming temporary part tmp_merge_20210119_262_293_2 to 20210119_262_293_2.\r\n2021.01.25 14:39:17.698466 [ 9729 ] {} <Trace> db.table (MergerMutator): Merged 7 parts: from 20210119_262_267_1 to 20210119_293_293_0\r\n2021.01.25 14:39:17.715249 [ 9711 ] {} <Trace> db.table: Created log entry for mutation 20210119_262_293_2_480\r\n2021.01.25 14:39:17.716625 [ 9732 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000202319 - log-0000202319\r\n2021.01.25 14:39:17.720744 [ 9732 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Pulled 1 entries to queue.\r\n2021.01.25 14:39:17.720815 [ 9725 ] {} <Trace> db.table: Executing log entry to mutate part 20210119_262_293_2 to 20210119_262_293_2_480\r\n2021.01.25 14:39:17.720839 [ 9725 ] {} <Debug> DiskLocal: Reserving 6.83 GiB on disk `default`, having unreserved 5.88 TiB.\r\n2021.01.25 14:39:17.721298 [ 9725 ] {} <Trace> db.table (MergerMutator): Mutating part 20210119_262_293_2 to mutation version 480\r\n2021.01.25 14:39:17.738997 [ 9725 ] {} <Trace> db.table: Renaming temporary part tmp_mut_20210119_262_293_2_480 to 20210119_262_293_2_480.\r\n2021.01.25 14:39:17.744894 [ 9740 ] {} <Trace> db.table (ReplicatedMergeTreeQueue): Will check if mutation 0000000013 is done\r\n2021.01.25 14:39:17.744931 [ 9740 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Trying to finalize 1 mutations\r\n2021.01.25 14:39:17.758855 [ 9740 ] {} <Trace> db.table (ReplicatedMergeTreeQueue): Mutation 0000000013 is done\r\n2021.01.25 14:39:17.758878 [ 9740 ] {} <Trace> db.table (ReplicatedMergeTreeQueue): Finishing data alter with version 53 for entry 0000000013\r\n```\r\nHappy end :)\n",
  "hints_text": "Double-checked, after the failed merge CH was still using the 'forgotten' parts in SELECTs\r\n```\r\n2021.01.19 22:53:56.746520 [ 25668 ] <Trace> db.table (SelectExecutor): Used generic exclusion search over index for part 20210119_262_267_1 with 484 steps\r\n2021.01.19 22:53:56.746798 [ 25668 ] <Trace> db.table (SelectExecutor): Used generic exclusion search over index for part 20210119_268_273_1 with 501 steps\r\n2021.01.19 22:53:56.747072 [ 25668 ] <Trace> db.table (SelectExecutor): Used generic exclusion search over index for part 20210119_274_279_1 with 494 steps\r\n2021.01.19 22:53:56.747356 [ 25668 ] <Trace> db.table (SelectExecutor): Used generic exclusion search over index for part 20210119_280_285_1 with 517 steps\r\n2021.01.19 22:53:56.747534 [ 19269 ] <Trace> db.table (SelectExecutor): Used generic exclusion search over index for part 20210119_292_292_0 with 150 steps\r\n2021.01.19 22:53:56.747606 [ 19269 ] <Trace> db.table (SelectExecutor): Used generic exclusion search over index for part 20210119_293_293_0 with 29 steps\r\n2021.01.19 22:53:56.747609 [ 25668 ] <Trace> db.table (SelectExecutor): Used generic exclusion search over index for part 20210119_286_291_1 with 440 steps\r\n2021.01.19 22:53:56.748184 [ 9785 ] <Trace> db.table (SelectExecutor): Used generic exclusion search over index for part 20210119_217_253_2 with 2961 steps\r\n```",
  "created_at": "2021-01-27T10:08:32Z"
}