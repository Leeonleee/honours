{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 35806,
  "instance_id": "ClickHouse__ClickHouse-35806",
  "issue_numbers": [
    "35790"
  ],
  "base_commit": "f3f335d5bf9fbf59238d5160723a8b0fd293aa8f",
  "patch": "diff --git a/src/DataTypes/ObjectUtils.cpp b/src/DataTypes/ObjectUtils.cpp\nindex 9004a5296e02..cbabc71a9656 100644\n--- a/src/DataTypes/ObjectUtils.cpp\n+++ b/src/DataTypes/ObjectUtils.cpp\n@@ -128,22 +128,21 @@ static auto extractVector(const std::vector<Tuple> & vec)\n     return res;\n }\n \n-void convertObjectsToTuples(NamesAndTypesList & columns_list, Block & block, const NamesAndTypesList & extended_storage_columns)\n+void convertObjectsToTuples(Block & block, const NamesAndTypesList & extended_storage_columns)\n {\n     std::unordered_map<String, DataTypePtr> storage_columns_map;\n     for (const auto & [name, type] : extended_storage_columns)\n         storage_columns_map[name] = type;\n \n-    for (auto & name_type : columns_list)\n+    for (auto & column : block)\n     {\n-        if (!isObject(name_type.type))\n+        if (!isObject(column.type))\n             continue;\n \n-        auto & column = block.getByName(name_type.name);\n         if (!isObject(column.type))\n             throw Exception(ErrorCodes::TYPE_MISMATCH,\n                 \"Type for column '{}' mismatch in columns list and in block. In list: {}, in block: {}\",\n-                name_type.name, name_type.type->getName(), column.type->getName());\n+                column.name, column.type->getName(), column.type->getName());\n \n         const auto & column_object = assert_cast<const ColumnObject &>(*column.column);\n         const auto & subcolumns = column_object.getSubcolumns();\n@@ -151,7 +150,7 @@ void convertObjectsToTuples(NamesAndTypesList & columns_list, Block & block, con\n         if (!column_object.isFinalized())\n             throw Exception(ErrorCodes::LOGICAL_ERROR,\n                 \"Cannot convert to tuple column '{}' from type {}. Column should be finalized first\",\n-                name_type.name, name_type.type->getName());\n+                column.name, column.type->getName());\n \n         PathsInData tuple_paths;\n         DataTypes tuple_types;\n@@ -164,12 +163,11 @@ void convertObjectsToTuples(NamesAndTypesList & columns_list, Block & block, con\n             tuple_columns.emplace_back(entry->data.getFinalizedColumnPtr());\n         }\n \n-        auto it = storage_columns_map.find(name_type.name);\n+        auto it = storage_columns_map.find(column.name);\n         if (it == storage_columns_map.end())\n-            throw Exception(ErrorCodes::LOGICAL_ERROR, \"Column '{}' not found in storage\", name_type.name);\n+            throw Exception(ErrorCodes::LOGICAL_ERROR, \"Column '{}' not found in storage\", column.name);\n \n         std::tie(column.column, column.type) = unflattenTuple(tuple_paths, tuple_types, tuple_columns);\n-        name_type.type = column.type;\n \n         /// Check that constructed Tuple type and type in storage are compatible.\n         getLeastCommonTypeForObject({column.type, it->second}, true);\ndiff --git a/src/DataTypes/ObjectUtils.h b/src/DataTypes/ObjectUtils.h\nindex 199a048c8cd6..1dbeac2b2443 100644\n--- a/src/DataTypes/ObjectUtils.h\n+++ b/src/DataTypes/ObjectUtils.h\n@@ -38,7 +38,7 @@ DataTypePtr getDataTypeByColumn(const IColumn & column);\n \n /// Converts Object types and columns to Tuples in @columns_list and @block\n /// and checks that types are consistent with types in @extended_storage_columns.\n-void convertObjectsToTuples(NamesAndTypesList & columns_list, Block & block, const NamesAndTypesList & extended_storage_columns);\n+void convertObjectsToTuples(Block & block, const NamesAndTypesList & extended_storage_columns);\n \n /// Checks that each path is not the prefix of any other path.\n void checkObjectHasNoAmbiguosPaths(const PathsInData & paths);\ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.cpp b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\nindex 4805a273c70f..fc05e2936849 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n@@ -145,7 +145,7 @@ void MergeTreeDataWriter::TemporaryPart::finalize()\n }\n \n BlocksWithPartition MergeTreeDataWriter::splitBlockIntoParts(\n-        const Block & block, size_t max_parts, const StorageMetadataPtr & metadata_snapshot, ContextPtr context)\n+    const Block & block, size_t max_parts, const StorageMetadataPtr & metadata_snapshot, ContextPtr context)\n {\n     BlocksWithPartition result;\n     if (!block || !block.rows())\n@@ -282,16 +282,12 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPart(\n {\n     TemporaryPart temp_part;\n     Block & block = block_with_partition.block;\n-    auto columns = metadata_snapshot->getColumns().getAllPhysical().filter(block.getNames());\n-    auto storage_snapshot = data.getStorageSnapshot(metadata_snapshot);\n \n-    if (!storage_snapshot->object_columns.empty())\n-    {\n-        auto extended_storage_columns = storage_snapshot->getColumns(\n-            GetColumnsOptions(GetColumnsOptions::AllPhysical).withExtendedObjects());\n+    auto columns = metadata_snapshot->getColumns().getAllPhysical().filter(block.getNames());\n \n-        convertObjectsToTuples(columns, block, extended_storage_columns);\n-    }\n+    for (auto & column : columns)\n+        if (isObject(column.type))\n+            column.type = block.getByName(column.name).type;\n \n     static const String TMP_PREFIX = \"tmp_insert_\";\n \n@@ -466,6 +462,16 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPart(\n     return temp_part;\n }\n \n+void MergeTreeDataWriter::deduceTypesOfObjectColumns(const StorageSnapshotPtr & storage_snapshot, Block & block)\n+{\n+    if (!storage_snapshot->object_columns.empty())\n+    {\n+        auto options = GetColumnsOptions(GetColumnsOptions::AllPhysical).withExtendedObjects();\n+        auto storage_columns = storage_snapshot->getColumns(options);\n+        convertObjectsToTuples(block, storage_columns);\n+    }\n+}\n+\n MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeProjectionPartImpl(\n     const String & part_name,\n     MergeTreeDataPartType part_type,\ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.h b/src/Storages/MergeTree/MergeTreeDataWriter.h\nindex ae46a94ccd71..7b6bf8fb1dbf 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.h\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.h\n@@ -42,14 +42,12 @@ class MergeTreeDataWriter\n       */\n     static BlocksWithPartition splitBlockIntoParts(const Block & block, size_t max_parts, const StorageMetadataPtr & metadata_snapshot, ContextPtr context);\n \n-    /** All rows must correspond to same partition.\n-      * Returns part with unique name starting with 'tmp_', yet not added to MergeTreeData.\n-      */\n-    MergeTreeData::MutableDataPartPtr writeTempPart(BlockWithPartition & block, const StorageMetadataPtr & metadata_snapshot, bool optimize_on_insert);\n+    static void deduceTypesOfObjectColumns(const StorageSnapshotPtr & storage_snapshot, Block & block);\n \n     /// This structure contains not completely written temporary part.\n     /// Some writes may happen asynchronously, e.g. for blob storages.\n     /// You should call finalize() to wait until all data is written.\n+\n     struct TemporaryPart\n     {\n         MergeTreeData::MutableDataPartPtr part;\n@@ -65,6 +63,9 @@ class MergeTreeDataWriter\n         void finalize();\n     };\n \n+    /** All rows must correspond to same partition.\n+      * Returns part with unique name starting with 'tmp_', yet not added to MergeTreeData.\n+      */\n     TemporaryPart writeTempPart(BlockWithPartition & block, const StorageMetadataPtr & metadata_snapshot, ContextPtr context);\n \n     /// For insertion.\ndiff --git a/src/Storages/MergeTree/MergeTreeSink.cpp b/src/Storages/MergeTree/MergeTreeSink.cpp\nindex 97bbfc17e9d4..7a4ecae24b3b 100644\n--- a/src/Storages/MergeTree/MergeTreeSink.cpp\n+++ b/src/Storages/MergeTree/MergeTreeSink.cpp\n@@ -50,7 +50,9 @@ struct MergeTreeSink::DelayedChunk\n void MergeTreeSink::consume(Chunk chunk)\n {\n     auto block = getHeader().cloneWithColumns(chunk.detachColumns());\n+    auto storage_snapshot = storage.getStorageSnapshot(metadata_snapshot);\n \n+    storage.writer.deduceTypesOfObjectColumns(storage_snapshot, block);\n     auto part_blocks = storage.writer.splitBlockIntoParts(block, max_parts_per_block, metadata_snapshot, context);\n \n     using DelayedPartitions = std::vector<MergeTreeSink::DelayedChunk::Partition>;\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp\nindex 550c586f7dee..63fa20710568 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeSink.cpp\n@@ -150,7 +150,8 @@ void ReplicatedMergeTreeSink::consume(Chunk chunk)\n     if (quorum)\n         checkQuorumPrecondition(zookeeper);\n \n-    const Settings & settings = context->getSettingsRef();\n+    auto storage_snapshot = storage.getStorageSnapshot(metadata_snapshot);\n+    storage.writer.deduceTypesOfObjectColumns(storage_snapshot, block);\n     auto part_blocks = storage.writer.splitBlockIntoParts(block, max_parts_per_block, metadata_snapshot, context);\n \n     using DelayedPartitions = std::vector<ReplicatedMergeTreeSink::DelayedChunk::Partition>;\n@@ -158,6 +159,7 @@ void ReplicatedMergeTreeSink::consume(Chunk chunk)\n \n     size_t streams = 0;\n     bool support_parallel_write = false;\n+    const Settings & settings = context->getSettingsRef();\n \n     for (auto & current_block : part_blocks)\n     {\ndiff --git a/src/Storages/StorageMemory.cpp b/src/Storages/StorageMemory.cpp\nindex 30be297194a4..a371ac1ccf82 100644\n--- a/src/Storages/StorageMemory.cpp\n+++ b/src/Storages/StorageMemory.cpp\n@@ -137,11 +137,10 @@ class MemorySink : public SinkToStorage\n         storage_snapshot->metadata->check(block, true);\n         if (!storage_snapshot->object_columns.empty())\n         {\n-            auto columns = storage_snapshot->metadata->getColumns().getAllPhysical().filter(block.getNames());\n             auto extended_storage_columns = storage_snapshot->getColumns(\n                 GetColumnsOptions(GetColumnsOptions::AllPhysical).withExtendedObjects());\n \n-            convertObjectsToTuples(columns, block, extended_storage_columns);\n+            convertObjectsToTuples(block, extended_storage_columns);\n         }\n \n         if (storage.compress)\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01825_type_json_partitions.reference b/tests/queries/0_stateless/01825_type_json_partitions.reference\nnew file mode 100644\nindex 000000000000..5a7ba251572b\n--- /dev/null\n+++ b/tests/queries/0_stateless/01825_type_json_partitions.reference\n@@ -0,0 +1,2 @@\n+{\"id\":1,\"obj\":{\"k1\":\"v1\",\"k2\":\"\"}}\n+{\"id\":2,\"obj\":{\"k1\":\"\",\"k2\":\"v2\"}}\ndiff --git a/tests/queries/0_stateless/01825_type_json_partitions.sql b/tests/queries/0_stateless/01825_type_json_partitions.sql\nnew file mode 100644\nindex 000000000000..27804e7edae1\n--- /dev/null\n+++ b/tests/queries/0_stateless/01825_type_json_partitions.sql\n@@ -0,0 +1,15 @@\n+-- Tags: no-fasttest\n+\n+DROP TABLE IF EXISTS t_json_partitions;\n+\n+SET allow_experimental_object_type = 1;\n+SET output_format_json_named_tuples_as_objects = 1;\n+\n+CREATE TABLE t_json_partitions (id UInt32, obj JSON)\n+ENGINE MergeTree ORDER BY id PARTITION BY id;\n+\n+INSERT INTO t_json_partitions FORMAT JSONEachRow {\"id\": 1, \"obj\": {\"k1\": \"v1\"}} {\"id\": 2, \"obj\": {\"k2\": \"v2\"}};\n+\n+SELECT * FROM t_json_partitions ORDER BY id FORMAT JSONEachRow;\n+\n+DROP TABLE t_json_partitions;\n",
  "problem_statement": "JSON field in partitioned table insert error when crossing partition boundaries\nGood day everyone! I have stumbled upon an interesting issue with  semi-structured data experimental feature . \r\n\r\n\r\nWhen inserting into a partitioned MergeTree with JSON column and  crossing partition boundaries  the  error is thrown \r\n`DB::Exception: ColumnObject must be converted to ColumnTuple before use. (LOGICAL_ERROR)`\r\n\r\nThe  issue can be reproduced on ` 22.3.2.1` release\r\n\r\n\r\n**How to reproduce**\r\n\r\n* Which ClickHouse server version to use -  22.3.2.1\r\n* Which interface to use, if matters - does not  seem to play any role\r\n* Non-default settings, if any  - allow_experimental_object_type enabled\r\n* `CREATE TABLE` statements for all tables involved \r\n```DROP TABLE IF EXISTS jsonTest;\r\nCREATE TABLE jsonTest\r\n(\r\n    `data` JSON,\r\n    `creationDateUnix` UInt32 CODEC(DoubleDelta, ZSTD(1))\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY toYYYYMM(toDate(creationDateUnix))\r\nORDER BY (creationDateUnix);\r\n\r\nDROP TABLE IF EXISTS source;\r\nCREATE TABLE source\r\n(\r\n    `sourceData` String,\r\n    `creationDateUnix` UInt32 CODEC(DoubleDelta, ZSTD(1))\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY toYYYYMM(toDate(creationDateUnix))\r\nORDER BY (creationDateUnix);\r\n\r\n\r\ntruncate source;\r\ninsert into source (sourceData,creationDateUnix)\r\nselect coalesce(actionDetailStr,'{}'), creationDateUnix from statOptJSONstr;   \r\n\r\n\r\ntruncate jsonTest; \r\ninsert into jsonTest(data, creationDateUnix)\r\nselect sourceData,creationDateUnix from source; -- this statement fails with \r\n DB::Exception: ColumnObject must be converted to ColumnTuple before use. (LOGICAL_ERROR) (version 22.3.2.1)\r\n ```\r\nI have noticed that if the  data belongs  to  only one partition  there  is no issue.\r\nFor  non-partitioned tables  issue is not reproducible  as well. \r\n\r\nfollowing statement  can be used  for  checking my  assumption above\r\n```\r\ninsert into jsonTest(data, creationDateUnix)\r\nselect  sourceData, creationDateUnix from source  Where  toYYYYMMDD(toDate(creationDateUnix)) between 20181130  and 20181201;\r\n```\r\n\r\n[Here  is my  dataset in parquet](https://drive.google.com/file/d/13fmXEcfLi-qGrBTz5UUB6hWOMUcPM6xb/view?usp=sharing)\r\n\r\n**Expected behavior**\r\n\r\nExpected  behavior is successful insertion of data\r\n\r\n**Error message and/or stacktrace**\r\n\r\nSQL Error [1002]: ClickHouse exception, code: 1002, host: localhost, port: 8123; Code: 49. DB::Exception: ColumnObject must be converted to ColumnTuple before use. (LOGICAL_ERROR) (version 22.3.2.1)\r\n\r\n\r\n\n",
  "hints_text": "",
  "created_at": "2022-03-31T13:33:38Z"
}