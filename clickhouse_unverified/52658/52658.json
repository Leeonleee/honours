{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 52658,
  "instance_id": "ClickHouse__ClickHouse-52658",
  "issue_numbers": [
    "48513"
  ],
  "base_commit": "9448d42aea6c5befae09cc923570fd6575a6d6f8",
  "patch": "diff --git a/docs/en/operations/backup.md b/docs/en/operations/backup.md\nindex 62f931a76b43..63bf3cfeb5c8 100644\n--- a/docs/en/operations/backup.md\n+++ b/docs/en/operations/backup.md\n@@ -84,6 +84,7 @@ The BACKUP and RESTORE statements take a list of DATABASE and TABLE names, a des\n     - `password` for the file on disk\n     - `base_backup`: the destination of the previous backup of this source.  For example, `Disk('backups', '1.zip')`\n     - `structure_only`: if enabled, allows to only backup or restore the CREATE statements without the data of tables\n+    - `s3_storage_class`: the storage class used for S3 backup. For example, `STANDARD`\n \n ### Usage examples\n \ndiff --git a/src/Backups/BackupFactory.h b/src/Backups/BackupFactory.h\nindex e95aeddb0863..a79c6d354fc2 100644\n--- a/src/Backups/BackupFactory.h\n+++ b/src/Backups/BackupFactory.h\n@@ -30,6 +30,7 @@ class BackupFactory : boost::noncopyable\n         String compression_method;\n         int compression_level = -1;\n         String password;\n+        String s3_storage_class;\n         ContextPtr context;\n         bool is_internal_backup = false;\n         std::shared_ptr<IBackupCoordination> backup_coordination;\ndiff --git a/src/Backups/BackupIO_S3.cpp b/src/Backups/BackupIO_S3.cpp\nindex d487ec6e80e3..0e2f457d75c6 100644\n--- a/src/Backups/BackupIO_S3.cpp\n+++ b/src/Backups/BackupIO_S3.cpp\n@@ -178,7 +178,7 @@ void BackupReaderS3::copyFileToDisk(const String & path_in_backup, size_t file_s\n \n \n BackupWriterS3::BackupWriterS3(\n-    const S3::URI & s3_uri_, const String & access_key_id_, const String & secret_access_key_, bool allow_s3_native_copy, const ContextPtr & context_)\n+    const S3::URI & s3_uri_, const String & access_key_id_, const String & secret_access_key_, bool allow_s3_native_copy, const String & storage_class_name, const ContextPtr & context_)\n     : BackupWriterDefault(&Poco::Logger::get(\"BackupWriterS3\"), context_)\n     , s3_uri(s3_uri_)\n     , client(makeS3Client(s3_uri_, access_key_id_, secret_access_key_, context_))\n@@ -188,6 +188,7 @@ BackupWriterS3::BackupWriterS3(\n     request_settings.updateFromSettings(context_->getSettingsRef());\n     request_settings.max_single_read_retries = context_->getSettingsRef().s3_max_single_read_retries; // FIXME: Avoid taking value for endpoint\n     request_settings.allow_native_copy = allow_s3_native_copy;\n+    request_settings.setStorageClassName(storage_class_name);\n }\n \n void BackupWriterS3::copyFileFromDisk(const String & path_in_backup, DiskPtr src_disk, const String & src_path,\ndiff --git a/src/Backups/BackupIO_S3.h b/src/Backups/BackupIO_S3.h\nindex a93d6119786f..8015dade60dc 100644\n--- a/src/Backups/BackupIO_S3.h\n+++ b/src/Backups/BackupIO_S3.h\n@@ -38,7 +38,7 @@ class BackupReaderS3 : public BackupReaderDefault\n class BackupWriterS3 : public BackupWriterDefault\n {\n public:\n-    BackupWriterS3(const S3::URI & s3_uri_, const String & access_key_id_, const String & secret_access_key_, bool allow_s3_native_copy, const ContextPtr & context_);\n+    BackupWriterS3(const S3::URI & s3_uri_, const String & access_key_id_, const String & secret_access_key_, bool allow_s3_native_copy, const String & storage_class_name, const ContextPtr & context_);\n     ~BackupWriterS3() override;\n \n     bool fileExists(const String & file_name) override;\ndiff --git a/src/Backups/BackupSettings.cpp b/src/Backups/BackupSettings.cpp\nindex b6d776d0347d..3138959191ed 100644\n--- a/src/Backups/BackupSettings.cpp\n+++ b/src/Backups/BackupSettings.cpp\n@@ -21,6 +21,7 @@ namespace ErrorCodes\n     M(String, id) \\\n     M(String, compression_method) \\\n     M(String, password) \\\n+    M(String, s3_storage_class) \\\n     M(Bool, structure_only) \\\n     M(Bool, async) \\\n     M(Bool, decrypt_files_from_encrypted_disks) \\\ndiff --git a/src/Backups/BackupSettings.h b/src/Backups/BackupSettings.h\nindex 7cec2d9693d4..dabfe9a600f2 100644\n--- a/src/Backups/BackupSettings.h\n+++ b/src/Backups/BackupSettings.h\n@@ -25,6 +25,9 @@ struct BackupSettings\n     /// Password used to encrypt the backup.\n     String password;\n \n+    /// S3 storage class.\n+    String s3_storage_class = \"\";\n+\n     /// If this is set to true then only create queries will be written to backup,\n     /// without the data of tables.\n     bool structure_only = false;\ndiff --git a/src/Backups/BackupsWorker.cpp b/src/Backups/BackupsWorker.cpp\nindex c08b110075eb..287560d1e5cf 100644\n--- a/src/Backups/BackupsWorker.cpp\n+++ b/src/Backups/BackupsWorker.cpp\n@@ -344,6 +344,7 @@ void BackupsWorker::doBackup(\n         backup_create_params.compression_method = backup_settings.compression_method;\n         backup_create_params.compression_level = backup_settings.compression_level;\n         backup_create_params.password = backup_settings.password;\n+        backup_create_params.s3_storage_class = backup_settings.s3_storage_class;\n         backup_create_params.is_internal_backup = backup_settings.internal;\n         backup_create_params.backup_coordination = backup_coordination;\n         backup_create_params.backup_uuid = backup_settings.backup_uuid;\ndiff --git a/src/Backups/registerBackupEngineS3.cpp b/src/Backups/registerBackupEngineS3.cpp\nindex bd705e4d70fa..5b6f7825157e 100644\n--- a/src/Backups/registerBackupEngineS3.cpp\n+++ b/src/Backups/registerBackupEngineS3.cpp\n@@ -112,7 +112,7 @@ void registerBackupEngineS3(BackupFactory & factory)\n         }\n         else\n         {\n-            auto writer = std::make_shared<BackupWriterS3>(S3::URI{s3_uri}, access_key_id, secret_access_key, params.allow_s3_native_copy, params.context);\n+            auto writer = std::make_shared<BackupWriterS3>(S3::URI{s3_uri}, access_key_id, secret_access_key, params.allow_s3_native_copy, params.s3_storage_class, params.context);\n             return std::make_unique<BackupImpl>(\n                 backup_name_for_logging,\n                 archive_params,\ndiff --git a/src/Storages/StorageS3Settings.h b/src/Storages/StorageS3Settings.h\nindex 581665a7dc5d..e3d577ca0b36 100644\n--- a/src/Storages/StorageS3Settings.h\n+++ b/src/Storages/StorageS3Settings.h\n@@ -77,6 +77,8 @@ struct S3Settings\n \n         const PartUploadSettings & getUploadSettings() const { return upload_settings; }\n \n+        void setStorageClassName(const String & storage_class_name) { upload_settings.storage_class_name = storage_class_name; }\n+\n         RequestSettings() = default;\n         explicit RequestSettings(const Settings & settings);\n         explicit RequestSettings(const NamedCollection & collection);\n",
  "test_patch": "diff --git a/tests/integration/test_backup_s3_storage_class/__init__.py b/tests/integration/test_backup_s3_storage_class/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_backup_s3_storage_class/test.py b/tests/integration/test_backup_s3_storage_class/test.py\nnew file mode 100644\nindex 000000000000..2b11f20afc68\n--- /dev/null\n+++ b/tests/integration/test_backup_s3_storage_class/test.py\n@@ -0,0 +1,47 @@\n+import pytest\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+node = cluster.add_instance(\n+    \"node\",\n+    stay_alive=True,\n+    with_minio=True,\n+)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_backup_s3_storage_class(started_cluster):\n+    node.query(\n+        \"\"\"\n+            CREATE TABLE test_s3_storage_class\n+            (\n+                `id` UInt64,\n+                `value` String\n+            )\n+            ENGINE = MergeTree\n+            ORDER BY id;\n+        \"\"\",\n+    )\n+    node.query(\n+        \"\"\"\n+            INSERT INTO test_s3_storage_class VALUES (1, 'a');\n+        \"\"\",\n+    )\n+    result = node.query(\n+        \"\"\"\n+            BACKUP TABLE test_s3_storage_class TO S3('http://minio1:9001/root/data', 'minio', 'minio123')\n+            SETTINGS s3_storage_class='STANDARD';\n+        \"\"\"\n+    )\n+\n+    minio = cluster.minio_client\n+    lst = list(minio.list_objects(cluster.minio_bucket, \"data/.backup\"))\n+    assert lst[0].storage_class == \"STANDARD\"\n",
  "problem_statement": "Backup to S3 ability to define storage class\n**Use case**\r\n\r\n```\r\nBACKUP TABLE database.tbl  TO S3(....,KEY, SECRET_KEY, s3_storage_class='STANDARD_IA')\r\n```\r\n\r\n\r\n**Describe the solution you'd like**\r\n\r\nIt will be possible to define storage class in s3 string\n",
  "hints_text": "I am working on it. If it's possible please assign me to this task.\nBTW in current C++ code implementation probably it's better to use it in this format:\r\n\r\n```sql\r\nBACKUP TABLE database.tbl  TO S3(....,KEY, SECRET_KEY)\r\nSETTINGS s3_storage_class='STANDARD_IA'\r\n```\r\n\r\nHere S3 is \"backup engine\" and in C++ code all arguments of engine are expected to be literals.\r\n\r\nIn case of usage s3_storage_class with other backup engines we may throw an exception.\r\n\r\n",
  "created_at": "2023-07-27T12:36:54Z",
  "modified_files": [
    "docs/en/operations/backup.md",
    "src/Backups/BackupFactory.h",
    "src/Backups/BackupIO_S3.cpp",
    "src/Backups/BackupIO_S3.h",
    "src/Backups/BackupSettings.cpp",
    "src/Backups/BackupSettings.h",
    "src/Backups/BackupsWorker.cpp",
    "src/Backups/registerBackupEngineS3.cpp",
    "src/Storages/StorageS3Settings.h"
  ],
  "modified_test_files": [
    "b/tests/integration/test_backup_s3_storage_class/test.py"
  ]
}