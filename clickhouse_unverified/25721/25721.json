{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 25721,
  "instance_id": "ClickHouse__ClickHouse-25721",
  "issue_numbers": [
    "16250",
    "20553"
  ],
  "base_commit": "d9f55a3f316682f5bbeec043f15ebc5b5dc0207e",
  "patch": "diff --git a/src/Interpreters/InterpreterSelectQuery.cpp b/src/Interpreters/InterpreterSelectQuery.cpp\nindex 82cd692eba4c..5180892a966a 100644\n--- a/src/Interpreters/InterpreterSelectQuery.cpp\n+++ b/src/Interpreters/InterpreterSelectQuery.cpp\n@@ -1927,11 +1927,13 @@ void InterpreterSelectQuery::executeFetchColumns(QueryProcessingStage::Enum proc\n                 }\n             }\n \n+            /// If we don't have filtration, we can pushdown limit to reading stage for optimizations.\n+            UInt64 limit = (query.hasFiltration() || query.groupBy()) ? 0 : getLimitForSorting(query, context);\n             if (query_info.projection)\n                 query_info.projection->input_order_info\n-                    = query_info.projection->order_optimizer->getInputOrder(query_info.projection->desc->metadata, context);\n+                    = query_info.projection->order_optimizer->getInputOrder(query_info.projection->desc->metadata, context, limit);\n             else\n-                query_info.input_order_info = query_info.order_optimizer->getInputOrder(metadata_snapshot, context);\n+                query_info.input_order_info = query_info.order_optimizer->getInputOrder(metadata_snapshot, context, limit);\n         }\n \n         StreamLocalLimits limits;\n@@ -2289,8 +2291,14 @@ void InterpreterSelectQuery::executeOrderOptimized(QueryPlan & query_plan, Input\n {\n     const Settings & settings = context->getSettingsRef();\n \n+    const auto & query = getSelectQuery();\n     auto finish_sorting_step = std::make_unique<FinishSortingStep>(\n-        query_plan.getCurrentDataStream(), input_sorting_info->order_key_prefix_descr, output_order_descr, settings.max_block_size, limit);\n+        query_plan.getCurrentDataStream(),\n+        input_sorting_info->order_key_prefix_descr,\n+        output_order_descr,\n+        settings.max_block_size,\n+        limit,\n+        query.hasFiltration());\n \n     query_plan.addStep(std::move(finish_sorting_step));\n }\ndiff --git a/src/Parsers/ASTSelectQuery.h b/src/Parsers/ASTSelectQuery.h\nindex db4d7e763209..c382da475397 100644\n--- a/src/Parsers/ASTSelectQuery.h\n+++ b/src/Parsers/ASTSelectQuery.h\n@@ -69,6 +69,8 @@ class ASTSelectQuery : public IAST\n     const ASTPtr limitLength()    const { return getExpression(Expression::LIMIT_LENGTH); }\n     const ASTPtr settings()       const { return getExpression(Expression::SETTINGS); }\n \n+    bool hasFiltration() const { return where() || prewhere() || having(); }\n+\n     /// Set/Reset/Remove expression.\n     void setExpression(Expression expr, ASTPtr && ast);\n \ndiff --git a/src/Processors/Merges/AggregatingSortedTransform.h b/src/Processors/Merges/AggregatingSortedTransform.h\nindex a0425d4c3765..e8bf90c2b314 100644\n--- a/src/Processors/Merges/AggregatingSortedTransform.h\n+++ b/src/Processors/Merges/AggregatingSortedTransform.h\n@@ -16,7 +16,7 @@ class AggregatingSortedTransform final : public IMergingTransform<AggregatingSor\n         const Block & header, size_t num_inputs,\n         SortDescription description_, size_t max_block_size)\n         : IMergingTransform(\n-            num_inputs, header, header, true,\n+            num_inputs, header, header, /*have_all_inputs_=*/ true, /*has_limit_below_one_block_=*/ false,\n             header,\n             num_inputs,\n             std::move(description_),\ndiff --git a/src/Processors/Merges/CollapsingSortedTransform.h b/src/Processors/Merges/CollapsingSortedTransform.h\nindex 9e6bd306eee0..87c466f31e89 100644\n--- a/src/Processors/Merges/CollapsingSortedTransform.h\n+++ b/src/Processors/Merges/CollapsingSortedTransform.h\n@@ -20,7 +20,7 @@ class CollapsingSortedTransform final : public IMergingTransform<CollapsingSorte\n         WriteBuffer * out_row_sources_buf_ = nullptr,\n         bool use_average_block_sizes = false)\n         : IMergingTransform(\n-            num_inputs, header, header, true,\n+            num_inputs, header, header, /*have_all_inputs_=*/ true, /*has_limit_below_one_block_=*/ false,\n             header,\n             num_inputs,\n             std::move(description_),\ndiff --git a/src/Processors/Merges/FinishAggregatingInOrderTransform.h b/src/Processors/Merges/FinishAggregatingInOrderTransform.h\nindex 4f9e53bd7d5b..6d5e334311fa 100644\n--- a/src/Processors/Merges/FinishAggregatingInOrderTransform.h\n+++ b/src/Processors/Merges/FinishAggregatingInOrderTransform.h\n@@ -19,7 +19,7 @@ class FinishAggregatingInOrderTransform final : public IMergingTransform<FinishA\n         SortDescription description,\n         size_t max_block_size)\n         : IMergingTransform(\n-            num_inputs, header, header, true,\n+            num_inputs, header, header, /*have_all_inputs_=*/ true, /*has_limit_below_one_block_=*/ false,\n             header,\n             num_inputs,\n             params,\ndiff --git a/src/Processors/Merges/GraphiteRollupSortedTransform.h b/src/Processors/Merges/GraphiteRollupSortedTransform.h\nindex 5104801aa0d4..46272f00eeda 100644\n--- a/src/Processors/Merges/GraphiteRollupSortedTransform.h\n+++ b/src/Processors/Merges/GraphiteRollupSortedTransform.h\n@@ -15,7 +15,7 @@ class GraphiteRollupSortedTransform final : public IMergingTransform<GraphiteRol\n         SortDescription description_, size_t max_block_size,\n         Graphite::Params params_, time_t time_of_merge_)\n         : IMergingTransform(\n-            num_inputs, header, header, true,\n+            num_inputs, header, header, /*have_all_inputs_=*/ true, /*has_limit_below_one_block_=*/ false,\n             header,\n             num_inputs,\n             std::move(description_),\ndiff --git a/src/Processors/Merges/IMergingTransform.cpp b/src/Processors/Merges/IMergingTransform.cpp\nindex eff786b150f2..cba78390c97f 100644\n--- a/src/Processors/Merges/IMergingTransform.cpp\n+++ b/src/Processors/Merges/IMergingTransform.cpp\n@@ -14,9 +14,11 @@ IMergingTransformBase::IMergingTransformBase(\n     size_t num_inputs,\n     const Block & input_header,\n     const Block & output_header,\n-    bool have_all_inputs_)\n+    bool have_all_inputs_,\n+    bool has_limit_below_one_block_)\n     : IProcessor(InputPorts(num_inputs, input_header), {output_header})\n     , have_all_inputs(have_all_inputs_)\n+    , has_limit_below_one_block(has_limit_below_one_block_)\n {\n }\n \n@@ -64,10 +66,7 @@ IProcessor::Status IMergingTransformBase::prepareInitializeInputs()\n             continue;\n \n         if (input_states[i].is_initialized)\n-        {\n-            // input.setNotNeeded();\n             continue;\n-        }\n \n         input.setNeeded();\n \n@@ -77,12 +76,17 @@ IProcessor::Status IMergingTransformBase::prepareInitializeInputs()\n             continue;\n         }\n \n-        auto chunk = input.pull();\n+        /// setNotNeeded after reading first chunk, because in optimismtic case\n+        /// (e.g. with optimized 'ORDER BY primary_key LIMIT n' and small 'n')\n+        /// we won't have to read any chunks anymore;\n+        auto chunk = input.pull(has_limit_below_one_block);\n         if (!chunk.hasRows())\n         {\n-\n             if (!input.isFinished())\n+            {\n+                input.setNeeded();\n                 all_inputs_has_data = false;\n+            }\n \n             continue;\n         }\ndiff --git a/src/Processors/Merges/IMergingTransform.h b/src/Processors/Merges/IMergingTransform.h\nindex ce673131ab6f..8b0a44ae0250 100644\n--- a/src/Processors/Merges/IMergingTransform.h\n+++ b/src/Processors/Merges/IMergingTransform.h\n@@ -16,7 +16,8 @@ class IMergingTransformBase : public IProcessor\n         size_t num_inputs,\n         const Block & input_header,\n         const Block & output_header,\n-        bool have_all_inputs_);\n+        bool have_all_inputs_,\n+        bool has_limit_below_one_block_);\n \n     OutputPort & getOutputPort() { return outputs.front(); }\n \n@@ -66,6 +67,7 @@ class IMergingTransformBase : public IProcessor\n     std::vector<InputState> input_states;\n     std::atomic<bool> have_all_inputs;\n     bool is_initialized = false;\n+    bool has_limit_below_one_block = false;\n \n     IProcessor::Status prepareInitializeInputs();\n };\n@@ -81,8 +83,9 @@ class IMergingTransform : public IMergingTransformBase\n         const Block & input_header,\n         const Block & output_header,\n         bool have_all_inputs_,\n+        bool has_limit_below_one_block_,\n         Args && ... args)\n-        : IMergingTransformBase(num_inputs, input_header, output_header, have_all_inputs_)\n+        : IMergingTransformBase(num_inputs, input_header, output_header, have_all_inputs_, has_limit_below_one_block_)\n         , algorithm(std::forward<Args>(args) ...)\n     {\n     }\ndiff --git a/src/Processors/Merges/MergingSortedTransform.cpp b/src/Processors/Merges/MergingSortedTransform.cpp\nindex ec1bdc59683f..92fafa4242c0 100644\n--- a/src/Processors/Merges/MergingSortedTransform.cpp\n+++ b/src/Processors/Merges/MergingSortedTransform.cpp\n@@ -13,12 +13,13 @@ MergingSortedTransform::MergingSortedTransform(\n     SortDescription  description_,\n     size_t max_block_size,\n     UInt64 limit_,\n+    bool has_limit_below_one_block_,\n     WriteBuffer * out_row_sources_buf_,\n     bool quiet_,\n     bool use_average_block_sizes,\n     bool have_all_inputs_)\n     : IMergingTransform(\n-        num_inputs, header, header, have_all_inputs_,\n+        num_inputs, header, header, have_all_inputs_, has_limit_below_one_block_,\n         header,\n         num_inputs,\n         std::move(description_),\ndiff --git a/src/Processors/Merges/MergingSortedTransform.h b/src/Processors/Merges/MergingSortedTransform.h\nindex 93bd36d8aec9..1fa9b1275bd6 100644\n--- a/src/Processors/Merges/MergingSortedTransform.h\n+++ b/src/Processors/Merges/MergingSortedTransform.h\n@@ -17,6 +17,7 @@ class MergingSortedTransform final : public IMergingTransform<MergingSortedAlgor\n         SortDescription description,\n         size_t max_block_size,\n         UInt64 limit_ = 0,\n+        bool has_limit_below_one_block_ = false,\n         WriteBuffer * out_row_sources_buf_ = nullptr,\n         bool quiet_ = false,\n         bool use_average_block_sizes = false,\ndiff --git a/src/Processors/Merges/ReplacingSortedTransform.h b/src/Processors/Merges/ReplacingSortedTransform.h\nindex 757e19e2cbeb..e760cdf0d2bc 100644\n--- a/src/Processors/Merges/ReplacingSortedTransform.h\n+++ b/src/Processors/Merges/ReplacingSortedTransform.h\n@@ -18,7 +18,7 @@ class ReplacingSortedTransform final : public IMergingTransform<ReplacingSortedA\n         WriteBuffer * out_row_sources_buf_ = nullptr,\n         bool use_average_block_sizes = false)\n         : IMergingTransform(\n-            num_inputs, header, header, true,\n+            num_inputs, header, header, /*have_all_inputs_=*/ true, /*has_limit_below_one_block_=*/ false,\n             header,\n             num_inputs,\n             std::move(description_),\ndiff --git a/src/Processors/Merges/SummingSortedTransform.h b/src/Processors/Merges/SummingSortedTransform.h\nindex 22361bb1a448..0287caed5aa4 100644\n--- a/src/Processors/Merges/SummingSortedTransform.h\n+++ b/src/Processors/Merges/SummingSortedTransform.h\n@@ -19,7 +19,7 @@ class SummingSortedTransform final : public IMergingTransform<SummingSortedAlgor\n         const Names & partition_key_columns,\n         size_t max_block_size)\n         : IMergingTransform(\n-            num_inputs, header, header, true,\n+            num_inputs, header, header, /*have_all_inputs_=*/ true, /*has_limit_below_one_block_=*/ false,\n             header,\n             num_inputs,\n             std::move(description_),\ndiff --git a/src/Processors/Merges/VersionedCollapsingTransform.h b/src/Processors/Merges/VersionedCollapsingTransform.h\nindex f593734c6037..f260e20f1da5 100644\n--- a/src/Processors/Merges/VersionedCollapsingTransform.h\n+++ b/src/Processors/Merges/VersionedCollapsingTransform.h\n@@ -19,7 +19,7 @@ class VersionedCollapsingTransform final : public IMergingTransform<VersionedCol\n         WriteBuffer * out_row_sources_buf_ = nullptr,\n         bool use_average_block_sizes = false)\n         : IMergingTransform(\n-            num_inputs, header, header, true,\n+            num_inputs, header, header, /*have_all_inputs_=*/ true, /*has_limit_below_one_block_=*/ false,\n             header,\n             num_inputs,\n             std::move(description_),\ndiff --git a/src/Processors/QueryPlan/FinishSortingStep.cpp b/src/Processors/QueryPlan/FinishSortingStep.cpp\nindex a2e056b30295..718eeb96cd84 100644\n--- a/src/Processors/QueryPlan/FinishSortingStep.cpp\n+++ b/src/Processors/QueryPlan/FinishSortingStep.cpp\n@@ -31,12 +31,14 @@ FinishSortingStep::FinishSortingStep(\n     SortDescription prefix_description_,\n     SortDescription result_description_,\n     size_t max_block_size_,\n-    UInt64 limit_)\n+    UInt64 limit_,\n+    bool has_filtration_)\n     : ITransformingStep(input_stream_, input_stream_.header, getTraits(limit_))\n     , prefix_description(std::move(prefix_description_))\n     , result_description(std::move(result_description_))\n     , max_block_size(max_block_size_)\n     , limit(limit_)\n+    , has_filtration(has_filtration_)\n {\n     /// TODO: check input_stream is sorted by prefix_description.\n     output_stream->sort_description = result_description;\n@@ -58,11 +60,14 @@ void FinishSortingStep::transformPipeline(QueryPipeline & pipeline, const BuildQ\n     if (pipeline.getNumStreams() > 1)\n     {\n         UInt64 limit_for_merging = (need_finish_sorting ? 0 : limit);\n+        bool has_limit_below_one_block = !has_filtration && limit_for_merging && limit_for_merging < max_block_size;\n         auto transform = std::make_shared<MergingSortedTransform>(\n                 pipeline.getHeader(),\n                 pipeline.getNumStreams(),\n                 prefix_description,\n-                max_block_size, limit_for_merging);\n+                max_block_size,\n+                limit_for_merging,\n+                has_limit_below_one_block);\n \n         pipeline.addTransform(std::move(transform));\n     }\ndiff --git a/src/Processors/QueryPlan/FinishSortingStep.h b/src/Processors/QueryPlan/FinishSortingStep.h\nindex 9fe031e792d1..5ea3a6d91b51 100644\n--- a/src/Processors/QueryPlan/FinishSortingStep.h\n+++ b/src/Processors/QueryPlan/FinishSortingStep.h\n@@ -13,8 +13,9 @@ class FinishSortingStep : public ITransformingStep\n         const DataStream & input_stream_,\n         SortDescription prefix_description_,\n         SortDescription result_description_,\n-        size_t max_block_size,\n-        UInt64 limit);\n+        size_t max_block_size_,\n+        UInt64 limit_,\n+        bool has_filtration_);\n \n     String getName() const override { return \"FinishSorting\"; }\n \n@@ -31,6 +32,7 @@ class FinishSortingStep : public ITransformingStep\n     SortDescription result_description;\n     size_t max_block_size;\n     UInt64 limit;\n+    bool has_filtration;\n };\n \n }\ndiff --git a/src/Processors/QueryPlan/ReadFromMergeTree.cpp b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\nindex d633c669e07d..f8c12449c7e6 100644\n--- a/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n+++ b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n@@ -13,7 +13,7 @@\n #include <Processors/Merges/ReplacingSortedTransform.h>\n #include <Processors/Merges/SummingSortedTransform.h>\n #include <Processors/Merges/VersionedCollapsingTransform.h>\n-#include <Storages/MergeTree/MergeTreeSelectProcessor.h>\n+#include <Storages/MergeTree/MergeTreeInOrderSelectProcessor.h>\n #include <Storages/MergeTree/MergeTreeReverseSelectProcessor.h>\n #include <Storages/MergeTree/MergeTreeThreadSelectProcessor.h>\n #include <Storages/MergeTree/MergeTreeDataSelectExecutor.h>\n@@ -179,26 +179,32 @@ template<typename TSource>\n ProcessorPtr ReadFromMergeTree::createSource(\n     const RangesInDataPart & part,\n     const Names & required_columns,\n-    bool use_uncompressed_cache)\n+    bool use_uncompressed_cache,\n+    bool has_limit_below_one_block)\n {\n     return std::make_shared<TSource>(\n             data, metadata_snapshot, part.data_part, max_block_size, preferred_block_size_bytes,\n-            preferred_max_column_in_block_size_bytes, required_columns, part.ranges, use_uncompressed_cache,\n-            prewhere_info, actions_settings, true, reader_settings, virt_column_names, part.part_index_in_query);\n+            preferred_max_column_in_block_size_bytes, required_columns, part.ranges, use_uncompressed_cache, prewhere_info,\n+            actions_settings, true, reader_settings, virt_column_names, part.part_index_in_query, has_limit_below_one_block);\n }\n \n Pipe ReadFromMergeTree::readInOrder(\n     RangesInDataParts parts_with_range,\n     Names required_columns,\n     ReadType read_type,\n-    bool use_uncompressed_cache)\n+    bool use_uncompressed_cache,\n+    UInt64 limit)\n {\n     Pipes pipes;\n+    /// For reading in order it makes sense to read only\n+    /// one range per task to reduce number of read rows.\n+    bool has_limit_below_one_block = read_type != ReadType::Default && limit && limit < max_block_size;\n+\n     for (const auto & part : parts_with_range)\n     {\n         auto source = read_type == ReadType::InReverseOrder\n-                    ? createSource<MergeTreeReverseSelectProcessor>(part, required_columns, use_uncompressed_cache)\n-                    : createSource<MergeTreeSelectProcessor>(part, required_columns, use_uncompressed_cache);\n+                    ? createSource<MergeTreeReverseSelectProcessor>(part, required_columns, use_uncompressed_cache, has_limit_below_one_block)\n+                    : createSource<MergeTreeInOrderSelectProcessor>(part, required_columns, use_uncompressed_cache, has_limit_below_one_block);\n \n         pipes.emplace_back(std::move(source));\n     }\n@@ -224,7 +230,7 @@ Pipe ReadFromMergeTree::read(\n         return readFromPool(parts_with_range, required_columns, max_streams,\n                             min_marks_for_concurrent_read, use_uncompressed_cache);\n \n-    auto pipe = readInOrder(parts_with_range, required_columns, read_type, use_uncompressed_cache);\n+    auto pipe = readInOrder(parts_with_range, required_columns, read_type, use_uncompressed_cache, 0);\n \n     /// Use ConcatProcessor to concat sources together.\n     /// It is needed to read in parts order (and so in PK order) if single thread is used.\n@@ -403,7 +409,6 @@ Pipe ReadFromMergeTree::spreadMarkRangesAmongStreamsWithOrder(\n         {\n             RangesInDataPart part = parts_with_ranges.back();\n             parts_with_ranges.pop_back();\n-\n             size_t & marks_in_part = info.sum_marks_in_parts.back();\n \n             /// We will not take too few rows from a part.\n@@ -418,8 +423,13 @@ Pipe ReadFromMergeTree::spreadMarkRangesAmongStreamsWithOrder(\n \n             MarkRanges ranges_to_get_from_part;\n \n+            /// We take full part if it contains enough marks or\n+            /// if we know limit and part contains less than 'limit' rows.\n+            bool take_full_part = marks_in_part <= need_marks\n+                || (input_order_info->limit && input_order_info->limit < part.getRowsCount());\n+\n             /// We take the whole part if it is small enough.\n-            if (marks_in_part <= need_marks)\n+            if (take_full_part)\n             {\n                 ranges_to_get_from_part = part.ranges;\n \n@@ -449,6 +459,7 @@ Pipe ReadFromMergeTree::spreadMarkRangesAmongStreamsWithOrder(\n                 }\n                 parts_with_ranges.emplace_back(part);\n             }\n+\n             ranges_to_get_from_part = split_ranges(ranges_to_get_from_part, input_order_info->direction);\n             new_parts.emplace_back(part.data_part, part.part_index_in_query, std::move(ranges_to_get_from_part));\n         }\n@@ -457,8 +468,8 @@ Pipe ReadFromMergeTree::spreadMarkRangesAmongStreamsWithOrder(\n                        ? ReadFromMergeTree::ReadType::InOrder\n                        : ReadFromMergeTree::ReadType::InReverseOrder;\n \n-        pipes.emplace_back(read(std::move(new_parts), column_names, read_type,\n-                           requested_num_streams, info.min_marks_for_concurrent_read, info.use_uncompressed_cache));\n+        pipes.emplace_back(readInOrder(std::move(new_parts), column_names, read_type,\n+                                        info.use_uncompressed_cache, input_order_info->limit));\n     }\n \n     if (need_preliminary_merge)\n@@ -486,7 +497,8 @@ Pipe ReadFromMergeTree::spreadMarkRangesAmongStreamsWithOrder(\n                         pipe.getHeader(),\n                         pipe.numOutputPorts(),\n                         sort_description,\n-                        max_block_size);\n+                        max_block_size,\n+                        0, true);\n \n                 pipe.addTransform(std::move(transform));\n             }\ndiff --git a/src/Processors/QueryPlan/ReadFromMergeTree.h b/src/Processors/QueryPlan/ReadFromMergeTree.h\nindex b82e027420bb..e83746c3ff09 100644\n--- a/src/Processors/QueryPlan/ReadFromMergeTree.h\n+++ b/src/Processors/QueryPlan/ReadFromMergeTree.h\n@@ -116,10 +116,10 @@ class ReadFromMergeTree final : public ISourceStep\n \n     Pipe read(RangesInDataParts parts_with_range, Names required_columns, ReadType read_type, size_t max_streams, size_t min_marks_for_concurrent_read, bool use_uncompressed_cache);\n     Pipe readFromPool(RangesInDataParts parts_with_ranges, Names required_columns, size_t max_streams, size_t min_marks_for_concurrent_read, bool use_uncompressed_cache);\n-    Pipe readInOrder(RangesInDataParts parts_with_range, Names required_columns, ReadType read_type, bool use_uncompressed_cache);\n+    Pipe readInOrder(RangesInDataParts parts_with_range, Names required_columns, ReadType read_type, bool use_uncompressed_cache, UInt64 limit);\n \n     template<typename TSource>\n-    ProcessorPtr createSource(const RangesInDataPart & part, const Names & required_columns, bool use_uncompressed_cache);\n+    ProcessorPtr createSource(const RangesInDataPart & part, const Names & required_columns, bool use_uncompressed_cache, bool has_limit_below_one_block);\n \n     Pipe spreadMarkRangesAmongStreams(\n         RangesInDataParts && parts_with_ranges,\ndiff --git a/src/Processors/Transforms/MergeSortingTransform.cpp b/src/Processors/Transforms/MergeSortingTransform.cpp\nindex 1806693db3aa..ca78a29071e0 100644\n--- a/src/Processors/Transforms/MergeSortingTransform.cpp\n+++ b/src/Processors/Transforms/MergeSortingTransform.cpp\n@@ -200,6 +200,7 @@ void MergeSortingTransform::consume(Chunk chunk)\n                     description,\n                     max_merged_block_size,\n                     limit,\n+                    false,\n                     nullptr,\n                     quiet,\n                     use_average_block_sizes,\ndiff --git a/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.cpp\nindex 549a8a4f772d..c91d60c5de78 100644\n--- a/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.cpp\n@@ -465,6 +465,19 @@ Block MergeTreeBaseSelectProcessor::transformHeader(\n     return block;\n }\n \n+std::unique_ptr<MergeTreeBlockSizePredictor> MergeTreeBaseSelectProcessor::getSizePredictor(\n+    const MergeTreeData::DataPartPtr & data_part,\n+    const MergeTreeReadTaskColumns & task_columns,\n+    const Block & sample_block)\n+{\n+    const auto & required_column_names = task_columns.columns.getNames();\n+    const auto & required_pre_column_names = task_columns.pre_columns.getNames();\n+    NameSet complete_column_names(required_column_names.begin(), required_column_names.end());\n+    complete_column_names.insert(required_pre_column_names.begin(), required_pre_column_names.end());\n+\n+    return std::make_unique<MergeTreeBlockSizePredictor>(\n+        data_part, Names(complete_column_names.begin(), complete_column_names.end()), sample_block);\n+}\n \n MergeTreeBaseSelectProcessor::~MergeTreeBaseSelectProcessor() = default;\n \ndiff --git a/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.h b/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.h\nindex 532dc48ec1ed..d102e4f07a4e 100644\n--- a/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.h\n+++ b/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.h\n@@ -37,6 +37,11 @@ class MergeTreeBaseSelectProcessor : public SourceWithProgress\n     static Block transformHeader(\n         Block block, const PrewhereInfoPtr & prewhere_info, const DataTypePtr & partition_value_type, const Names & virtual_columns);\n \n+    static std::unique_ptr<MergeTreeBlockSizePredictor> getSizePredictor(\n+        const MergeTreeData::DataPartPtr & data_part,\n+        const MergeTreeReadTaskColumns & task_columns,\n+        const Block & sample_block);\n+\n protected:\n     Chunk generate() final;\n \ndiff --git a/src/Storages/MergeTree/MergeTreeBlockReadUtils.h b/src/Storages/MergeTree/MergeTreeBlockReadUtils.h\nindex 31d609e42426..4c4081bd83b1 100644\n--- a/src/Storages/MergeTree/MergeTreeBlockReadUtils.h\n+++ b/src/Storages/MergeTree/MergeTreeBlockReadUtils.h\n@@ -70,7 +70,7 @@ struct MergeTreeReadTaskColumns\n     /// column names to read during PREWHERE\n     NamesAndTypesList pre_columns;\n     /// resulting block may require reordering in accordance with `ordered_names`\n-    bool should_reorder;\n+    bool should_reorder = false;\n };\n \n MergeTreeReadTaskColumns getReadTaskColumns(\ndiff --git a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\nindex a777c244426d..6279d2d7d6f8 100644\n--- a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n@@ -894,7 +894,7 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor\n     {\n         case MergeTreeData::MergingParams::Ordinary:\n             merged_transform = std::make_unique<MergingSortedTransform>(\n-                header, pipes.size(), sort_description, merge_block_size, 0, rows_sources_write_buf.get(), true, blocks_are_granules_size);\n+                header, pipes.size(), sort_description, merge_block_size, 0, false, rows_sources_write_buf.get(), true, blocks_are_granules_size);\n             break;\n \n         case MergeTreeData::MergingParams::Collapsing:\ndiff --git a/src/Storages/MergeTree/MergeTreeInOrderSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeInOrderSelectProcessor.cpp\nnew file mode 100644\nindex 000000000000..48a9d62d8726\n--- /dev/null\n+++ b/src/Storages/MergeTree/MergeTreeInOrderSelectProcessor.cpp\n@@ -0,0 +1,54 @@\n+#include <Storages/MergeTree/MergeTreeInOrderSelectProcessor.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int MEMORY_LIMIT_EXCEEDED;\n+}\n+\n+bool MergeTreeInOrderSelectProcessor::getNewTask()\n+try\n+{\n+    if (all_mark_ranges.empty())\n+    {\n+        finish();\n+        return false;\n+    }\n+\n+    if (!reader)\n+        initializeReaders();\n+\n+    MarkRanges mark_ranges_for_task;\n+    /// If we need to read few rows, set one range per task to reduce number of read data.\n+    if (has_limit_below_one_block)\n+    {\n+        mark_ranges_for_task = { std::move(all_mark_ranges.front()) };\n+        all_mark_ranges.pop_front();\n+    }\n+    else\n+    {\n+        mark_ranges_for_task = std::move(all_mark_ranges);\n+        all_mark_ranges.clear();\n+    }\n+\n+    auto size_predictor = (preferred_block_size_bytes == 0) ? nullptr\n+        : getSizePredictor(data_part, task_columns, sample_block);\n+\n+    task = std::make_unique<MergeTreeReadTask>(\n+        data_part, mark_ranges_for_task, part_index_in_query, ordered_names, column_name_set, task_columns.columns,\n+        task_columns.pre_columns, prewhere_info && prewhere_info->remove_prewhere_column,\n+        task_columns.should_reorder, std::move(size_predictor));\n+\n+    return true;\n+}\n+catch (...)\n+{\n+    /// Suspicion of the broken part. A part is added to the queue for verification.\n+    if (getCurrentExceptionCode() != ErrorCodes::MEMORY_LIMIT_EXCEEDED)\n+        storage.reportBrokenPart(data_part->name);\n+    throw;\n+}\n+\n+}\ndiff --git a/src/Storages/MergeTree/MergeTreeInOrderSelectProcessor.h b/src/Storages/MergeTree/MergeTreeInOrderSelectProcessor.h\nnew file mode 100644\nindex 000000000000..ecf648b02916\n--- /dev/null\n+++ b/src/Storages/MergeTree/MergeTreeInOrderSelectProcessor.h\n@@ -0,0 +1,31 @@\n+#pragma once\n+#include <Storages/MergeTree/MergeTreeSelectProcessor.h>\n+\n+namespace DB\n+{\n+\n+\n+/// Used to read data from single part with select query in order of primary key.\n+/// Cares about PREWHERE, virtual columns, indexes etc.\n+/// To read data from multiple parts, Storage (MergeTree) creates multiple such objects.\n+class MergeTreeInOrderSelectProcessor final : public MergeTreeSelectProcessor\n+{\n+public:\n+    template <typename... Args>\n+    MergeTreeInOrderSelectProcessor(Args &&... args)\n+        : MergeTreeSelectProcessor{std::forward<Args>(args)...}\n+    {\n+        LOG_DEBUG(log, \"Reading {} ranges in order from part {}, approx. {} rows starting from {}\",\n+            all_mark_ranges.size(), data_part->name, total_rows,\n+            data_part->index_granularity.getMarkStartingRow(all_mark_ranges.front().begin));\n+    }\n+\n+    String getName() const override { return \"MergeTreeInOrder\"; }\n+\n+private:\n+    bool getNewTask() override;\n+\n+    Poco::Logger * log = &Poco::Logger::get(\"MergeTreeInOrderSelectProcessor\");\n+};\n+\n+}\ndiff --git a/src/Storages/MergeTree/MergeTreeReadPool.cpp b/src/Storages/MergeTree/MergeTreeReadPool.cpp\nindex f5ae51626763..b18f22c3ab13 100644\n--- a/src/Storages/MergeTree/MergeTreeReadPool.cpp\n+++ b/src/Storages/MergeTree/MergeTreeReadPool.cpp\n@@ -228,29 +228,20 @@ std::vector<size_t> MergeTreeReadPool::fillPerPartInfo(\n \n         per_part_sum_marks.push_back(sum_marks);\n \n-        auto [required_columns, required_pre_columns, should_reorder] =\n-            getReadTaskColumns(data, metadata_snapshot, part.data_part, column_names, prewhere_info, check_columns);\n+        auto task_columns = getReadTaskColumns(data, metadata_snapshot, part.data_part, column_names, prewhere_info, check_columns);\n \n-        if (predict_block_size_bytes)\n-        {\n-            const auto & required_column_names = required_columns.getNames();\n-            const auto & required_pre_column_names = required_pre_columns.getNames();\n-            NameSet complete_column_names(required_column_names.begin(), required_column_names.end());\n-            complete_column_names.insert(required_pre_column_names.begin(), required_pre_column_names.end());\n+        auto size_predictor = !predict_block_size_bytes ? nullptr\n+            : MergeTreeBaseSelectProcessor::getSizePredictor(part.data_part, task_columns, sample_block);\n \n-            per_part_size_predictor.emplace_back(std::make_unique<MergeTreeBlockSizePredictor>(\n-                part.data_part, Names(complete_column_names.begin(), complete_column_names.end()), sample_block));\n-        }\n-        else\n-            per_part_size_predictor.emplace_back(nullptr);\n+        per_part_size_predictor.emplace_back(std::move(size_predictor));\n \n         /// will be used to distinguish between PREWHERE and WHERE columns when applying filter\n-        const auto & required_column_names = required_columns.getNames();\n+        const auto & required_column_names = task_columns.columns.getNames();\n         per_part_column_name_set.emplace_back(required_column_names.begin(), required_column_names.end());\n \n-        per_part_pre_columns.push_back(std::move(required_pre_columns));\n-        per_part_columns.push_back(std::move(required_columns));\n-        per_part_should_reorder.push_back(should_reorder);\n+        per_part_pre_columns.push_back(std::move(task_columns.pre_columns));\n+        per_part_columns.push_back(std::move(task_columns.columns));\n+        per_part_should_reorder.push_back(task_columns.should_reorder);\n \n         parts_with_idx.push_back({ part.data_part, part.part_index_in_query });\n     }\ndiff --git a/src/Storages/MergeTree/MergeTreeReverseSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeReverseSelectProcessor.cpp\nindex d546b2a95afe..16ce9823ebbf 100644\n--- a/src/Storages/MergeTree/MergeTreeReverseSelectProcessor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeReverseSelectProcessor.cpp\n@@ -1,8 +1,4 @@\n #include <Storages/MergeTree/MergeTreeReverseSelectProcessor.h>\n-#include <Storages/MergeTree/MergeTreeBaseSelectProcessor.h>\n-#include <Storages/MergeTree/IMergeTreeReader.h>\n-#include <Interpreters/Context.h>\n-\n \n namespace DB\n {\n@@ -12,74 +8,10 @@ namespace ErrorCodes\n     extern const int MEMORY_LIMIT_EXCEEDED;\n }\n \n-MergeTreeReverseSelectProcessor::MergeTreeReverseSelectProcessor(\n-    const MergeTreeData & storage_,\n-    const StorageMetadataPtr & metadata_snapshot_,\n-    const MergeTreeData::DataPartPtr & owned_data_part_,\n-    UInt64 max_block_size_rows_,\n-    size_t preferred_block_size_bytes_,\n-    size_t preferred_max_column_in_block_size_bytes_,\n-    Names required_columns_,\n-    MarkRanges mark_ranges_,\n-    bool use_uncompressed_cache_,\n-    const PrewhereInfoPtr & prewhere_info_,\n-    ExpressionActionsSettings actions_settings,\n-    bool check_columns,\n-    const MergeTreeReaderSettings & reader_settings_,\n-    const Names & virt_column_names_,\n-    size_t part_index_in_query_,\n-    bool quiet)\n-    :\n-    MergeTreeBaseSelectProcessor{\n-        metadata_snapshot_->getSampleBlockForColumns(required_columns_, storage_.getVirtuals(), storage_.getStorageID()),\n-        storage_, metadata_snapshot_, prewhere_info_, std::move(actions_settings), max_block_size_rows_,\n-        preferred_block_size_bytes_, preferred_max_column_in_block_size_bytes_,\n-        reader_settings_, use_uncompressed_cache_, virt_column_names_},\n-    required_columns{std::move(required_columns_)},\n-    data_part{owned_data_part_},\n-    all_mark_ranges(std::move(mark_ranges_)),\n-    part_index_in_query(part_index_in_query_),\n-    path(data_part->getFullRelativePath())\n-{\n-    /// Let's estimate total number of rows for progress bar.\n-    for (const auto & range : all_mark_ranges)\n-        total_marks_count += range.end - range.begin;\n-\n-    size_t total_rows = data_part->index_granularity.getRowsCountInRanges(all_mark_ranges);\n-\n-    if (!quiet)\n-        LOG_DEBUG(log, \"Reading {} ranges in reverse order from part {}, approx. {} rows starting from {}\",\n-            all_mark_ranges.size(), data_part->name, total_rows,\n-            data_part->index_granularity.getMarkStartingRow(all_mark_ranges.front().begin));\n-\n-    addTotalRowsApprox(total_rows);\n-\n-    ordered_names = header_without_virtual_columns.getNames();\n-\n-    task_columns = getReadTaskColumns(storage, metadata_snapshot, data_part, required_columns, prewhere_info, check_columns);\n-\n-    /// will be used to distinguish between PREWHERE and WHERE columns when applying filter\n-    const auto & column_names = task_columns.columns.getNames();\n-    column_name_set = NameSet{column_names.begin(), column_names.end()};\n-\n-    if (use_uncompressed_cache)\n-        owned_uncompressed_cache = storage.getContext()->getUncompressedCache();\n-\n-    owned_mark_cache = storage.getContext()->getMarkCache();\n-\n-    reader = data_part->getReader(task_columns.columns, metadata_snapshot,\n-        all_mark_ranges, owned_uncompressed_cache.get(),\n-        owned_mark_cache.get(), reader_settings);\n-\n-    if (prewhere_info)\n-        pre_reader = data_part->getReader(task_columns.pre_columns, metadata_snapshot, all_mark_ranges,\n-            owned_uncompressed_cache.get(), owned_mark_cache.get(), reader_settings);\n-}\n-\n bool MergeTreeReverseSelectProcessor::getNewTask()\n try\n {\n-    if ((chunks.empty() && all_mark_ranges.empty()) || total_marks_count == 0)\n+    if (chunks.empty() && all_mark_ranges.empty())\n     {\n         finish();\n         return false;\n@@ -90,21 +22,15 @@ try\n     if (all_mark_ranges.empty())\n         return true;\n \n+    if (!reader)\n+        initializeReaders();\n+\n     /// Read ranges from right to left.\n     MarkRanges mark_ranges_for_task = { all_mark_ranges.back() };\n     all_mark_ranges.pop_back();\n \n-    std::unique_ptr<MergeTreeBlockSizePredictor> size_predictor;\n-    if (preferred_block_size_bytes)\n-    {\n-        const auto & required_column_names = task_columns.columns.getNames();\n-        const auto & required_pre_column_names = task_columns.pre_columns.getNames();\n-        NameSet complete_column_names(required_column_names.begin(), required_column_names.end());\n-        complete_column_names.insert(required_pre_column_names.begin(), required_pre_column_names.end());\n-\n-        size_predictor = std::make_unique<MergeTreeBlockSizePredictor>(\n-            data_part, Names(complete_column_names.begin(), complete_column_names.end()), metadata_snapshot->getSampleBlock());\n-    }\n+    auto size_predictor = (preferred_block_size_bytes == 0) ? nullptr\n+        : getSizePredictor(data_part, task_columns, sample_block);\n \n     task = std::make_unique<MergeTreeReadTask>(\n         data_part, mark_ranges_for_task, part_index_in_query, ordered_names, column_name_set,\n@@ -150,17 +76,4 @@ Chunk MergeTreeReverseSelectProcessor::readFromPart()\n     return res;\n }\n \n-void MergeTreeReverseSelectProcessor::finish()\n-{\n-    /** Close the files (before destroying the object).\n-    * When many sources are created, but simultaneously reading only a few of them,\n-    * buffers don't waste memory.\n-    */\n-    reader.reset();\n-    pre_reader.reset();\n-    data_part.reset();\n-}\n-\n-MergeTreeReverseSelectProcessor::~MergeTreeReverseSelectProcessor() = default;\n-\n }\ndiff --git a/src/Storages/MergeTree/MergeTreeReverseSelectProcessor.h b/src/Storages/MergeTree/MergeTreeReverseSelectProcessor.h\nindex a76564302a4a..18ab51c03a02 100644\n--- a/src/Storages/MergeTree/MergeTreeReverseSelectProcessor.h\n+++ b/src/Storages/MergeTree/MergeTreeReverseSelectProcessor.h\n@@ -1,77 +1,33 @@\n #pragma once\n-\n-#include <Storages/MergeTree/MergeTreeThreadSelectProcessor.h>\n-#include <Storages/MergeTree/MergeTreeData.h>\n-#include <Storages/MergeTree/MarkRange.h>\n-#include <Storages/MergeTree/MergeTreeBlockReadUtils.h>\n-#include <Storages/SelectQueryInfo.h>\n+#include <Storages/MergeTree/MergeTreeSelectProcessor.h>\n \n \n namespace DB\n {\n \n-\n /// Used to read data from single part with select query\n+/// in reverse order of primary key.\n /// Cares about PREWHERE, virtual columns, indexes etc.\n /// To read data from multiple parts, Storage (MergeTree) creates multiple such objects.\n-class MergeTreeReverseSelectProcessor : public MergeTreeBaseSelectProcessor\n+class MergeTreeReverseSelectProcessor final : public MergeTreeSelectProcessor\n {\n public:\n-    MergeTreeReverseSelectProcessor(\n-        const MergeTreeData & storage,\n-        const StorageMetadataPtr & metadata_snapshot,\n-        const MergeTreeData::DataPartPtr & owned_data_part,\n-        UInt64 max_block_size_rows,\n-        size_t preferred_block_size_bytes,\n-        size_t preferred_max_column_in_block_size_bytes,\n-        Names required_columns_,\n-        MarkRanges mark_ranges,\n-        bool use_uncompressed_cache,\n-        const PrewhereInfoPtr & prewhere_info,\n-        ExpressionActionsSettings actions_settings,\n-        bool check_columns,\n-        const MergeTreeReaderSettings & reader_settings,\n-        const Names & virt_column_names = {},\n-        size_t part_index_in_query = 0,\n-        bool quiet = false);\n-\n-    ~MergeTreeReverseSelectProcessor() override;\n+    template <typename... Args>\n+    MergeTreeReverseSelectProcessor(Args &&... args)\n+        : MergeTreeSelectProcessor{std::forward<Args>(args)...}\n+    {\n+        LOG_DEBUG(log, \"Reading {} ranges in reverse order from part {}, approx. {} rows starting from {}\",\n+            all_mark_ranges.size(), data_part->name, total_rows,\n+            data_part->index_granularity.getMarkStartingRow(all_mark_ranges.front().begin));\n+    }\n \n     String getName() const override { return \"MergeTreeReverse\"; }\n \n-    /// Closes readers and unlock part locks\n-    void finish();\n-\n-protected:\n-\n+private:\n     bool getNewTask() override;\n     Chunk readFromPart() override;\n \n-private:\n-    Block header;\n-\n-    /// Used by Task\n-    Names required_columns;\n-    /// Names from header. Used in order to order columns in read blocks.\n-    Names ordered_names;\n-    NameSet column_name_set;\n-\n-    MergeTreeReadTaskColumns task_columns;\n-\n-    /// Data part will not be removed if the pointer owns it\n-    MergeTreeData::DataPartPtr data_part;\n-\n-    /// Mark ranges we should read (in ascending order)\n-    MarkRanges all_mark_ranges;\n-    /// Total number of marks we should read\n-    size_t total_marks_count = 0;\n-    /// Value of _part_index virtual column (used only in SelectExecutor)\n-    size_t part_index_in_query = 0;\n-\n-    String path;\n-\n     Chunks chunks;\n-\n     Poco::Logger * log = &Poco::Logger::get(\"MergeTreeReverseSelectProcessor\");\n };\n \ndiff --git a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\nindex 1e4b61e13d9e..98077605f89e 100644\n--- a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\n@@ -7,11 +7,6 @@\n namespace DB\n {\n \n-namespace ErrorCodes\n-{\n-    extern const int MEMORY_LIMIT_EXCEEDED;\n-}\n-\n MergeTreeSelectProcessor::MergeTreeSelectProcessor(\n     const MergeTreeData & storage_,\n     const StorageMetadataPtr & metadata_snapshot_,\n@@ -28,97 +23,49 @@ MergeTreeSelectProcessor::MergeTreeSelectProcessor(\n     const MergeTreeReaderSettings & reader_settings_,\n     const Names & virt_column_names_,\n     size_t part_index_in_query_,\n-    bool quiet)\n-    :\n-    MergeTreeBaseSelectProcessor{\n+    bool has_limit_below_one_block_)\n+    : MergeTreeBaseSelectProcessor{\n         metadata_snapshot_->getSampleBlockForColumns(required_columns_, storage_.getVirtuals(), storage_.getStorageID()),\n         storage_, metadata_snapshot_, prewhere_info_, std::move(actions_settings), max_block_size_rows_,\n         preferred_block_size_bytes_, preferred_max_column_in_block_size_bytes_,\n         reader_settings_, use_uncompressed_cache_, virt_column_names_},\n     required_columns{std::move(required_columns_)},\n     data_part{owned_data_part_},\n+    sample_block(metadata_snapshot_->getSampleBlock()),\n     all_mark_ranges(std::move(mark_ranges_)),\n     part_index_in_query(part_index_in_query_),\n-    check_columns(check_columns_)\n+    has_limit_below_one_block(has_limit_below_one_block_),\n+    check_columns(check_columns_),\n+    total_rows(data_part->index_granularity.getRowsCountInRanges(all_mark_ranges))\n {\n-    /// Let's estimate total number of rows for progress bar.\n-    for (const auto & range : all_mark_ranges)\n-        total_marks_count += range.end - range.begin;\n-\n-    size_t total_rows = data_part->index_granularity.getRowsCountInRanges(all_mark_ranges);\n-\n-    if (!quiet)\n-        LOG_DEBUG(log, \"Reading {} ranges from part {}, approx. {} rows starting from {}\",\n-            all_mark_ranges.size(), data_part->name, total_rows,\n-            data_part->index_granularity.getMarkStartingRow(all_mark_ranges.front().begin));\n-\n     addTotalRowsApprox(total_rows);\n     ordered_names = header_without_virtual_columns.getNames();\n }\n \n-\n-bool MergeTreeSelectProcessor::getNewTask()\n-try\n+void MergeTreeSelectProcessor::initializeReaders()\n {\n-    /// Produce no more than one task\n-    if (!is_first_task || total_marks_count == 0)\n-    {\n-        finish();\n-        return false;\n-    }\n-    is_first_task = false;\n-\n     task_columns = getReadTaskColumns(\n         storage, metadata_snapshot, data_part,\n         required_columns, prewhere_info, check_columns);\n \n-    std::unique_ptr<MergeTreeBlockSizePredictor> size_predictor;\n-    if (preferred_block_size_bytes)\n-    {\n-        const auto & required_column_names = task_columns.columns.getNames();\n-        const auto & required_pre_column_names = task_columns.pre_columns.getNames();\n-        NameSet complete_column_names(required_column_names.begin(), required_column_names.end());\n-        complete_column_names.insert(required_pre_column_names.begin(), required_pre_column_names.end());\n-\n-        size_predictor = std::make_unique<MergeTreeBlockSizePredictor>(\n-            data_part, Names(complete_column_names.begin(), complete_column_names.end()), metadata_snapshot->getSampleBlock());\n-    }\n-\n-    /// will be used to distinguish between PREWHERE and WHERE columns when applying filter\n+    /// Will be used to distinguish between PREWHERE and WHERE columns when applying filter\n     const auto & column_names = task_columns.columns.getNames();\n     column_name_set = NameSet{column_names.begin(), column_names.end()};\n \n-    task = std::make_unique<MergeTreeReadTask>(\n-        data_part, all_mark_ranges, part_index_in_query, ordered_names, column_name_set, task_columns.columns,\n-        task_columns.pre_columns, prewhere_info && prewhere_info->remove_prewhere_column,\n-        task_columns.should_reorder, std::move(size_predictor));\n+    if (use_uncompressed_cache)\n+        owned_uncompressed_cache = storage.getContext()->getUncompressedCache();\n \n-    if (!reader)\n-    {\n-        if (use_uncompressed_cache)\n-            owned_uncompressed_cache = storage.getContext()->getUncompressedCache();\n+    owned_mark_cache = storage.getContext()->getMarkCache();\n \n-        owned_mark_cache = storage.getContext()->getMarkCache();\n+    reader = data_part->getReader(task_columns.columns, metadata_snapshot, all_mark_ranges,\n+        owned_uncompressed_cache.get(), owned_mark_cache.get(), reader_settings);\n \n-        reader = data_part->getReader(task_columns.columns, metadata_snapshot, all_mark_ranges,\n+    if (prewhere_info)\n+        pre_reader = data_part->getReader(task_columns.pre_columns, metadata_snapshot, all_mark_ranges,\n             owned_uncompressed_cache.get(), owned_mark_cache.get(), reader_settings);\n \n-        if (prewhere_info)\n-            pre_reader = data_part->getReader(task_columns.pre_columns, metadata_snapshot, all_mark_ranges,\n-                owned_uncompressed_cache.get(), owned_mark_cache.get(), reader_settings);\n-    }\n-\n-    return true;\n-}\n-catch (...)\n-{\n-    /// Suspicion of the broken part. A part is added to the queue for verification.\n-    if (getCurrentExceptionCode() != ErrorCodes::MEMORY_LIMIT_EXCEEDED)\n-        storage.reportBrokenPart(data_part->name);\n-    throw;\n }\n \n-\n void MergeTreeSelectProcessor::finish()\n {\n     /** Close the files (before destroying the object).\n@@ -130,8 +77,6 @@ void MergeTreeSelectProcessor::finish()\n     data_part.reset();\n }\n \n-\n MergeTreeSelectProcessor::~MergeTreeSelectProcessor() = default;\n \n-\n }\ndiff --git a/src/Storages/MergeTree/MergeTreeSelectProcessor.h b/src/Storages/MergeTree/MergeTreeSelectProcessor.h\nindex e9712bd5aded..ea4cd349cbaa 100644\n--- a/src/Storages/MergeTree/MergeTreeSelectProcessor.h\n+++ b/src/Storages/MergeTree/MergeTreeSelectProcessor.h\n@@ -1,5 +1,5 @@\n #pragma once\n-#include <Storages/MergeTree/MergeTreeThreadSelectProcessor.h>\n+#include <Storages/MergeTree/MergeTreeBaseSelectProcessor.h>\n #include <Storages/MergeTree/MergeTreeData.h>\n #include <Storages/MergeTree/MarkRange.h>\n #include <Storages/MergeTree/MergeTreeBlockReadUtils.h>\n@@ -28,24 +28,21 @@ class MergeTreeSelectProcessor : public MergeTreeBaseSelectProcessor\n         bool use_uncompressed_cache,\n         const PrewhereInfoPtr & prewhere_info,\n         ExpressionActionsSettings actions_settings,\n-        bool check_columns,\n+        bool check_columns_,\n         const MergeTreeReaderSettings & reader_settings,\n         const Names & virt_column_names = {},\n-        size_t part_index_in_query = 0,\n-        bool quiet = false);\n+        size_t part_index_in_query_ = 0,\n+        bool has_limit_below_one_block_ = false);\n \n     ~MergeTreeSelectProcessor() override;\n \n-    String getName() const override { return \"MergeTree\"; }\n-\n     /// Closes readers and unlock part locks\n     void finish();\n \n protected:\n-\n-    bool getNewTask() override;\n-\n-private:\n+    /// Defer initialization from constructor, because it may be heavy\n+    /// and it's better to do it lazily in `getNewTask`, which is executing in parallel.\n+    void initializeReaders();\n \n     /// Used by Task\n     Names required_columns;\n@@ -58,17 +55,19 @@ class MergeTreeSelectProcessor : public MergeTreeBaseSelectProcessor\n     /// Data part will not be removed if the pointer owns it\n     MergeTreeData::DataPartPtr data_part;\n \n+    /// Cache getSampleBlock call, which might be heavy.\n+    Block sample_block;\n+\n     /// Mark ranges we should read (in ascending order)\n     MarkRanges all_mark_ranges;\n-    /// Total number of marks we should read\n-    size_t total_marks_count = 0;\n     /// Value of _part_index virtual column (used only in SelectExecutor)\n     size_t part_index_in_query = 0;\n+    /// If true, every task will be created only with one range.\n+    /// It reduces amount of read data for queries with small LIMIT.\n+    bool has_limit_below_one_block = false;\n \n     bool check_columns;\n-    bool is_first_task = true;\n-\n-    Poco::Logger * log = &Poco::Logger::get(\"MergeTreeSelectProcessor\");\n+    size_t total_rows = 0;\n };\n \n }\ndiff --git a/src/Storages/ReadInOrderOptimizer.cpp b/src/Storages/ReadInOrderOptimizer.cpp\nindex 87273330b345..912d284bfc09 100644\n--- a/src/Storages/ReadInOrderOptimizer.cpp\n+++ b/src/Storages/ReadInOrderOptimizer.cpp\n@@ -37,7 +37,7 @@ ReadInOrderOptimizer::ReadInOrderOptimizer(\n     array_join_result_to_source = syntax_result->array_join_result_to_source;\n }\n \n-InputOrderInfoPtr ReadInOrderOptimizer::getInputOrder(const StorageMetadataPtr & metadata_snapshot, ContextPtr context) const\n+InputOrderInfoPtr ReadInOrderOptimizer::getInputOrder(const StorageMetadataPtr & metadata_snapshot, ContextPtr context, UInt64 limit) const\n {\n     Names sorting_key_columns = metadata_snapshot->getSortingKeyColumns();\n     if (!metadata_snapshot->hasSortingKey())\n@@ -155,7 +155,8 @@ InputOrderInfoPtr ReadInOrderOptimizer::getInputOrder(const StorageMetadataPtr &\n \n     if (order_key_prefix_descr.empty())\n         return {};\n-    return std::make_shared<InputOrderInfo>(std::move(order_key_prefix_descr), read_direction);\n+\n+    return std::make_shared<InputOrderInfo>(std::move(order_key_prefix_descr), read_direction, limit);\n }\n \n }\ndiff --git a/src/Storages/ReadInOrderOptimizer.h b/src/Storages/ReadInOrderOptimizer.h\nindex 0abf2923a981..2686d0818559 100644\n--- a/src/Storages/ReadInOrderOptimizer.h\n+++ b/src/Storages/ReadInOrderOptimizer.h\n@@ -22,7 +22,7 @@ class ReadInOrderOptimizer\n         const SortDescription & required_sort_description,\n         const TreeRewriterResultPtr & syntax_result);\n \n-    InputOrderInfoPtr getInputOrder(const StorageMetadataPtr & metadata_snapshot, ContextPtr context) const;\n+    InputOrderInfoPtr getInputOrder(const StorageMetadataPtr & metadata_snapshot, ContextPtr context, UInt64 limit = 0) const;\n \n private:\n     /// Actions for every element of order expression to analyze functions for monotonicity\ndiff --git a/src/Storages/SelectQueryInfo.h b/src/Storages/SelectQueryInfo.h\nindex cf2c4d72f591..3b3c0fa12588 100644\n--- a/src/Storages/SelectQueryInfo.h\n+++ b/src/Storages/SelectQueryInfo.h\n@@ -83,9 +83,10 @@ struct InputOrderInfo\n {\n     SortDescription order_key_prefix_descr;\n     int direction;\n+    UInt64 limit;\n \n-    InputOrderInfo(const SortDescription & order_key_prefix_descr_, int direction_)\n-        : order_key_prefix_descr(order_key_prefix_descr_), direction(direction_) {}\n+    InputOrderInfo(const SortDescription & order_key_prefix_descr_, int direction_, UInt64 limit_)\n+        : order_key_prefix_descr(order_key_prefix_descr_), direction(direction_), limit(limit_) {}\n \n     bool operator ==(const InputOrderInfo & other) const\n     {\ndiff --git a/src/Storages/ya.make b/src/Storages/ya.make\nindex e794124362bb..476449e8e6cb 100644\n--- a/src/Storages/ya.make\n+++ b/src/Storages/ya.make\n@@ -58,6 +58,7 @@ SRCS(\n     MergeTree/MergeTreeDataSelectExecutor.cpp\n     MergeTree/MergeTreeDataWriter.cpp\n     MergeTree/MergeTreeDeduplicationLog.cpp\n+    MergeTree/MergeTreeInOrderSelectProcessor.cpp\n     MergeTree/MergeTreeIndexAggregatorBloomFilter.cpp\n     MergeTree/MergeTreeIndexBloomFilter.cpp\n     MergeTree/MergeTreeIndexConditionBloomFilter.cpp\n",
  "test_patch": "diff --git a/src/DataStreams/tests/gtest_blocks_size_merging_streams.cpp b/src/DataStreams/tests/gtest_blocks_size_merging_streams.cpp\nindex ed1ac6adbff3..aa4c717a28b2 100644\n--- a/src/DataStreams/tests/gtest_blocks_size_merging_streams.cpp\n+++ b/src/DataStreams/tests/gtest_blocks_size_merging_streams.cpp\n@@ -83,7 +83,7 @@ TEST(MergingSortedTest, SimpleBlockSizeTest)\n     EXPECT_EQ(pipe.numOutputPorts(), 3);\n \n     auto transform = std::make_shared<MergingSortedTransform>(pipe.getHeader(), pipe.numOutputPorts(), sort_description,\n-            DEFAULT_MERGE_BLOCK_SIZE, 0, nullptr, false, true);\n+            DEFAULT_MERGE_BLOCK_SIZE, 0, false, nullptr, false, true);\n \n     pipe.addTransform(std::move(transform));\n \n@@ -128,7 +128,7 @@ TEST(MergingSortedTest, MoreInterestingBlockSizes)\n     EXPECT_EQ(pipe.numOutputPorts(), 3);\n \n     auto transform = std::make_shared<MergingSortedTransform>(pipe.getHeader(), pipe.numOutputPorts(), sort_description,\n-            DEFAULT_MERGE_BLOCK_SIZE, 0, nullptr, false, true);\n+            DEFAULT_MERGE_BLOCK_SIZE, 0, false, nullptr, false, true);\n \n     pipe.addTransform(std::move(transform));\n \ndiff --git a/tests/queries/0_stateless/01551_mergetree_read_in_order_spread.reference b/tests/queries/0_stateless/01551_mergetree_read_in_order_spread.reference\nindex 2843b305f0ab..cdc595a3c57c 100644\n--- a/tests/queries/0_stateless/01551_mergetree_read_in_order_spread.reference\n+++ b/tests/queries/0_stateless/01551_mergetree_read_in_order_spread.reference\n@@ -9,9 +9,9 @@ ExpressionTransform\n           (SettingQuotaAndLimits)\n             (ReadFromMergeTree)\n             ExpressionTransform \u00d7 4\n-              MergeTree 0 \u2192 1\n+              MergeTreeInOrder 0 \u2192 1\n                 MergingSortedTransform 2 \u2192 1\n                   ExpressionTransform \u00d7 2\n-                    MergeTree \u00d7 2 0 \u2192 1\n+                    MergeTreeInOrder \u00d7 2 0 \u2192 1\n                       ExpressionTransform\n-                        MergeTree 0 \u2192 1\n+                        MergeTreeInOrder 0 \u2192 1\ndiff --git a/tests/queries/0_stateless/01861_explain_pipeline.reference b/tests/queries/0_stateless/01861_explain_pipeline.reference\nindex 9d62fb9f6b82..63ba55f5a045 100644\n--- a/tests/queries/0_stateless/01861_explain_pipeline.reference\n+++ b/tests/queries/0_stateless/01861_explain_pipeline.reference\n@@ -5,7 +5,7 @@ ExpressionTransform\n     ExpressionTransform\n       ReplacingSorted 2 \u2192 1\n         ExpressionTransform \u00d7 2\n-          MergeTree \u00d7 2 0 \u2192 1\n+          MergeTreeInOrder \u00d7 2 0 \u2192 1\n 0\t0\n 1\t1\n 2\t2\n@@ -22,4 +22,4 @@ ExpressionTransform \u00d7 2\n         Copy \u00d7 2 1 \u2192 2\n           AddingSelector \u00d7 2\n             ExpressionTransform \u00d7 2\n-              MergeTree \u00d7 2 0 \u2192 1\n+              MergeTreeInOrder \u00d7 2 0 \u2192 1\ndiff --git a/tests/queries/0_stateless/01926_order_by_desc_limit.reference b/tests/queries/0_stateless/01926_order_by_desc_limit.reference\nnew file mode 100644\nindex 000000000000..6ed281c757a9\n--- /dev/null\n+++ b/tests/queries/0_stateless/01926_order_by_desc_limit.reference\n@@ -0,0 +1,2 @@\n+1\n+1\ndiff --git a/tests/queries/0_stateless/01926_order_by_desc_limit.sql b/tests/queries/0_stateless/01926_order_by_desc_limit.sql\nnew file mode 100644\nindex 000000000000..7ea102e11e96\n--- /dev/null\n+++ b/tests/queries/0_stateless/01926_order_by_desc_limit.sql\n@@ -0,0 +1,21 @@\n+DROP TABLE IF EXISTS order_by_desc;\n+\n+CREATE TABLE order_by_desc (u UInt32, s String)\n+ENGINE MergeTree ORDER BY u PARTITION BY u % 100\n+SETTINGS index_granularity = 1024;\n+\n+INSERT INTO order_by_desc SELECT number, repeat('a', 1024) FROM numbers(1024 * 300);\n+OPTIMIZE TABLE order_by_desc FINAL;\n+\n+SELECT s FROM order_by_desc ORDER BY u DESC LIMIT 10 FORMAT Null\n+SETTINGS max_memory_usage = '400M';\n+\n+SELECT s FROM order_by_desc ORDER BY u LIMIT 10 FORMAT Null\n+SETTINGS max_memory_usage = '400M';\n+\n+SYSTEM FLUSH LOGS;\n+\n+SELECT read_rows < 110000 FROM system.query_log\n+WHERE type = 'QueryFinish' AND current_database = currentDatabase()\n+AND event_time > now() - INTERVAL 10 SECOND\n+AND lower(query) LIKE lower('SELECT s FROM order_by_desc ORDER BY u%');\n",
  "problem_statement": "[20.11] optimize_read_in_order works only in reverse direction.\n**Describe the bug**\r\nOptimize read in order doesn't work in specific conditions.\r\n\r\n**How to reproduce**\r\nClickhouse server 20.11.1.4941\r\n\r\n```\r\nCREATE TABLE test_asof(key UInt32, ts DateTime) ENGINE=MergeTree PARTITION BY toYYYYMM(ts) ORDER BY (key,ts);\r\nINSERT INTO test_asof SELECT number % 100000 as key, now() - toIntervalMonth(intDiv(number, 100000)) as ts FROM numbers(1000000);\r\nset max_threads=1;\r\n\r\nSELECT *\r\nFROM test_asof\r\nORDER BY\r\n    key DESC,\r\n    ts DESC\r\nLIMIT 1\r\nFORMAT Null\r\n\r\nQuery id: d8a698d1-567b-4c01-956b-440fd7b5e70e\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.008 sec. Processed 262.72 thousand rows, 2.10 MB (34.62 million rows/s., 276.97 MB/s.)\r\n\r\nSELECT *\r\nFROM test_asof\r\nORDER BY\r\n    key DESC,\r\n    ts ASC\r\nLIMIT 1\r\nFORMAT Null\r\n\r\nQuery id: 98dfb13c-716b-42dd-b4ee-5778f48d4690\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.026 sec. Processed 590.40 thousand rows, 4.72 MB (22.73 million rows/s., 181.85 MB/s.)\r\n\r\nSELECT *\r\nFROM test_asof\r\nORDER BY\r\n    key ASC,\r\n    ts ASC\r\nLIMIT 1\r\nFORMAT Null\r\n\r\nQuery id: 1843cc1b-4a72-4820-9385-48cbaea1ed12\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.012 sec. Processed 1.00 million rows, 8.00 MB (84.13 million rows/s., 673.03 MB/s.)\r\n```\r\n\r\n**Expected behavior**\r\noptimize_read_in_order works in both directions.\r\n\nSelect * memory usage optimization\nI use ClickHouse 20.12.3.3 to store normalized logs. A table consists of 160 columns, primary key and order by key is Timestamp (int64).\r\n\r\nOften I need to find some logs extracting all table columns. When I use the following query to find logs in 1 month period (approx. 2 TB of compressed data) on a single node - it consumes 20+ GB of memory:\r\n\r\nSELECT * FROM events WHERE Column1 = 'value1' AND Column2 = 'value2' AND Timestamp > [START] AND Timestamp < [END] ORDER BY Timestamp DESC LIMIT **250**;\r\n\r\nThe wider search period is, the more RAM is consumed. The less columns appear in SELECT, the less memory consumed.\r\n\r\nFolks from CH telegram group told me that part of this memory is allocated for column buffers (1MB for each column that appears in SELECT) by each thread. Let's say query is executed by 32 threads: 32t * 160c * 1MB = 5 GB RAM. So it is not clear why CH needs another 15 GB to execute this query. \r\n\r\nIs there a way to use a pipeline like the following one?\r\n\r\n1. Read only columns that appear in WHERE and ORDER BY clauses from disk;\r\n2. Mark locations of each row that satisfy WHERE clause, heap sort on-the-fly;\r\n3. Extract marked rows after scan is finished.\r\n\r\nThis way I wouldn't need tens of GB of RAM to perform deep \"historical\" searches.\r\n\r\nIf that is not possible at the moment, are there any plans to introduce such a pipeline in a future releases?\r\n\n",
  "hints_text": "Actually it's works, but for some reason a much more data is reading, than it's expected.\r\n\r\n```sql\r\nEXPLAIN PIPELINE\r\nSELECT *\r\nFROM test_asof\r\nORDER BY\r\n    key ASC,\r\n    ts ASC\r\nLIMIT 1\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 (Expression)                      \u2502\r\n\u2502 ExpressionTransform               \u2502\r\n\u2502   (Limit)                         \u2502\r\n\u2502   Limit                           \u2502\r\n\u2502     (FinishSorting)               \u2502\r\n\u2502     MergingSortedTransform 10 \u2192 1 \u2502\r\n\u2502       (Expression)                \u2502\r\n\u2502       ExpressionTransform \u00d7 10    \u2502\r\n\u2502         (ReadFromStorage)         \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nIf 10M rows are inserted instead of 1M, then it's visible.\r\n\r\n```sql\r\n\r\nINSERT INTO test_asof SELECT number % 100000 as key, now() - toIntervalMonth(intDiv(number, 1000000)) as ts FROM numbers(10000000);\r\n\r\nSET max_threads = 1;\r\n\r\nSELECT *\r\nFROM test_asof\r\nORDER BY\r\n    key ASC,\r\n    ts ASC\r\nLIMIT 1\r\nFORMAT Null;\r\n\r\n0 rows in set. Elapsed: 0.028 sec. Processed 2.37 million rows, 18.99 MB (83.97 million rows/s., 671.74 MB/s.);\r\n```\r\n\r\n\r\n\r\n\n1. Let's check `EXPLAIN` output. It should use \"read in order\" query plan if `Timestamp` column is the first column in primary key. Otherwise large amount of memory will be used for sorting.\r\n2. Maybe lowering block size will help: `SET max_block_size = 8192` or lower.\r\n\r\n```\r\nIs there a way to use a pipeline like the following one?\r\n\r\n    Read only columns that appear in WHERE and ORDER BY clauses from disk;\r\n    Mark locations of each row that satisfy WHERE clause, heap sort on-the-fly;\r\n    Extract marked rows after scan is finished.\r\n```\r\n\r\nThis algorithm is not implemented in ClickHouse.\r\n\r\n> If that is not possible at the moment, are there any plans to introduce such a pipeline in a future releases?\r\n\r\nIt's possible but non-trivial.\nThank you for an answer. \r\n\r\nHere is the output of EXPLAIN for similar query against a distributed table:\r\n\r\n```EXPLAIN\r\nSELECT *\r\nFROM events\r\nWHERE (Timestamp >= 1613573815452) AND (Timestamp <= 1613576700442)\r\nORDER BY Timestamp DESC\r\nLIMIT 250\r\n\r\nQuery id: b605856a-d07b-49ce-b544-4bf33c2eb278\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression (Projection)                                                       \u2502\r\n\u2502   Limit (preliminary LIMIT)                                                   \u2502\r\n\u2502     MergingSorted (Merge sorted streams for ORDER BY)                         \u2502\r\n\u2502       SettingQuotaAndLimits (Set limits and quota after reading from storage) \u2502\r\n\u2502         Union                                                                 \u2502\r\n\u2502           ReadFromPreparedSource (Read from remote replica)                   \u2502\r\n\u2502           ReadFromPreparedSource (Read from delayed local replica)            \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nAnd EXPLAIN for the same query against a local replicated table:\r\n\r\n```\r\nEXPLAIN\r\nSELECT *\r\nFROM events_local\r\nWHERE (Timestamp >= 1613573815452) AND (Timestamp <= 1613576700442)\r\nORDER BY Timestamp DESC\r\nLIMIT 250\r\n\r\nQuery id: 0c3879ad-462e-4525-ac96-d4a16b650978\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression (Projection)                                                           \u2502\r\n\u2502   Limit (preliminary LIMIT)                                                       \u2502\r\n\u2502     FinishSorting                                                                 \u2502\r\n\u2502       Expression (Before ORDER BY and SELECT)                                     \u2502\r\n\u2502         Filter (WHERE)                                                            \u2502\r\n\u2502           SettingQuotaAndLimits (Set limits and quota after reading from storage) \u2502\r\n\u2502             Union                                                                 \u2502\r\n\u2502               ReverseRows                                                         \u2502\r\n\u2502                 ReadFromStorage (MergeTree  with order)                           \u2502\r\n\u2502               ReverseRows                                                         \u2502\r\n\u2502                 ReadFromStorage (MergeTree  with order)                           \u2502\r\n\u2502               ReverseRows                                                         \u2502\r\n\u2502                 ReadFromStorage (MergeTree  with order)                           \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nOptimize read in order is enabled:\r\n\r\n```\r\nSELECT *\r\nFROM system.settings\r\nWHERE name = 'optimize_read_in_order'\r\n\r\nQuery id: 059bb96f-03f5-4478-9f66-56d65e3dc73e\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nname:        optimize_read_in_order\r\nvalue:       1\r\n\r\n```\r\n\r\nTimestamp column is a primary key. A fragment of output of a query against system.tables:\r\n\r\n```\r\nname:                       events_local\r\n...\r\npartition_key:              (toYYYYMMDD(toDateTime(Timestamp / 1000)), SpaceID)\r\nsorting_key:                Timestamp\r\nprimary_key:                Timestamp\r\nsampling_key:               Timestamp\r\nstorage_policy:             default\r\n....\r\n```\nOk. The query plan is correct.\r\n\r\nNow let's go to\r\n> 2. Maybe lowering block size will help: SET max_block_size = 8192 or lower.\r\n\r\nMaybe there are very large values in the table?\nSetting max_block_size to 8192 helps to reduce memory usage. Here is a query for 2 month period:\r\n\r\n```\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nis_initial_query:     1\r\nuser:                 default\r\nquery_id:             d0e4e907-f2df-4d44-a8d5-5765aa7735d3\r\nelapsed:              889.8091651\r\nis_cancelled:         0\r\nread_rows:            0\r\nread_bytes:           0\r\ntotal_rows_approx:    0\r\nwritten_rows:         0\r\nwritten_bytes:        0\r\nmemory_usage:         1539539856\r\npeak_memory_usage:    1539539856\r\nquery:                SELECT * FROM `events_local` WHERE Timestamp >= 1609448400000 AND Timestamp <= 1613825407279 AND (DeviceProduct = \u2018...' OR DeviceProduct = \u2018...' OR ServiceID = '3bf38e66-dd86-47bb-8839-fc8c3c12505b' OR ServiceID = '89d4716c-1666-4745-9e59-9ee4a5ccd71c' OR ServiceID = '5d2a94e6-75d3-486a-90bf-dcd1b38eadc6' OR ServiceID = '2d1f5f7b-33b2-4dda-adf8-aef8f94976e9' OR DeviceProduct = \u2018...') ORDER BY Timestamp DESC LIMIT 250\r\nProfileEvents.Names:  ['Query','SelectQuery','FileOpen','Seek','ReadBufferFromFileDescriptorRead','ReadBufferFromFileDescriptorReadBytes','ReadCompressedBytes','CompressedReadBufferBlocks','CompressedReadBufferBytes','IOBufferAllocs','IOBufferAllocBytes','ArenaAllocChunks','ArenaAllocBytes','FunctionExecute','MarkCacheHits','MarkCacheMisses','CreatedReadBufferOrdinary','DiskReadElapsedMicroseconds','SelectedParts','SelectedRanges','SelectedMarks','ContextLock','RWLockAcquiredReadLocks','RealTimeMicroseconds','UserTimeMicroseconds','SystemTimeMicroseconds','SoftPageFaults','OSCPUWaitMicroseconds','OSCPUVirtualTimeMicroseconds','OSReadChars','OSWriteChars']\r\nProfileEvents.Values: [1,1,268153,131530,263061,33439521600,36,1,10,268154,35968729162,4,16384,2860,5092,131530,268153,738458090,928,928,8786329,2836,1,5161249,4502249,139672,3,2672,4634931,482030,744]\r\nSettings.Names:       ['max_block_size','use_uncompressed_cache','load_balancing','distributed_aggregation_memory_efficient','max_bytes_before_external_group_by','max_bytes_before_external_sort','max_memory_usage']\r\nSettings.Values:      ['8192','0','random','1','20000000000','20000000000','40000000000\u2019]\r\n```\r\nQuery processing takes about 15 minutes. But memory usage is unstable. The same query for a 1 month period sometimes consumes about 5-7 GB RAM and completes in 60 seconds (cache?). But query for 2 month often does not cross 1 GB boundary with identical max_block_size.\r\n\r\n`Maybe there are very large values in the table?`\r\n\r\nOne column has maximum size of 16 KB (application-level limit). Other string columns are limited to 1 KB. But this is a rare case. Most log records (when serialized to JSON) are about 500-1000 bytes long.",
  "created_at": "2021-06-25T16:27:06Z",
  "modified_files": [
    "src/Interpreters/InterpreterSelectQuery.cpp",
    "src/Parsers/ASTSelectQuery.h",
    "src/Processors/Merges/AggregatingSortedTransform.h",
    "src/Processors/Merges/CollapsingSortedTransform.h",
    "src/Processors/Merges/FinishAggregatingInOrderTransform.h",
    "src/Processors/Merges/GraphiteRollupSortedTransform.h",
    "src/Processors/Merges/IMergingTransform.cpp",
    "src/Processors/Merges/IMergingTransform.h",
    "src/Processors/Merges/MergingSortedTransform.cpp",
    "src/Processors/Merges/MergingSortedTransform.h",
    "src/Processors/Merges/ReplacingSortedTransform.h",
    "src/Processors/Merges/SummingSortedTransform.h",
    "src/Processors/Merges/VersionedCollapsingTransform.h",
    "src/Processors/QueryPlan/FinishSortingStep.cpp",
    "src/Processors/QueryPlan/FinishSortingStep.h",
    "src/Processors/QueryPlan/ReadFromMergeTree.cpp",
    "src/Processors/QueryPlan/ReadFromMergeTree.h",
    "src/Processors/Transforms/MergeSortingTransform.cpp",
    "src/Storages/MergeTree/MergeTreeBaseSelectProcessor.cpp",
    "src/Storages/MergeTree/MergeTreeBaseSelectProcessor.h",
    "src/Storages/MergeTree/MergeTreeBlockReadUtils.h",
    "src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp",
    "b/src/Storages/MergeTree/MergeTreeInOrderSelectProcessor.cpp",
    "b/src/Storages/MergeTree/MergeTreeInOrderSelectProcessor.h",
    "src/Storages/MergeTree/MergeTreeReadPool.cpp",
    "src/Storages/MergeTree/MergeTreeReverseSelectProcessor.cpp",
    "src/Storages/MergeTree/MergeTreeReverseSelectProcessor.h",
    "src/Storages/MergeTree/MergeTreeSelectProcessor.cpp",
    "src/Storages/MergeTree/MergeTreeSelectProcessor.h",
    "src/Storages/ReadInOrderOptimizer.cpp",
    "src/Storages/ReadInOrderOptimizer.h",
    "src/Storages/SelectQueryInfo.h",
    "src/Storages/ya.make"
  ],
  "modified_test_files": [
    "src/DataStreams/tests/gtest_blocks_size_merging_streams.cpp",
    "tests/queries/0_stateless/01551_mergetree_read_in_order_spread.reference",
    "tests/queries/0_stateless/01861_explain_pipeline.reference",
    "b/tests/queries/0_stateless/01926_order_by_desc_limit.reference",
    "b/tests/queries/0_stateless/01926_order_by_desc_limit.sql"
  ]
}