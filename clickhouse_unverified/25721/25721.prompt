You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
[20.11] optimize_read_in_order works only in reverse direction.
**Describe the bug**
Optimize read in order doesn't work in specific conditions.

**How to reproduce**
Clickhouse server 20.11.1.4941

```
CREATE TABLE test_asof(key UInt32, ts DateTime) ENGINE=MergeTree PARTITION BY toYYYYMM(ts) ORDER BY (key,ts);
INSERT INTO test_asof SELECT number % 100000 as key, now() - toIntervalMonth(intDiv(number, 100000)) as ts FROM numbers(1000000);
set max_threads=1;

SELECT *
FROM test_asof
ORDER BY
    key DESC,
    ts DESC
LIMIT 1
FORMAT Null

Query id: d8a698d1-567b-4c01-956b-440fd7b5e70e

Ok.

0 rows in set. Elapsed: 0.008 sec. Processed 262.72 thousand rows, 2.10 MB (34.62 million rows/s., 276.97 MB/s.)

SELECT *
FROM test_asof
ORDER BY
    key DESC,
    ts ASC
LIMIT 1
FORMAT Null

Query id: 98dfb13c-716b-42dd-b4ee-5778f48d4690

Ok.

0 rows in set. Elapsed: 0.026 sec. Processed 590.40 thousand rows, 4.72 MB (22.73 million rows/s., 181.85 MB/s.)

SELECT *
FROM test_asof
ORDER BY
    key ASC,
    ts ASC
LIMIT 1
FORMAT Null

Query id: 1843cc1b-4a72-4820-9385-48cbaea1ed12

Ok.

0 rows in set. Elapsed: 0.012 sec. Processed 1.00 million rows, 8.00 MB (84.13 million rows/s., 673.03 MB/s.)
```

**Expected behavior**
optimize_read_in_order works in both directions.

Select * memory usage optimization
I use ClickHouse 20.12.3.3 to store normalized logs. A table consists of 160 columns, primary key and order by key is Timestamp (int64).

Often I need to find some logs extracting all table columns. When I use the following query to find logs in 1 month period (approx. 2 TB of compressed data) on a single node - it consumes 20+ GB of memory:

SELECT * FROM events WHERE Column1 = 'value1' AND Column2 = 'value2' AND Timestamp > [START] AND Timestamp < [END] ORDER BY Timestamp DESC LIMIT **250**;

The wider search period is, the more RAM is consumed. The less columns appear in SELECT, the less memory consumed.

Folks from CH telegram group told me that part of this memory is allocated for column buffers (1MB for each column that appears in SELECT) by each thread. Let's say query is executed by 32 threads: 32t * 160c * 1MB = 5 GB RAM. So it is not clear why CH needs another 15 GB to execute this query. 

Is there a way to use a pipeline like the following one?

1. Read only columns that appear in WHERE and ORDER BY clauses from disk;
2. Mark locations of each row that satisfy WHERE clause, heap sort on-the-fly;
3. Extract marked rows after scan is finished.

This way I wouldn't need tens of GB of RAM to perform deep "historical" searches.

If that is not possible at the moment, are there any plans to introduce such a pipeline in a future releases?
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
