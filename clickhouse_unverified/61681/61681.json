{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 61681,
  "instance_id": "ClickHouse__ClickHouse-61681",
  "issue_numbers": [
    "61071"
  ],
  "base_commit": "03e722864125121cf229c9323bdb7fc6f6029645",
  "patch": "diff --git a/docker/packager/binary-builder/Dockerfile b/docker/packager/binary-builder/Dockerfile\nindex c9442accd7e3..73ec4275f12b 100644\n--- a/docker/packager/binary-builder/Dockerfile\n+++ b/docker/packager/binary-builder/Dockerfile\n@@ -61,7 +61,7 @@ RUN arch=${TARGETARCH:-amd64} \\\n     && rm /tmp/nfpm.deb\n \n ARG GO_VERSION=1.19.10\n-# We need go for clickhouse-diagnostics\n+# We needed go for clickhouse-diagnostics (it is not used anymore)\n RUN arch=${TARGETARCH:-amd64} \\\n     && curl -Lo /tmp/go.tgz \"https://go.dev/dl/go${GO_VERSION}.linux-${arch}.tar.gz\" \\\n     && tar -xzf /tmp/go.tgz -C /usr/local/ \\\ndiff --git a/docker/packager/binary-builder/build.sh b/docker/packager/binary-builder/build.sh\nindex b63643419fe9..032aceb0af3d 100755\n--- a/docker/packager/binary-builder/build.sh\n+++ b/docker/packager/binary-builder/build.sh\n@@ -36,22 +36,6 @@ rm -f CMakeCache.txt\n \n if [ -n \"$MAKE_DEB\" ]; then\n   rm -rf /build/packages/root\n-  # NOTE: this is for backward compatibility with previous releases,\n-  # that does not diagnostics tool (only script).\n-  if [ -d /build/programs/diagnostics ]; then\n-    if [ -z \"$SANITIZER\" ]; then\n-      # We need to check if clickhouse-diagnostics is fine and build it\n-      (\n-        cd /build/programs/diagnostics\n-        make test-no-docker\n-        GOARCH=\"${DEB_ARCH}\" CGO_ENABLED=0 make VERSION=\"$VERSION_STRING\" build\n-        mv clickhouse-diagnostics ..\n-      )\n-    else\n-      echo -e \"#!/bin/sh\\necho 'Not implemented for this type of package'\" > /build/programs/clickhouse-diagnostics\n-      chmod +x /build/programs/clickhouse-diagnostics\n-    fi\n-  fi\n fi\n \n \n@@ -121,8 +105,6 @@ if [ -n \"$MAKE_DEB\" ]; then\n   # No quotes because I want it to expand to nothing if empty.\n   # shellcheck disable=SC2086\n   DESTDIR=/build/packages/root ninja $NINJA_FLAGS programs/install\n-  cp /build/programs/clickhouse-diagnostics /build/packages/root/usr/bin\n-  cp /build/programs/clickhouse-diagnostics /output\n   bash -x /build/packages/build\n fi\n \ndiff --git a/packages/clickhouse-common-static.yaml b/packages/clickhouse-common-static.yaml\nindex 238126f95fda..383ad39591cd 100644\n--- a/packages/clickhouse-common-static.yaml\n+++ b/packages/clickhouse-common-static.yaml\n@@ -34,8 +34,6 @@ suggests:\n contents:\n - src: root/usr/bin/clickhouse\n   dst: /usr/bin/clickhouse\n-- src: root/usr/bin/clickhouse-diagnostics\n-  dst: /usr/bin/clickhouse-diagnostics\n - src: root/usr/bin/clickhouse-extract-from-config\n   dst: /usr/bin/clickhouse-extract-from-config\n - src: root/usr/bin/clickhouse-library-bridge\ndiff --git a/programs/diagnostics/.gitignore b/programs/diagnostics/.gitignore\ndeleted file mode 100644\nindex 5e0b0165f381..000000000000\n--- a/programs/diagnostics/.gitignore\n+++ /dev/null\n@@ -1,30 +0,0 @@\n-# If you prefer the allow list template instead of the deny list, see community template:\n-# https://github.com/github/gitignore/blob/main/community/Golang/Go.AllowList.gitignore\n-#\n-# Binaries for programs and plugins\n-*.exe\n-*.exe~\n-*.dll\n-*.so\n-*.dylib\n-\n-# Test binary, built with `go test -c`\n-*.test\n-\n-# Output of the go coverage tool, specifically when used with LiteIDE\n-*.out\n-\n-# Dependency directories (remove the comment below to include it)\n-# vendor/\n-\n-# Go workspace file\n-go.work\n-\n-.idea\n-clickhouse-diagnostics\n-output\n-vendor\n-bin\n-profile.cov\n-clickhouse-diagnostics.yml\n-dist/\ndiff --git a/programs/diagnostics/CONTRIBUTION.md b/programs/diagnostics/CONTRIBUTION.md\ndeleted file mode 100644\nindex 00fb073cefec..000000000000\n--- a/programs/diagnostics/CONTRIBUTION.md\n+++ /dev/null\n@@ -1,49 +0,0 @@\n-# Contribution\n-\n-We keep things simple. Execute all commands in this folder.\n-\n-## Requirements\n-\n-- docker - tested on version 20.10.12.\n-- golang >= go1.17.6\n-\n-## Building\n-\n-Creates a binary `clickhouse-diagnostics` in the local folder. Build will be versioned according to a timestamp. For a versioned release see [Releasing](#releasing).\n-\n-```bash\n-make build\n-```\n-\n-## Linting\n-\n-We use [golangci-lint](https://golangci-lint.run/). We use a container to run so no need to install.\n-\n-```bash\n-make lint-go\n-```\n-\n-## Running Tests\n-\n-```bash\n-make test\n-```\n-\n-For a coverage report,\n-\n-```bash\n-make test-coverage\n-```\n-\n-## Adding Collectors\n-\n-TODO\n-\n-\n-## Adding Outputs\n-\n-TODO\n-\n-## Frames\n-\n-## Parameter Types\ndiff --git a/programs/diagnostics/Makefile b/programs/diagnostics/Makefile\ndeleted file mode 100644\nindex 2e85002b871d..000000000000\n--- a/programs/diagnostics/Makefile\n+++ /dev/null\n@@ -1,65 +0,0 @@\n-GOCMD=go\n-GOTEST=$(GOCMD) test\n-BINARY_NAME=clickhouse-diagnostics\n-BUILD_DIR=dist\n-\n-TIMESTAMP := $(shell date +%Y%m%d-%H%M)\n-COMMIT := $(shell git rev-parse --short HEAD)\n-MODULE := github.com/ClickHouse/ClickHouse/programs/diagnostics\n-VERSION := v.dev-${TIMESTAMP}\n-DEVLDFLAGS = -ldflags \"-X ${MODULE}/cmd.Version=${VERSION} -X ${MODULE}/cmd.Commit=${COMMIT}\"\n-\n-# override with env variable to test other versions e.g. 21.11.10.1\n-CLICKHOUSE_VERSION ?= latest\n-\n-GREEN  := $(shell tput -Txterm setaf 2)\n-YELLOW := $(shell tput -Txterm setaf 3)\n-WHITE  := $(shell tput -Txterm setaf 7)\n-CYAN   := $(shell tput -Txterm setaf 6)\n-RESET  := $(shell tput -Txterm sgr0)\n-\n-.PHONY: all test build vendor release lint-go test-coverages dep\n-\n-all: help\n-\n-release: ## Release is delegated to goreleaser\n-\t$(shell goreleaser release --rm-dist)\n-\n-## Build:\n-build: ## Build a binary for local use\n-\t# timestamped version\n-\t$(GOCMD) build ${DEVLDFLAGS} -o $(BINARY_NAME) ./cmd/clickhouse-diagnostics\n-\n-clean: ## Remove build related file\n-\trm ${BINARY_NAME}\n-\trm -f checkstyle-report.xml ./coverage.xml ./profile.cov\n-\n-vendor: ## Copy of all packages needed to support builds and tests in the vendor directory\n-\t$(GOCMD) mod vendor\n-\n-test: ## Run the tests of the project\n-\tCLICKHOUSE_VERSION=$(CLICKHOUSE_VERSION) $(GOTEST) -v -race `go list ./... | grep -v ./internal/platform/test`\n-\n-test-no-docker: ## Don't run tests depending on dockerd\n-\tCLICKHOUSE_VERSION=$(CLICKHOUSE_VERSION) $(GOTEST) -v -race -tags no_docker `go list ./... | grep -v ./internal/platform/test`\n-\n-lint-go: ## Use golintci-lint\n-\tdocker run --rm -v $(shell pwd):/app -w /app golangci/golangci-lint:latest-alpine golangci-lint run\n-\n-test-coverage: ## Run the tests of the project and export the coverage\n-\tCLICKHOUSE_VERSION=$(CLICKHOUSE_VERSION) $(GOTEST) -cover -covermode=count -coverprofile=profile.cov `go list ./... | grep -v ./internal/platform/test`\n-\t$(GOCMD) tool cover -func profile.cov\n-\n-dep:\n-  $(shell go mod download)\n-\n-help: ## Show this help.\n-\t@echo ''\n-\t@echo 'Usage:'\n-\t@echo '  ${YELLOW}make${RESET} ${GREEN}<target>${RESET}'\n-\t@echo ''\n-\t@echo 'Targets:'\n-\t@awk 'BEGIN {FS = \":.*?## \"} { \\\n-\t\tif (/^[a-zA-Z_-]+:.*?##.*$$/) {printf \"    ${YELLOW}%-20s${GREEN}%s${RESET}\\n\", $$1, $$2} \\\n-\t\telse if (/^## .*$$/) {printf \"  ${CYAN}%s${RESET}\\n\", substr($$1,4)} \\\n-\t\t}' $(MAKEFILE_LIST)\ndiff --git a/programs/diagnostics/README.md b/programs/diagnostics/README.md\ndeleted file mode 100644\nindex f800bb0648ea..000000000000\n--- a/programs/diagnostics/README.md\n+++ /dev/null\n@@ -1,167 +0,0 @@\n-# Clickhouse Diagnostics Tool\n-\n-## Purpose\n-\n-This tool provides a means of obtaining a diagnostic bundle from a ClickHouse instance. This bundle can be provided to your nearest ClickHouse support provider in order to assist with the diagnosis of issues.\n-\n-## Design Philosophy\n-\n-- **No local dependencies** to run. We compile to a platform-independent binary, hence Go.\n-- **Minimize resource overhead**. Improvements always welcome.\n-- **Extendable framework**. At its core, the tool provides collectors and outputs. Collectors are independent and are responsible for collecting a specific dataset e.g. system configuration. Outputs produce the diagnostic bundle in a specific format. It should be trivial to add both for contributors. See [Collectors](#collectors) and [Outputs](#outputs) for more details.\n-- **Convertible output formats**. Outputs produce diagnostic bundles in different formats e.g. archive, simple report etc. Where possible, it should be possible to convert between these formats. For example, an administrator may provide a bundle as an archive to their support provider who in turn wishes to visualise this as a report or even in ClickHouse itself...\n-- **Something is better than nothing**. Collectors execute independently. We never fail a collection because one fails - preferring to warn the user only. There are good reasons for a collector failure e.g. insufficient permissions or missing data.\n-- **Execute anywhere** - Ideally, this tool is executed on a ClickHouse host. Some collectors e.g. configuration file collection or system information, rely on this. However, collectors will obtain as much information remotely from the database as possible if executed remotely from the cluster - warning where collection fails. **We do currently require ClickHouse to be running, connecting over the native port**.\n-\n-We recommend reading [Permissions, Warnings & Locality](#permissions-warnings--locality).\n-\n-## Usage\n-\n-### Collection\n-\n-The `collect` command allows the collection of a diagnostic bundle. In its simplest form, assuming ClickHouse is running locally on default ports with no password:\n-\n-```bash\n-clickhouse-diagnostics collect\n-```\n-\n-This will use the default collectors and the simple output. This output produces a timestamped archive bundle in `gz` format in a sub folder named after the host. This folder name can be controlled via the parameter `--id` or configured directly for the simple output parameter `output.simple.folder` (this allows a specific directory to be specified).\n-\n-Collectors, Outputs and ClickHouse connection credentials can be specified as shown below:\n-\n-```bash\n-clickhouse-diagnostics collect --password random --username default --collector=system_db,system --output=simple --id my_cluster_name\n-```\n-\n-This collects the system database and host information from the cluster running locally. The archive bundle will be produced under a folder `my_cluster_name`.\n-\n-For further details, use the in built help (the commands below are equivalent):\n-\n-```bash\n-clickhouse-diagnostics collect --help\n-./clickhouse-diagnostics help collect\n-```\n-\n-### Help & Finding parameters for collectors & outputs\n-\n-Collectors and outputs have their own parameters not listed under the help for the command for the `collect` command. These can be identified using the `help` command. Specifically,\n-\n-For more information about a specific collector.\n-\n-```bash\n-Use \"clickhouse-diagnostics help --collector [collector]\" \n-```\n-\n-For more information about a specific output.\n-\n-```bash\n-Use \"clickhouse-diagnostics help --output [output]\" \n-```\n-\n-### Convert\n-\n-Coming soon to a cluster near you...\n-\n-## Collectors\n-\n-We currently support the following collectors. A `*` indicates this collector is enabled by default:\n-\n-- `system_db*` - Collects all tables in the system database, except those which have been excluded and up to a specified row limit.\n-- `system*` - Collects summary OS and hardware statistics for the host.\n-- `config*` - Collects the ClickHouse configuration from the local filesystem. A best effort is made using process information if ClickHouse is not installed locally. `include_path` are also considered. \n-- `db_logs*` - Collects the ClickHouse logs directly from the database.\n-- `logs*` - Collects the ClickHouse logs directly from the database.\n-- `summary*` - Collects summary statistics on the database based on a set of known useful queries. This represents the easiest collector to extend - contributions are welcome to this set which can be found [here](https://github.com/ClickHouse/ClickHouse/blob/master/programs/diagnostics/internal/collectors/clickhouse/queries.json).\n-- `file` - Collects files based on glob patterns. Does not collect directories. To preview files which will be collected try, `clickhouse-diagnostics collect --collectors=file --collector.file.file_pattern=<glob path> --output report`\n-- `command` - Collects the output of a user specified command. To preview output, `clickhouse-diagnostics collect --collectors=command --collector.command.command=\"<command>\" --output report`\n-- `zookeeper_db` - Collects information about zookeeper using the `system.zookeeper` table, recursively iterating the zookeeper tree/table. Note: changing the default parameter values can cause extremely high load to be placed on the database. Use with caution. By default, uses the glob `/clickhouse/{task_queue}/**` to match zookeeper paths and iterates to a max depth of 8.\n-\n-## Outputs\n-\n-We currently support the following outputs. The `simple` output is currently the default:\n-\n-- `simple` - Writes out the diagnostic bundle as files in a structured directory, optionally producing a compressed archive.\n-- `report` - Writes out the diagnostic bundle to the terminal as a simple report. Supports an ascii table format or markdown.\n-- `clickhouse` - **Under development**. This will allow a bundle to be stored in a cluster allowing visualization in common tooling e.g. Grafana.\n-\n-## Simple Output\n-\n-Since the `simple` output is the default we provide additional details here. \n-This output produces a timestamped archive by default in `gz` format under a directory created with either the hostname of the specified collection `--id`. As shown below, a specific folder can also be specified. Compression can also be disabled, leaving just the contents of the folder:\n-\n-```bash\n-./clickhouse-diagnostics help --output simple\n-\n-Writes out the diagnostic bundle as files in a structured directory, optionally producing a compressed archive.\n-\n-Usage:\n-  --output=simple [flags]\n-\n-Flags:\n-      --output.simple.directory string   Directory in which to create dump. Defaults to the current directory. (default \"./\")\n-      --output.simple.format string      Format of exported files (default \"csv\")\n-      --output.simple.skip_archive       Don't compress output to an archive\n-```\n-\n-The archive itself contains a folder for each collector. Each collector can potentially produce many discrete sets of data, known as frames. Each of these typically results in a single file within the collector's folder. For example, each query for the `summary` collector results in a correspondingly named file within the `summary` folder. \n-\n-## Permissions, Warnings & Locality\n-\n-Some collectors either require specific permissions for complete collection or should be executed on a ClickHouse host. We aim to collate these requirements below:\n-\n-- `system_db` - This collect aims to collect all tables in the `system` database. Some tables may fail if certain features are not enabled. Specifically,[allow_introspection_functions](https://clickhouse.com/docs/en/operations/settings/settings/#settings-allow_introspection_functions) is required to collect the `stack_traces` table. [access_management](https://clickhouse.com/docs/en/operations/settings/settings-users/#access_management-user-setting) must be set for the ClickHouse user specified for collection, to permit access to access management tables e.g. `quota_usage`.\n-- `db_logs`- The ClickHouse user must have access to the tables `query_log`,`query_thread_log` and `text_log`.\n-- `logs` - The system user under which the tool is executed must have access to the logs directory. It must therefore also be executed on the target ClickHouse server directly for this collector work. In cases where the logs directory is not a default location e.g. `/var/log/clickhouse-server` we will attempt to establish the location from the ClickHouse configuration. This requires permissions to read the configuration files - which in most cases requires specific permissions to be granted to the run user if you are not comfortable executing the tool under sudo or the `clickhouse` user.\n-- `summary`- This collector executes pre-recorded queries. Some of these read tables concerning access management, thus requiring the ClickHouse user to have the [access_management](https://clickhouse.com/docs/en/operations/settings/settings-users/#access_management-user-setting) permission.\n-- `config` - This collector reads and copies the local configuration files. It thus requires permissions to read the configuration files - which in most cases requires specific permissions to be granted to the run user if you are not comfortable executing the tool under sudo or the `clickhouse` user.\n-\n-**If a collector cannot collect specific data because of either execution location or permissions, it will log a warning to the terminal.**\n-\n-## Logging\n-\n-All logs are output to `stderr`. `stdout` is used exclusively for outputs to print information.\n-\n-## Configuration file\n-\n-In addition to supporting parameters via the command line, a configuration file can be specified via the `--config`, `-f` flag. \n-\n-By default, we look for a configuration file `clickhouse-diagnostics.yml` in the same directory as the binary. If not present, we revert to command line flags.\n-\n-**Values set via the command line values always take precedence over those in the configuration file.**\n-\n-All parameters can be set via the configuration file and can in most cases be converted to a yaml hierarchy, where periods indicate a nesting. For example,\n-\n-`--collector.system_db.row_limit=1`\n-\n-becomes\n-\n-```yaml\n-collector:\n-  system_db:\n-    row_limit: 1\n-```\n-\n-The following exceptions exist to avoid collisions:\n-\n-| Command | Parameter  | Configuration File |\n-|---------|------------|--------------------|\n-| collect | output     | collect.output     |\n-| collect | collectors | collect.collectors |\n-\n-## FAQ\n-\n-1. Does the collector need root permissions?\n-\n-    No. However, to read some local files e.g. configurations, the tool should be executed as the `clickhouse` user.\n-\n-2. What ClickHouse database permissions does the collector need?\n-\n-    Read permissions on all system tables are required in most cases - although only specific collectors need this. [Access management permissions]((https://clickhouse.com/docs/en/operations/settings/settings-users/#access_management-user-setting)) will ensure full collection.\n-\n-3. Is any processing done on logs for anonimization purposes?\n-\n-    Currently no. ClickHouse should not log sensitive information to logs e.g. passwords.\n-\n-4. Is sensitive information removed from configuration files e.g. passwords?\n-\n-    Yes. We remove both passwords and hashed passwords. Please raise an issue if you require further information to be anonimized. We appreciate this is a sensitive topic.\ndiff --git a/programs/diagnostics/cmd/clickhouse-diagnostics/main.go b/programs/diagnostics/cmd/clickhouse-diagnostics/main.go\ndeleted file mode 100644\nindex 0a849a9f520a..000000000000\n--- a/programs/diagnostics/cmd/clickhouse-diagnostics/main.go\n+++ /dev/null\n@@ -1,9 +0,0 @@\n-package main\n-\n-import (\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/cmd\"\n-)\n-\n-func main() {\n-\tcmd.Execute()\n-}\ndiff --git a/programs/diagnostics/cmd/collect.go b/programs/diagnostics/cmd/collect.go\ndeleted file mode 100644\nindex 503d8e41fb7a..000000000000\n--- a/programs/diagnostics/cmd/collect.go\n+++ /dev/null\n@@ -1,159 +0,0 @@\n-package cmd\n-\n-import (\n-\t\"fmt\"\n-\t\"os\"\n-\t\"strings\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/cmd/params\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors\"\n-\t_ \"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse\"\n-\t_ \"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/system\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs\"\n-\t_ \"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs/file\"\n-\t_ \"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs/terminal\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils\"\n-\t\"github.com/rs/zerolog/log\"\n-\t\"github.com/spf13/cobra\"\n-\t\"github.com/spf13/pflag\"\n-\t\"github.com/spf13/viper\"\n-)\n-\n-var id string\n-var output = params.StringOptionsVar{\n-\tOptions: outputs.GetOutputNames(),\n-\tValue:   \"simple\",\n-}\n-\n-// access credentials\n-var host string\n-var port uint16\n-var username string\n-var password string\n-\n-var collectorNames = params.StringSliceOptionsVar{\n-\tOptions: collectors.GetCollectorNames(false),\n-\tValues:  collectors.GetCollectorNames(true),\n-}\n-\n-// holds the collector params passed by the cli\n-var collectorParams params.ParamMap\n-\n-// holds the output params passed by the cli\n-var outputParams params.ParamMap\n-\n-const collectHelpTemplate = `Usage:{{if .Runnable}}\n-  {{.UseLine}}{{end}}{{if .HasAvailableSubCommands}}\n-  {{.CommandPath}} [command]{{end}}{{if gt (len .Aliases) 0}}\n-\n-Aliases:\n-  {{.NameAndAliases}}{{end}}{{if .HasExample}}\n-\n-Examples:\n-{{.Example}}{{end}}{{if .HasAvailableSubCommands}}\n-\n-Available Commands:{{range .Commands}}{{if (or .IsAvailableCommand (eq .Name \"help\"))}}\n-  {{rpad .Name .NamePadding }} {{.Short}}{{end}}{{end}}{{end}}{{if .HasAvailableLocalFlags}}\n-\n-Flags:\n-{{.LocalFlags.FlagUsages | trimTrailingWhitespaces}}{{end}}{{if .HasAvailableInheritedFlags}}\n-\n-Global Flags:\n-{{.InheritedFlags.FlagUsages | trimTrailingWhitespaces}}{{end}}\n-\n-Additional help topics:\n-\tUse \"{{.CommandPath}} [command] --help\" for more information about a command.\n-\tUse \"{{.Parent.Name}} help --collector [collector]\" for more information about a specific collector.\n-\tUse \"{{.Parent.Name}} help --output [output]\" for more information about a specific output.\n-`\n-\n-func init() {\n-\tcollectCmd.Flags().StringVar(&id, \"id\", getHostName(), \"Id of diagnostic bundle\")\n-\n-\t// access credentials\n-\tcollectCmd.Flags().StringVar(&host, \"host\", \"localhost\", \"ClickHouse host\")\n-\tcollectCmd.Flags().Uint16VarP(&port, \"port\", \"p\", 9000, \"ClickHouse native port\")\n-\tcollectCmd.Flags().StringVarP(&username, \"username\", \"u\", \"\", \"ClickHouse username\")\n-\tcollectCmd.Flags().StringVar(&password, \"password\", \"\", \"ClickHouse password\")\n-\t// collectors and outputs\n-\tcollectCmd.Flags().VarP(&output, \"output\", \"o\", fmt.Sprintf(\"Output Format for the diagnostic Bundle, options: [%s]\\n\", strings.Join(output.Options, \",\")))\n-\tcollectCmd.Flags().VarP(&collectorNames, \"collectors\", \"c\", fmt.Sprintf(\"Collectors to use, options: [%s]\\n\", strings.Join(collectorNames.Options, \",\")))\n-\n-\tcollectorConfigs, err := collectors.BuildConfigurationOptions()\n-\tif err != nil {\n-\t\tlog.Fatal().Err(err).Msg(\"Unable to build collector configurations\")\n-\t}\n-\tcollectorParams = params.NewParamMap(collectorConfigs)\n-\n-\toutputConfigs, err := outputs.BuildConfigurationOptions()\n-\tif err != nil {\n-\t\tlog.Fatal().Err(err).Msg(\"Unable to build output configurations\")\n-\t}\n-\tparams.AddParamMapToCmd(collectorParams, collectCmd, \"collector\", true)\n-\n-\toutputParams = params.NewParamMap(outputConfigs)\n-\tparams.AddParamMapToCmd(outputParams, collectCmd, \"output\", true)\n-\n-\tcollectCmd.SetFlagErrorFunc(handleFlagErrors)\n-\tcollectCmd.SetHelpTemplate(collectHelpTemplate)\n-\trootCmd.AddCommand(collectCmd)\n-}\n-\n-var collectCmd = &cobra.Command{\n-\tUse:   \"collect\",\n-\tShort: \"Collect a diagnostic bundle\",\n-\tLong:  `Collect a ClickHouse diagnostic bundle for a specified ClickHouse instance`,\n-\tPreRun: func(cmd *cobra.Command, args []string) {\n-\t\tbindFlagsToConfig(cmd)\n-\t},\n-\tExample: fmt.Sprintf(`%s collect --username default --collector=%s --output=simple`, rootCmd.Name(), strings.Join(collectorNames.Options[:2], \",\")),\n-\tRun: func(cmd *cobra.Command, args []string) {\n-\t\tlog.Info().Msgf(\"executing collect command with %v collectors and %s output\", collectorNames.Values, output.Value)\n-\t\toutputConfig := params.ConvertParamsToConfig(outputParams)[output.Value]\n-\t\trunConfig := internal.NewRunConfiguration(id, host, port, username, password, output.Value, outputConfig, collectorNames.Values, params.ConvertParamsToConfig(collectorParams))\n-\t\tinternal.Capture(runConfig)\n-\t\tos.Exit(0)\n-\t},\n-}\n-\n-func getHostName() string {\n-\tname, err := os.Hostname()\n-\tif err != nil {\n-\t\tname = \"clickhouse-diagnostics\"\n-\t}\n-\treturn name\n-}\n-\n-// these flags are nested under the cmd name in the config file to prevent collisions\n-var flagsToNest = []string{\"output\", \"collectors\"}\n-\n-// this saves us binding each command manually to viper\n-func bindFlagsToConfig(cmd *cobra.Command) {\n-\tcmd.Flags().VisitAll(func(f *pflag.Flag) {\n-\t\terr := viper.BindEnv(f.Name, fmt.Sprintf(\"%s_%s\", envPrefix,\n-\t\t\tstrings.ToUpper(strings.Replace(f.Name, \".\", \"_\", -1))))\n-\t\tif err != nil {\n-\t\t\tlog.Error().Msgf(\"Unable to bind %s to config\", f.Name)\n-\t\t}\n-\t\tconfigFlagName := f.Name\n-\t\tif utils.Contains(flagsToNest, f.Name) {\n-\t\t\tconfigFlagName = fmt.Sprintf(\"%s.%s\", cmd.Use, configFlagName)\n-\t\t}\n-\t\terr = viper.BindPFlag(configFlagName, f)\n-\t\tif err != nil {\n-\t\t\tlog.Error().Msgf(\"Unable to bind %s to config\", f.Name)\n-\t\t}\n-\t\t// here we prefer the config value when the param is not set on the cmd line\n-\t\tif !f.Changed && viper.IsSet(configFlagName) {\n-\t\t\tval := viper.Get(configFlagName)\n-\t\t\tlog.Debug().Msgf(\"Setting parameter %s from configuration file\", f.Name)\n-\t\t\terr = cmd.Flags().Set(f.Name, fmt.Sprintf(\"%v\", val))\n-\t\t\tif err != nil {\n-\t\t\t\tlog.Error().Msgf(\"Unable to read \\\"%s\\\" value from config\", f.Name)\n-\t\t\t} else {\n-\t\t\t\tlog.Debug().Msgf(\"Set parameter \\\"%s\\\" from configuration\", f.Name)\n-\t\t\t}\n-\t\t}\n-\t})\n-}\ndiff --git a/programs/diagnostics/cmd/convert.go b/programs/diagnostics/cmd/convert.go\ndeleted file mode 100644\nindex 1d619dd05e2a..000000000000\n--- a/programs/diagnostics/cmd/convert.go\n+++ /dev/null\n@@ -1,1 +0,0 @@\n-package cmd\ndiff --git a/programs/diagnostics/cmd/help.go b/programs/diagnostics/cmd/help.go\ndeleted file mode 100644\nindex 750576dda252..000000000000\n--- a/programs/diagnostics/cmd/help.go\n+++ /dev/null\n@@ -1,124 +0,0 @@\n-package cmd\n-\n-import (\n-\t\"fmt\"\n-\t\"os\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/cmd/params\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/rs/zerolog/log\"\n-\t\"github.com/spf13/cobra\"\n-)\n-\n-var cHelp = params.StringOptionsVar{\n-\tOptions: collectors.GetCollectorNames(false),\n-\tValue:   \"\",\n-}\n-var oHelp = params.StringOptionsVar{\n-\tOptions: outputs.GetOutputNames(),\n-\tValue:   \"\",\n-}\n-\n-func init() {\n-\thelpCmd.Flags().VarP(&cHelp, \"collector\", \"c\", \"Specify collector to get description of available flags\")\n-\thelpCmd.Flags().VarP(&oHelp, \"output\", \"o\", \"Specify output to get description of available flags\")\n-\thelpCmd.SetUsageTemplate(`Usage:{{if .Runnable}}\n-  {{.UseLine}}{{end}}{{if .HasExample}}\n-\n-Examples:\n-{{.Example}}{{end}}\n-\n-Available Commands:{{range .Parent.Commands}}{{if (or .IsAvailableCommand (eq .Name \"help\"))}}\n-  {{rpad .Name .NamePadding }} {{.Short}}{{end}}{{end}}{{if .HasAvailableLocalFlags}}\n-\n-Flags:\n-{{.LocalFlags.FlagUsages | trimTrailingWhitespaces}}{{end}}\n-\n-Alternatively use \"{{.CommandPath}} [command] --help\" for more information about a command.\n-`)\n-\thelpCmd.SetFlagErrorFunc(handleFlagErrors)\n-\n-}\n-\n-var helpCmd = &cobra.Command{\n-\tUse:   \"help [command]\",\n-\tShort: \"Help about any command, collector or output\",\n-\tLong:  `Help provides help for any command, collector or output in the application.`,\n-\tExample: fmt.Sprintf(`%[1]v help collect\n-%[1]v help --collector=config\n-%[1]v help --output=simple`, rootCmd.Name()),\n-\tRun: func(c *cobra.Command, args []string) {\n-\t\tif len(args) != 0 {\n-\t\t\t//find the command on which help is requested\n-\t\t\tcmd, _, e := c.Root().Find(args)\n-\t\t\tif cmd == nil || e != nil {\n-\t\t\t\tc.Printf(\"Unknown help topic %#q\\n\", args)\n-\t\t\t\tcobra.CheckErr(c.Root().Usage())\n-\t\t\t} else {\n-\t\t\t\tcmd.InitDefaultHelpFlag()\n-\t\t\t\tcobra.CheckErr(cmd.Help())\n-\t\t\t}\n-\t\t\treturn\n-\t\t}\n-\t\tif cHelp.Value != \"\" && oHelp.Value != \"\" {\n-\t\t\tlog.Error().Msg(\"Specify either --collector or --output not both\")\n-\t\t\t_ = c.Help()\n-\t\t\tos.Exit(1)\n-\t\t}\n-\t\tif cHelp.Value != \"\" {\n-\t\t\tcollector, err := collectors.GetCollectorByName(cHelp.Value)\n-\t\t\tif err != nil {\n-\t\t\t\tlog.Fatal().Err(err).Msgf(\"Unable to initialize collector %s\", cHelp.Value)\n-\t\t\t}\n-\t\t\tconfigHelp(collector.Configuration(), \"collector\", cHelp.Value, collector.Description())\n-\t\t} else if oHelp.Value != \"\" {\n-\t\t\toutput, err := outputs.GetOutputByName(oHelp.Value)\n-\t\t\tif err != nil {\n-\t\t\t\tlog.Fatal().Err(err).Msgf(\"Unable to initialize output %s\", oHelp.Value)\n-\t\t\t}\n-\t\t\tconfigHelp(output.Configuration(), \"output\", oHelp.Value, output.Description())\n-\t\t} else {\n-\t\t\t_ = c.Help()\n-\t\t}\n-\t\tos.Exit(0)\n-\t},\n-}\n-\n-func configHelp(conf config.Configuration, componentType, name, description string) {\n-\n-\tparamMap := params.NewParamMap(map[string]config.Configuration{\n-\t\tname: conf,\n-\t})\n-\ttempHelpCmd := &cobra.Command{\n-\t\tUse:           fmt.Sprintf(\"--%s=%s\", componentType, name),\n-\t\tShort:         fmt.Sprintf(\"Help about the %s collector\", name),\n-\t\tLong:          description,\n-\t\tSilenceErrors: true,\n-\t\tRun: func(c *cobra.Command, args []string) {\n-\t\t\t_ = c.Help()\n-\t\t},\n-\t}\n-\tparams.AddParamMapToCmd(paramMap, tempHelpCmd, componentType, false)\n-\t// this is workaround to hide the help flag\n-\ttempHelpCmd.Flags().BoolP(\"help\", \"h\", false, \"Dummy help\")\n-\ttempHelpCmd.Flags().Lookup(\"help\").Hidden = true\n-\ttempHelpCmd.SetUsageTemplate(`\n-{{.Long}}\n-\n-Usage:{{if .Runnable}}\n-  {{.UseLine}}{{end}}{{if .HasExample}}\n-\n-Examples:\n-{{.Example}}{{end}}\n-\n-Flags:{{if .HasAvailableLocalFlags}}\n-{{.LocalFlags.FlagUsages | trimTrailingWhitespaces}}{{else}}\n-\n-\tNo configuration flags available\n-{{end}}\n-`)\n-\n-\t_ = tempHelpCmd.Execute()\n-}\ndiff --git a/programs/diagnostics/cmd/params/params.go b/programs/diagnostics/cmd/params/params.go\ndeleted file mode 100644\nindex c4464aab5d24..000000000000\n--- a/programs/diagnostics/cmd/params/params.go\n+++ /dev/null\n@@ -1,281 +0,0 @@\n-package params\n-\n-import (\n-\t\"bytes\"\n-\t\"encoding/csv\"\n-\t\"fmt\"\n-\t\"strings\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils\"\n-\t\"github.com/spf13/cobra\"\n-)\n-\n-type cliParamType uint8\n-\n-const (\n-\tString cliParamType = iota\n-\tStringList\n-\tStringOptionsList\n-\tInteger\n-\tBoolean\n-)\n-\n-type CliParam struct {\n-\tDescription string\n-\tDefault     interface{}\n-\t//this should always be an address to a value - as required by cobra\n-\tValue interface{}\n-\tType  cliParamType\n-}\n-\n-type ParamMap map[string]map[string]CliParam\n-\n-func NewParamMap(configs map[string]config.Configuration) ParamMap {\n-\tparamMap := make(ParamMap)\n-\tfor name, configuration := range configs {\n-\t\tfor _, param := range configuration.Params {\n-\t\t\tswitch p := param.(type) {\n-\t\t\tcase config.StringParam:\n-\t\t\t\tparamMap = paramMap.createStringParam(name, p)\n-\t\t\tcase config.StringListParam:\n-\t\t\t\tparamMap = paramMap.createStringListParam(name, p)\n-\t\t\tcase config.StringOptions:\n-\t\t\t\tparamMap = paramMap.createStringOptionsParam(name, p)\n-\t\t\tcase config.IntParam:\n-\t\t\t\tparamMap = paramMap.createIntegerParam(name, p)\n-\t\t\tcase config.BoolParam:\n-\t\t\t\tparamMap = paramMap.createBoolParam(name, p)\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn paramMap\n-}\n-\n-func (m ParamMap) createBoolParam(rootKey string, bParam config.BoolParam) ParamMap {\n-\tif _, ok := m[rootKey]; !ok {\n-\t\tm[rootKey] = make(map[string]CliParam)\n-\t}\n-\tvar value bool\n-\tparam := CliParam{\n-\t\tDescription: bParam.Description(),\n-\t\tDefault:     bParam.Value,\n-\t\tValue:       &value,\n-\t\tType:        Boolean,\n-\t}\n-\tm[rootKey][bParam.Name()] = param\n-\treturn m\n-}\n-\n-func (m ParamMap) createStringParam(rootKey string, sParam config.StringParam) ParamMap {\n-\tif _, ok := m[rootKey]; !ok {\n-\t\tm[rootKey] = make(map[string]CliParam)\n-\t}\n-\tvar value string\n-\tparam := CliParam{\n-\t\tDescription: sParam.Description(),\n-\t\tDefault:     sParam.Value,\n-\t\tValue:       &value,\n-\t\tType:        String,\n-\t}\n-\tm[rootKey][sParam.Name()] = param\n-\treturn m\n-}\n-\n-func (m ParamMap) createStringListParam(rootKey string, lParam config.StringListParam) ParamMap {\n-\tif _, ok := m[rootKey]; !ok {\n-\t\tm[rootKey] = make(map[string]CliParam)\n-\t}\n-\tvar value []string\n-\tparam := CliParam{\n-\t\tDescription: lParam.Description(),\n-\t\tDefault:     lParam.Values,\n-\t\tValue:       &value,\n-\t\tType:        StringList,\n-\t}\n-\tm[rootKey][lParam.Name()] = param\n-\treturn m\n-}\n-\n-func (m ParamMap) createStringOptionsParam(rootKey string, oParam config.StringOptions) ParamMap {\n-\tif _, ok := m[rootKey]; !ok {\n-\t\tm[rootKey] = make(map[string]CliParam)\n-\t}\n-\tvalue := StringOptionsVar{\n-\t\tOptions: oParam.Options,\n-\t\tValue:   oParam.Value,\n-\t}\n-\tparam := CliParam{\n-\t\tDescription: oParam.Description(),\n-\t\tDefault:     oParam.Value,\n-\t\tValue:       &value,\n-\t\tType:        StringOptionsList,\n-\t}\n-\tm[rootKey][oParam.Name()] = param\n-\treturn m\n-}\n-\n-func (m ParamMap) createIntegerParam(rootKey string, iParam config.IntParam) ParamMap {\n-\tif _, ok := m[rootKey]; !ok {\n-\t\tm[rootKey] = make(map[string]CliParam)\n-\t}\n-\tvar value int64\n-\tparam := CliParam{\n-\t\tDescription: iParam.Description(),\n-\t\tDefault:     iParam.Value,\n-\t\tValue:       &value,\n-\t\tType:        Integer,\n-\t}\n-\tm[rootKey][iParam.Name()] = param\n-\treturn m\n-}\n-\n-func (c CliParam) GetConfigParam(name string) config.ConfigParam {\n-\t// this is a config being passed to a collector - required can be false\n-\tparam := config.NewParam(name, c.Description, false)\n-\tswitch c.Type {\n-\tcase String:\n-\t\treturn config.StringParam{\n-\t\t\tParam: param,\n-\t\t\t// values will be pointers\n-\t\t\tValue: *(c.Value.(*string)),\n-\t\t}\n-\tcase StringList:\n-\t\treturn config.StringListParam{\n-\t\t\tParam:  param,\n-\t\t\tValues: *(c.Value.(*[]string)),\n-\t\t}\n-\tcase StringOptionsList:\n-\t\toptionsVar := *(c.Value.(*StringOptionsVar))\n-\t\treturn config.StringOptions{\n-\t\t\tParam:   param,\n-\t\t\tOptions: optionsVar.Options,\n-\t\t\tValue:   optionsVar.Value,\n-\t\t}\n-\tcase Integer:\n-\t\treturn config.IntParam{\n-\t\t\tParam: param,\n-\t\t\tValue: *(c.Value.(*int64)),\n-\t\t}\n-\tcase Boolean:\n-\t\treturn config.BoolParam{\n-\t\t\tParam: param,\n-\t\t\tValue: *(c.Value.(*bool)),\n-\t\t}\n-\t}\n-\treturn param\n-}\n-\n-type StringOptionsVar struct {\n-\tOptions []string\n-\tValue   string\n-}\n-\n-func (o StringOptionsVar) String() string {\n-\treturn o.Value\n-}\n-\n-func (o *StringOptionsVar) Set(p string) error {\n-\tisIncluded := func(opts []string, val string) bool {\n-\t\tfor _, opt := range opts {\n-\t\t\tif val == opt {\n-\t\t\t\treturn true\n-\t\t\t}\n-\t\t}\n-\t\treturn false\n-\t}\n-\tif !isIncluded(o.Options, p) {\n-\t\treturn fmt.Errorf(\"%s is not included in options: %v\", p, o.Options)\n-\t}\n-\to.Value = p\n-\treturn nil\n-}\n-\n-func (o *StringOptionsVar) Type() string {\n-\treturn \"string\"\n-}\n-\n-type StringSliceOptionsVar struct {\n-\tOptions []string\n-\tValues  []string\n-}\n-\n-func (o StringSliceOptionsVar) String() string {\n-\tstr, _ := writeAsCSV(o.Values)\n-\treturn \"[\" + str + \"]\"\n-}\n-\n-func (o *StringSliceOptionsVar) Set(val string) error {\n-\tvalues, err := readAsCSV(val)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tvValues := utils.Distinct(values, o.Options)\n-\tif len(vValues) > 0 {\n-\t\treturn fmt.Errorf(\"%v are not included in options: %v\", vValues, o.Options)\n-\t}\n-\to.Values = values\n-\treturn nil\n-}\n-\n-func (o *StringSliceOptionsVar) Type() string {\n-\treturn \"stringSlice\"\n-}\n-\n-func writeAsCSV(vals []string) (string, error) {\n-\tb := &bytes.Buffer{}\n-\tw := csv.NewWriter(b)\n-\terr := w.Write(vals)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\tw.Flush()\n-\treturn strings.TrimSuffix(b.String(), \"\\n\"), nil\n-}\n-\n-func readAsCSV(val string) ([]string, error) {\n-\tif val == \"\" {\n-\t\treturn []string{}, nil\n-\t}\n-\tstringReader := strings.NewReader(val)\n-\tcsvReader := csv.NewReader(stringReader)\n-\treturn csvReader.Read()\n-}\n-\n-func AddParamMapToCmd(paramMap ParamMap, cmd *cobra.Command, prefix string, hide bool) {\n-\tfor rootKey, childMap := range paramMap {\n-\t\tfor childKey, value := range childMap {\n-\t\t\tparamName := fmt.Sprintf(\"%s.%s.%s\", prefix, rootKey, childKey)\n-\t\t\tswitch value.Type {\n-\t\t\tcase String:\n-\t\t\t\tcmd.Flags().StringVar(value.Value.(*string), paramName, value.Default.(string), value.Description)\n-\t\t\tcase StringList:\n-\t\t\t\tcmd.Flags().StringSliceVar(value.Value.(*[]string), paramName, value.Default.([]string), value.Description)\n-\t\t\tcase StringOptionsList:\n-\t\t\t\tcmd.Flags().Var(value.Value.(*StringOptionsVar), paramName, value.Description)\n-\t\t\tcase Integer:\n-\t\t\t\tcmd.Flags().Int64Var(value.Value.(*int64), paramName, value.Default.(int64), value.Description)\n-\t\t\tcase Boolean:\n-\t\t\t\tcmd.Flags().BoolVar(value.Value.(*bool), paramName, value.Default.(bool), value.Description)\n-\t\t\t}\n-\t\t\t// this ensures flags from collectors and outputs are not shown as they will pollute the output\n-\t\t\tif hide {\n-\t\t\t\t_ = cmd.Flags().MarkHidden(paramName)\n-\t\t\t}\n-\t\t}\n-\t}\n-}\n-\n-func ConvertParamsToConfig(paramMap ParamMap) map[string]config.Configuration {\n-\tconfiguration := make(map[string]config.Configuration)\n-\tfor rootKey, childMap := range paramMap {\n-\t\tif _, ok := configuration[rootKey]; !ok {\n-\t\t\tconfiguration[rootKey] = config.Configuration{}\n-\t\t}\n-\t\tfor childKey, value := range childMap {\n-\t\t\tconfigParam := value.GetConfigParam(childKey)\n-\t\t\tconfiguration[rootKey] = config.Configuration{Params: append(configuration[rootKey].Params, configParam)}\n-\t\t}\n-\t}\n-\treturn configuration\n-}\ndiff --git a/programs/diagnostics/cmd/root.go b/programs/diagnostics/cmd/root.go\ndeleted file mode 100644\nindex 4cf329d54389..000000000000\n--- a/programs/diagnostics/cmd/root.go\n+++ /dev/null\n@@ -1,174 +0,0 @@\n-package cmd\n-\n-import (\n-\t\"fmt\"\n-\t\"net/http\"\n-\t_ \"net/http/pprof\"\n-\t\"os\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils\"\n-\t\"github.com/pkg/errors\"\n-\t\"github.com/rs/zerolog\"\n-\t\"github.com/rs/zerolog/log\"\n-\t\"github.com/spf13/cobra\"\n-\t\"github.com/spf13/viper\"\n-)\n-\n-func enableDebug() {\n-\tif debug {\n-\t\tzerolog.SetGlobalLevel(zerolog.DebugLevel)\n-\t\tgo func() {\n-\t\t\terr := http.ListenAndServe(\"localhost:8080\", nil)\n-\t\t\tif err != nil {\n-\t\t\t\tlog.Error().Err(err).Msg(\"unable to start debugger\")\n-\t\t\t} else {\n-\t\t\t\tlog.Debug().Msg(\"debugger has been started on port 8080\")\n-\t\t\t}\n-\t\t}()\n-\t}\n-}\n-\n-var rootCmd = &cobra.Command{\n-\tUse:   \"clickhouse-diagnostics\",\n-\tShort: \"Capture and convert ClickHouse diagnostic bundles.\",\n-\tLong:  `Captures ClickHouse diagnostic bundles to a number of supported formats, including file and ClickHouse itself. Converts bundles between formats.`,\n-\tPersistentPreRunE: func(cmd *cobra.Command, args []string) error {\n-\t\tenableDebug()\n-\t\terr := initializeConfig()\n-\t\tif err != nil {\n-\t\t\tlog.Error().Err(err)\n-\t\t\tos.Exit(1)\n-\t\t}\n-\t\treturn nil\n-\t},\n-\tExample: `clickhouse-diagnostics collect`,\n-}\n-\n-const (\n-\tcolorRed = iota + 31\n-\tcolorGreen\n-\tcolorYellow\n-\tcolorMagenta = 35\n-\n-\tcolorBold = 1\n-)\n-\n-const TimeFormat = time.RFC3339\n-\n-var debug bool\n-var configFiles []string\n-\n-const (\n-\t// The environment variable prefix of all environment variables bound to our command line flags.\n-\t// For example, --output is bound to CLICKHOUSE_DIAGNOSTIC_OUTPUT.\n-\tenvPrefix = \"CLICKHOUSE_DIAGNOSTIC\"\n-)\n-\n-func init() {\n-\trootCmd.PersistentFlags().BoolVarP(&debug, \"debug\", \"d\", false, \"Enable debug mode\")\n-\trootCmd.PersistentFlags().StringSliceVarP(&configFiles, \"config\", \"f\", []string{\"clickhouse-diagnostics.yml\", \"/etc/clickhouse-diagnostics.yml\"}, \"Configuration file path\")\n-\t// set a usage template to ensure flags on root are listed as global\n-\trootCmd.SetUsageTemplate(`Usage:{{if .Runnable}}\n-  {{.UseLine}}{{end}}{{if .HasAvailableSubCommands}}\n-  {{.CommandPath}} [command]{{end}}{{if gt (len .Aliases) 0}}\n-\n-Aliases:\n-  {{.NameAndAliases}}{{end}}{{if .HasExample}}\n-\n-Examples:\n-{{.Example}}{{end}}{{if .HasAvailableSubCommands}}\n-\n-Available Commands:{{range .Commands}}{{if (or .IsAvailableCommand (eq .Name \"help\"))}}\n-  {{rpad .Name .NamePadding }} {{.Short}}{{end}}{{end}}{{end}}{{if .HasAvailableLocalFlags}}\n-\n-Global Flags:\n-{{.LocalFlags.FlagUsages | trimTrailingWhitespaces}}{{end}}{{if .HasAvailableInheritedFlags}}\n-\n-Additional help topics:{{range .Commands}}{{if .IsAdditionalHelpTopicCommand}}\n-  {{rpad .CommandPath .CommandPathPadding}} {{.Short}}{{end}}{{end}}{{end}}{{if .HasAvailableSubCommands}}\n-\n-Use \"{{.CommandPath}} [command] --help\" for more information about a command.{{end}}\n-`)\n-\trootCmd.SetFlagErrorFunc(handleFlagErrors)\n-\n-}\n-\n-func Execute() {\n-\t// logs go to stderr - stdout is exclusive for outputs e.g. tables\n-\toutput := zerolog.ConsoleWriter{Out: os.Stderr, TimeFormat: TimeFormat}\n-\t// override the colors\n-\toutput.FormatLevel = func(i interface{}) string {\n-\t\tvar l string\n-\t\tif ll, ok := i.(string); ok {\n-\t\t\tswitch ll {\n-\t\t\tcase zerolog.LevelTraceValue:\n-\t\t\t\tl = colorize(\"TRC\", colorMagenta)\n-\t\t\tcase zerolog.LevelDebugValue:\n-\t\t\t\tl = colorize(\"DBG\", colorMagenta)\n-\t\t\tcase zerolog.LevelInfoValue:\n-\t\t\t\tl = colorize(\"INF\", colorGreen)\n-\t\t\tcase zerolog.LevelWarnValue:\n-\t\t\t\tl = colorize(colorize(\"WRN\", colorYellow), colorBold)\n-\t\t\tcase zerolog.LevelErrorValue:\n-\t\t\t\tl = colorize(colorize(\"ERR\", colorRed), colorBold)\n-\t\t\tcase zerolog.LevelFatalValue:\n-\t\t\t\tl = colorize(colorize(\"FTL\", colorRed), colorBold)\n-\t\t\tcase zerolog.LevelPanicValue:\n-\t\t\t\tl = colorize(colorize(\"PNC\", colorRed), colorBold)\n-\t\t\tdefault:\n-\t\t\t\tl = colorize(\"???\", colorBold)\n-\t\t\t}\n-\t\t} else {\n-\t\t\tif i == nil {\n-\t\t\t\tl = colorize(\"???\", colorBold)\n-\t\t\t} else {\n-\t\t\t\tl = strings.ToUpper(fmt.Sprintf(\"%s\", i))[0:3]\n-\t\t\t}\n-\t\t}\n-\t\treturn l\n-\t}\n-\toutput.FormatTimestamp = func(i interface{}) string {\n-\t\ttt := i.(string)\n-\t\treturn colorize(tt, colorGreen)\n-\t}\n-\tlog.Logger = log.Output(output)\n-\tzerolog.SetGlobalLevel(zerolog.InfoLevel)\n-\trootCmd.SetHelpCommand(helpCmd)\n-\tif err := rootCmd.Execute(); err != nil {\n-\t\tlog.Fatal().Err(err)\n-\t}\n-}\n-\n-// colorize returns the string s wrapped in ANSI code c\n-func colorize(s interface{}, c int) string {\n-\treturn fmt.Sprintf(\"\\x1b[%dm%v\\x1b[0m\", c, s)\n-}\n-\n-func handleFlagErrors(cmd *cobra.Command, err error) error {\n-\tfmt.Println(colorize(colorize(fmt.Sprintf(\"Error: %s\\n\", err), colorRed), colorBold))\n-\t_ = cmd.Help()\n-\tos.Exit(1)\n-\treturn nil\n-}\n-\n-func initializeConfig() error {\n-\t// we use the first config file we find\n-\tvar configFile string\n-\tfor _, confFile := range configFiles {\n-\t\tif ok, _ := utils.FileExists(confFile); ok {\n-\t\t\tconfigFile = confFile\n-\t\t\tbreak\n-\t\t}\n-\t}\n-\tif configFile == \"\" {\n-\t\tlog.Warn().Msgf(\"config file in %s not found - config file will be ignored\", configFiles)\n-\t\treturn nil\n-\t}\n-\tviper.SetConfigFile(configFile)\n-\tif err := viper.ReadInConfig(); err != nil {\n-\t\treturn errors.Wrapf(err, \"Unable to read configuration file at %s\", configFile)\n-\t}\n-\treturn nil\n-}\ndiff --git a/programs/diagnostics/cmd/version.go b/programs/diagnostics/cmd/version.go\ndeleted file mode 100644\nindex b1c0b44171b9..000000000000\n--- a/programs/diagnostics/cmd/version.go\n+++ /dev/null\n@@ -1,24 +0,0 @@\n-package cmd\n-\n-import (\n-\t\"fmt\"\n-\t\"github.com/spf13/cobra\"\n-)\n-\n-var (\n-\tVersion = \"\" // set at compile time with -ldflags \"-X versserv/cmd.Version=x.y.yz\"\n-\tCommit  = \"\"\n-)\n-\n-func init() {\n-\trootCmd.AddCommand(versionCmd)\n-}\n-\n-var versionCmd = &cobra.Command{\n-\tUse:   \"version\",\n-\tShort: \"Print the version number of clickhouse-diagnostics\",\n-\tLong:  `All software has versions. This is clickhouse-diagnostics`,\n-\tRun: func(cmd *cobra.Command, args []string) {\n-\t\tfmt.Printf(\"Clickhouse Diagnostics %s (%s)\\n\", Version, Commit)\n-\t},\n-}\ndiff --git a/programs/diagnostics/go.mod b/programs/diagnostics/go.mod\ndeleted file mode 100644\nindex 34c6b0037ae0..000000000000\n--- a/programs/diagnostics/go.mod\n+++ /dev/null\n@@ -1,89 +0,0 @@\n-module github.com/ClickHouse/ClickHouse/programs/diagnostics\n-\n-go 1.19\n-\n-require (\n-\tgithub.com/ClickHouse/clickhouse-go/v2 v2.0.12\n-\tgithub.com/DATA-DOG/go-sqlmock v1.5.0\n-\tgithub.com/Masterminds/semver v1.5.0\n-\tgithub.com/bmatcuk/doublestar/v4 v4.0.2\n-\tgithub.com/docker/go-connections v0.4.0\n-\tgithub.com/elastic/gosigar v0.14.2\n-\tgithub.com/google/shlex v0.0.0-20191202100458-e7afc7fbc510\n-\tgithub.com/jaypipes/ghw v0.8.0\n-\tgithub.com/matishsiao/goInfo v0.0.0-20210923090445-da2e3fa8d45f\n-\tgithub.com/mholt/archiver/v4 v4.0.0-alpha.4\n-\tgithub.com/olekukonko/tablewriter v0.0.5\n-\tgithub.com/pkg/errors v0.9.1\n-\tgithub.com/rs/zerolog v1.26.1\n-\tgithub.com/spf13/cobra v1.3.0\n-\tgithub.com/spf13/pflag v1.0.5\n-\tgithub.com/spf13/viper v1.10.1\n-\tgithub.com/stretchr/testify v1.8.1\n-\tgithub.com/testcontainers/testcontainers-go v0.18.0\n-\tgithub.com/yargevad/filepathx v1.0.0\n-\tgopkg.in/yaml.v3 v3.0.1\n-)\n-\n-require (\n-\tgithub.com/Azure/go-ansiterm v0.0.0-20210617225240-d185dfc1b5a1 // indirect\n-\tgithub.com/Microsoft/go-winio v0.5.2 // indirect\n-\tgithub.com/StackExchange/wmi v0.0.0-20190523213315-cbe66965904d // indirect\n-\tgithub.com/andybalholm/brotli v1.0.4 // indirect\n-\tgithub.com/cenkalti/backoff/v4 v4.2.0 // indirect\n-\tgithub.com/containerd/containerd v1.6.17 // indirect\n-\tgithub.com/davecgh/go-spew v1.1.1 // indirect\n-\tgithub.com/distribution/distribution v2.8.2+incompatible // indirect\n-\tgithub.com/docker/distribution v2.8.1+incompatible // indirect\n-\tgithub.com/docker/docker v23.0.0+incompatible // indirect\n-\tgithub.com/docker/go-units v0.5.0 // indirect\n-\tgithub.com/dsnet/compress v0.0.1 // indirect\n-\tgithub.com/fsnotify/fsnotify v1.5.4 // indirect\n-\tgithub.com/ghodss/yaml v1.0.0 // indirect\n-\tgithub.com/go-ole/go-ole v1.2.4 // indirect\n-\tgithub.com/gogo/protobuf v1.3.2 // indirect\n-\tgithub.com/golang/protobuf v1.5.2 // indirect\n-\tgithub.com/golang/snappy v0.0.4 // indirect\n-\tgithub.com/google/uuid v1.3.0 // indirect\n-\tgithub.com/hashicorp/hcl v1.0.0 // indirect\n-\tgithub.com/inconshreveable/mousetrap v1.0.0 // indirect\n-\tgithub.com/jaypipes/pcidb v0.6.0 // indirect\n-\tgithub.com/klauspost/compress v1.13.6 // indirect\n-\tgithub.com/klauspost/pgzip v1.2.5 // indirect\n-\tgithub.com/kr/text v0.2.0 // indirect\n-\tgithub.com/magiconair/properties v1.8.7 // indirect\n-\tgithub.com/mattn/go-runewidth v0.0.9 // indirect\n-\tgithub.com/mitchellh/go-homedir v1.1.0 // indirect\n-\tgithub.com/mitchellh/mapstructure v1.4.3 // indirect\n-\tgithub.com/moby/patternmatcher v0.5.0 // indirect\n-\tgithub.com/moby/sys/sequential v0.5.0 // indirect\n-\tgithub.com/moby/term v0.0.0-20221128092401-c43b287e0e0f // indirect\n-\tgithub.com/morikuni/aec v1.0.0 // indirect\n-\tgithub.com/nwaples/rardecode/v2 v2.0.0-beta.2 // indirect\n-\tgithub.com/opencontainers/go-digest v1.0.0 // indirect\n-\tgithub.com/opencontainers/image-spec v1.1.0-rc2 // indirect\n-\tgithub.com/opencontainers/runc v1.1.3 // indirect\n-\tgithub.com/paulmach/orb v0.4.0 // indirect\n-\tgithub.com/pelletier/go-toml v1.9.5 // indirect\n-\tgithub.com/pierrec/lz4/v4 v4.1.14 // indirect\n-\tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n-\tgithub.com/shopspring/decimal v1.3.1 // indirect\n-\tgithub.com/sirupsen/logrus v1.9.0 // indirect\n-\tgithub.com/spf13/afero v1.8.0 // indirect\n-\tgithub.com/spf13/cast v1.4.1 // indirect\n-\tgithub.com/spf13/jwalterweatherman v1.1.0 // indirect\n-\tgithub.com/subosito/gotenv v1.2.0 // indirect\n-\tgithub.com/therootcompany/xz v1.0.1 // indirect\n-\tgithub.com/ulikunitz/xz v0.5.10 // indirect\n-\tgo.opentelemetry.io/otel v1.4.1 // indirect\n-\tgo.opentelemetry.io/otel/trace v1.4.1 // indirect\n-\tgolang.org/x/net v0.0.0-20220906165146-f3363e06e74c // indirect\n-\tgolang.org/x/sys v0.5.0 // indirect\n-\tgolang.org/x/text v0.7.0 // indirect\n-\tgoogle.golang.org/genproto v0.0.0-20220617124728-180714bec0ad // indirect\n-\tgoogle.golang.org/grpc v1.47.0 // indirect\n-\tgoogle.golang.org/protobuf v1.28.0 // indirect\n-\tgopkg.in/ini.v1 v1.66.2 // indirect\n-\tgopkg.in/yaml.v2 v2.4.0 // indirect\n-\thowett.net/plist v0.0.0-20181124034731-591f970eefbb // indirect\n-)\ndiff --git a/programs/diagnostics/go.sum b/programs/diagnostics/go.sum\ndeleted file mode 100644\nindex a95dfb4fd2b8..000000000000\n--- a/programs/diagnostics/go.sum\n+++ /dev/null\n@@ -1,992 +0,0 @@\n-cloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\n-cloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\n-cloud.google.com/go v0.38.0/go.mod h1:990N+gfupTy94rShfmMCWGDn0LpTmnzTp2qbd1dvSRU=\n-cloud.google.com/go v0.44.1/go.mod h1:iSa0KzasP4Uvy3f1mN/7PiObzGgflwredwwASm/v6AU=\n-cloud.google.com/go v0.44.2/go.mod h1:60680Gw3Yr4ikxnPRS/oxxkBccT6SA1yMk63TGekxKY=\n-cloud.google.com/go v0.44.3/go.mod h1:60680Gw3Yr4ikxnPRS/oxxkBccT6SA1yMk63TGekxKY=\n-cloud.google.com/go v0.45.1/go.mod h1:RpBamKRgapWJb87xiFSdk4g1CME7QZg3uwTez+TSTjc=\n-cloud.google.com/go v0.46.3/go.mod h1:a6bKKbmY7er1mI7TEI4lsAkts/mkhTSZK8w33B4RAg0=\n-cloud.google.com/go v0.50.0/go.mod h1:r9sluTvynVuxRIOHXQEHMFffphuXHOMZMycpNR5e6To=\n-cloud.google.com/go v0.52.0/go.mod h1:pXajvRH/6o3+F9jDHZWQ5PbGhn+o8w9qiu/CffaVdO4=\n-cloud.google.com/go v0.53.0/go.mod h1:fp/UouUEsRkN6ryDKNW/Upv/JBKnv6WDthjR6+vze6M=\n-cloud.google.com/go v0.54.0/go.mod h1:1rq2OEkV3YMf6n/9ZvGWI3GWw0VoqH/1x2nd8Is/bPc=\n-cloud.google.com/go v0.56.0/go.mod h1:jr7tqZxxKOVYizybht9+26Z/gUq7tiRzu+ACVAMbKVk=\n-cloud.google.com/go v0.57.0/go.mod h1:oXiQ6Rzq3RAkkY7N6t3TcE6jE+CIBBbA36lwQ1JyzZs=\n-cloud.google.com/go v0.62.0/go.mod h1:jmCYTdRCQuc1PHIIJ/maLInMho30T/Y0M4hTdTShOYc=\n-cloud.google.com/go v0.65.0/go.mod h1:O5N8zS7uWy9vkA9vayVHs65eM1ubvY4h553ofrNHObY=\n-cloud.google.com/go v0.72.0/go.mod h1:M+5Vjvlc2wnp6tjzE102Dw08nGShTscUx2nZMufOKPI=\n-cloud.google.com/go v0.74.0/go.mod h1:VV1xSbzvo+9QJOxLDaJfTjx5e+MePCpCWwvftOeQmWk=\n-cloud.google.com/go v0.75.0/go.mod h1:VGuuCn7PG0dwsd5XPVm2Mm3wlh3EL55/79EKB6hlPTY=\n-cloud.google.com/go v0.78.0/go.mod h1:QjdrLG0uq+YwhjoVOLsS1t7TW8fs36kLs4XO5R5ECHg=\n-cloud.google.com/go v0.79.0/go.mod h1:3bzgcEeQlzbuEAYu4mrWhKqWjmpprinYgKJLgKHnbb8=\n-cloud.google.com/go v0.81.0/go.mod h1:mk/AM35KwGk/Nm2YSeZbxXdrNK3KZOYHmLkOqC2V6E0=\n-cloud.google.com/go v0.83.0/go.mod h1:Z7MJUsANfY0pYPdw0lbnivPx4/vhy/e2FEkSkF7vAVY=\n-cloud.google.com/go v0.84.0/go.mod h1:RazrYuxIK6Kb7YrzzhPoLmCVzl7Sup4NrbKPg8KHSUM=\n-cloud.google.com/go v0.87.0/go.mod h1:TpDYlFy7vuLzZMMZ+B6iRiELaY7z/gJPaqbMx6mlWcY=\n-cloud.google.com/go v0.90.0/go.mod h1:kRX0mNRHe0e2rC6oNakvwQqzyDmg57xJ+SZU1eT2aDQ=\n-cloud.google.com/go v0.93.3/go.mod h1:8utlLll2EF5XMAV15woO4lSbWQlk8rer9aLOfLh7+YI=\n-cloud.google.com/go v0.94.1/go.mod h1:qAlAugsXlC+JWO+Bke5vCtc9ONxjQT3drlTTnAplMW4=\n-cloud.google.com/go v0.97.0/go.mod h1:GF7l59pYBVlXQIBLx3a761cZ41F9bBH3JUlihCt2Udc=\n-cloud.google.com/go v0.98.0/go.mod h1:ua6Ush4NALrHk5QXDWnjvZHN93OuF0HfuEPq9I1X0cM=\n-cloud.google.com/go v0.99.0/go.mod h1:w0Xx2nLzqWJPuozYQX+hFfCSI8WioryfRDzkoI/Y2ZA=\n-cloud.google.com/go/bigquery v1.0.1/go.mod h1:i/xbL2UlR5RvWAURpBYZTtm/cXjCha9lbfbpx4poX+o=\n-cloud.google.com/go/bigquery v1.3.0/go.mod h1:PjpwJnslEMmckchkHFfq+HTD2DmtT67aNFKH1/VBDHE=\n-cloud.google.com/go/bigquery v1.4.0/go.mod h1:S8dzgnTigyfTmLBfrtrhyYhwRxG72rYxvftPBK2Dvzc=\n-cloud.google.com/go/bigquery v1.5.0/go.mod h1:snEHRnqQbz117VIFhE8bmtwIDY80NLUZUMb4Nv6dBIg=\n-cloud.google.com/go/bigquery v1.7.0/go.mod h1://okPTzCYNXSlb24MZs83e2Do+h+VXtc4gLoIoXIAPc=\n-cloud.google.com/go/bigquery v1.8.0/go.mod h1:J5hqkt3O0uAFnINi6JXValWIb1v0goeZM77hZzJN/fQ=\n-cloud.google.com/go/datastore v1.0.0/go.mod h1:LXYbyblFSglQ5pkeyhO+Qmw7ukd3C+pD7TKLgZqpHYE=\n-cloud.google.com/go/datastore v1.1.0/go.mod h1:umbIZjpQpHh4hmRpGhH4tLFup+FVzqBi1b3c64qFpCk=\n-cloud.google.com/go/firestore v1.6.1/go.mod h1:asNXNOzBdyVQmEU+ggO8UPodTkEVFW5Qx+rwHnAz+EY=\n-cloud.google.com/go/pubsub v1.0.1/go.mod h1:R0Gpsv3s54REJCy4fxDixWD93lHJMoZTyQ2kNxGRt3I=\n-cloud.google.com/go/pubsub v1.1.0/go.mod h1:EwwdRX2sKPjnvnqCa270oGRyludottCI76h+R3AArQw=\n-cloud.google.com/go/pubsub v1.2.0/go.mod h1:jhfEVHT8odbXTkndysNHCcx0awwzvfOlguIAii9o8iA=\n-cloud.google.com/go/pubsub v1.3.1/go.mod h1:i+ucay31+CNRpDW4Lu78I4xXG+O1r/MAHgjpRVR+TSU=\n-cloud.google.com/go/storage v1.0.0/go.mod h1:IhtSnM/ZTZV8YYJWCY8RULGVqBDmpoyjwiyrjsg+URw=\n-cloud.google.com/go/storage v1.5.0/go.mod h1:tpKbwo567HUNpVclU5sGELwQWBDZ8gh0ZeosJ0Rtdos=\n-cloud.google.com/go/storage v1.6.0/go.mod h1:N7U0C8pVQ/+NIKOBQyamJIeKQKkZ+mxpohlUTyfDhBk=\n-cloud.google.com/go/storage v1.8.0/go.mod h1:Wv1Oy7z6Yz3DshWRJFhqM/UCfaWIRTdp0RXyy7KQOVs=\n-cloud.google.com/go/storage v1.10.0/go.mod h1:FLPqc6j+Ki4BU591ie1oL6qBQGu2Bl/tZ9ullr3+Kg0=\n-cloud.google.com/go/storage v1.14.0/go.mod h1:GrKmX003DSIwi9o29oFT7YDnHYwZoctc3fOKtUw0Xmo=\n-dmitri.shuralyov.com/gpu/mtl v0.0.0-20190408044501-666a987793e9/go.mod h1:H6x//7gZCb22OMCxBHrMx7a5I7Hp++hsVxbQ4BYO7hU=\n-github.com/Azure/go-ansiterm v0.0.0-20210617225240-d185dfc1b5a1 h1:UQHMgLO+TxOElx5B5HZ4hJQsoJ/PvUvKRhJHDQXO8P8=\n-github.com/Azure/go-ansiterm v0.0.0-20210617225240-d185dfc1b5a1/go.mod h1:xomTg63KZ2rFqZQzSB4Vz2SUXa1BpHTVz9L5PTmPC4E=\n-github.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=\n-github.com/BurntSushi/xgb v0.0.0-20160522181843-27f122750802/go.mod h1:IVnqGOEym/WlBOVXweHU+Q+/VP0lqqI8lqeDx9IjBqo=\n-github.com/ClickHouse/clickhouse-go v1.5.3/go.mod h1:EaI/sW7Azgz9UATzd5ZdZHRUhHgv5+JMS9NSr2smCJI=\n-github.com/ClickHouse/clickhouse-go/v2 v2.0.12 h1:Nbl/NZwoM6LGJm7smNBgvtdr/rxjlIssSW3eG/Nmb9E=\n-github.com/ClickHouse/clickhouse-go/v2 v2.0.12/go.mod h1:u4RoNQLLM2W6hNSPYrIESLJqaWSInZVmfM+MlaAhXcg=\n-github.com/DATA-DOG/go-sqlmock v1.5.0 h1:Shsta01QNfFxHCfpW6YH2STWB0MudeXXEWMr20OEh60=\n-github.com/DATA-DOG/go-sqlmock v1.5.0/go.mod h1:f/Ixk793poVmq4qj/V1dPUg2JEAKC73Q5eFN3EC/SaM=\n-github.com/DataDog/datadog-go v3.2.0+incompatible/go.mod h1:LButxg5PwREeZtORoXG3tL4fMGNddJ+vMq1mwgfaqoQ=\n-github.com/Masterminds/semver v1.5.0 h1:H65muMkzWKEuNDnfl9d70GUjFniHKHRbFPGBuZ3QEww=\n-github.com/Masterminds/semver v1.5.0/go.mod h1:MB6lktGJrhw8PrUyiEoblNEGEQ+RzHPF078ddwwvV3Y=\n-github.com/Microsoft/go-winio v0.5.2 h1:a9IhgEQBCUEk6QCdml9CiJGhAws+YwffDHEMp1VMrpA=\n-github.com/Microsoft/go-winio v0.5.2/go.mod h1:WpS1mjBmmwHBEWmogvA2mj8546UReBk4v8QkMxJ6pZY=\n-github.com/Microsoft/hcsshim v0.9.6 h1:VwnDOgLeoi2du6dAznfmspNqTiwczvjv4K7NxuY9jsY=\n-github.com/OneOfOne/xxhash v1.2.2/go.mod h1:HSdplMjZKSmBqAxg5vPj2TmRDmfkzw+cTzAElWljhcU=\n-github.com/StackExchange/wmi v0.0.0-20190523213315-cbe66965904d h1:G0m3OIz70MZUWq3EgK3CesDbo8upS2Vm9/P3FtgI+Jk=\n-github.com/StackExchange/wmi v0.0.0-20190523213315-cbe66965904d/go.mod h1:3eOhrUMpNV+6aFIbp5/iudMxNCF27Vw2OZgy4xEx0Fg=\n-github.com/alecthomas/template v0.0.0-20160405071501-a0175ee3bccc/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=\n-github.com/alecthomas/template v0.0.0-20190718012654-fb15b899a751/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=\n-github.com/alecthomas/units v0.0.0-20151022065526-2efee857e7cf/go.mod h1:ybxpYRFXyAe+OPACYpWeL0wqObRcbAqCMya13uyzqw0=\n-github.com/alecthomas/units v0.0.0-20190717042225-c3de453c63f4/go.mod h1:ybxpYRFXyAe+OPACYpWeL0wqObRcbAqCMya13uyzqw0=\n-github.com/andybalholm/brotli v1.0.4 h1:V7DdXeJtZscaqfNuAdSRuRFzuiKlHSC/Zh3zl9qY3JY=\n-github.com/andybalholm/brotli v1.0.4/go.mod h1:fO7iG3H7G2nSZ7m0zPUDn85XEX2GTukHGRSepvi9Eig=\n-github.com/antihax/optional v1.0.0/go.mod h1:uupD/76wgC+ih3iEmQUL+0Ugr19nfwCT1kdvxnR2qWY=\n-github.com/armon/circbuf v0.0.0-20150827004946-bbbad097214e/go.mod h1:3U/XgcO3hCbHZ8TKRvWD2dDTCfh9M9ya+I9JpbB7O8o=\n-github.com/armon/go-metrics v0.0.0-20180917152333-f0300d1749da/go.mod h1:Q73ZrmVTwzkszR9V5SSuryQ31EELlFMUz1kKyl939pY=\n-github.com/armon/go-metrics v0.3.10/go.mod h1:4O98XIr/9W0sxpJ8UaYkvjk10Iff7SnFrb4QAOwNTFc=\n-github.com/armon/go-radix v0.0.0-20180808171621-7fddfc383310/go.mod h1:ufUuZ+zHj4x4TnLV4JWEpy2hxWSpsRywHrMgIH9cCH8=\n-github.com/armon/go-radix v1.0.0/go.mod h1:ufUuZ+zHj4x4TnLV4JWEpy2hxWSpsRywHrMgIH9cCH8=\n-github.com/beorn7/perks v0.0.0-20180321164747-3a771d992973/go.mod h1:Dwedo/Wpr24TaqPxmxbtue+5NUziq4I4S80YR8gNf3Q=\n-github.com/beorn7/perks v1.0.0/go.mod h1:KWe93zE9D1o94FZ5RNwFwVgaQK1VOXiVxmqh+CedLV8=\n-github.com/beorn7/perks v1.0.1/go.mod h1:G2ZrVWU2WbWT9wwq4/hrbKbnv/1ERSJQ0ibhJ6rlkpw=\n-github.com/bgentry/speakeasy v0.1.0/go.mod h1:+zsyZBPWlz7T6j88CTgSN5bM796AkVf0kBD4zp0CCIs=\n-github.com/bkaradzic/go-lz4 v1.0.0/go.mod h1:0YdlkowM3VswSROI7qDxhRvJ3sLhlFrRRwjwegp5jy4=\n-github.com/bmatcuk/doublestar/v4 v4.0.2 h1:X0krlUVAVmtr2cRoTqR8aDMrDqnB36ht8wpWTiQ3jsA=\n-github.com/bmatcuk/doublestar/v4 v4.0.2/go.mod h1:xBQ8jztBU6kakFMg+8WGxn0c6z1fTSPVIjEY1Wr7jzc=\n-github.com/cenkalti/backoff/v4 v4.2.0 h1:HN5dHm3WBOgndBH6E8V0q2jIYIR3s9yglV8k/+MN3u4=\n-github.com/cenkalti/backoff/v4 v4.2.0/go.mod h1:Y3VNntkOUPxTVeUxJ/G5vcM//AlwfmyYozVcomhLiZE=\n-github.com/census-instrumentation/opencensus-proto v0.2.1/go.mod h1:f6KPmirojxKA12rnyqOA5BBL4O983OfeGPqjHWSTneU=\n-github.com/census-instrumentation/opencensus-proto v0.3.0/go.mod h1:f6KPmirojxKA12rnyqOA5BBL4O983OfeGPqjHWSTneU=\n-github.com/cespare/xxhash v1.1.0/go.mod h1:XrSqR1VqqWfGrhpAt58auRo0WTKS1nRRg3ghfAqPWnc=\n-github.com/cespare/xxhash/v2 v2.1.1/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\n-github.com/cespare/xxhash/v2 v2.1.2/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\n-github.com/checkpoint-restore/go-criu/v5 v5.3.0/go.mod h1:E/eQpaFtUKGOOSEBZgmKAcn+zUUwWxqcaKZlF54wK8E=\n-github.com/chzyer/logex v1.1.10/go.mod h1:+Ywpsq7O8HXn0nuIou7OrIPyXbp3wmkHB+jjWRnGsAI=\n-github.com/chzyer/readline v0.0.0-20180603132655-2972be24d48e/go.mod h1:nSuG5e5PlCu98SY8svDHJxuZscDgtXS6KTTbou5AhLI=\n-github.com/chzyer/test v0.0.0-20180213035817-a1ea475d72b1/go.mod h1:Q3SI9o4m/ZMnBNeIyt5eFwwo7qiLfzFZmjNmxjkiQlU=\n-github.com/cilium/ebpf v0.7.0/go.mod h1:/oI2+1shJiTGAMgl6/RgJr36Eo1jzrRcAWbcXO2usCA=\n-github.com/circonus-labs/circonus-gometrics v2.3.1+incompatible/go.mod h1:nmEj6Dob7S7YxXgwXpfOuvO54S+tGdZdw9fuRZt25Ag=\n-github.com/circonus-labs/circonusllhist v0.1.3/go.mod h1:kMXHVDlOchFAehlya5ePtbp5jckzBHf4XRpQvBOLI+I=\n-github.com/client9/misspell v0.3.4/go.mod h1:qj6jICC3Q7zFZvVWo7KLAzC3yx5G7kyvSDkc90ppPyw=\n-github.com/cloudflare/golz4 v0.0.0-20150217214814-ef862a3cdc58/go.mod h1:EOBUe0h4xcZ5GoxqC5SDxFQ8gwyZPKQoEzownBlhI80=\n-github.com/cncf/udpa/go v0.0.0-20191209042840-269d4d468f6f/go.mod h1:M8M6+tZqaGXZJjfX53e64911xZQV5JYwmTeXPW+k8Sc=\n-github.com/cncf/udpa/go v0.0.0-20200629203442-efcf912fb354/go.mod h1:WmhPx2Nbnhtbo57+VJT5O0JRkEi1Wbu0z5j0R8u5Hbk=\n-github.com/cncf/udpa/go v0.0.0-20201120205902-5459f2c99403/go.mod h1:WmhPx2Nbnhtbo57+VJT5O0JRkEi1Wbu0z5j0R8u5Hbk=\n-github.com/cncf/udpa/go v0.0.0-20210930031921-04548b0d99d4/go.mod h1:6pvJx4me5XPnfI9Z40ddWsdw2W/uZgQLFXToKeRcDiI=\n-github.com/cncf/xds/go v0.0.0-20210312221358-fbca930ec8ed/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\n-github.com/cncf/xds/go v0.0.0-20210805033703-aa0b78936158/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\n-github.com/cncf/xds/go v0.0.0-20210922020428-25de7278fc84/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\n-github.com/cncf/xds/go v0.0.0-20211001041855-01bcc9b48dfe/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\n-github.com/cncf/xds/go v0.0.0-20211011173535-cb28da3451f1/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\n-github.com/cncf/xds/go v0.0.0-20211130200136-a8f946100490/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\n-github.com/containerd/console v1.0.3/go.mod h1:7LqA/THxQ86k76b8c/EMSiaJ3h1eZkMkXar0TQ1gf3U=\n-github.com/containerd/containerd v1.6.17 h1:XDnJIeJW0cLf6v7/+N+6L9kGrChHeXekZp2VHu6OpiY=\n-github.com/containerd/containerd v1.6.17/go.mod h1:1RdCUu95+gc2v9t3IL+zIlpClSmew7/0YS8O5eQZrOw=\n-github.com/containerd/continuity v0.3.0 h1:nisirsYROK15TAMVukJOUyGJjz4BNQJBVsNvAXZJ/eg=\n-github.com/coreos/go-semver v0.3.0/go.mod h1:nnelYz7RCh+5ahJtPPxZlU+153eP4D4r3EedlOD2RNk=\n-github.com/coreos/go-systemd/v22 v22.3.2/go.mod h1:Y58oyj3AT4RCenI/lSvhwexgC+NSVTIJ3seZv2GcEnc=\n-github.com/cpuguy83/go-md2man/v2 v2.0.0-20190314233015-f79a8a8ca69d/go.mod h1:maD7wRr/U5Z6m/iR4s+kqSMx2CaBsrgA7czyZG/E6dU=\n-github.com/cpuguy83/go-md2man/v2 v2.0.1/go.mod h1:tgQtvFlXSQOSOSIRvRPT7W67SCa46tRHOmNcaadrF8o=\n-github.com/creack/pty v1.1.9/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=\n-github.com/creack/pty v1.1.17 h1:QeVUsEDNrLBW4tMgZHvxy18sKtr6VI492kBhUfhDJNI=\n-github.com/cyphar/filepath-securejoin v0.2.3/go.mod h1:aPGpWjXOXUn2NCNjFvBE6aRxGGx79pTxQpKOJNYHHl4=\n-github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\n-github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\n-github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\n-github.com/distribution/distribution v2.8.2+incompatible h1:k9+4DKdOG+quPFZXT/mUsiQrGu9vYCp+dXpuPkuqhk8=\n-github.com/distribution/distribution v2.8.2+incompatible/go.mod h1:EgLm2NgWtdKgzF9NpMzUKgzmR7AMmb0VQi2B+ZzDRjc=\n-github.com/docker/distribution v2.8.1+incompatible h1:Q50tZOPR6T/hjNsyc9g8/syEs6bk8XXApsHjKukMl68=\n-github.com/docker/distribution v2.8.1+incompatible/go.mod h1:J2gT2udsDAN96Uj4KfcMRqY0/ypR+oyYUYmja8H+y+w=\n-github.com/docker/docker v23.0.0+incompatible h1:L6c28tNyqZ4/ub9AZC9d5QUuunoHHfEH4/Ue+h/E5nE=\n-github.com/docker/docker v23.0.0+incompatible/go.mod h1:eEKB0N0r5NX/I1kEveEz05bcu8tLC/8azJZsviup8Sk=\n-github.com/docker/go-connections v0.4.0 h1:El9xVISelRB7BuFusrZozjnkIM5YnzCViNKohAFqRJQ=\n-github.com/docker/go-connections v0.4.0/go.mod h1:Gbd7IOopHjR8Iph03tsViu4nIes5XhDvyHbTtUxmeec=\n-github.com/docker/go-units v0.4.0/go.mod h1:fgPhTUdO+D/Jk86RDLlptpiXQzgHJF7gydDDbaIK4Dk=\n-github.com/docker/go-units v0.5.0 h1:69rxXcBk27SvSaaxTtLh/8llcHD8vYHT7WSdRZ/jvr4=\n-github.com/docker/go-units v0.5.0/go.mod h1:fgPhTUdO+D/Jk86RDLlptpiXQzgHJF7gydDDbaIK4Dk=\n-github.com/dsnet/compress v0.0.1 h1:PlZu0n3Tuv04TzpfPbrnI0HW/YwodEXDS+oPKahKF0Q=\n-github.com/dsnet/compress v0.0.1/go.mod h1:Aw8dCMJ7RioblQeTqt88akK31OvO8Dhf5JflhBbQEHo=\n-github.com/dsnet/golib v0.0.0-20171103203638-1ea166775780/go.mod h1:Lj+Z9rebOhdfkVLjJ8T6VcRQv3SXugXy999NBtR9aFY=\n-github.com/elastic/gosigar v0.14.2 h1:Dg80n8cr90OZ7x+bAax/QjoW/XqTI11RmA79ZwIm9/4=\n-github.com/elastic/gosigar v0.14.2/go.mod h1:iXRIGg2tLnu7LBdpqzyQfGDEidKCfWcCMS0WKyPWoMs=\n-github.com/envoyproxy/go-control-plane v0.9.0/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\n-github.com/envoyproxy/go-control-plane v0.9.1-0.20191026205805-5f8ba28d4473/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\n-github.com/envoyproxy/go-control-plane v0.9.4/go.mod h1:6rpuAdCZL397s3pYoYcLgu1mIlRU8Am5FuJP05cCM98=\n-github.com/envoyproxy/go-control-plane v0.9.7/go.mod h1:cwu0lG7PUMfa9snN8LXBig5ynNVH9qI8YYLbd1fK2po=\n-github.com/envoyproxy/go-control-plane v0.9.9-0.20201210154907-fd9021fe5dad/go.mod h1:cXg6YxExXjJnVBQHBLXeUAgxn2UodCpnH306RInaBQk=\n-github.com/envoyproxy/go-control-plane v0.9.9-0.20210217033140-668b12f5399d/go.mod h1:cXg6YxExXjJnVBQHBLXeUAgxn2UodCpnH306RInaBQk=\n-github.com/envoyproxy/go-control-plane v0.9.9-0.20210512163311-63b5d3c536b0/go.mod h1:hliV/p42l8fGbc6Y9bQ70uLwIvmJyVE5k4iMKlh8wCQ=\n-github.com/envoyproxy/go-control-plane v0.9.10-0.20210907150352-cf90f659a021/go.mod h1:AFq3mo9L8Lqqiid3OhADV3RfLJnjiw63cSpi+fDTRC0=\n-github.com/envoyproxy/go-control-plane v0.10.1/go.mod h1:AY7fTTXNdv/aJ2O5jwpxAPOWUZ7hQAEvzN5Pf27BkQQ=\n-github.com/envoyproxy/go-control-plane v0.10.2-0.20220325020618-49ff273808a1/go.mod h1:KJwIaB5Mv44NWtYuAOFCVOjcI94vtpEz2JU/D2v6IjE=\n-github.com/envoyproxy/protoc-gen-validate v0.1.0/go.mod h1:iSmxcyjqTsJpI2R4NaDN7+kN2VEUnK/pcBlmesArF7c=\n-github.com/envoyproxy/protoc-gen-validate v0.6.2/go.mod h1:2t7qjJNvHPx8IjnBOzl9E9/baC+qXE/TeeyBRzgJDws=\n-github.com/fatih/color v1.7.0/go.mod h1:Zm6kSWBoL9eyXnKyktHP6abPY2pDugNf5KwzbycvMj4=\n-github.com/fatih/color v1.9.0/go.mod h1:eQcE1qtQxscV5RaZvpXrrb8Drkc3/DdQ+uUYCNjL+zU=\n-github.com/fatih/color v1.13.0/go.mod h1:kLAiJbzzSOZDVNGyDpeOxJ47H46qBXwg5ILebYFFOfk=\n-github.com/frankban/quicktest v1.11.3/go.mod h1:wRf/ReqHper53s+kmmSZizM8NamnL3IM0I9ntUbOk+k=\n-github.com/fsnotify/fsnotify v1.5.1/go.mod h1:T3375wBYaZdLLcVNkcVbzGHY7f1l/uK5T5Ai1i3InKU=\n-github.com/fsnotify/fsnotify v1.5.4 h1:jRbGcIw6P2Meqdwuo0H1p6JVLbL5DHKAKlYndzMwVZI=\n-github.com/fsnotify/fsnotify v1.5.4/go.mod h1:OVB6XrOHzAwXMpEM7uPOzcehqUV2UqJxmVXmkdnm1bU=\n-github.com/ghodss/yaml v1.0.0 h1:wQHKEahhL6wmXdzwWG11gIVCkOv05bNOh+Rxn0yngAk=\n-github.com/ghodss/yaml v1.0.0/go.mod h1:4dBDuWmgqj2HViK6kFavaiC9ZROes6MMH2rRYeMEF04=\n-github.com/go-gl/glfw v0.0.0-20190409004039-e6da0acd62b1/go.mod h1:vR7hzQXu2zJy9AVAgeJqvqgH9Q5CA+iKCZ2gyEVpxRU=\n-github.com/go-gl/glfw/v3.3/glfw v0.0.0-20191125211704-12ad95a8df72/go.mod h1:tQ2UAYgL5IevRw8kRxooKSPJfGvJ9fJQFa0TUsXzTg8=\n-github.com/go-gl/glfw/v3.3/glfw v0.0.0-20200222043503-6f7a984d4dc4/go.mod h1:tQ2UAYgL5IevRw8kRxooKSPJfGvJ9fJQFa0TUsXzTg8=\n-github.com/go-kit/kit v0.8.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=\n-github.com/go-kit/kit v0.9.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=\n-github.com/go-logfmt/logfmt v0.3.0/go.mod h1:Qt1PoO58o5twSAckw1HlFXLmHsOX5/0LbT9GBnD5lWE=\n-github.com/go-logfmt/logfmt v0.4.0/go.mod h1:3RMwSq7FuexP4Kalkev3ejPJsZTpXXBr9+V4qmtdjCk=\n-github.com/go-logr/logr v1.2.2/go.mod h1:jdQByPbusPIv2/zmleS9BjJVeZ6kBagPoEUsqbVz/1A=\n-github.com/go-logr/stdr v1.2.2/go.mod h1:mMo/vtBO5dYbehREoey6XUKy/eSumjCCveDpRre4VKE=\n-github.com/go-ole/go-ole v1.2.4 h1:nNBDSCOigTSiarFpYE9J/KtEA1IOW4CNeqT9TQDqCxI=\n-github.com/go-ole/go-ole v1.2.4/go.mod h1:XCwSNxSkXRo4vlyPy93sltvi/qJq0jqQhjqQNIwKuxM=\n-github.com/go-sql-driver/mysql v1.4.0/go.mod h1:zAC/RDZ24gD3HViQzih4MyKcchzm+sOG5ZlKdlhCg5w=\n-github.com/go-stack/stack v1.8.0/go.mod h1:v0f6uXyyMGvRgIKkXu+yp6POWl0qKG85gN/melR3HDY=\n-github.com/godbus/dbus/v5 v5.0.4/go.mod h1:xhWf0FNVPg57R7Z0UbKHbJfkEywrmjJnf7w5xrFpKfA=\n-github.com/godbus/dbus/v5 v5.0.6/go.mod h1:xhWf0FNVPg57R7Z0UbKHbJfkEywrmjJnf7w5xrFpKfA=\n-github.com/gogo/protobuf v1.1.1/go.mod h1:r8qH/GZQm5c6nD/R0oafs1akxWv10x8SbQlK7atdtwQ=\n-github.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=\n-github.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=\n-github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=\n-github.com/golang/groupcache v0.0.0-20190702054246-869f871628b6/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\n-github.com/golang/groupcache v0.0.0-20191227052852-215e87163ea7/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\n-github.com/golang/groupcache v0.0.0-20200121045136-8c9f03a8e57e/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\n-github.com/golang/groupcache v0.0.0-20210331224755-41bb18bfe9da/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\n-github.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\n-github.com/golang/mock v1.2.0/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\n-github.com/golang/mock v1.3.1/go.mod h1:sBzyDLLjw3U8JLTeZvSv8jJB+tU5PVekmnlKIyFUx0Y=\n-github.com/golang/mock v1.4.0/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\n-github.com/golang/mock v1.4.1/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\n-github.com/golang/mock v1.4.3/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\n-github.com/golang/mock v1.4.4/go.mod h1:l3mdAwkq5BuhzHwde/uurv3sEJeZMXNpwsxVWU71h+4=\n-github.com/golang/mock v1.5.0/go.mod h1:CWnOUgYIOo4TcNZ0wHX3YZCqsaM1I1Jvs6v3mP3KVu8=\n-github.com/golang/mock v1.6.0/go.mod h1:p6yTPP+5HYm5mzsMV8JkE6ZKdX+/wYM6Hr+LicevLPs=\n-github.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\n-github.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\n-github.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\n-github.com/golang/protobuf v1.3.3/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=\n-github.com/golang/protobuf v1.3.4/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=\n-github.com/golang/protobuf v1.3.5/go.mod h1:6O5/vntMXwX2lRkT1hjjk0nAC1IDOTvTlVgjlRvqsdk=\n-github.com/golang/protobuf v1.4.0-rc.1/go.mod h1:ceaxUfeHdC40wWswd/P6IGgMaK3YpKi5j83Wpe3EHw8=\n-github.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208/go.mod h1:xKAWHe0F5eneWXFV3EuXVDTCmh+JuBKY0li0aMyXATA=\n-github.com/golang/protobuf v1.4.0-rc.2/go.mod h1:LlEzMj4AhA7rCAGe4KMBDvJI+AwstrUpVNzEA03Pprs=\n-github.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0/go.mod h1:WU3c8KckQ9AFe+yFwt9sWVRKCVIyN9cPHBJSNnbL67w=\n-github.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=\n-github.com/golang/protobuf v1.4.1/go.mod h1:U8fpvMrcmy5pZrNK1lt4xCsGvpyWQ/VVv6QDs8UjoX8=\n-github.com/golang/protobuf v1.4.2/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\n-github.com/golang/protobuf v1.4.3/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\n-github.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=\n-github.com/golang/protobuf v1.5.1/go.mod h1:DopwsBzvsk0Fs44TXzsVbJyPhcCPeIwnvohx4u74HPM=\n-github.com/golang/protobuf v1.5.2 h1:ROPKBNFfQgOUMifHyP+KYbvpjbdoFNs+aK7DXlji0Tw=\n-github.com/golang/protobuf v1.5.2/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=\n-github.com/golang/snappy v0.0.3/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\n-github.com/golang/snappy v0.0.4 h1:yAGX7huGHXlcLOEtBnF4w7FQwA26wojNCwOYAEhLjQM=\n-github.com/golang/snappy v0.0.4/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\n-github.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\n-github.com/google/btree v1.0.0/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\n-github.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=\n-github.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\n-github.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\n-github.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\n-github.com/google/go-cmp v0.4.1/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\n-github.com/google/go-cmp v0.5.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\n-github.com/google/go-cmp v0.5.1/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\n-github.com/google/go-cmp v0.5.2/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\n-github.com/google/go-cmp v0.5.3/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\n-github.com/google/go-cmp v0.5.4/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\n-github.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\n-github.com/google/go-cmp v0.5.6/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\n-github.com/google/go-cmp v0.5.7/go.mod h1:n+brtR0CgQNWTVd5ZUFpTBC8YFBDLK/h/bpaJ8/DtOE=\n-github.com/google/go-cmp v0.5.9 h1:O2Tfq5qg4qc4AmwVlvv0oLiVAGB7enBSJ2x2DqQFi38=\n-github.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=\n-github.com/google/martian v2.1.0+incompatible/go.mod h1:9I4somxYTbIHy5NJKHRl3wXiIaQGbYVAs8BPL6v8lEs=\n-github.com/google/martian/v3 v3.0.0/go.mod h1:y5Zk1BBys9G+gd6Jrk0W3cC1+ELVxBWuIGO+w/tUAp0=\n-github.com/google/martian/v3 v3.1.0/go.mod h1:y5Zk1BBys9G+gd6Jrk0W3cC1+ELVxBWuIGO+w/tUAp0=\n-github.com/google/martian/v3 v3.2.1/go.mod h1:oBOf6HBosgwRXnUGWUB05QECsc6uvmMiJ3+6W4l/CUk=\n-github.com/google/pprof v0.0.0-20181206194817-3ea8567a2e57/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=\n-github.com/google/pprof v0.0.0-20190515194954-54271f7e092f/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=\n-github.com/google/pprof v0.0.0-20191218002539-d4f498aebedc/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\n-github.com/google/pprof v0.0.0-20200212024743-f11f1df84d12/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\n-github.com/google/pprof v0.0.0-20200229191704-1ebb73c60ed3/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\n-github.com/google/pprof v0.0.0-20200430221834-fc25d7d30c6d/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\n-github.com/google/pprof v0.0.0-20200708004538-1a94d8640e99/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\n-github.com/google/pprof v0.0.0-20201023163331-3e6fc7fc9c4c/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=\n-github.com/google/pprof v0.0.0-20201203190320-1bf35d6f28c2/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=\n-github.com/google/pprof v0.0.0-20201218002935-b9804c9f04c2/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=\n-github.com/google/pprof v0.0.0-20210122040257-d980be63207e/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=\n-github.com/google/pprof v0.0.0-20210226084205-cbba55b83ad5/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=\n-github.com/google/pprof v0.0.0-20210601050228-01bbb1931b22/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=\n-github.com/google/pprof v0.0.0-20210609004039-a478d1d731e9/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=\n-github.com/google/pprof v0.0.0-20210720184732-4bb14d4b1be1/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=\n-github.com/google/renameio v0.1.0/go.mod h1:KWCgfxg9yswjAJkECMjeO8J8rahYeXnNhOm40UhjYkI=\n-github.com/google/shlex v0.0.0-20191202100458-e7afc7fbc510 h1:El6M4kTTCOh6aBiKaUGG7oYTSPP8MxqL4YI3kZKwcP4=\n-github.com/google/shlex v0.0.0-20191202100458-e7afc7fbc510/go.mod h1:pupxD2MaaD3pAXIBCelhxNneeOaAeabZDe5s4K6zSpQ=\n-github.com/google/uuid v1.1.2/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\n-github.com/google/uuid v1.3.0 h1:t6JiXgmwXMjEs8VusXIJk2BXHsn+wx8BZdTaoZ5fu7I=\n-github.com/google/uuid v1.3.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\n-github.com/googleapis/gax-go/v2 v2.0.4/go.mod h1:0Wqv26UfaUD9n4G6kQubkQ+KchISgw+vpHVxEJEs9eg=\n-github.com/googleapis/gax-go/v2 v2.0.5/go.mod h1:DWXyrwAJ9X0FpwwEdw+IPEYBICEFu5mhpdKc/us6bOk=\n-github.com/googleapis/gax-go/v2 v2.1.0/go.mod h1:Q3nei7sK6ybPYH7twZdmQpAd1MKb7pfu6SK+H1/DsU0=\n-github.com/googleapis/gax-go/v2 v2.1.1/go.mod h1:hddJymUZASv3XPyGkUpKj8pPO47Rmb0eJc8R6ouapiM=\n-github.com/googleapis/google-cloud-go-testing v0.0.0-20200911160855-bcd43fbb19e8/go.mod h1:dvDLG8qkwmyD9a/MJJN3XJcT3xFxOKAvTZGvuZmac9g=\n-github.com/gorilla/handlers v1.4.2/go.mod h1:Qkdc/uu4tH4g6mTK6auzZ766c4CA0Ng8+o/OAirnOIQ=\n-github.com/gorilla/websocket v1.4.1/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=\n-github.com/grpc-ecosystem/grpc-gateway v1.16.0/go.mod h1:BDjrQk3hbvj6Nolgz8mAMFbcEtjT1g+wF4CSlocrBnw=\n-github.com/hashicorp/consul/api v1.11.0/go.mod h1:XjsvQN+RJGWI2TWy1/kqaE16HrR2J/FWgkYjdZQsX9M=\n-github.com/hashicorp/consul/sdk v0.8.0/go.mod h1:GBvyrGALthsZObzUGsfgHZQDXjg4lOjagTIwIR1vPms=\n-github.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\n-github.com/hashicorp/go-cleanhttp v0.5.0/go.mod h1:JpRdi6/HCYpAwUzNwuwqhbovhLtngrth3wmdIIUrZ80=\n-github.com/hashicorp/go-cleanhttp v0.5.1/go.mod h1:JpRdi6/HCYpAwUzNwuwqhbovhLtngrth3wmdIIUrZ80=\n-github.com/hashicorp/go-cleanhttp v0.5.2/go.mod h1:kO/YDlP8L1346E6Sodw+PrpBSV4/SoxCXGY6BqNFT48=\n-github.com/hashicorp/go-hclog v0.12.0/go.mod h1:whpDNt7SSdeAju8AWKIWsul05p54N/39EeqMAyrmvFQ=\n-github.com/hashicorp/go-hclog v1.0.0/go.mod h1:whpDNt7SSdeAju8AWKIWsul05p54N/39EeqMAyrmvFQ=\n-github.com/hashicorp/go-immutable-radix v1.0.0/go.mod h1:0y9vanUI8NX6FsYoO3zeMjhV/C5i9g4Q3DwcSNZ4P60=\n-github.com/hashicorp/go-immutable-radix v1.3.1/go.mod h1:0y9vanUI8NX6FsYoO3zeMjhV/C5i9g4Q3DwcSNZ4P60=\n-github.com/hashicorp/go-msgpack v0.5.3/go.mod h1:ahLV/dePpqEmjfWmKiqvPkv/twdG7iPBM1vqhUKIvfM=\n-github.com/hashicorp/go-multierror v1.0.0/go.mod h1:dHtQlpGsu+cZNNAkkCN/P3hoUDHhCYQXV3UM06sGGrk=\n-github.com/hashicorp/go-multierror v1.1.0/go.mod h1:spPvp8C1qA32ftKqdAHm4hHTbPw+vmowP0z+KUhOZdA=\n-github.com/hashicorp/go-retryablehttp v0.5.3/go.mod h1:9B5zBasrRhHXnJnui7y6sL7es7NDiJgTc6Er0maI1Xs=\n-github.com/hashicorp/go-rootcerts v1.0.2/go.mod h1:pqUvnprVnM5bf7AOirdbb01K4ccR319Vf4pU3K5EGc8=\n-github.com/hashicorp/go-sockaddr v1.0.0/go.mod h1:7Xibr9yA9JjQq1JpNB2Vw7kxv8xerXegt+ozgdvDeDU=\n-github.com/hashicorp/go-syslog v1.0.0/go.mod h1:qPfqrKkXGihmCqbJM2mZgkZGvKG1dFdvsLplgctolz4=\n-github.com/hashicorp/go-uuid v1.0.0/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/bN7x4byOro=\n-github.com/hashicorp/go-uuid v1.0.1/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/bN7x4byOro=\n-github.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\n-github.com/hashicorp/golang-lru v0.5.1/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\n-github.com/hashicorp/golang-lru v0.5.4/go.mod h1:iADmTwqILo4mZ8BN3D2Q6+9jd8WM5uGBxy+E8yxSoD4=\n-github.com/hashicorp/hcl v1.0.0 h1:0Anlzjpi4vEasTeNFn2mLJgTSwt0+6sfsiTG8qcWGx4=\n-github.com/hashicorp/hcl v1.0.0/go.mod h1:E5yfLk+7swimpb2L/Alb/PJmXilQ/rhwaUYs4T20WEQ=\n-github.com/hashicorp/logutils v1.0.0/go.mod h1:QIAnNjmIWmVIIkWDTG1z5v++HQmx9WQRO+LraFDTW64=\n-github.com/hashicorp/mdns v1.0.1/go.mod h1:4gW7WsVCke5TE7EPeYliwHlRUyBtfCwuFwuMg2DmyNY=\n-github.com/hashicorp/mdns v1.0.4/go.mod h1:mtBihi+LeNXGtG8L9dX59gAEa12BDtBQSp4v/YAJqrc=\n-github.com/hashicorp/memberlist v0.2.2/go.mod h1:MS2lj3INKhZjWNqd3N0m3J+Jxf3DAOnAH9VT3Sh9MUE=\n-github.com/hashicorp/memberlist v0.3.0/go.mod h1:MS2lj3INKhZjWNqd3N0m3J+Jxf3DAOnAH9VT3Sh9MUE=\n-github.com/hashicorp/serf v0.9.5/go.mod h1:UWDWwZeL5cuWDJdl0C6wrvrUwEqtQ4ZKBKKENpqIUyk=\n-github.com/hashicorp/serf v0.9.6/go.mod h1:TXZNMjZQijwlDvp+r0b63xZ45H7JmCmgg4gpTwn9UV4=\n-github.com/iancoleman/strcase v0.2.0/go.mod h1:iwCmte+B7n89clKwxIoIXy/HfoL7AsD47ZCWhYzw7ho=\n-github.com/ianlancetaylor/demangle v0.0.0-20181102032728-5e5cf60278f6/go.mod h1:aSSvb/t6k1mPoxDqO4vJh6VOCGPwU4O0C2/Eqndh1Sc=\n-github.com/ianlancetaylor/demangle v0.0.0-20200824232613-28f6c0f3b639/go.mod h1:aSSvb/t6k1mPoxDqO4vJh6VOCGPwU4O0C2/Eqndh1Sc=\n-github.com/inconshreveable/mousetrap v1.0.0 h1:Z8tu5sraLXCXIcARxBp/8cbvlwVa7Z1NHg9XEKhtSvM=\n-github.com/inconshreveable/mousetrap v1.0.0/go.mod h1:PxqpIevigyE2G7u3NXJIT2ANytuPF1OarO4DADm73n8=\n-github.com/jaypipes/ghw v0.8.0 h1:02q1pTm9CD83vuhBsEZZhOCS128pq87uyaQeJZkp3sQ=\n-github.com/jaypipes/ghw v0.8.0/go.mod h1:+gR9bjm3W/HnFi90liF+Fj9GpCe/Dsibl9Im8KmC7c4=\n-github.com/jaypipes/pcidb v0.6.0 h1:VIM7GKVaW4qba30cvB67xSCgJPTzkG8Kzw/cbs5PHWU=\n-github.com/jaypipes/pcidb v0.6.0/go.mod h1:L2RGk04sfRhp5wvHO0gfRAMoLY/F3PKv/nwJeVoho0o=\n-github.com/jessevdk/go-flags v1.4.0/go.mod h1:4FA24M0QyGHXBuZZK/XkWh8h0e1EYbRYJSGM75WSRxI=\n-github.com/jmoiron/sqlx v1.2.0/go.mod h1:1FEQNm3xlJgrMD+FBdI9+xvCksHtbpVBBw5dYhBSsks=\n-github.com/json-iterator/go v1.1.6/go.mod h1:+SdeFBvtyEkXs7REEP0seUULqWtbJapLOCVDaaPEHmU=\n-github.com/json-iterator/go v1.1.9/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\n-github.com/json-iterator/go v1.1.11/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\n-github.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=\n-github.com/jstemmer/go-junit-report v0.0.0-20190106144839-af01ea7f8024/go.mod h1:6v2b51hI/fHJwM22ozAgKL4VKDeJcHhJFhtBdhmNjmU=\n-github.com/jstemmer/go-junit-report v0.9.1/go.mod h1:Brl9GWCQeLvo8nXZwPNNblvFj/XSXhF0NWZEnDohbsk=\n-github.com/julienschmidt/httprouter v1.2.0/go.mod h1:SYymIcj16QtmaHHD7aYtjjsJG7VTCxuUUipMqKk8s4w=\n-github.com/kisielk/errcheck v1.5.0/go.mod h1:pFxgyoBC7bSaBwPgfKdkLd5X25qrDl4LWUI2bnpBCr8=\n-github.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=\n-github.com/klauspost/compress v1.4.1/go.mod h1:RyIbtBH6LamlWaDj8nUwkbUhJ87Yi3uG0guNDohfE1A=\n-github.com/klauspost/compress v1.13.6 h1:P76CopJELS0TiO2mebmnzgWaajssP/EszplttgQxcgc=\n-github.com/klauspost/compress v1.13.6/go.mod h1:/3/Vjq9QcHkK5uEr5lBEmyoZ1iFhe47etQ6QUkpK6sk=\n-github.com/klauspost/cpuid v1.2.0/go.mod h1:Pj4uuM528wm8OyEC2QMXAi2YiTZ96dNQPGgoMS4s3ek=\n-github.com/klauspost/pgzip v1.2.5 h1:qnWYvvKqedOF2ulHpMG72XQol4ILEJ8k2wwRl/Km8oE=\n-github.com/klauspost/pgzip v1.2.5/go.mod h1:Ch1tH69qFZu15pkjo5kYi6mth2Zzwzt50oCQKQE9RUs=\n-github.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\n-github.com/kr/fs v0.1.0/go.mod h1:FFnZGqtBN9Gxj7eW1uZ42v5BccTP0vu6NEaFoC2HwRg=\n-github.com/kr/logfmt v0.0.0-20140226030751-b84e30acd515/go.mod h1:+0opPa2QZZtGFBFZlji/RkVcI2GknAs/DXo4wKdlNEc=\n-github.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\n-github.com/kr/pretty v0.2.0/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=\n-github.com/kr/pretty v0.2.1/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=\n-github.com/kr/pretty v0.3.0 h1:WgNl7dwNpEZ6jJ9k1snq4pZsg7DOEN8hP9Xw0Tsjwk0=\n-github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\n-github.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\n-github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=\n-github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=\n-github.com/lib/pq v1.0.0/go.mod h1:5WUZQaWbwv1U+lTReE5YruASi9Al49XbQIvNi/34Woo=\n-github.com/lyft/protoc-gen-star v0.5.3/go.mod h1:V0xaHgaf5oCCqmcxYcWiDfTiKsZsRc87/1qhoTACD8w=\n-github.com/magiconair/properties v1.8.5/go.mod h1:y3VJvCyxH9uVvJTWEGAELF3aiYNyPKd5NZ3oSwXrF60=\n-github.com/magiconair/properties v1.8.7 h1:IeQXZAiQcpL9mgcAe1Nu6cX9LLw6ExEHKjN0VQdvPDY=\n-github.com/magiconair/properties v1.8.7/go.mod h1:Dhd985XPs7jluiymwWYZ0G4Z61jb3vdS329zhj2hYo0=\n-github.com/matishsiao/goInfo v0.0.0-20210923090445-da2e3fa8d45f h1:B0OD7nYl2FPQEVrw8g2uyc1lGEzNbvrKh7fspGZcbvY=\n-github.com/matishsiao/goInfo v0.0.0-20210923090445-da2e3fa8d45f/go.mod h1:aEt7p9Rvh67BYApmZwNDPpgircTO2kgdmDUoF/1QmwA=\n-github.com/mattn/go-colorable v0.0.9/go.mod h1:9vuHe8Xs5qXnSaW/c/ABM9alt+Vo+STaOChaDxuIBZU=\n-github.com/mattn/go-colorable v0.1.4/go.mod h1:U0ppj6V5qS13XJ6of8GYAs25YV2eR4EVcfRqFIhoBtE=\n-github.com/mattn/go-colorable v0.1.6/go.mod h1:u6P/XSegPjTcexA+o6vUJrdnUu04hMope9wVRipJSqc=\n-github.com/mattn/go-colorable v0.1.9/go.mod h1:u6P/XSegPjTcexA+o6vUJrdnUu04hMope9wVRipJSqc=\n-github.com/mattn/go-colorable v0.1.12/go.mod h1:u5H1YNBxpqRaxsYJYSkiCWKzEfiAb1Gb520KVy5xxl4=\n-github.com/mattn/go-isatty v0.0.3/go.mod h1:M+lRXTBqGeGNdLjl/ufCoiOlB5xdOkqRJdNxMWT7Zi4=\n-github.com/mattn/go-isatty v0.0.8/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=\n-github.com/mattn/go-isatty v0.0.10/go.mod h1:qgIWMr58cqv1PHHyhnkY9lrL7etaEgOFcMEpPG5Rm84=\n-github.com/mattn/go-isatty v0.0.11/go.mod h1:PhnuNfih5lzO57/f3n+odYbM4JtupLOxQOAqxQCu2WE=\n-github.com/mattn/go-isatty v0.0.12/go.mod h1:cbi8OIDigv2wuxKPP5vlRcQ1OAZbq2CE4Kysco4FUpU=\n-github.com/mattn/go-isatty v0.0.14/go.mod h1:7GGIvUiUoEMVVmxf/4nioHXj79iQHKdU27kJ6hsGG94=\n-github.com/mattn/go-runewidth v0.0.9 h1:Lm995f3rfxdpd6TSmuVCHVb/QhupuXlYr8sCI/QdE+0=\n-github.com/mattn/go-runewidth v0.0.9/go.mod h1:H031xJmbD/WCDINGzjvQ9THkh0rPKHF+m2gUSrubnMI=\n-github.com/mattn/go-sqlite3 v1.9.0/go.mod h1:FPy6KqzDD04eiIsT53CuJW3U88zkxoIYsOqkbpncsNc=\n-github.com/matttproud/golang_protobuf_extensions v1.0.1/go.mod h1:D8He9yQNgCq6Z5Ld7szi9bcBfOoFv/3dc6xSMkL2PC0=\n-github.com/mholt/archiver/v4 v4.0.0-alpha.4 h1:QJ4UuWgavPynEX3LXxClHDRGzYcgcvTtAMp8az7spuw=\n-github.com/mholt/archiver/v4 v4.0.0-alpha.4/go.mod h1:J7SYS/UTAtnO3I49RQEf+2FYZVwo7XBOh9Im43VrjNs=\n-github.com/miekg/dns v1.0.14/go.mod h1:W1PPwlIAgtquWBMBEV9nkV9Cazfe8ScdGz/Lj7v3Nrg=\n-github.com/miekg/dns v1.1.26/go.mod h1:bPDLeHnStXmXAq1m/Ch/hvfNHr14JKNPMBo3VZKjuso=\n-github.com/miekg/dns v1.1.41/go.mod h1:p6aan82bvRIyn+zDIv9xYNUpwa73JcSh9BKwknJysuI=\n-github.com/mitchellh/cli v1.1.0/go.mod h1:xcISNoH86gajksDmfB23e/pu+B+GeFRMYmoHXxx3xhI=\n-github.com/mitchellh/go-homedir v1.0.0/go.mod h1:SfyaCUpYCn1Vlf4IUYiD9fPX4A5wJrkLzIz1N1q0pr0=\n-github.com/mitchellh/go-homedir v1.1.0 h1:lukF9ziXFxDFPkA1vsr5zpc1XuPDn/wFntq5mG+4E0Y=\n-github.com/mitchellh/go-homedir v1.1.0/go.mod h1:SfyaCUpYCn1Vlf4IUYiD9fPX4A5wJrkLzIz1N1q0pr0=\n-github.com/mitchellh/go-testing-interface v1.0.0/go.mod h1:kRemZodwjscx+RGhAo8eIhFbs2+BFgRtFPeD/KE+zxI=\n-github.com/mitchellh/mapstructure v0.0.0-20160808181253-ca63d7c062ee/go.mod h1:FVVH3fgwuzCH5S8UJGiWEs2h04kUh9fWfEaFds41c1Y=\n-github.com/mitchellh/mapstructure v1.1.2/go.mod h1:FVVH3fgwuzCH5S8UJGiWEs2h04kUh9fWfEaFds41c1Y=\n-github.com/mitchellh/mapstructure v1.4.3 h1:OVowDSCllw/YjdLkam3/sm7wEtOy59d8ndGgCcyj8cs=\n-github.com/mitchellh/mapstructure v1.4.3/go.mod h1:bFUtVrKA4DC2yAKiSyO/QUcy7e+RRV2QTWOzhPopBRo=\n-github.com/mkevac/debugcharts v0.0.0-20191222103121-ae1c48aa8615/go.mod h1:Ad7oeElCZqA1Ufj0U9/liOF4BtVepxRcTvr2ey7zTvM=\n-github.com/moby/patternmatcher v0.5.0 h1:YCZgJOeULcxLw1Q+sVR636pmS7sPEn1Qo2iAN6M7DBo=\n-github.com/moby/patternmatcher v0.5.0/go.mod h1:hDPoyOpDY7OrrMDLaYoY3hf52gNCR/YOUYxkhApJIxc=\n-github.com/moby/sys/mountinfo v0.5.0/go.mod h1:3bMD3Rg+zkqx8MRYPi7Pyb0Ie97QEBmdxbhnCLlSvSU=\n-github.com/moby/sys/sequential v0.5.0 h1:OPvI35Lzn9K04PBbCLW0g4LcFAJgHsvXsRyewg5lXtc=\n-github.com/moby/sys/sequential v0.5.0/go.mod h1:tH2cOOs5V9MlPiXcQzRC+eEyab644PWKGRYaaV5ZZlo=\n-github.com/moby/term v0.0.0-20221128092401-c43b287e0e0f h1:J/7hjLaHLD7epG0m6TBMGmp4NQ+ibBYLfeyJWdAIFLA=\n-github.com/moby/term v0.0.0-20221128092401-c43b287e0e0f/go.mod h1:15ce4BGCFxt7I5NQKT+HV0yEDxmf6fSysfEDiVo3zFM=\n-github.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\n-github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\n-github.com/modern-go/reflect2 v0.0.0-20180701023420-4b7aa43c6742/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=\n-github.com/modern-go/reflect2 v1.0.1/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=\n-github.com/modern-go/reflect2 v1.0.2/go.mod h1:yWuevngMOJpCy52FWWMvUC8ws7m/LJsjYzDa0/r8luk=\n-github.com/morikuni/aec v1.0.0 h1:nP9CBfwrvYnBRgY6qfDQkygYDmYwOilePFkwzv4dU8A=\n-github.com/morikuni/aec v1.0.0/go.mod h1:BbKIizmSmc5MMPqRYbxO4ZU0S0+P200+tUnFx7PXmsc=\n-github.com/mrunalp/fileutils v0.5.0/go.mod h1:M1WthSahJixYnrXQl/DFQuteStB1weuxD2QJNHXfbSQ=\n-github.com/mwitkow/go-conntrack v0.0.0-20161129095857-cc309e4a2223/go.mod h1:qRWi+5nqEBWmkhHvq77mSJWrCKwh8bxhgT7d/eI7P4U=\n-github.com/nwaples/rardecode/v2 v2.0.0-beta.2 h1:e3mzJFJs4k83GXBEiTaQ5HgSc/kOK8q0rDaRO0MPaOk=\n-github.com/nwaples/rardecode/v2 v2.0.0-beta.2/go.mod h1:yntwv/HfMc/Hbvtq9I19D1n58te3h6KsqCf3GxyfBGY=\n-github.com/olekukonko/tablewriter v0.0.5 h1:P2Ga83D34wi1o9J6Wh1mRuqd4mF/x/lgBS7N7AbDhec=\n-github.com/olekukonko/tablewriter v0.0.5/go.mod h1:hPp6KlRPjbx+hW8ykQs1w3UBbZlj6HuIJcUGPhkA7kY=\n-github.com/opencontainers/go-digest v1.0.0 h1:apOUWs51W5PlhuyGyz9FCeeBIOUDA/6nW8Oi/yOhh5U=\n-github.com/opencontainers/go-digest v1.0.0/go.mod h1:0JzlMkj0TRzQZfJkVvzbP0HBR3IKzErnv2BNG4W4MAM=\n-github.com/opencontainers/image-spec v1.1.0-rc2 h1:2zx/Stx4Wc5pIPDvIxHXvXtQFW/7XWJGmnM7r3wg034=\n-github.com/opencontainers/image-spec v1.1.0-rc2/go.mod h1:3OVijpioIKYWTqjiG0zfF6wvoJ4fAXGbjdZuI2NgsRQ=\n-github.com/opencontainers/runc v1.1.3 h1:vIXrkId+0/J2Ymu2m7VjGvbSlAId9XNRPhn2p4b+d8w=\n-github.com/opencontainers/runc v1.1.3/go.mod h1:1J5XiS+vdZ3wCyZybsuxXZWGrgSr8fFJHLXuG2PsnNg=\n-github.com/opencontainers/runtime-spec v1.0.3-0.20210326190908-1c3f411f0417/go.mod h1:jwyrGlmzljRJv/Fgzds9SsS/C5hL+LL3ko9hs6T5lQ0=\n-github.com/opencontainers/selinux v1.10.0/go.mod h1:2i0OySw99QjzBBQByd1Gr9gSjvuho1lHsJxIJ3gGbJI=\n-github.com/pascaldekloe/goe v0.0.0-20180627143212-57f6aae5913c/go.mod h1:lzWF7FIEvWOWxwDKqyGYQf6ZUaNfKdP144TG7ZOy1lc=\n-github.com/pascaldekloe/goe v0.1.0/go.mod h1:lzWF7FIEvWOWxwDKqyGYQf6ZUaNfKdP144TG7ZOy1lc=\n-github.com/paulmach/orb v0.4.0 h1:ilp1MQjRapLJ1+qcays1nZpe0mvkCY+b8JU/qBKRZ1A=\n-github.com/paulmach/orb v0.4.0/go.mod h1:FkcWtplUAIVqAuhAOV2d3rpbnQyliDOjOcLW9dUrfdU=\n-github.com/paulmach/protoscan v0.2.1-0.20210522164731-4e53c6875432/go.mod h1:2sV+uZ/oQh66m4XJVZm5iqUZ62BN88Ex1E+TTS0nLzI=\n-github.com/pelletier/go-toml v1.9.4/go.mod h1:u1nR/EPcESfeI/szUZKdtJ0xRNbUoANCkoOuaOx1Y+c=\n-github.com/pelletier/go-toml v1.9.5 h1:4yBQzkHv+7BHq2PQUZF3Mx0IYxG7LsP222s7Agd3ve8=\n-github.com/pelletier/go-toml v1.9.5/go.mod h1:u1nR/EPcESfeI/szUZKdtJ0xRNbUoANCkoOuaOx1Y+c=\n-github.com/pierrec/lz4 v2.0.5+incompatible/go.mod h1:pdkljMzZIN41W+lC3N2tnIh5sFi+IEE17M5jbnwPHcY=\n-github.com/pierrec/lz4/v4 v4.1.14 h1:+fL8AQEZtz/ijeNnpduH0bROTu0O3NZAlPjQxGn8LwE=\n-github.com/pierrec/lz4/v4 v4.1.14/go.mod h1:gZWDp/Ze/IJXGXf23ltt2EXimqmTUXEy0GFuRQyBid4=\n-github.com/pkg/errors v0.8.0/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n-github.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n-github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\n-github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n-github.com/pkg/sftp v1.10.1/go.mod h1:lYOWFsE0bwd1+KfKJaKeuokY15vzFx25BLbzYYoAxZI=\n-github.com/pkg/sftp v1.13.1/go.mod h1:3HaPG6Dq1ILlpPZRO0HVMrsydcdLt6HRDccSgb87qRg=\n-github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\n-github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\n-github.com/posener/complete v1.1.1/go.mod h1:em0nMJCgc9GFtwrmVmEMR/ZL6WyhyjMBndrE9hABlRI=\n-github.com/posener/complete v1.2.3/go.mod h1:WZIdtGGp+qx0sLrYKtIRAruyNpv6hFCicSgv7Sy7s/s=\n-github.com/prometheus/client_golang v0.9.1/go.mod h1:7SWBe2y4D6OKWSNQJUaRYU/AaXPKyh/dDVn+NZz0KFw=\n-github.com/prometheus/client_golang v1.0.0/go.mod h1:db9x61etRT2tGnBNRi70OPL5FsnadC4Ky3P0J6CfImo=\n-github.com/prometheus/client_golang v1.4.0/go.mod h1:e9GMxYsXl05ICDXkRhurwBS4Q3OK1iX/F2sw+iXX5zU=\n-github.com/prometheus/client_model v0.0.0-20180712105110-5c3871d89910/go.mod h1:MbSGuTsp3dbXC40dX6PRTWyKYBIrTGTE9sqQNg2J8bo=\n-github.com/prometheus/client_model v0.0.0-20190129233127-fd36f4220a90/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\n-github.com/prometheus/client_model v0.0.0-20190812154241-14fe0d1b01d4/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\n-github.com/prometheus/client_model v0.2.0/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\n-github.com/prometheus/common v0.4.1/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=\n-github.com/prometheus/common v0.9.1/go.mod h1:yhUN8i9wzaXS3w1O07YhxHEBxD+W35wd8bs7vj7HSQ4=\n-github.com/prometheus/procfs v0.0.0-20181005140218-185b4288413d/go.mod h1:c3At6R/oaqEKCNdg8wHV1ftS6bRYblBhIjjI8uT2IGk=\n-github.com/prometheus/procfs v0.0.2/go.mod h1:TjEm7ze935MbeOT/UhFTIMYKhuLP4wbCsTZCD3I8kEA=\n-github.com/prometheus/procfs v0.0.8/go.mod h1:7Qr8sr6344vo1JqZ6HhLceV9o3AJ1Ff+GxbHq6oeK9A=\n-github.com/rogpeppe/fastuuid v1.2.0/go.mod h1:jVj6XXZzXRy/MSR5jhDC/2q6DgLz+nrA6LYCDYWNEvQ=\n-github.com/rogpeppe/go-internal v1.3.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\n-github.com/rogpeppe/go-internal v1.8.1 h1:geMPLpDpQOgVyCg5z5GoRwLHepNdb71NXb67XFkP+Eg=\n-github.com/rs/xid v1.3.0/go.mod h1:trrq9SKmegXys3aeAKXMUTdJsYXVwGY3RLcfgqegfbg=\n-github.com/rs/zerolog v1.26.1 h1:/ihwxqH+4z8UxyI70wM1z9yCvkWcfz/a3mj48k/Zngc=\n-github.com/rs/zerolog v1.26.1/go.mod h1:/wSSJWX7lVrsOwlbyTRSOJvqRlc+WjWlfes+CiJ+tmc=\n-github.com/russross/blackfriday/v2 v2.0.1/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=\n-github.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=\n-github.com/ryanuber/columnize v0.0.0-20160712163229-9b3edd62028f/go.mod h1:sm1tb6uqfes/u+d4ooFouqFdy9/2g9QGwK3SQygK0Ts=\n-github.com/sagikazarmark/crypt v0.3.0/go.mod h1:uD/D+6UF4SrIR1uGEv7bBNkNqLGqUr43MRiaGWX1Nig=\n-github.com/sean-/seed v0.0.0-20170313163322-e2103e2c3529/go.mod h1:DxrIzT+xaE7yg65j358z/aeFdxmN0P9QXhEzd20vsDc=\n-github.com/seccomp/libseccomp-golang v0.9.2-0.20220502022130-f33da4d89646/go.mod h1:JA8cRccbGaA1s33RQf7Y1+q9gHmZX1yB/z9WDN1C6fg=\n-github.com/shirou/gopsutil v2.19.11+incompatible/go.mod h1:5b4v6he4MtMOwMlS0TUMTu2PcXUg8+E1lC7eC3UO/RA=\n-github.com/shirou/w32 v0.0.0-20160930032740-bb4de0191aa4/go.mod h1:qsXQc7+bwAM3Q1u/4XEfrquwF8Lw7D7y5cD8CuHnfIc=\n-github.com/shopspring/decimal v1.3.1 h1:2Usl1nmF/WZucqkFZhnfFYxxxu8LG21F6nPQBE5gKV8=\n-github.com/shopspring/decimal v1.3.1/go.mod h1:DKyhrW/HYNuLGql+MJL6WCR6knT2jwCFRcu2hWCYk4o=\n-github.com/shurcooL/sanitized_anchor_name v1.0.0/go.mod h1:1NzhyTcUVG4SuEtjjoZeVRXNmyL/1OwPU0+IJeTBvfc=\n-github.com/sirupsen/logrus v1.2.0/go.mod h1:LxeOpSwHxABJmUn/MG1IvRgCAasNZTLOkJPxbbu5VWo=\n-github.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE=\n-github.com/sirupsen/logrus v1.7.0/go.mod h1:yWOB1SBYBC5VeMP7gHvWumXLIWorT60ONWic61uBYv0=\n-github.com/sirupsen/logrus v1.8.1/go.mod h1:yWOB1SBYBC5VeMP7gHvWumXLIWorT60ONWic61uBYv0=\n-github.com/sirupsen/logrus v1.9.0 h1:trlNQbNUG3OdDrDil03MCb1H2o9nJ1x4/5LYw7byDE0=\n-github.com/sirupsen/logrus v1.9.0/go.mod h1:naHLuLoDiP4jHNo9R0sCBMtWGeIprob74mVsIT4qYEQ=\n-github.com/spaolacci/murmur3 v0.0.0-20180118202830-f09979ecbc72/go.mod h1:JwIasOWyU6f++ZhiEuf87xNszmSA2myDM2Kzu9HwQUA=\n-github.com/spf13/afero v1.3.3/go.mod h1:5KUK8ByomD5Ti5Artl0RtHeI5pTF7MIDuXL3yY520V4=\n-github.com/spf13/afero v1.6.0/go.mod h1:Ai8FlHk4v/PARR026UzYexafAt9roJ7LcLMAmO6Z93I=\n-github.com/spf13/afero v1.8.0 h1:5MmtuhAgYeU6qpa7w7bP0dv6MBYuup0vekhSpSkoq60=\n-github.com/spf13/afero v1.8.0/go.mod h1:CtAatgMJh6bJEIs48Ay/FOnkljP3WeGUG0MC1RfAqwo=\n-github.com/spf13/cast v1.4.1 h1:s0hze+J0196ZfEMTs80N7UlFt0BDuQ7Q+JDnHiMWKdA=\n-github.com/spf13/cast v1.4.1/go.mod h1:Qx5cxh0v+4UWYiBimWS+eyWzqEqokIECu5etghLkUJE=\n-github.com/spf13/cobra v0.0.3/go.mod h1:1l0Ry5zgKvJasoi3XT1TypsSe7PqH0Sj9dhYf7v3XqQ=\n-github.com/spf13/cobra v1.3.0 h1:R7cSvGu+Vv+qX0gW5R/85dx2kmmJT5z5NM8ifdYjdn0=\n-github.com/spf13/cobra v1.3.0/go.mod h1:BrRVncBjOJa/eUcVVm9CE+oC6as8k+VYr4NY7WCi9V4=\n-github.com/spf13/jwalterweatherman v1.1.0 h1:ue6voC5bR5F8YxI5S67j9i582FU4Qvo2bmqnqMYADFk=\n-github.com/spf13/jwalterweatherman v1.1.0/go.mod h1:aNWZUN0dPAAO/Ljvb5BEdw96iTZ0EXowPYD95IqWIGo=\n-github.com/spf13/pflag v1.0.2/go.mod h1:DYY7MBk1bdzusC3SYhjObp+wFpr4gzcvqqNjLnInEg4=\n-github.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=\n-github.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=\n-github.com/spf13/viper v1.10.0/go.mod h1:SoyBPwAtKDzypXNDFKN5kzH7ppppbGZtls1UpIy5AsM=\n-github.com/spf13/viper v1.10.1 h1:nuJZuYpG7gTj/XqiUwg8bA0cp1+M2mC3J4g5luUYBKk=\n-github.com/spf13/viper v1.10.1/go.mod h1:IGlFPqhNAPKRxohIzWpI5QEy4kuI7tcl5WvR+8qy1rU=\n-github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\n-github.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\n-github.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=\n-github.com/stretchr/objx v0.5.0/go.mod h1:Yh+to48EsGEfYuaHDzXPcE3xhTkx73EhmCGUpEOglKo=\n-github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\n-github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\n-github.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=\n-github.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=\n-github.com/stretchr/testify v1.6.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\n-github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\n-github.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\n-github.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=\n-github.com/stretchr/testify v1.8.1 h1:w7B6lhMri9wdJUVmEZPGGhZzrYTPvgJArz7wNPgYKsk=\n-github.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=\n-github.com/subosito/gotenv v1.2.0 h1:Slr1R9HxAlEKefgq5jn9U+DnETlIUa6HfgEzj0g5d7s=\n-github.com/subosito/gotenv v1.2.0/go.mod h1:N0PQaV/YGNqwC0u51sEeR/aUtSLEXKX9iv69rRypqCw=\n-github.com/syndtr/gocapability v0.0.0-20200815063812-42c35b437635/go.mod h1:hkRG7XYTFWNJGYcbNJQlaLq0fg1yr4J4t/NcTQtrfww=\n-github.com/testcontainers/testcontainers-go v0.18.0 h1:8RXrcIQv5xX/uBOSmZd297gzvA7F0yuRA37/918o7Yg=\n-github.com/testcontainers/testcontainers-go v0.18.0/go.mod h1:rLC7hR2SWRjJZZNrUYiTKvUXCziNxzZiYtz9icTWYNQ=\n-github.com/therootcompany/xz v1.0.1 h1:CmOtsn1CbtmyYiusbfmhmkpAAETj0wBIH6kCYaX+xzw=\n-github.com/therootcompany/xz v1.0.1/go.mod h1:3K3UH1yCKgBneZYhuQUvJ9HPD19UEXEI0BWbMn8qNMY=\n-github.com/tv42/httpunix v0.0.0-20150427012821-b75d8614f926/go.mod h1:9ESjWnEqriFuLhtthL60Sar/7RFoluCcXsuvEwTV5KM=\n-github.com/ulikunitz/xz v0.5.6/go.mod h1:2bypXElzHzzJZwzH67Y6wb67pO62Rzfn7BSiF4ABRW8=\n-github.com/ulikunitz/xz v0.5.10 h1:t92gobL9l3HE202wg3rlk19F6X+JOxl9BBrCCMYEYd8=\n-github.com/ulikunitz/xz v0.5.10/go.mod h1:nbz6k7qbPmH4IRqmfOplQw/tblSgqTqBwxkY0oWt/14=\n-github.com/urfave/cli v1.22.1/go.mod h1:Gos4lmkARVdJ6EkW0WaNv/tZAAMe9V7XWyB60NtXRu0=\n-github.com/vishvananda/netlink v1.1.0/go.mod h1:cTgwzPIzzgDAYoQrMm0EdrjRUBkTqKYppBueQtXaqoE=\n-github.com/vishvananda/netns v0.0.0-20191106174202-0a2b9b5464df/go.mod h1:JP3t17pCcGlemwknint6hfoeCVQrEMVwxRLRjXpq+BU=\n-github.com/yargevad/filepathx v1.0.0 h1:SYcT+N3tYGi+NvazubCNlvgIPbzAk7i7y2dwg3I5FYc=\n-github.com/yargevad/filepathx v1.0.0/go.mod h1:BprfX/gpYNJHJfc35GjRRpVcwWXS89gGulUIU5tK3tA=\n-github.com/yuin/goldmark v1.1.25/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\n-github.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\n-github.com/yuin/goldmark v1.1.32/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\n-github.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\n-github.com/yuin/goldmark v1.3.5/go.mod h1:mwnBkeHKe2W/ZEtQ+71ViKU8L12m81fl3OWwC1Zlc8k=\n-github.com/yuin/goldmark v1.4.0/go.mod h1:mwnBkeHKe2W/ZEtQ+71ViKU8L12m81fl3OWwC1Zlc8k=\n-go.etcd.io/etcd/api/v3 v3.5.1/go.mod h1:cbVKeC6lCfl7j/8jBhAK6aIYO9XOjdptoxU/nLQcPvs=\n-go.etcd.io/etcd/client/pkg/v3 v3.5.1/go.mod h1:IJHfcCEKxYu1Os13ZdwCwIUTUVGYTSAM3YSwc9/Ac1g=\n-go.etcd.io/etcd/client/v2 v2.305.1/go.mod h1:pMEacxZW7o8pg4CrFE7pquyCJJzZvkvdD2RibOCCCGs=\n-go.opencensus.io v0.21.0/go.mod h1:mSImk1erAIZhrmZN+AvHh14ztQfjbGwt4TtuofqLduU=\n-go.opencensus.io v0.22.0/go.mod h1:+kGneAE2xo2IficOXnaByMWTGM9T73dGwxeWcUqIpI8=\n-go.opencensus.io v0.22.2/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\n-go.opencensus.io v0.22.3/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\n-go.opencensus.io v0.22.4/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\n-go.opencensus.io v0.22.5/go.mod h1:5pWMHQbX5EPX2/62yrJeAkowc+lfs/XD7Uxpq3pI6kk=\n-go.opencensus.io v0.23.0/go.mod h1:XItmlyltB5F7CS4xOC1DcqMoFqwtC6OG2xF7mCv7P7E=\n-go.opentelemetry.io/otel v1.4.1 h1:QbINgGDDcoQUoMJa2mMaWno49lja9sHwp6aoa2n3a4g=\n-go.opentelemetry.io/otel v1.4.1/go.mod h1:StM6F/0fSwpd8dKWDCdRr7uRvEPYdW0hBSlbdTiUde4=\n-go.opentelemetry.io/otel/trace v1.4.1 h1:O+16qcdTrT7zxv2J6GejTPFinSwA++cYerC5iSiF8EQ=\n-go.opentelemetry.io/otel/trace v1.4.1/go.mod h1:iYEVbroFCNut9QkwEczV9vMRPHNKSSwYZjulEtsmhFc=\n-go.opentelemetry.io/proto/otlp v0.7.0/go.mod h1:PqfVotwruBrMGOCsRd/89rSnXhoiJIqeYNgFYFoEGnI=\n-go.uber.org/atomic v1.7.0/go.mod h1:fEN4uk6kAWBTFdckzkM89CLk9XfWZrxpCo0nPH17wJc=\n-go.uber.org/multierr v1.6.0/go.mod h1:cdWPpRnG4AhwMwsgIHip0KRBQjJy5kYEpYjJxpXp9iU=\n-go.uber.org/zap v1.17.0/go.mod h1:MXVU+bhUf/A7Xi2HNOnopQOrmycQ5Ih87HtOu4q5SSo=\n-golang.org/x/crypto v0.0.0-20180904163835-0709b304e793/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\n-golang.org/x/crypto v0.0.0-20181029021203-45a5f77698d3/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\n-golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\n-golang.org/x/crypto v0.0.0-20190510104115-cbcb75029529/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\n-golang.org/x/crypto v0.0.0-20190605123033-f99c8df09eb5/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\n-golang.org/x/crypto v0.0.0-20190820162420-60c769a6c586/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\n-golang.org/x/crypto v0.0.0-20190923035154-9ee001bba392/go.mod h1:/lpIB1dKB+9EgE3H3cr1v9wB50oz8l4C4h62xy7jSTY=\n-golang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\n-golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\n-golang.org/x/crypto v0.0.0-20210421170649-83a5a9bb288b/go.mod h1:T9bdIzuCu7OtxOm1hfPfRQxPLYneinmdGuTeoZ9dtd4=\n-golang.org/x/crypto v0.0.0-20210817164053-32db794688a5/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=\n-golang.org/x/crypto v0.0.0-20211108221036-ceb1ce70b4fa/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=\n-golang.org/x/crypto v0.0.0-20211215165025-cf75a172585e/go.mod h1:P+XmwS30IXTQdn5tA2iutPOUgjI07+tq3H3K9MVA1s8=\n-golang.org/x/exp v0.0.0-20190121172915-509febef88a4/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\n-golang.org/x/exp v0.0.0-20190306152737-a1d7652674e8/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\n-golang.org/x/exp v0.0.0-20190510132918-efd6b22b2522/go.mod h1:ZjyILWgesfNpC6sMxTJOJm9Kp84zZh5NQWvqDGG3Qr8=\n-golang.org/x/exp v0.0.0-20190829153037-c13cbed26979/go.mod h1:86+5VVa7VpoJ4kLfm080zCjGlMRFzhUhsZKEZO7MGek=\n-golang.org/x/exp v0.0.0-20191030013958-a1ab85dbe136/go.mod h1:JXzH8nQsPlswgeRAPE3MuO9GYsAcnJvJ4vnMwN/5qkY=\n-golang.org/x/exp v0.0.0-20191129062945-2f5052295587/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\n-golang.org/x/exp v0.0.0-20191227195350-da58074b4299/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\n-golang.org/x/exp v0.0.0-20200119233911-0405dc783f0a/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\n-golang.org/x/exp v0.0.0-20200207192155-f17229e696bd/go.mod h1:J/WKrq2StrnmMY6+EHIKF9dgMWnmCNThgcyBT1FY9mM=\n-golang.org/x/exp v0.0.0-20200224162631-6cc2880d07d6/go.mod h1:3jZMyOhIsHpP37uCMkUooju7aAi5cS1Q23tOzKc+0MU=\n-golang.org/x/image v0.0.0-20190227222117-0694c2d4d067/go.mod h1:kZ7UVZpmo3dzQBMxlp+ypCbDeSB+sBbTgSJuh5dn5js=\n-golang.org/x/image v0.0.0-20190802002840-cff245a6509b/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\n-golang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\n-golang.org/x/lint v0.0.0-20190227174305-5b3e6a55c961/go.mod h1:wehouNa3lNwaWXcvxsM5YxQ5yQlVC4a0KAMCusXpPoU=\n-golang.org/x/lint v0.0.0-20190301231843-5614ed5bae6f/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\n-golang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\n-golang.org/x/lint v0.0.0-20190409202823-959b441ac422/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\n-golang.org/x/lint v0.0.0-20190909230951-414d861bb4ac/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\n-golang.org/x/lint v0.0.0-20190930215403-16217165b5de/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\n-golang.org/x/lint v0.0.0-20191125180803-fdd1cda4f05f/go.mod h1:5qLYkcX4OjUUV8bRuDixDT3tpyyb+LUpUlRWLxfhWrs=\n-golang.org/x/lint v0.0.0-20200130185559-910be7a94367/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\n-golang.org/x/lint v0.0.0-20200302205851-738671d3881b/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\n-golang.org/x/lint v0.0.0-20201208152925-83fdc39ff7b5/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\n-golang.org/x/lint v0.0.0-20210508222113-6edffad5e616/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\n-golang.org/x/mobile v0.0.0-20190312151609-d3739f865fa6/go.mod h1:z+o9i4GpDbdi3rU15maQ/Ox0txvL9dWGYEHz965HBQE=\n-golang.org/x/mobile v0.0.0-20190719004257-d2bd2a29d028/go.mod h1:E/iHnbuqvinMTCcRqshq8CkpyQDoeVncDDYHnLhea+o=\n-golang.org/x/mod v0.0.0-20190513183733-4bf6d317e70e/go.mod h1:mXi4GBBbnImb6dmsKGUJ2LatrhH/nqhxcFungHvyanc=\n-golang.org/x/mod v0.1.0/go.mod h1:0QHyrYULN0/3qlju5TqG8bIK38QM8yzMo5ekMj3DlcY=\n-golang.org/x/mod v0.1.1-0.20191105210325-c90efee705ee/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=\n-golang.org/x/mod v0.1.1-0.20191107180719-034126e5016b/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=\n-golang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\n-golang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\n-golang.org/x/mod v0.4.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\n-golang.org/x/mod v0.4.1/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\n-golang.org/x/mod v0.4.2/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\n-golang.org/x/mod v0.5.0/go.mod h1:5OXOZSfqPIIbmVBIIKWRFfZjPR0E5r58TLhUjH0a2Ro=\n-golang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20180826012351-8a410e7b638d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20181023162649-9b4f9f5ad519/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20181114220301-adae6a3d119a/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20190213061140-3a22650c66bd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n-golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n-golang.org/x/net v0.0.0-20190501004415-9ce7a6920f09/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n-golang.org/x/net v0.0.0-20190503192946-f4e77d36d62c/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n-golang.org/x/net v0.0.0-20190603091049-60506f45cf65/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=\n-golang.org/x/net v0.0.0-20190613194153-d28f0bde5980/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20190628185345-da137c7871d7/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20190724013045-ca1201d0de80/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20190923162816-aa69164e4478/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20191209160850-c0dbc17a3553/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20200114155413-6afb5195e5aa/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20200202094626-16171245cfb2/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20200222125558-5a598a2470a0/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20200301022130-244492dfa37a/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20200324143707-d3edc9973b7e/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\n-golang.org/x/net v0.0.0-20200501053045-e0ff5e5a1de5/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\n-golang.org/x/net v0.0.0-20200506145744-7e3656a0809f/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\n-golang.org/x/net v0.0.0-20200513185701-a91f0712d120/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\n-golang.org/x/net v0.0.0-20200520182314-0ba52f642ac2/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\n-golang.org/x/net v0.0.0-20200625001655-4c5254603344/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\n-golang.org/x/net v0.0.0-20200707034311-ab3426394381/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\n-golang.org/x/net v0.0.0-20200822124328-c89045814202/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\n-golang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=\n-golang.org/x/net v0.0.0-20201031054903-ff519b6c9102/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=\n-golang.org/x/net v0.0.0-20201110031124-69a78807bb2b/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=\n-golang.org/x/net v0.0.0-20201209123823-ac852fbbde11/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\n-golang.org/x/net v0.0.0-20201224014010-6772e930b67b/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\n-golang.org/x/net v0.0.0-20210119194325-5f4716e94777/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\n-golang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\n-golang.org/x/net v0.0.0-20210316092652-d523dce5a7f4/go.mod h1:RBQZq4jEuRlivfhVLdyRGr576XBO4/greRjx4P4O3yc=\n-golang.org/x/net v0.0.0-20210405180319-a5a99cb37ef4/go.mod h1:p54w0d4576C0XHj96bSt6lcn1PtDYWL6XObtHCRCNQM=\n-golang.org/x/net v0.0.0-20210410081132-afb366fc7cd1/go.mod h1:9tjilg8BloeKEkVJvy7fQ90B1CfIiPueXVOjqfkSzI8=\n-golang.org/x/net v0.0.0-20210503060351-7fd8e65b6420/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\n-golang.org/x/net v0.0.0-20210805182204-aaa1db679c0d/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\n-golang.org/x/net v0.0.0-20210813160813-60bc85c4be6d/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\n-golang.org/x/net v0.0.0-20220906165146-f3363e06e74c h1:yKufUcDwucU5urd+50/Opbt4AYpqthk7wHpHok8f1lo=\n-golang.org/x/net v0.0.0-20220906165146-f3363e06e74c/go.mod h1:YDH+HFinaLZZlnHAfSS6ZXJJ9M9t4Dl22yv3iI2vPwk=\n-golang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=\n-golang.org/x/oauth2 v0.0.0-20190226205417-e64efc72b421/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n-golang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n-golang.org/x/oauth2 v0.0.0-20191202225959-858c2ad4c8b6/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n-golang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n-golang.org/x/oauth2 v0.0.0-20200902213428-5d25da1a8d43/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\n-golang.org/x/oauth2 v0.0.0-20201109201403-9fd604954f58/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\n-golang.org/x/oauth2 v0.0.0-20201208152858-08078c50e5b5/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\n-golang.org/x/oauth2 v0.0.0-20210218202405-ba52d332ba99/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\n-golang.org/x/oauth2 v0.0.0-20210220000619-9bb904979d93/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\n-golang.org/x/oauth2 v0.0.0-20210313182246-cd4f82c27b84/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\n-golang.org/x/oauth2 v0.0.0-20210514164344-f6687ab2804c/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\n-golang.org/x/oauth2 v0.0.0-20210628180205-a41e5a781914/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\n-golang.org/x/oauth2 v0.0.0-20210805134026-6f1e6394065a/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\n-golang.org/x/oauth2 v0.0.0-20210819190943-2bc19b11175f/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\n-golang.org/x/oauth2 v0.0.0-20211005180243-6b3c2da341f1/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\n-golang.org/x/oauth2 v0.0.0-20211104180415-d3ed0bb246c8/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\n-golang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.0.0-20190227155943-e225da77a7e6/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.0.0-20200317015054-43a5402ce75a/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.0.0-20200625203802-6e8e738ad208/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.0.0-20201207232520-09787c993a3a/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.0.0-20210220032951-036812b2e83c/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sys v0.0.0-20180810173357-98c5dad5d1a0/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n-golang.org/x/sys v0.0.0-20180823144017-11551d06cbcc/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n-golang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n-golang.org/x/sys v0.0.0-20180905080454-ebe1bf3edb33/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n-golang.org/x/sys v0.0.0-20181026203630-95b1ffbd15a5/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n-golang.org/x/sys v0.0.0-20181116152217-5ac8a444bdc5/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n-golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n-golang.org/x/sys v0.0.0-20190222072716-a9d3bda3a223/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n-golang.org/x/sys v0.0.0-20190312061237-fead79001313/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190502145724-3ef323f4f1fd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190507160741-ecd444e8653b/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190606165138-5da285871e9c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190606203320-7fc4e5ec1444/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190624142023-c5567b49c5d0/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190726091711-fc99dfbffb4e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190922100055-0a153f010e69/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190924154521-2837fb4f24fe/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20191001151750-bb3f8db39f24/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20191008105621-543471e840be/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20191026070338-33540a1f6037/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20191115151921-52ab43148777/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20191204072324-ce4227a45e2e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20191220220014-0732a990476f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20191228213918-04cbcbbfeed8/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200113162924-86b910548bc1/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200116001909-b77594299b42/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200122134326-e047566fdf82/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200124204421-9fbb57f87de9/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200202164722-d101bd2416d5/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200212091648-12a6c2dcc1e4/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200223170610-d5e6a3e2c0ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200302150141-5c8b2ff67527/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200331124033-c3d80250170d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200501052902-10377860bb8e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200511232937-7e40ca221e25/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200515095857-1151b9dac4a9/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200523222454-059865788121/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200803210538-64077c9b5642/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200905004654-be1d3432aa8f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20201201145000-ef89a241ccb3/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20210104204734-6f8348627aad/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20210119212857-b64e53b001e4/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20210124154548-22da62e12c0c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20210220050731-9a76102bfb43/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20210225134936-a50acf3fe073/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20210303074136-134d130e1a04/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20210305230114-8fe3ee5dd75b/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20210315160823-c6e025ad8005/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20210320140829-1e4c9ba3b0c4/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20210330210617-4fbd30eecc44/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20210403161142-5e06dd20ab57/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20210423082822-04245dca01da/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20210423185535-09eb48e85fd7/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20210510120138-977fb7262007/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20210514084401-e8d321eab015/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20210603125802-9665404d3644/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20210616094352-59db8d763f22/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20210630005230-0f9fa26af87c/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20210806184541-e5e7981a1069/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20210809222454-d867a43fc93e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20210816183151-1e6c022a8912/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20210823070655-63515b42dcdf/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20210906170528-6f6e22806c34/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20210908233432-aa78b53d3365/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20210927094055-39ccf1dd6fa6/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20211007075335-d3039528d8ac/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20211025201205-69cdffdb9359/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20211110154304-99a53858aa08/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20211116061358-0a5406a5449c/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20211124211545-fe61309f8881/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20211205182925-97ca703d548d/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20220412211240-33da011f77ad/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20220715151400-c0bba94af5f8/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.5.0 h1:MUK/U/4lj1t1oPg0HfuXDN/Z1wv31ZJ/YcPiGccS4DU=\n-golang.org/x/sys v0.5.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=\n-golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n-golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n-golang.org/x/text v0.3.1-0.20180807135948-17ff2d5776d2/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n-golang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\n-golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\n-golang.org/x/text v0.3.4/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\n-golang.org/x/text v0.3.5/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\n-golang.org/x/text v0.3.6/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\n-golang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=\n-golang.org/x/text v0.7.0 h1:4BRB4x83lYWy72KwLD/qYDuTu7q9PjSagHvijDw7cLo=\n-golang.org/x/text v0.7.0/go.mod h1:mrYo+phRRbMaCq/xk9113O4dZlRixOauAjOtrjsXDZ8=\n-golang.org/x/time v0.0.0-20181108054448-85acf8d2951c/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n-golang.org/x/time v0.0.0-20190308202827-9d24e82272b4/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n-golang.org/x/time v0.0.0-20191024005414-555d28b269f0/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n-golang.org/x/time v0.0.0-20220210224613-90d013bbcef8 h1:vVKdlvoWBphwdxWKrFZEuM0kGgGLxUOYcY4U/2Vjg44=\n-golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n-golang.org/x/tools v0.0.0-20190114222345-bf090417da8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n-golang.org/x/tools v0.0.0-20190226205152-f727befe758c/go.mod h1:9Yl7xja0Znq3iFh3HoIrodX9oNMXvdceNzlUR8zjMvY=\n-golang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n-golang.org/x/tools v0.0.0-20190312151545-0bb0c0a6e846/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n-golang.org/x/tools v0.0.0-20190312170243-e65039ee4138/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n-golang.org/x/tools v0.0.0-20190425150028-36563e24a262/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\n-golang.org/x/tools v0.0.0-20190506145303-2d16b83fe98c/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\n-golang.org/x/tools v0.0.0-20190524140312-2c0ae7006135/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\n-golang.org/x/tools v0.0.0-20190606124116-d0a3d012864b/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\n-golang.org/x/tools v0.0.0-20190621195816-6e04913cbbac/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\n-golang.org/x/tools v0.0.0-20190628153133-6cdbf07be9d0/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\n-golang.org/x/tools v0.0.0-20190816200558-6889da9d5479/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\n-golang.org/x/tools v0.0.0-20190907020128-2ca718005c18/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\n-golang.org/x/tools v0.0.0-20190911174233-4f2ddba30aff/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\n-golang.org/x/tools v0.0.0-20191012152004-8de300cfc20a/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\n-golang.org/x/tools v0.0.0-20191113191852-77e3bb0ad9e7/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\n-golang.org/x/tools v0.0.0-20191115202509-3a792d9c32b2/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\n-golang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\n-golang.org/x/tools v0.0.0-20191125144606-a911d9008d1f/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\n-golang.org/x/tools v0.0.0-20191130070609-6e064ea0cf2d/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\n-golang.org/x/tools v0.0.0-20191216173652-a0e659d51361/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\n-golang.org/x/tools v0.0.0-20191227053925-7b8e75db28f4/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\n-golang.org/x/tools v0.0.0-20200117161641-43d50277825c/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\n-golang.org/x/tools v0.0.0-20200122220014-bf1340f18c4a/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\n-golang.org/x/tools v0.0.0-20200130002326-2f3ba24bd6e7/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\n-golang.org/x/tools v0.0.0-20200204074204-1cc6d1ef6c74/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\n-golang.org/x/tools v0.0.0-20200207183749-b753a1ba74fa/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\n-golang.org/x/tools v0.0.0-20200212150539-ea181f53ac56/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\n-golang.org/x/tools v0.0.0-20200224181240-023911ca70b2/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\n-golang.org/x/tools v0.0.0-20200227222343-706bc42d1f0d/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\n-golang.org/x/tools v0.0.0-20200304193943-95d2e580d8eb/go.mod h1:o4KQGtdN14AW+yjsvvwRTJJuXz8XRtIHtEnmAXLyFUw=\n-golang.org/x/tools v0.0.0-20200312045724-11d5b4c81c7d/go.mod h1:o4KQGtdN14AW+yjsvvwRTJJuXz8XRtIHtEnmAXLyFUw=\n-golang.org/x/tools v0.0.0-20200331025713-a30bf2db82d4/go.mod h1:Sl4aGygMT6LrqrWclx+PTx3U+LnKx/seiNR+3G19Ar8=\n-golang.org/x/tools v0.0.0-20200501065659-ab2804fb9c9d/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\n-golang.org/x/tools v0.0.0-20200512131952-2bc93b1c0c88/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\n-golang.org/x/tools v0.0.0-20200515010526-7d3b6ebf133d/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\n-golang.org/x/tools v0.0.0-20200618134242-20370b0cb4b2/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\n-golang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\n-golang.org/x/tools v0.0.0-20200729194436-6467de6f59a7/go.mod h1:njjCfa9FT2d7l9Bc6FUM5FLjQPp3cFF28FI3qnDFljA=\n-golang.org/x/tools v0.0.0-20200804011535-6c149bb5ef0d/go.mod h1:njjCfa9FT2d7l9Bc6FUM5FLjQPp3cFF28FI3qnDFljA=\n-golang.org/x/tools v0.0.0-20200825202427-b303f430e36d/go.mod h1:njjCfa9FT2d7l9Bc6FUM5FLjQPp3cFF28FI3qnDFljA=\n-golang.org/x/tools v0.0.0-20200904185747-39188db58858/go.mod h1:Cj7w3i3Rnn0Xh82ur9kSqwfTHTeVxaDqrfMjpcNT6bE=\n-golang.org/x/tools v0.0.0-20201110124207-079ba7bd75cd/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\n-golang.org/x/tools v0.0.0-20201201161351-ac6f37ff4c2a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\n-golang.org/x/tools v0.0.0-20201208233053-a543418bbed2/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\n-golang.org/x/tools v0.0.0-20210105154028-b0ab187a4818/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\n-golang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\n-golang.org/x/tools v0.0.0-20210108195828-e2f9c7f1fc8e/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\n-golang.org/x/tools v0.1.0/go.mod h1:xkSsbof2nBLbhDlRMhhhyNLN/zl3eTqcnHD5viDpcZ0=\n-golang.org/x/tools v0.1.1/go.mod h1:o0xws9oXOQQZyjljx8fwUC0k7L1pTE6eaCbjGeHmOkk=\n-golang.org/x/tools v0.1.2/go.mod h1:o0xws9oXOQQZyjljx8fwUC0k7L1pTE6eaCbjGeHmOkk=\n-golang.org/x/tools v0.1.3/go.mod h1:o0xws9oXOQQZyjljx8fwUC0k7L1pTE6eaCbjGeHmOkk=\n-golang.org/x/tools v0.1.4/go.mod h1:o0xws9oXOQQZyjljx8fwUC0k7L1pTE6eaCbjGeHmOkk=\n-golang.org/x/tools v0.1.5/go.mod h1:o0xws9oXOQQZyjljx8fwUC0k7L1pTE6eaCbjGeHmOkk=\n-golang.org/x/tools v0.1.7/go.mod h1:LGqMHiF4EqQNHR1JncWGqT5BVaXmza+X+BDGol+dOxo=\n-golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\n-golang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\n-golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\n-golang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\n-google.golang.org/api v0.4.0/go.mod h1:8k5glujaEP+g9n7WNsDg8QP6cUVNI86fCNMcbazEtwE=\n-google.golang.org/api v0.7.0/go.mod h1:WtwebWUNSVBH/HAw79HIFXZNqEvBhG+Ra+ax0hx3E3M=\n-google.golang.org/api v0.8.0/go.mod h1:o4eAsZoiT+ibD93RtjEohWalFOjRDx6CVaqeizhEnKg=\n-google.golang.org/api v0.9.0/go.mod h1:o4eAsZoiT+ibD93RtjEohWalFOjRDx6CVaqeizhEnKg=\n-google.golang.org/api v0.13.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\n-google.golang.org/api v0.14.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\n-google.golang.org/api v0.15.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\n-google.golang.org/api v0.17.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\n-google.golang.org/api v0.18.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\n-google.golang.org/api v0.19.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\n-google.golang.org/api v0.20.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\n-google.golang.org/api v0.22.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\n-google.golang.org/api v0.24.0/go.mod h1:lIXQywCXRcnZPGlsd8NbLnOjtAoL6em04bJ9+z0MncE=\n-google.golang.org/api v0.28.0/go.mod h1:lIXQywCXRcnZPGlsd8NbLnOjtAoL6em04bJ9+z0MncE=\n-google.golang.org/api v0.29.0/go.mod h1:Lcubydp8VUV7KeIHD9z2Bys/sm/vGKnG1UHuDBSrHWM=\n-google.golang.org/api v0.30.0/go.mod h1:QGmEvQ87FHZNiUVJkT14jQNYJ4ZJjdRF23ZXz5138Fc=\n-google.golang.org/api v0.35.0/go.mod h1:/XrVsuzM0rZmrsbjJutiuftIzeuTQcEeaYcSk/mQ1dg=\n-google.golang.org/api v0.36.0/go.mod h1:+z5ficQTmoYpPn8LCUNVpK5I7hwkpjbcgqA7I34qYtE=\n-google.golang.org/api v0.40.0/go.mod h1:fYKFpnQN0DsDSKRVRcQSDQNtqWPfM9i+zNPxepjRCQ8=\n-google.golang.org/api v0.41.0/go.mod h1:RkxM5lITDfTzmyKFPt+wGrCJbVfniCr2ool8kTBzRTU=\n-google.golang.org/api v0.43.0/go.mod h1:nQsDGjRXMo4lvh5hP0TKqF244gqhGcr/YSIykhUk/94=\n-google.golang.org/api v0.47.0/go.mod h1:Wbvgpq1HddcWVtzsVLyfLp8lDg6AA241LmgIL59tHXo=\n-google.golang.org/api v0.48.0/go.mod h1:71Pr1vy+TAZRPkPs/xlCf5SsU8WjuAWv1Pfjbtukyy4=\n-google.golang.org/api v0.50.0/go.mod h1:4bNT5pAuq5ji4SRZm+5QIkjny9JAyVD/3gaSihNefaw=\n-google.golang.org/api v0.51.0/go.mod h1:t4HdrdoNgyN5cbEfm7Lum0lcLDLiise1F8qDKX00sOU=\n-google.golang.org/api v0.54.0/go.mod h1:7C4bFFOvVDGXjfDTAsgGwDgAxRDeQ4X8NvUedIt6z3k=\n-google.golang.org/api v0.55.0/go.mod h1:38yMfeP1kfjsl8isn0tliTjIb1rJXcQi4UXlbqivdVE=\n-google.golang.org/api v0.56.0/go.mod h1:38yMfeP1kfjsl8isn0tliTjIb1rJXcQi4UXlbqivdVE=\n-google.golang.org/api v0.57.0/go.mod h1:dVPlbZyBo2/OjBpmvNdpn2GRm6rPy75jyU7bmhdrMgI=\n-google.golang.org/api v0.59.0/go.mod h1:sT2boj7M9YJxZzgeZqXogmhfmRWDtPzT31xkieUbuZU=\n-google.golang.org/api v0.61.0/go.mod h1:xQRti5UdCmoCEqFxcz93fTl338AVqDgyaDRuOZ3hg9I=\n-google.golang.org/api v0.62.0/go.mod h1:dKmwPCydfsad4qCH08MSdgWjfHOyfpd4VtDGgRFdavw=\n-google.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=\n-google.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\n-google.golang.org/appengine v1.5.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\n-google.golang.org/appengine v1.6.1/go.mod h1:i06prIuMbXzDqacNJfV5OdTW448YApPu5ww/cMBSeb0=\n-google.golang.org/appengine v1.6.5/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\n-google.golang.org/appengine v1.6.6/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\n-google.golang.org/appengine v1.6.7/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\n-google.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=\n-google.golang.org/genproto v0.0.0-20190307195333-5fe7a883aa19/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\n-google.golang.org/genproto v0.0.0-20190418145605-e7d98fc518a7/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\n-google.golang.org/genproto v0.0.0-20190425155659-357c62f0e4bb/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\n-google.golang.org/genproto v0.0.0-20190502173448-54afdca5d873/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\n-google.golang.org/genproto v0.0.0-20190801165951-fa694d86fc64/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=\n-google.golang.org/genproto v0.0.0-20190819201941-24fa4b261c55/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=\n-google.golang.org/genproto v0.0.0-20190911173649-1774047e7e51/go.mod h1:IbNlFCBrqXvoKpeg0TB2l7cyZUmoaFKYIwrEpbDKLA8=\n-google.golang.org/genproto v0.0.0-20191108220845-16a3f7862a1a/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\n-google.golang.org/genproto v0.0.0-20191115194625-c23dd37a84c9/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\n-google.golang.org/genproto v0.0.0-20191216164720-4f79533eabd1/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\n-google.golang.org/genproto v0.0.0-20191230161307-f3c370f40bfb/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\n-google.golang.org/genproto v0.0.0-20200115191322-ca5a22157cba/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\n-google.golang.org/genproto v0.0.0-20200122232147-0452cf42e150/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\n-google.golang.org/genproto v0.0.0-20200204135345-fa8e72b47b90/go.mod h1:GmwEX6Z4W5gMy59cAlVYjN9JhxgbQH6Gn+gFDQe2lzA=\n-google.golang.org/genproto v0.0.0-20200212174721-66ed5ce911ce/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\n-google.golang.org/genproto v0.0.0-20200224152610-e50cd9704f63/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\n-google.golang.org/genproto v0.0.0-20200228133532-8c2c7df3a383/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\n-google.golang.org/genproto v0.0.0-20200305110556-506484158171/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\n-google.golang.org/genproto v0.0.0-20200312145019-da6875a35672/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\n-google.golang.org/genproto v0.0.0-20200331122359-1ee6d9798940/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\n-google.golang.org/genproto v0.0.0-20200430143042-b979b6f78d84/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\n-google.golang.org/genproto v0.0.0-20200511104702-f5ebc3bea380/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\n-google.golang.org/genproto v0.0.0-20200513103714-09dca8ec2884/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\n-google.golang.org/genproto v0.0.0-20200515170657-fc4c6c6a6587/go.mod h1:YsZOwe1myG/8QRHRsmBRE1LrgQY60beZKjly0O1fX9U=\n-google.golang.org/genproto v0.0.0-20200526211855-cb27e3aa2013/go.mod h1:NbSheEEYHJ7i3ixzK3sjbqSGDJWnxyFXZblF3eUsNvo=\n-google.golang.org/genproto v0.0.0-20200618031413-b414f8b61790/go.mod h1:jDfRM7FcilCzHH/e9qn6dsT145K34l5v+OpcnNgKAAA=\n-google.golang.org/genproto v0.0.0-20200729003335-053ba62fc06f/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\n-google.golang.org/genproto v0.0.0-20200804131852-c06518451d9c/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\n-google.golang.org/genproto v0.0.0-20200825200019-8632dd797987/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\n-google.golang.org/genproto v0.0.0-20200904004341-0bd0a958aa1d/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\n-google.golang.org/genproto v0.0.0-20201109203340-2640f1f9cdfb/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\n-google.golang.org/genproto v0.0.0-20201201144952-b05cb90ed32e/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\n-google.golang.org/genproto v0.0.0-20201210142538-e3217bee35cc/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\n-google.golang.org/genproto v0.0.0-20201214200347-8c77b98c765d/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\n-google.golang.org/genproto v0.0.0-20210108203827-ffc7fda8c3d7/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\n-google.golang.org/genproto v0.0.0-20210222152913-aa3ee6e6a81c/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\n-google.golang.org/genproto v0.0.0-20210226172003-ab064af71705/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\n-google.golang.org/genproto v0.0.0-20210303154014-9728d6b83eeb/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\n-google.golang.org/genproto v0.0.0-20210310155132-4ce2db91004e/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\n-google.golang.org/genproto v0.0.0-20210319143718-93e7006c17a6/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\n-google.golang.org/genproto v0.0.0-20210402141018-6c239bbf2bb1/go.mod h1:9lPAdzaEmUacj36I+k7YKbEc5CXzPIeORRgDAUOu28A=\n-google.golang.org/genproto v0.0.0-20210513213006-bf773b8c8384/go.mod h1:P3QM42oQyzQSnHPnZ/vqoCdDmzH28fzWByN9asMeM8A=\n-google.golang.org/genproto v0.0.0-20210602131652-f16073e35f0c/go.mod h1:UODoCrxHCcBojKKwX1terBiRUaqAsFqJiF615XL43r0=\n-google.golang.org/genproto v0.0.0-20210604141403-392c879c8b08/go.mod h1:UODoCrxHCcBojKKwX1terBiRUaqAsFqJiF615XL43r0=\n-google.golang.org/genproto v0.0.0-20210608205507-b6d2f5bf0d7d/go.mod h1:UODoCrxHCcBojKKwX1terBiRUaqAsFqJiF615XL43r0=\n-google.golang.org/genproto v0.0.0-20210624195500-8bfb893ecb84/go.mod h1:SzzZ/N+nwJDaO1kznhnlzqS8ocJICar6hYhVyhi++24=\n-google.golang.org/genproto v0.0.0-20210713002101-d411969a0d9a/go.mod h1:AxrInvYm1dci+enl5hChSFPOmmUF1+uAa/UsgNRWd7k=\n-google.golang.org/genproto v0.0.0-20210716133855-ce7ef5c701ea/go.mod h1:AxrInvYm1dci+enl5hChSFPOmmUF1+uAa/UsgNRWd7k=\n-google.golang.org/genproto v0.0.0-20210728212813-7823e685a01f/go.mod h1:ob2IJxKrgPT52GcgX759i1sleT07tiKowYBGbczaW48=\n-google.golang.org/genproto v0.0.0-20210805201207-89edb61ffb67/go.mod h1:ob2IJxKrgPT52GcgX759i1sleT07tiKowYBGbczaW48=\n-google.golang.org/genproto v0.0.0-20210813162853-db860fec028c/go.mod h1:cFeNkxwySK631ADgubI+/XFU/xp8FD5KIVV4rj8UC5w=\n-google.golang.org/genproto v0.0.0-20210821163610-241b8fcbd6c8/go.mod h1:eFjDcFEctNawg4eG61bRv87N7iHBWyVhJu7u1kqDUXY=\n-google.golang.org/genproto v0.0.0-20210828152312-66f60bf46e71/go.mod h1:eFjDcFEctNawg4eG61bRv87N7iHBWyVhJu7u1kqDUXY=\n-google.golang.org/genproto v0.0.0-20210831024726-fe130286e0e2/go.mod h1:eFjDcFEctNawg4eG61bRv87N7iHBWyVhJu7u1kqDUXY=\n-google.golang.org/genproto v0.0.0-20210903162649-d08c68adba83/go.mod h1:eFjDcFEctNawg4eG61bRv87N7iHBWyVhJu7u1kqDUXY=\n-google.golang.org/genproto v0.0.0-20210909211513-a8c4777a87af/go.mod h1:eFjDcFEctNawg4eG61bRv87N7iHBWyVhJu7u1kqDUXY=\n-google.golang.org/genproto v0.0.0-20210924002016-3dee208752a0/go.mod h1:5CzLGKJ67TSI2B9POpiiyGha0AjJvZIUgRMt1dSmuhc=\n-google.golang.org/genproto v0.0.0-20211008145708-270636b82663/go.mod h1:5CzLGKJ67TSI2B9POpiiyGha0AjJvZIUgRMt1dSmuhc=\n-google.golang.org/genproto v0.0.0-20211028162531-8db9c33dc351/go.mod h1:5CzLGKJ67TSI2B9POpiiyGha0AjJvZIUgRMt1dSmuhc=\n-google.golang.org/genproto v0.0.0-20211118181313-81c1377c94b1/go.mod h1:5CzLGKJ67TSI2B9POpiiyGha0AjJvZIUgRMt1dSmuhc=\n-google.golang.org/genproto v0.0.0-20211129164237-f09f9a12af12/go.mod h1:5CzLGKJ67TSI2B9POpiiyGha0AjJvZIUgRMt1dSmuhc=\n-google.golang.org/genproto v0.0.0-20211203200212-54befc351ae9/go.mod h1:5CzLGKJ67TSI2B9POpiiyGha0AjJvZIUgRMt1dSmuhc=\n-google.golang.org/genproto v0.0.0-20211206160659-862468c7d6e0/go.mod h1:5CzLGKJ67TSI2B9POpiiyGha0AjJvZIUgRMt1dSmuhc=\n-google.golang.org/genproto v0.0.0-20211208223120-3a66f561d7aa/go.mod h1:5CzLGKJ67TSI2B9POpiiyGha0AjJvZIUgRMt1dSmuhc=\n-google.golang.org/genproto v0.0.0-20220617124728-180714bec0ad h1:kqrS+lhvaMHCxul6sKQvKJ8nAAhlVItmZV822hYFH/U=\n-google.golang.org/genproto v0.0.0-20220617124728-180714bec0ad/go.mod h1:KEWEmljWE5zPzLBa/oHl6DaEt9LmfH6WtH1OHIvleBA=\n-google.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=\n-google.golang.org/grpc v1.20.1/go.mod h1:10oTOabMzJvdu6/UiuZezV6QK5dSlG84ov/aaiqXj38=\n-google.golang.org/grpc v1.21.1/go.mod h1:oYelfM1adQP15Ek0mdvEgi9Df8B9CZIaU1084ijfRaM=\n-google.golang.org/grpc v1.23.0/go.mod h1:Y5yQAOtifL1yxbo5wqy6BxZv8vAUGQwXBOALyacEbxg=\n-google.golang.org/grpc v1.25.1/go.mod h1:c3i+UQWmh7LiEpx4sFZnkU36qjEYZ0imhYfXVyQciAY=\n-google.golang.org/grpc v1.26.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\n-google.golang.org/grpc v1.27.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\n-google.golang.org/grpc v1.27.1/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\n-google.golang.org/grpc v1.28.0/go.mod h1:rpkK4SK4GF4Ach/+MFLZUBavHOvF2JJB5uozKKal+60=\n-google.golang.org/grpc v1.29.1/go.mod h1:itym6AZVZYACWQqET3MqgPpjcuV5QH3BxFS3IjizoKk=\n-google.golang.org/grpc v1.30.0/go.mod h1:N36X2cJ7JwdamYAgDz+s+rVMFjt3numwzf/HckM8pak=\n-google.golang.org/grpc v1.31.0/go.mod h1:N36X2cJ7JwdamYAgDz+s+rVMFjt3numwzf/HckM8pak=\n-google.golang.org/grpc v1.31.1/go.mod h1:N36X2cJ7JwdamYAgDz+s+rVMFjt3numwzf/HckM8pak=\n-google.golang.org/grpc v1.33.1/go.mod h1:fr5YgcSWrqhRRxogOsw7RzIpsmvOZ6IcH4kBYTpR3n0=\n-google.golang.org/grpc v1.33.2/go.mod h1:JMHMWHQWaTccqQQlmk3MJZS+GWXOdAesneDmEnv2fbc=\n-google.golang.org/grpc v1.34.0/go.mod h1:WotjhfgOW/POjDeRt8vscBtXq+2VjORFy659qA51WJ8=\n-google.golang.org/grpc v1.35.0/go.mod h1:qjiiYl8FncCW8feJPdyg3v6XW24KsRHe+dy9BAGRRjU=\n-google.golang.org/grpc v1.36.0/go.mod h1:qjiiYl8FncCW8feJPdyg3v6XW24KsRHe+dy9BAGRRjU=\n-google.golang.org/grpc v1.36.1/go.mod h1:qjiiYl8FncCW8feJPdyg3v6XW24KsRHe+dy9BAGRRjU=\n-google.golang.org/grpc v1.37.0/go.mod h1:NREThFqKR1f3iQ6oBuvc5LadQuXVGo9rkm5ZGrQdJfM=\n-google.golang.org/grpc v1.37.1/go.mod h1:NREThFqKR1f3iQ6oBuvc5LadQuXVGo9rkm5ZGrQdJfM=\n-google.golang.org/grpc v1.38.0/go.mod h1:NREThFqKR1f3iQ6oBuvc5LadQuXVGo9rkm5ZGrQdJfM=\n-google.golang.org/grpc v1.39.0/go.mod h1:PImNr+rS9TWYb2O4/emRugxiyHZ5JyHW5F+RPnDzfrE=\n-google.golang.org/grpc v1.39.1/go.mod h1:PImNr+rS9TWYb2O4/emRugxiyHZ5JyHW5F+RPnDzfrE=\n-google.golang.org/grpc v1.40.0/go.mod h1:ogyxbiOoUXAkP+4+xa6PZSE9DZgIHtSpzjDTB9KAK34=\n-google.golang.org/grpc v1.40.1/go.mod h1:ogyxbiOoUXAkP+4+xa6PZSE9DZgIHtSpzjDTB9KAK34=\n-google.golang.org/grpc v1.42.0/go.mod h1:k+4IHHFw41K8+bbowsex27ge2rCb65oeWqe4jJ590SU=\n-google.golang.org/grpc v1.47.0 h1:9n77onPX5F3qfFCqjy9dhn8PbNQsIKeVU04J9G7umt8=\n-google.golang.org/grpc v1.47.0/go.mod h1:vN9eftEi1UMyUsIF80+uQXhHjbXYbm0uXoFCACuMGWk=\n-google.golang.org/grpc/cmd/protoc-gen-go-grpc v1.1.0/go.mod h1:6Kw0yEErY5E/yWrBtf03jp27GLLJujG4z/JK95pnjjw=\n-google.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd/go.mod h1:DFci5gLYBciE7Vtevhsrf46CRTquxDuWsQurQQe4oz8=\n-google.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64/go.mod h1:kwYJMbMJ01Woi6D6+Kah6886xMZcty6N08ah7+eCXa0=\n-google.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60/go.mod h1:cfTl7dwQJ+fmap5saPgwCLgHXTUD7jkjRqWcaiX5VyM=\n-google.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967/go.mod h1:A+miEFZTKqfCUM6K7xSMQL9OKL/b6hQv+e19PK+JZNE=\n-google.golang.org/protobuf v1.21.0/go.mod h1:47Nbq4nVaFHyn7ilMalzfO3qCViNmqZ2kzikPIcrTAo=\n-google.golang.org/protobuf v1.22.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\n-google.golang.org/protobuf v1.23.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\n-google.golang.org/protobuf v1.23.1-0.20200526195155-81db48ad09cc/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\n-google.golang.org/protobuf v1.24.0/go.mod h1:r/3tXBNzIEhYS9I1OUVjXDlt8tc493IdKGjtUeSXeh4=\n-google.golang.org/protobuf v1.25.0/go.mod h1:9JNX74DMeImyA3h4bdi1ymwjUzf21/xIlbajtzgsN7c=\n-google.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=\n-google.golang.org/protobuf v1.26.0/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=\n-google.golang.org/protobuf v1.27.1/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=\n-google.golang.org/protobuf v1.28.0 h1:w43yiav+6bVFTBQFZX0r7ipe9JQ1QsbMgHwbBziscLw=\n-google.golang.org/protobuf v1.28.0/go.mod h1:HV8QOd/L58Z+nl8r43ehVNZIU/HEI6OcFqwMG9pJV4I=\n-gopkg.in/alecthomas/kingpin.v2 v2.2.6/go.mod h1:FMv+mEhP44yOT+4EoQTLFTRgOQ1FBLkstjWtayDeSgw=\n-gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n-gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n-gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n-gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=\n-gopkg.in/errgo.v2 v2.1.0/go.mod h1:hNsd1EY+bozCKY1Ytp96fpM3vjJbqLJn88ws8XvfDNI=\n-gopkg.in/ini.v1 v1.66.2 h1:XfR1dOYubytKy4Shzc2LHrrGhU0lDCfDGG1yLPmpgsI=\n-gopkg.in/ini.v1 v1.66.2/go.mod h1:pNLf8WUiyNEtQjuu5G5vTm06TEv9tsIgeAvK8hOrP4k=\n-gopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n-gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n-gopkg.in/yaml.v2 v2.2.3/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n-gopkg.in/yaml.v2 v2.2.4/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n-gopkg.in/yaml.v2 v2.2.5/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n-gopkg.in/yaml.v2 v2.2.8/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n-gopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=\n-gopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=\n-gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n-gopkg.in/yaml.v3 v3.0.0-20210107192922-496545a6307b/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n-gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\n-gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n-gotest.tools/v3 v3.0.3 h1:4AuOwCGf4lLR9u3YOe2awrHygurzhO/HeQ6laiA6Sx0=\n-honnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n-honnef.co/go/tools v0.0.0-20190106161140-3f1c8253044a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n-honnef.co/go/tools v0.0.0-20190418001031-e561f6794a2a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n-honnef.co/go/tools v0.0.0-20190523083050-ea95bdfd59fc/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n-honnef.co/go/tools v0.0.1-2019.2.3/go.mod h1:a3bituU0lyd329TUQxRnasdCoJDkEUEAqEt0JzvZhAg=\n-honnef.co/go/tools v0.0.1-2020.1.3/go.mod h1:X/FiERA/W4tHapMX5mGpAtMSVEeEUOyHaw9vFzvIQ3k=\n-honnef.co/go/tools v0.0.1-2020.1.4/go.mod h1:X/FiERA/W4tHapMX5mGpAtMSVEeEUOyHaw9vFzvIQ3k=\n-howett.net/plist v0.0.0-20181124034731-591f970eefbb h1:jhnBjNi9UFpfpl8YZhA9CrOqpnJdvzuiHsl/dnxl11M=\n-howett.net/plist v0.0.0-20181124034731-591f970eefbb/go.mod h1:vMygbs4qMhSZSc4lCUl2OEE+rDiIIJAIdR4m7MiMcm0=\n-rsc.io/binaryregexp v0.2.0/go.mod h1:qTv7/COck+e2FymRvadv62gMdZztPaShugOCi3I+8D8=\n-rsc.io/quote/v3 v3.1.0/go.mod h1:yEA65RcK8LyAZtP9Kv3t0HmxON59tX3rD+tICJqUlj0=\n-rsc.io/sampler v1.3.0/go.mod h1:T1hPZKmBbMNahiBKFy5HrXp6adAjACjK9JXDnKaTXpA=\ndiff --git a/programs/diagnostics/internal/collectors/clickhouse/config.go b/programs/diagnostics/internal/collectors/clickhouse/config.go\ndeleted file mode 100644\nindex 92368bce6f3e..000000000000\n--- a/programs/diagnostics/internal/collectors/clickhouse/config.go\n+++ /dev/null\n@@ -1,113 +0,0 @@\n-package clickhouse\n-\n-import (\n-\t\"fmt\"\n-\t\"path/filepath\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils\"\n-\t\"github.com/pkg/errors\"\n-)\n-\n-type ConfigCollector struct {\n-\tresourceManager *platform.ResourceManager\n-}\n-\n-func NewConfigCollector(m *platform.ResourceManager) *ConfigCollector {\n-\treturn &ConfigCollector{\n-\t\tresourceManager: m,\n-\t}\n-}\n-\n-const DefaultConfigLocation = \"/etc/clickhouse-server/\"\n-const ProcessedConfigurationLocation = \"/var/lib/clickhouse/preprocessed_configs\"\n-\n-func (c ConfigCollector) Collect(conf config.Configuration) (*data.DiagnosticBundle, error) {\n-\tconf, err := conf.ValidateConfig(c.Configuration())\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\tdirectory, err := config.ReadStringValue(conf, \"directory\")\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\n-\tif directory != \"\" {\n-\t\t// user has specified a directory - we therefore skip all other efforts to locate the config\n-\t\tframe, errs := data.NewConfigFileFrame(directory)\n-\t\treturn &data.DiagnosticBundle{\n-\t\t\tFrames: map[string]data.Frame{\n-\t\t\t\t\"user_specified\": frame,\n-\t\t\t},\n-\t\t\tErrors: data.FrameErrors{Errors: errs},\n-\t\t}, nil\n-\t}\n-\tconfigCandidates, err := FindConfigurationFiles()\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, errors.Wrapf(err, \"Unable to find configuration files\")\n-\t}\n-\tframes := make(map[string]data.Frame)\n-\tvar frameErrors []error\n-\tfor frameName, confDir := range configCandidates {\n-\t\tframe, errs := data.NewConfigFileFrame(confDir)\n-\t\tframeErrors = append(frameErrors, errs...)\n-\t\tframes[frameName] = frame\n-\t}\n-\treturn &data.DiagnosticBundle{\n-\t\tFrames: frames,\n-\t\tErrors: data.FrameErrors{Errors: frameErrors},\n-\t}, err\n-}\n-\n-func FindConfigurationFiles() (map[string]string, error) {\n-\tconfigCandidates := map[string]string{\n-\t\t\"default\":      DefaultConfigLocation,\n-\t\t\"preprocessed\": ProcessedConfigurationLocation,\n-\t}\n-\t// we don't know specifically where the config is but try to find via processes\n-\tprocessConfigs, err := utils.FindConfigsFromClickHouseProcesses()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tfor i, path := range processConfigs {\n-\t\tconfDir := filepath.Dir(path)\n-\t\tif len(processConfigs) == 1 {\n-\t\t\tconfigCandidates[\"process\"] = confDir\n-\t\t\tbreak\n-\t\t}\n-\t\tconfigCandidates[fmt.Sprintf(\"process_%d\", i)] = confDir\n-\t}\n-\treturn configCandidates, nil\n-}\n-\n-func (c ConfigCollector) Configuration() config.Configuration {\n-\treturn config.Configuration{\n-\t\tParams: []config.ConfigParam{\n-\t\t\tconfig.StringParam{\n-\t\t\t\tValue:      \"\",\n-\t\t\t\tParam:      config.NewParam(\"directory\", \"Specify the location of the configuration files for ClickHouse Server e.g. /etc/clickhouse-server/\", false),\n-\t\t\t\tAllowEmpty: true,\n-\t\t\t},\n-\t\t},\n-\t}\n-}\n-\n-func (c ConfigCollector) Description() string {\n-\treturn \"Collects the ClickHouse configuration from the local filesystem.\"\n-}\n-\n-func (c ConfigCollector) IsDefault() bool {\n-\treturn true\n-}\n-\n-// here we register the collector for use\n-func init() {\n-\tcollectors.Register(\"config\", func() (collectors.Collector, error) {\n-\t\treturn &ConfigCollector{\n-\t\t\tresourceManager: platform.GetResourceManager(),\n-\t\t}, nil\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/collectors/clickhouse/db_logs.go b/programs/diagnostics/internal/collectors/clickhouse/db_logs.go\ndeleted file mode 100644\nindex 3253f504c1bc..000000000000\n--- a/programs/diagnostics/internal/collectors/clickhouse/db_logs.go\n+++ /dev/null\n@@ -1,108 +0,0 @@\n-package clickhouse\n-\n-import (\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/pkg/errors\"\n-)\n-\n-type DBLogTable struct {\n-\torderBy        data.OrderBy\n-\texcludeColumns []string\n-}\n-\n-var DbLogTables = map[string]DBLogTable{\n-\t\"query_log\": {\n-\t\torderBy: data.OrderBy{\n-\t\t\tColumn: \"event_time_microseconds\",\n-\t\t\tOrder:  data.Asc,\n-\t\t},\n-\t\texcludeColumns: []string{},\n-\t},\n-\t\"query_thread_log\": {\n-\t\torderBy: data.OrderBy{\n-\t\t\tColumn: \"event_time_microseconds\",\n-\t\t\tOrder:  data.Asc,\n-\t\t},\n-\t\texcludeColumns: []string{},\n-\t},\n-\t\"text_log\": {\n-\t\torderBy: data.OrderBy{\n-\t\t\tColumn: \"event_time_microseconds\",\n-\t\t\tOrder:  data.Asc,\n-\t\t},\n-\t\texcludeColumns: []string{},\n-\t},\n-}\n-\n-// This collector collects db logs\n-\n-type DBLogsCollector struct {\n-\tresourceManager *platform.ResourceManager\n-}\n-\n-func NewDBLogsCollector(m *platform.ResourceManager) *DBLogsCollector {\n-\treturn &DBLogsCollector{\n-\t\tresourceManager: m,\n-\t}\n-}\n-\n-func (dc *DBLogsCollector) Collect(conf config.Configuration) (*data.DiagnosticBundle, error) {\n-\tconf, err := conf.ValidateConfig(dc.Configuration())\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\trowLimit, err := config.ReadIntValue(conf, \"row_limit\")\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\n-\tframes := make(map[string]data.Frame)\n-\tvar frameErrors []error\n-\tfor logTable, tableConfig := range DbLogTables {\n-\t\tframe, err := dc.resourceManager.DbClient.ReadTable(\"system\", logTable, tableConfig.excludeColumns, tableConfig.orderBy, rowLimit)\n-\t\tif err != nil {\n-\t\t\tframeErrors = append(frameErrors, errors.Wrapf(err, \"Unable to collect %s\", logTable))\n-\t\t} else {\n-\t\t\tframes[logTable] = frame\n-\t\t}\n-\t}\n-\n-\tfErrors := data.FrameErrors{\n-\t\tErrors: frameErrors,\n-\t}\n-\treturn &data.DiagnosticBundle{\n-\t\tFrames: frames,\n-\t\tErrors: fErrors,\n-\t}, nil\n-}\n-\n-func (dc *DBLogsCollector) Configuration() config.Configuration {\n-\treturn config.Configuration{\n-\t\tParams: []config.ConfigParam{\n-\t\t\tconfig.IntParam{\n-\t\t\t\tValue: 100000,\n-\t\t\t\tParam: config.NewParam(\"row_limit\", \"Maximum number of log rows to collect. Negative values mean unlimited\", false),\n-\t\t\t},\n-\t\t},\n-\t}\n-}\n-\n-func (dc *DBLogsCollector) IsDefault() bool {\n-\treturn true\n-}\n-\n-func (dc DBLogsCollector) Description() string {\n-\treturn \"Collects the ClickHouse logs directly from the database.\"\n-}\n-\n-// here we register the collector for use\n-func init() {\n-\tcollectors.Register(\"db_logs\", func() (collectors.Collector, error) {\n-\t\treturn &DBLogsCollector{\n-\t\t\tresourceManager: platform.GetResourceManager(),\n-\t\t}, nil\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/collectors/clickhouse/logs.go b/programs/diagnostics/internal/collectors/clickhouse/logs.go\ndeleted file mode 100644\nindex 8436a392c478..000000000000\n--- a/programs/diagnostics/internal/collectors/clickhouse/logs.go\n+++ /dev/null\n@@ -1,140 +0,0 @@\n-package clickhouse\n-\n-import (\n-\t\"fmt\"\n-\t\"path/filepath\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils\"\n-)\n-\n-// This collector collects logs\n-\n-type LogsCollector struct {\n-\tresourceManager *platform.ResourceManager\n-}\n-\n-func NewLogsCollector(m *platform.ResourceManager) *LogsCollector {\n-\treturn &LogsCollector{\n-\t\tresourceManager: m,\n-\t}\n-}\n-\n-var DefaultLogsLocation = filepath.Clean(\"/var/log/clickhouse-server/\")\n-\n-func (lc *LogsCollector) Collect(conf config.Configuration) (*data.DiagnosticBundle, error) {\n-\tconf, err := conf.ValidateConfig(lc.Configuration())\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\tdirectory, err := config.ReadStringValue(conf, \"directory\")\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\tcollectArchives, err := config.ReadBoolValue(conf, \"collect_archives\")\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\tlogPatterns := []string{\"*.log\"}\n-\tif collectArchives {\n-\t\tlogPatterns = append(logPatterns, \"*.gz\")\n-\t}\n-\n-\tif directory != \"\" {\n-\t\t// user has specified a directory - we therefore skip all other efforts to locate the logs\n-\t\tframe, errs := data.NewFileDirectoryFrame(directory, logPatterns)\n-\t\treturn &data.DiagnosticBundle{\n-\t\t\tFrames: map[string]data.Frame{\n-\t\t\t\t\"user_specified\": frame,\n-\t\t\t},\n-\t\t\tErrors: data.FrameErrors{Errors: errs},\n-\t\t}, nil\n-\t}\n-\t// add the default\n-\tframes := make(map[string]data.Frame)\n-\tdirFrame, frameErrors := data.NewFileDirectoryFrame(DefaultLogsLocation, logPatterns)\n-\tframes[\"default\"] = dirFrame\n-\tlogFolders, errs := FindLogFileCandidates()\n-\tframeErrors = append(frameErrors, errs...)\n-\ti := 0\n-\tfor folder, paths := range logFolders {\n-\t\t// we will collect the default location anyway above so skip these\n-\t\tif folder != DefaultLogsLocation {\n-\t\t\tif collectArchives {\n-\t\t\t\tpaths = append(paths, \"*.gz\")\n-\t\t\t}\n-\t\t\tdirFrame, errs := data.NewFileDirectoryFrame(folder, paths)\n-\t\t\tframes[fmt.Sprintf(\"logs-%d\", i)] = dirFrame\n-\t\t\tframeErrors = append(frameErrors, errs...)\n-\t\t}\n-\t}\n-\treturn &data.DiagnosticBundle{\n-\t\tFrames: frames,\n-\t\tErrors: data.FrameErrors{Errors: frameErrors},\n-\t}, err\n-}\n-\n-func (lc *LogsCollector) Configuration() config.Configuration {\n-\treturn config.Configuration{\n-\t\tParams: []config.ConfigParam{\n-\t\t\tconfig.StringParam{\n-\t\t\t\tValue:      \"\",\n-\t\t\t\tParam:      config.NewParam(\"directory\", \"Specify the location of the log files for ClickHouse Server e.g. /var/log/clickhouse-server/\", false),\n-\t\t\t\tAllowEmpty: true,\n-\t\t\t},\n-\t\t\tconfig.BoolParam{\n-\t\t\t\tParam: config.NewParam(\"collect_archives\", \"Collect compressed log archive files\", false),\n-\t\t\t},\n-\t\t},\n-\t}\n-}\n-\n-func FindLogFileCandidates() (logFolders map[string][]string, configErrors []error) {\n-\t// we need the config to determine the location of the logs\n-\tconfigCandidates := make(map[string]data.ConfigFileFrame)\n-\tconfigFiles, err := FindConfigurationFiles()\n-\tlogFolders = make(map[string][]string)\n-\tif err != nil {\n-\t\tconfigErrors = append(configErrors, err)\n-\t\treturn logFolders, configErrors\n-\t}\n-\tfor _, folder := range configFiles {\n-\t\tconfigFrame, errs := data.NewConfigFileFrame(folder)\n-\t\tconfigErrors = append(configErrors, errs...)\n-\t\tconfigCandidates[filepath.Clean(folder)] = configFrame\n-\t}\n-\n-\tfor _, config := range configCandidates {\n-\t\tpaths, errs := config.FindLogPaths()\n-\t\tfor _, path := range paths {\n-\t\t\tfolder := filepath.Dir(path)\n-\t\t\tfilename := filepath.Base(path)\n-\t\t\tif _, ok := logFolders[folder]; !ok {\n-\t\t\t\tlogFolders[folder] = []string{}\n-\t\t\t}\n-\t\t\tlogFolders[folder] = utils.Unique(append(logFolders[folder], filename))\n-\t\t}\n-\t\tconfigErrors = append(configErrors, errs...)\n-\t}\n-\treturn logFolders, configErrors\n-}\n-\n-func (lc *LogsCollector) IsDefault() bool {\n-\treturn true\n-}\n-\n-func (lc LogsCollector) Description() string {\n-\treturn \"Collects the ClickHouse logs directly from the database.\"\n-}\n-\n-// here we register the collector for use\n-func init() {\n-\tcollectors.Register(\"logs\", func() (collectors.Collector, error) {\n-\t\treturn &LogsCollector{\n-\t\t\tresourceManager: platform.GetResourceManager(),\n-\t\t}, nil\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/collectors/clickhouse/queries.json b/programs/diagnostics/internal/collectors/clickhouse/queries.json\ndeleted file mode 100644\nindex f5cf4362c9e3..000000000000\n--- a/programs/diagnostics/internal/collectors/clickhouse/queries.json\n+++ /dev/null\n@@ -1,153 +0,0 @@\n-{\n-  \"queries\": {\n-    \"version\": [\n-      {\n-        \"statement\": \"SELECT version()\"\n-      }\n-    ],\n-    \"databases\": [\n-      {\n-        \"statement\": \"SELECT name, engine, tables, partitions, parts, formatReadableSize(bytes_on_disk) \\\"disk_size\\\" FROM system.databases db LEFT JOIN ( SELECT database, uniq(table) \\\"tables\\\", uniq(table, partition) \\\"partitions\\\", count() AS parts, sum(bytes_on_disk) \\\"bytes_on_disk\\\" FROM system.parts WHERE active GROUP BY database ) AS db_stats ON db.name = db_stats.database ORDER BY bytes_on_disk DESC LIMIT {{.Limit}}\"\n-      }\n-    ],\n-    \"access\": [\n-      {\n-        \"statement\": \"SHOW ACCESS\"\n-      }\n-    ],\n-    \"quotas\": [\n-      {\n-        \"statement\": \"SHOW QUOTA\"\n-      }\n-    ],\n-    \"db_engines\": [\n-      {\n-        \"statement\": \"SELECT engine, count() \\\"count\\\" FROM system.databases GROUP BY engine\"\n-      }\n-    ],\n-    \"table_engines\": [\n-      {\n-        \"statement\": \"SELECT engine, count() \\\"count\\\" FROM system.tables WHERE database != 'system' GROUP BY engine\"\n-      }\n-    ],\n-    \"dictionaries\": [\n-      {\n-        \"statement\": \"SELECT source, type, status, count() \\\"count\\\" FROM system.dictionaries GROUP BY source, type, status ORDER BY status DESC, source\"\n-      }\n-    ],\n-    \"replicated_tables_by_delay\": [\n-      {\n-        \"statement\": \"SELECT database, table, is_leader, is_readonly, absolute_delay, queue_size, inserts_in_queue, merges_in_queue FROM system.replicas ORDER BY absolute_delay DESC LIMIT {{.Limit}}\"\n-      }\n-    ],\n-    \"replication_queue_by_oldest\": [\n-      {\n-        \"statement\": \"SELECT database, table, replica_name, position, node_name, type, source_replica, parts_to_merge, new_part_name, create_time, required_quorum, is_detach, is_currently_executing, num_tries, last_attempt_time, last_exception, concat( 'time: ', toString(last_postpone_time), ', number: ', toString(num_postponed), ', reason: ', postpone_reason ) postpone FROM system.replication_queue ORDER BY create_time ASC LIMIT {{.Limit}}\"\n-      }\n-    ],\n-    \"replicated_fetches\": [\n-      {\n-        \"statement\": \"SELECT database, table, round(elapsed, 1) \\\"elapsed\\\", round(100 * progress, 1) \\\"progress\\\", partition_id, result_part_name, result_part_path, total_size_bytes_compressed, bytes_read_compressed, source_replica_path, source_replica_hostname, source_replica_port, interserver_scheme, to_detached, thread_id FROM system.replicated_fetches\"\n-      }\n-    ],\n-    \"tables_by_max_partition_count\": [\n-      {\n-        \"statement\": \"SELECT database, table, count() \\\"partitions\\\", sum(part_count) \\\"parts\\\", max(part_count) \\\"max_parts_per_partition\\\" FROM ( SELECT database, table, partition, count() \\\"part_count\\\" FROM system.parts WHERE active GROUP BY database, table, partition ) partitions GROUP BY database, table ORDER BY max_parts_per_partition DESC LIMIT {{.Limit}}\"\n-      }\n-    ],\n-    \"stack_traces\": [\n-      {\n-        \"statement\": \"SELECT '\\\\n' || arrayStringConcat( arrayMap( x, y -> concat(x, ': ', y), arrayMap(x -> addressToLine(x), trace), arrayMap(x -> demangle(addressToSymbol(x)), trace) ), '\\\\n' ) AS trace FROM system.stack_trace\"\n-      }\n-    ],\n-    \"crash_log\": [\n-      {\n-        \"statement\": \"SELECT event_time, signal, thread_id, query_id, '\\\\n' || arrayStringConcat(trace_full, '\\\\n') AS trace, version FROM system.crash_log ORDER BY event_time DESC\"\n-      }\n-    ],\n-    \"merges\": [\n-      {\n-        \"statement\": \"SELECT database, table, round(elapsed, 1) \\\"elapsed\\\", round(100 * progress, 1) \\\"progress\\\", is_mutation, partition_id, result_part_path, source_part_paths, num_parts, formatReadableSize(total_size_bytes_compressed) \\\"total_size_compressed\\\", formatReadableSize(bytes_read_uncompressed) \\\"read_uncompressed\\\", formatReadableSize(bytes_written_uncompressed) \\\"written_uncompressed\\\", columns_written, formatReadableSize(memory_usage) \\\"memory_usage\\\", thread_id FROM system.merges\",\n-        \"constraint\": \">=20.3\"\n-      },\n-      {\n-        \"statement\": \"SELECT database, table, round(elapsed, 1) \\\"elapsed\\\", round(100 * progress, 1) \\\"progress\\\", is_mutation, partition_id, num_parts, formatReadableSize(total_size_bytes_compressed) \\\"total_size_compressed\\\", formatReadableSize(bytes_read_uncompressed) \\\"read_uncompressed\\\", formatReadableSize(bytes_written_uncompressed) \\\"written_uncompressed\\\", columns_written, formatReadableSize(memory_usage) \\\"memory_usage\\\" FROM system.merges\"\n-      }\n-    ],\n-    \"mutations\": [\n-      {\n-        \"statement\": \"SELECT database, table, mutation_id, command, create_time, parts_to_do_names, parts_to_do, is_done, latest_failed_part, latest_fail_time, latest_fail_reason FROM system.mutations WHERE NOT is_done ORDER BY create_time DESC\",\n-        \"constraint\": \">=20.3\"\n-      },\n-      {\n-        \"statement\": \"SELECT database, table, mutation_id, command, create_time, parts_to_do, is_done, latest_failed_part, latest_fail_time, latest_fail_reason FROM system.mutations WHERE NOT is_done ORDER BY create_time DESC\"\n-      }\n-    ],\n-    \"recent_data_parts\": [\n-      {\n-        \"statement\": \"SELECT database, table, engine, partition_id, name, part_type, active, level, disk_name, path, marks, rows, bytes_on_disk, data_compressed_bytes, data_uncompressed_bytes, marks_bytes, modification_time, remove_time, refcount, is_frozen, min_date, max_date, min_time, max_time, min_block_number, max_block_number FROM system.parts WHERE modification_time > now() - INTERVAL 3 MINUTE ORDER BY modification_time DESC\",\n-        \"constraint\": \">=20.3\"\n-      },\n-      {\n-        \"statement\": \"SELECT database, table, engine, partition_id, name, active, level, path, marks, rows, bytes_on_disk, data_compressed_bytes, data_uncompressed_bytes, marks_bytes, modification_time, remove_time, refcount, is_frozen, min_date, max_date, min_time, max_time, min_block_number, max_block_number FROM system.parts WHERE modification_time > now() - INTERVAL 3 MINUTE ORDER BY modification_time DESC\"\n-      }\n-    ],\n-    \"detached_parts\": [\n-      {\n-        \"statement\": \"SELECT database, table, partition_id, name, disk, reason, min_block_number, max_block_number, level FROM system.detached_parts\"\n-      }\n-    ],\n-    \"processes\": [\n-      {\n-        \"statement\": \"SELECT elapsed, query_id, normalizeQuery(query) AS normalized_query, is_cancelled, concat( toString(read_rows), ' rows / ', formatReadableSize(read_bytes) ) AS read, concat( toString(written_rows), ' rows / ', formatReadableSize(written_bytes) ) AS written, formatReadableSize(memory_usage) AS \\\"memory usage\\\", user, multiIf( empty(client_name), http_user_agent, concat( client_name, ' ', toString(client_version_major), '.', toString(client_version_minor), '.', toString(client_version_patch) ) ) AS client, thread_ids, ProfileEvents, Settings FROM system.processes ORDER BY elapsed DESC\",\n-        \"constraint\": \">=21.8\"\n-      },\n-      {\n-        \"statement\": \"SELECT elapsed, query_id, normalizeQuery(query) AS normalized_query, is_cancelled, concat( toString(read_rows), ' rows / ', formatReadableSize(read_bytes) ) AS read, concat( toString(written_rows), ' rows / ', formatReadableSize(written_bytes) ) AS written, formatReadableSize(memory_usage) AS \\\"memory usage\\\", user, multiIf( empty(client_name), http_user_agent, concat( client_name, ' ', toString(client_version_major), '.', toString(client_version_minor), '.', toString(client_version_patch) ) ) AS client, thread_ids, ProfileEvents.Names, ProfileEvents.Values, Settings.Names, Settings.Values FROM system.processes ORDER BY elapsed DESC\",\n-        \"constraint\": \">=21.3\"\n-      },\n-      {\n-        \"statement\": \"SELECT elapsed, query_id, normalizeQuery(query) AS normalized_query, is_cancelled, concat( toString(read_rows), ' rows / ', formatReadableSize(read_bytes) ) AS read, concat( toString(written_rows), ' rows / ', formatReadableSize(written_bytes) ) AS written, formatReadableSize(memory_usage) AS \\\"memory usage\\\", user, multiIf( empty(client_name), http_user_agent, concat( client_name, ' ', toString(client_version_major), '.', toString(client_version_minor), '.', toString(client_version_patch) ) ) AS client, ProfileEvents.Names, ProfileEvents.Values, Settings.Names, Settings.Values FROM system.processes ORDER BY elapsed DESC\"\n-      }\n-    ],\n-    \"top_queries_by_duration\": [\n-      {\n-        \"statement\": \"SELECT type, query_start_time, query_duration_ms, query_id, query_kind, is_initial_query, normalizeQuery(query) AS normalized_query, concat( toString(read_rows), ' rows / ', formatReadableSize(read_bytes) ) AS read, concat( toString(written_rows), ' rows / ', formatReadableSize(written_bytes) ) AS written, concat( toString(result_rows), ' rows / ', formatReadableSize(result_bytes) ) AS result, formatReadableSize(memory_usage) AS \\\"memory usage\\\", exception, '\\\\n' || stack_trace AS stack_trace, user, initial_user, multiIf( empty(client_name), http_user_agent, concat( client_name, ' ', toString(client_version_major), '.', toString(client_version_minor), '.', toString(client_version_patch) ) ) AS client, client_hostname, databases, tables, columns, used_aggregate_functions, used_aggregate_function_combinators, used_database_engines, used_data_type_families, used_dictionaries, used_formats, used_functions, used_storages, used_table_functions, thread_ids, ProfileEvents, Settings FROM system.query_log WHERE type != 'QueryStart' AND event_date >= today() - 1 AND event_time >= now() - INTERVAL 1 DAY ORDER BY query_duration_ms DESC LIMIT {{.Limit}}\",\n-        \"constraint\": \">=21.8\"\n-      },\n-      {\n-        \"statement\": \"SELECT type, query_start_time, query_duration_ms, query_id, query_kind, is_initial_query, normalizeQuery(query) AS normalized_query, concat( toString(read_rows), ' rows / ', formatReadableSize(read_bytes) ) AS read, concat( toString(written_rows), ' rows / ', formatReadableSize(written_bytes) ) AS written, concat( toString(result_rows), ' rows / ', formatReadableSize(result_bytes) ) AS result, formatReadableSize(memory_usage) AS \\\"memory usage\\\", exception, '\\\\n' || stack_trace AS stack_trace, user, initial_user, multiIf( empty(client_name), http_user_agent, concat( client_name, ' ', toString(client_version_major), '.', toString(client_version_minor), '.', toString(client_version_patch) ) ) AS client, client_hostname, databases, tables, columns, used_aggregate_functions, used_aggregate_function_combinators, used_database_engines, used_data_type_families, used_dictionaries, used_formats, used_functions, used_storages, used_table_functions, thread_ids, ProfileEvents.Names, ProfileEvents.Values, Settings.Names, Settings.Values FROM system.query_log WHERE type != 'QueryStart' AND event_date >= today() - 1 AND event_time >= now() - INTERVAL 1 DAY ORDER BY query_duration_ms DESC LIMIT {{.Limit}}\",\n-        \"constraint\": \">=21.3\"\n-      },\n-      {\n-        \"statement\": \"SELECT type, query_start_time, query_duration_ms, query_id, query_kind, is_initial_query, normalizeQuery(query) AS normalized_query, concat( toString(read_rows), ' rows / ', formatReadableSize(read_bytes) ) AS read, concat( toString(written_rows), ' rows / ', formatReadableSize(written_bytes) ) AS written, concat( toString(result_rows), ' rows / ', formatReadableSize(result_bytes) ) AS result, formatReadableSize(memory_usage) AS \\\"memory usage\\\", exception, '\\\\n' || stack_trace AS stack_trace, user, initial_user, multiIf( empty(client_name), http_user_agent, concat( client_name, ' ', toString(client_version_major), '.', toString(client_version_minor), '.', toString(client_version_patch) ) ) AS client, client_hostname, ProfileEvents.Names, ProfileEvents.Values, Settings.Names, Settings.Values FROM system.query_log WHERE type != 'QueryStart' AND event_date >= today() - 1 AND event_time >= now() - INTERVAL 1 DAY ORDER BY query_duration_ms DESC LIMIT {{.Limit}}\"\n-      }\n-    ],\n-    \"top_queries_by_memory\": [\n-      {\n-        \"statement\": \"SELECT type, query_start_time, query_duration_ms, query_id, query_kind, is_initial_query, normalizeQuery(query) AS normalized_query, concat( toString(read_rows), ' rows / ', formatReadableSize(read_bytes) ) AS read, concat( toString(written_rows), ' rows / ', formatReadableSize(written_bytes) ) AS written, concat( toString(result_rows), ' rows / ', formatReadableSize(result_bytes) ) AS result, formatReadableSize(memory_usage) AS \\\"memory usage\\\", exception, '\\\\n' || stack_trace AS stack_trace, user, initial_user, multiIf( empty(client_name), http_user_agent, concat( client_name, ' ', toString(client_version_major), '.', toString(client_version_minor), '.', toString(client_version_patch) ) ) AS client, client_hostname, databases, tables, columns, used_aggregate_functions, used_aggregate_function_combinators, used_database_engines, used_data_type_families, used_dictionaries, used_formats, used_functions, used_storages, used_table_functions, thread_ids, ProfileEvents, Settings FROM system.query_log WHERE type != 'QueryStart' AND event_date >= today() - 1 AND event_time >= now() - INTERVAL 1 DAY ORDER BY memory_usage DESC LIMIT {{.Limit}}\",\n-        \"constraint\": \">=21.8\"\n-      },\n-      {\n-        \"statement\": \"SELECT type, query_start_time, query_duration_ms, query_id, query_kind, is_initial_query, normalizeQuery(query) AS normalized_query, concat( toString(read_rows), ' rows / ', formatReadableSize(read_bytes) ) AS read, concat( toString(written_rows), ' rows / ', formatReadableSize(written_bytes) ) AS written, concat( toString(result_rows), ' rows / ', formatReadableSize(result_bytes) ) AS result, formatReadableSize(memory_usage) AS \\\"memory usage\\\", exception, '\\\\n' || stack_trace AS stack_trace, user, initial_user, multiIf( empty(client_name), http_user_agent, concat( client_name, ' ', toString(client_version_major), '.', toString(client_version_minor), '.', toString(client_version_patch) ) ) AS client, client_hostname, databases, tables, columns, used_aggregate_functions, used_aggregate_function_combinators, used_database_engines, used_data_type_families, used_dictionaries, used_formats, used_functions, used_storages, used_table_functions, thread_ids, ProfileEvents.Names, ProfileEvents.Values, Settings.Names, Settings.Values FROM system.query_log WHERE type != 'QueryStart' AND event_date >= today() - 1 AND event_time >= now() - INTERVAL 1 DAY ORDER BY memory_usage DESC LIMIT {{.Limit}}\",\n-        \"constraint\": \">=21.3\"\n-      },\n-      {\n-        \"statement\": \"SELECT type, query_start_time, query_duration_ms, query_id, query_kind, is_initial_query, normalizeQuery(query) AS normalized_query, concat( toString(read_rows), ' rows / ', formatReadableSize(read_bytes) ) AS read, concat( toString(written_rows), ' rows / ', formatReadableSize(written_bytes) ) AS written, concat( toString(result_rows), ' rows / ', formatReadableSize(result_bytes) ) AS result, formatReadableSize(memory_usage) AS \\\"memory usage\\\", exception, '\\\\n' || stack_trace AS stack_trace, user, initial_user, multiIf( empty(client_name), http_user_agent, concat( client_name, ' ', toString(client_version_major), '.', toString(client_version_minor), '.', toString(client_version_patch) ) ) AS client, client_hostname, ProfileEvents.Names, ProfileEvents.Values, Settings.Names, Settings.Values FROM system.query_log WHERE type != 'QueryStart' AND event_date >= today() - 1 AND event_time >= now() - INTERVAL 1 DAY ORDER BY memory_usage DESC LIMIT {{.Limit}}\"\n-      }\n-    ],\n-    \"failed_queries\": [\n-      {\n-        \"statement\": \"SELECT type, query_start_time, query_duration_ms, query_id, query_kind, is_initial_query, normalizeQuery(query) AS normalized_query, concat( toString(read_rows), ' rows / ', formatReadableSize(read_bytes) ) AS read, concat( toString(written_rows), ' rows / ', formatReadableSize(written_bytes) ) AS written, concat( toString(result_rows), ' rows / ', formatReadableSize(result_bytes) ) AS result, formatReadableSize(memory_usage) AS \\\"memory usage\\\", exception, '\\\\n' || stack_trace AS stack_trace, user, initial_user, multiIf( empty(client_name), http_user_agent, concat( client_name, ' ', toString(client_version_major), '.', toString(client_version_minor), '.', toString(client_version_patch) ) ) AS client, client_hostname, databases, tables, columns, used_aggregate_functions, used_aggregate_function_combinators, used_database_engines, used_data_type_families, used_dictionaries, used_formats, used_functions, used_storages, used_table_functions, thread_ids, ProfileEvents, Settings FROM system.query_log WHERE type != 'QueryStart' AND event_date >= today() - 1 AND event_time >= now() - INTERVAL 1 DAY AND exception != '' ORDER BY query_start_time DESC LIMIT {{.Limit}}\",\n-        \"constraint\": \">=21.8\"\n-      },\n-      {\n-        \"statement\": \"SELECT type, query_start_time, query_duration_ms, query_id, query_kind, is_initial_query, normalizeQuery(query) AS normalized_query, concat( toString(read_rows), ' rows / ', formatReadableSize(read_bytes) ) AS read, concat( toString(written_rows), ' rows / ', formatReadableSize(written_bytes) ) AS written, concat( toString(result_rows), ' rows / ', formatReadableSize(result_bytes) ) AS result, formatReadableSize(memory_usage) AS \\\"memory usage\\\", exception, '\\\\n' || stack_trace AS stack_trace, user, initial_user, multiIf( empty(client_name), http_user_agent, concat( client_name, ' ', toString(client_version_major), '.', toString(client_version_minor), '.', toString(client_version_patch) ) ) AS client, client_hostname, databases, tables, columns, used_aggregate_functions, used_aggregate_function_combinators, used_database_engines, used_data_type_families, used_dictionaries, used_formats, used_functions, used_storages, used_table_functions, thread_ids, ProfileEvents.Names, ProfileEvents.Values, Settings.Names, Settings.Values FROM system.query_log WHERE type != 'QueryStart' AND event_date >= today() - 1 AND event_time >= now() - INTERVAL 1 DAY AND exception != '' ORDER BY query_start_time DESC LIMIT {{.Limit}}\",\n-        \"constraint\": \">=21.3\"\n-      },\n-      {\n-        \"statement\": \"SELECT type, query_start_time, query_duration_ms, query_id, query_kind, is_initial_query, normalizeQuery(query) AS normalized_query, concat( toString(read_rows), ' rows / ', formatReadableSize(read_bytes) ) AS read, concat( toString(written_rows), ' rows / ', formatReadableSize(written_bytes) ) AS written, concat( toString(result_rows), ' rows / ', formatReadableSize(result_bytes) ) AS result, formatReadableSize(memory_usage) AS \\\"memory usage\\\", exception, '\\\\n' || stack_trace AS stack_trace, user, initial_user, multiIf( empty(client_name), http_user_agent, concat( client_name, ' ', toString(client_version_major), '.', toString(client_version_minor), '.', toString(client_version_patch) ) ) AS client, client_hostname, ProfileEvents.Names, ProfileEvents.Values, Settings.Names, Settings.Values FROM system.query_log WHERE type != 'QueryStart' AND event_date >= today() - 1 AND event_time >= now() - INTERVAL 1 DAY AND exception != '' ORDER BY query_start_time DESC LIMIT {{.Limit}}\"\n-      }\n-    ]\n-  }\n-}\ndiff --git a/programs/diagnostics/internal/collectors/clickhouse/summary.go b/programs/diagnostics/internal/collectors/clickhouse/summary.go\ndeleted file mode 100644\nindex 0b6dd3aff20b..000000000000\n--- a/programs/diagnostics/internal/collectors/clickhouse/summary.go\n+++ /dev/null\n@@ -1,159 +0,0 @@\n-package clickhouse\n-\n-import (\n-\t\"bytes\"\n-\t_ \"embed\"\n-\t\"encoding/json\"\n-\t\"strings\"\n-\t\"text/template\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/Masterminds/semver\"\n-\t\"github.com/pkg/errors\"\n-)\n-\n-// This collector collects the system db from database\n-\n-type SummaryCollector struct {\n-\tresourceManager *platform.ResourceManager\n-}\n-\n-type querySet struct {\n-\tQueries map[string][]query `json:\"queries\"`\n-}\n-\n-type query struct {\n-\tStatement  string `json:\"statement\"`\n-\tConstraint string `json:\"constraint\"`\n-}\n-\n-type ParameterTemplate struct {\n-\tLimit int64\n-}\n-\n-//go:embed queries.json\n-var queryFile []byte\n-\n-func NewSummaryCollector(m *platform.ResourceManager) *SummaryCollector {\n-\treturn &SummaryCollector{\n-\t\tresourceManager: m,\n-\t}\n-}\n-\n-func (sc *SummaryCollector) Collect(conf config.Configuration) (*data.DiagnosticBundle, error) {\n-\tconf, err := conf.ValidateConfig(sc.Configuration())\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\tvar queries querySet\n-\terr = json.Unmarshal(queryFile, &queries)\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, errors.Wrap(err, \"Unable to read queries from disk\")\n-\t}\n-\tlimit, err := config.ReadIntValue(conf, \"row_limit\")\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\n-\tparamTemplate := ParameterTemplate{\n-\t\tLimit: limit,\n-\t}\n-\tframes := make(map[string]data.Frame)\n-\n-\tserverVersion, err := getServerSemVersion(sc)\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, errors.Wrapf(err, \"Unable to read server version\")\n-\t}\n-\n-\tvar frameErrors []error\n-\tfor queryId, sqlQueries := range queries.Queries {\n-\t\t//  we find the first matching query that satisfies the current version. Empty version means ANY version is\n-\t\t// supported\n-\t\tfor _, sqlQuery := range sqlQueries {\n-\t\t\tvar queryConstraint *semver.Constraints\n-\t\t\tif sqlQuery.Constraint != \"\" {\n-\t\t\t\tqueryConstraint, err = semver.NewConstraint(sqlQuery.Constraint)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\t//we try another one\n-\t\t\t\t\tframeErrors = append(frameErrors, errors.Wrapf(err, \"Unable to parse version %s for query %s\", sqlQuery.Constraint, queryId))\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tif sqlQuery.Constraint == \"\" || queryConstraint.Check(serverVersion) {\n-\t\t\t\ttmpl, err := template.New(queryId).Parse(sqlQuery.Statement)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\tframeErrors = append(frameErrors, errors.Wrapf(err, \"Unable to parse query %s\", queryId))\n-\t\t\t\t\t//we try another one\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t\tbuf := new(bytes.Buffer)\n-\t\t\t\terr = tmpl.Execute(buf, paramTemplate)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\tframeErrors = append(frameErrors, errors.Wrapf(err, \"Unable to process query %s template\", queryId))\n-\t\t\t\t\t//we try another one\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t\tframe, err := sc.resourceManager.DbClient.ExecuteStatement(queryId, buf.String())\n-\t\t\t\tif err != nil {\n-\t\t\t\t\tframeErrors = append(frameErrors, errors.Wrapf(err, \"Unable to execute query %s\", queryId))\n-\t\t\t\t\t//we try another one\n-\t\t\t\t} else {\n-\t\t\t\t\tframes[queryId] = frame\n-\t\t\t\t\t// only 1 query executed\n-\t\t\t\t\tbreak\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n-\t}\n-\n-\tfErrors := data.FrameErrors{\n-\t\tErrors: frameErrors,\n-\t}\n-\treturn &data.DiagnosticBundle{\n-\t\tFrames: frames,\n-\t\tErrors: fErrors,\n-\t}, nil\n-}\n-\n-func getServerSemVersion(sc *SummaryCollector) (*semver.Version, error) {\n-\tserverVersion, err := sc.resourceManager.DbClient.Version()\n-\tif err != nil {\n-\t\treturn &semver.Version{}, err\n-\t}\n-\t//drop the build number - it is not a semantic version\n-\tversionComponents := strings.Split(serverVersion, \".\")\n-\tserverVersion = strings.Join(versionComponents[:len(versionComponents)-1], \".\")\n-\treturn semver.NewVersion(serverVersion)\n-}\n-\n-func (sc *SummaryCollector) Configuration() config.Configuration {\n-\treturn config.Configuration{\n-\t\tParams: []config.ConfigParam{\n-\t\t\tconfig.IntParam{\n-\t\t\t\tValue: 20,\n-\t\t\t\tParam: config.NewParam(\"row_limit\", \"Limit rows on supported queries.\", false),\n-\t\t\t},\n-\t\t},\n-\t}\n-}\n-\n-func (sc *SummaryCollector) IsDefault() bool {\n-\treturn true\n-}\n-\n-func (sc *SummaryCollector) Description() string {\n-\treturn \"Collects summary statistics on the database based on a set of known useful queries.\"\n-}\n-\n-// here we register the collector for use\n-func init() {\n-\tcollectors.Register(\"summary\", func() (collectors.Collector, error) {\n-\t\treturn &SummaryCollector{\n-\t\t\tresourceManager: platform.GetResourceManager(),\n-\t\t}, nil\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/collectors/clickhouse/system.go b/programs/diagnostics/internal/collectors/clickhouse/system.go\ndeleted file mode 100644\nindex d47cfd924f3d..000000000000\n--- a/programs/diagnostics/internal/collectors/clickhouse/system.go\n+++ /dev/null\n@@ -1,165 +0,0 @@\n-package clickhouse\n-\n-import (\n-\t\"fmt\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils\"\n-\t\"github.com/pkg/errors\"\n-)\n-\n-// This collector collects the system db from database\n-\n-type SystemDatabaseCollector struct {\n-\tresourceManager *platform.ResourceManager\n-}\n-\n-const SystemDatabase = \"system\"\n-\n-// ExcludeColumns columns if we need - this will be refined over time [table_name][columnA, columnB]\n-var ExcludeColumns = map[string][]string{}\n-\n-// BannedTables - Hardcoded list. These are always excluded even if the user doesn't specify in exclude_tables.\n-//Attempts to export will work but we will warn\n-var BannedTables = []string{\"numbers\", \"zeros\"}\n-\n-// OrderBy contains a map of tables to an order by clause - by default we don't order table dumps\n-var OrderBy = map[string]data.OrderBy{\n-\t\"errors\": {\n-\t\tColumn: \"last_error_message\",\n-\t\tOrder:  data.Desc,\n-\t},\n-\t\"replication_queue\": {\n-\t\tColumn: \"create_time\",\n-\t\tOrder:  data.Asc,\n-\t},\n-}\n-\n-func NewSystemDatabaseCollector(m *platform.ResourceManager) *SystemDatabaseCollector {\n-\treturn &SystemDatabaseCollector{\n-\t\tresourceManager: m,\n-\t}\n-}\n-\n-func (sc *SystemDatabaseCollector) Collect(conf config.Configuration) (*data.DiagnosticBundle, error) {\n-\tconf, err := conf.ValidateConfig(sc.Configuration())\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\tincludeTables, err := config.ReadStringListValues(conf, \"include_tables\")\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\texcludeTables, err := config.ReadStringListValues(conf, \"exclude_tables\")\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\trowLimit, err := config.ReadIntValue(conf, \"row_limit\")\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\texcludeTables = checkBannedTables(includeTables, excludeTables)\n-\tds, err := sc.readSystemAllTables(includeTables, excludeTables, rowLimit)\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\treturn ds, nil\n-}\n-\n-// all banned tables are added to excluded if not present and not specified in included. Returns new exclude_tables list.\n-func checkBannedTables(includeTables []string, excludeTables []string) []string {\n-\tfor _, bannedTable := range BannedTables {\n-\t\t//if its specified we don't add to our exclude list - explicitly included tables take precedence\n-\t\tif !utils.Contains(includeTables, bannedTable) && !utils.Contains(excludeTables, bannedTable) {\n-\t\t\texcludeTables = append(excludeTables, bannedTable)\n-\t\t}\n-\t}\n-\treturn excludeTables\n-}\n-\n-func (sc *SystemDatabaseCollector) readSystemAllTables(include []string, exclude []string, limit int64) (*data.DiagnosticBundle, error) {\n-\ttableNames, err := sc.resourceManager.DbClient.ReadTableNamesForDatabase(SystemDatabase)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tvar frameErrors []error\n-\tif include != nil {\n-\t\t// nil means include everything\n-\t\ttableNames = utils.Intersection(tableNames, include)\n-\t\tif len(tableNames) != len(include) {\n-\t\t\t// we warn that some included tables aren't present in db\n-\t\t\tframeErrors = append(frameErrors, fmt.Errorf(\"some tables specified in the include_tables are not in the system database and will not be exported: %v\",\n-\t\t\t\tutils.Distinct(include, tableNames)))\n-\t\t}\n-\t}\n-\n-\t// exclude tables unless specified in includes\n-\texcludedTables := utils.Distinct(exclude, include)\n-\ttableNames = utils.Distinct(tableNames, excludedTables)\n-\tframes := make(map[string]data.Frame)\n-\n-\tfor _, tableName := range tableNames {\n-\t\tvar excludeColumns []string\n-\t\tif _, ok := ExcludeColumns[tableName]; ok {\n-\t\t\texcludeColumns = ExcludeColumns[tableName]\n-\t\t}\n-\t\torderBy := data.OrderBy{}\n-\t\tif _, ok := OrderBy[tableName]; ok {\n-\t\t\torderBy = OrderBy[tableName]\n-\t\t}\n-\t\tframe, err := sc.resourceManager.DbClient.ReadTable(SystemDatabase, tableName, excludeColumns, orderBy, limit)\n-\t\tif err != nil {\n-\t\t\tframeErrors = append(frameErrors, errors.Wrapf(err, \"Unable to collect %s\", tableName))\n-\t\t} else {\n-\t\t\tframes[tableName] = frame\n-\t\t}\n-\t}\n-\n-\tfErrors := data.FrameErrors{\n-\t\tErrors: frameErrors,\n-\t}\n-\treturn &data.DiagnosticBundle{\n-\t\tFrames: frames,\n-\t\tErrors: fErrors,\n-\t}, nil\n-}\n-\n-func (sc *SystemDatabaseCollector) Configuration() config.Configuration {\n-\treturn config.Configuration{\n-\t\tParams: []config.ConfigParam{\n-\t\t\tconfig.StringListParam{\n-\t\t\t\t// nil means include everything\n-\t\t\t\tValues: nil,\n-\t\t\t\tParam:  config.NewParam(\"include_tables\", \"Specify list of tables to collect. Takes precedence over exclude_tables. If not specified (default) all tables except exclude_tables.\", false),\n-\t\t\t},\n-\t\t\tconfig.StringListParam{\n-\t\t\t\tValues: []string{\"licenses\", \"distributed_ddl_queue\", \"query_thread_log\", \"query_log\", \"asynchronous_metric_log\", \"zookeeper\", \"aggregate_function_combinators\", \"collations\", \"contributors\", \"data_type_families\", \"formats\", \"graphite_retentions\", \"numbers\", \"numbers_mt\", \"one\", \"parts_columns\", \"projection_parts\", \"projection_parts_columns\", \"table_engines\", \"time_zones\", \"zeros\", \"zeros_mt\"},\n-\t\t\t\tParam:  config.NewParam(\"exclude_tables\", \"Specify list of tables to not collect.\", false),\n-\t\t\t},\n-\t\t\tconfig.IntParam{\n-\t\t\t\tValue: 100000,\n-\t\t\t\tParam: config.NewParam(\"row_limit\", \"Maximum number of rows to collect from any table. Negative values mean unlimited.\", false),\n-\t\t\t},\n-\t\t},\n-\t}\n-}\n-\n-func (sc *SystemDatabaseCollector) IsDefault() bool {\n-\treturn true\n-}\n-\n-func (sc *SystemDatabaseCollector) Description() string {\n-\treturn \"Collects all tables in the system database, except those which have been excluded.\"\n-}\n-\n-// here we register the collector for use\n-func init() {\n-\tcollectors.Register(\"system_db\", func() (collectors.Collector, error) {\n-\t\treturn &SystemDatabaseCollector{\n-\t\t\tresourceManager: platform.GetResourceManager(),\n-\t\t}, nil\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/collectors/clickhouse/zookeeper.go b/programs/diagnostics/internal/collectors/clickhouse/zookeeper.go\ndeleted file mode 100644\nindex 78aefeaa0c15..000000000000\n--- a/programs/diagnostics/internal/collectors/clickhouse/zookeeper.go\n+++ /dev/null\n@@ -1,153 +0,0 @@\n-package clickhouse\n-\n-import (\n-\t\"fmt\"\n-\t\"strings\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/bmatcuk/doublestar/v4\"\n-\t\"github.com/pkg/errors\"\n-\t\"github.com/rs/zerolog/log\"\n-)\n-\n-// This collector collects the system zookeeper db\n-\n-type ZookeeperCollector struct {\n-\tresourceManager *platform.ResourceManager\n-}\n-\n-func NewZookeeperCollector(m *platform.ResourceManager) *ZookeeperCollector {\n-\treturn &ZookeeperCollector{\n-\t\tresourceManager: m,\n-\t}\n-}\n-\n-func (zkc *ZookeeperCollector) Collect(conf config.Configuration) (*data.DiagnosticBundle, error) {\n-\tconf, err := conf.ValidateConfig(zkc.Configuration())\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\n-\tpathPattern, err := config.ReadStringValue(conf, \"path_pattern\")\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\tdefaultPattern, _ := zkc.Configuration().GetConfigParam(\"path_pattern\")\n-\tif defaultPattern.(config.StringParam).Value != pathPattern {\n-\t\tlog.Warn().Msgf(\"Using non default zookeeper glob pattern [%s] - this can potentially cause high query load\", pathPattern)\n-\t}\n-\tmaxDepth, err := config.ReadIntValue(conf, \"max_depth\")\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\trowLimit, err := config.ReadIntValue(conf, \"row_limit\")\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\t// we use doublestar for globs as it provides us with ** but also allows us to identify prefix or base paths\n-\tif !doublestar.ValidatePattern(pathPattern) {\n-\t\treturn &data.DiagnosticBundle{}, errors.Wrapf(err, \"%s is not a valid pattern\", pathPattern)\n-\t}\n-\tbase, _ := doublestar.SplitPattern(pathPattern)\n-\tframes := make(map[string]data.Frame)\n-\thFrame, frameErrors := zkc.collectSubFrames(base, pathPattern, rowLimit, 0, maxDepth)\n-\tfErrors := data.FrameErrors{\n-\t\tErrors: frameErrors,\n-\t}\n-\tframes[\"zookeeper_db\"] = hFrame\n-\treturn &data.DiagnosticBundle{\n-\t\tFrames: frames,\n-\t\tErrors: fErrors,\n-\t}, nil\n-}\n-\n-// recursively iterates over the zookeeper sub tables to a max depth, applying the filter and max rows per table\n-func (zkc *ZookeeperCollector) collectSubFrames(path, pathPattern string, rowLimit, currentDepth, maxDepth int64) (data.HierarchicalFrame, []error) {\n-\tvar frameErrors []error\n-\tvar subFrames []data.HierarchicalFrame\n-\n-\tcurrentDepth += 1\n-\tif currentDepth == maxDepth {\n-\t\treturn data.HierarchicalFrame{}, frameErrors\n-\t}\n-\tmatch, err := doublestar.PathMatch(pathPattern, path)\n-\tif err != nil {\n-\t\tframeErrors = append(frameErrors, errors.Wrapf(err, \"Path match failed for pattern %s with path %s\", pathPattern, path))\n-\t\treturn data.HierarchicalFrame{}, frameErrors\n-\t}\n-\t// we allow a single level to be examined or we never get going\n-\tif !match && currentDepth > 1 {\n-\t\treturn data.HierarchicalFrame{}, frameErrors\n-\t}\n-\tframe, err := zkc.resourceManager.DbClient.ExecuteStatement(path, fmt.Sprintf(\"SELECT name FROM system.zookeeper WHERE path='%s' LIMIT %d\", path, rowLimit))\n-\tif err != nil {\n-\t\tframeErrors = append(frameErrors, errors.Wrapf(err, \"Unable to read zookeeper table path for sub paths %s\", path))\n-\t\treturn data.HierarchicalFrame{}, frameErrors\n-\t}\n-\n-\t// this isn't ideal, we add re-execute the query to our collection as this will be consumed by the output lazily\n-\toutputFrame, err := zkc.resourceManager.DbClient.ExecuteStatement(path, fmt.Sprintf(\"SELECT * FROM system.zookeeper WHERE path='%s' LIMIT %d\", path, rowLimit))\n-\tif err != nil {\n-\t\tframeErrors = append(frameErrors, errors.Wrapf(err, \"Unable to read zookeeper table path %s\", path))\n-\t\treturn data.HierarchicalFrame{}, frameErrors\n-\t}\n-\tframeComponents := strings.Split(path, \"/\")\n-\tframeId := frameComponents[len(frameComponents)-1]\n-\n-\tfor {\n-\t\tvalues, ok, err := frame.Next()\n-\t\tif err != nil {\n-\t\t\tframeErrors = append(frameErrors, errors.Wrapf(err, \"unable to read frame %s\", frame.Name()))\n-\t\t\treturn data.NewHierarchicalFrame(frameId, outputFrame, subFrames), frameErrors\n-\t\t}\n-\t\tif !ok {\n-\t\t\treturn data.NewHierarchicalFrame(frameId, outputFrame, subFrames), frameErrors\n-\t\t}\n-\t\tsubName := fmt.Sprintf(\"%v\", values[0])\n-\t\tsubPath := fmt.Sprintf(\"%s/%s\", path, subName)\n-\t\tsubFrame, errs := zkc.collectSubFrames(subPath, pathPattern, rowLimit, currentDepth, maxDepth)\n-\t\tif subFrame.Name() != \"\" {\n-\t\t\tsubFrames = append(subFrames, subFrame)\n-\t\t}\n-\t\tframeErrors = append(frameErrors, errs...)\n-\t}\n-}\n-\n-func (zkc *ZookeeperCollector) Configuration() config.Configuration {\n-\treturn config.Configuration{\n-\t\tParams: []config.ConfigParam{\n-\t\t\tconfig.StringParam{\n-\t\t\t\tValue: \"/clickhouse/{task_queue}/**\",\n-\t\t\t\tParam: config.NewParam(\"path_pattern\", \"Glob pattern for zookeeper path matching. Change with caution.\", false),\n-\t\t\t},\n-\t\t\tconfig.IntParam{\n-\t\t\t\tValue: 8,\n-\t\t\t\tParam: config.NewParam(\"max_depth\", \"Max depth for zookeeper navigation.\", false),\n-\t\t\t},\n-\t\t\tconfig.IntParam{\n-\t\t\t\tValue: 10,\n-\t\t\t\tParam: config.NewParam(\"row_limit\", \"Maximum number of rows/sub nodes to collect/expand from any zookeeper leaf. Negative values mean unlimited.\", false),\n-\t\t\t},\n-\t\t},\n-\t}\n-}\n-\n-func (zkc *ZookeeperCollector) IsDefault() bool {\n-\treturn false\n-}\n-\n-func (zkc *ZookeeperCollector) Description() string {\n-\treturn \"Collects Zookeeper information available within ClickHouse.\"\n-}\n-\n-// here we register the collector for use\n-func init() {\n-\tcollectors.Register(\"zookeeper_db\", func() (collectors.Collector, error) {\n-\t\treturn &ZookeeperCollector{\n-\t\t\tresourceManager: platform.GetResourceManager(),\n-\t\t}, nil\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/collectors/registry.go b/programs/diagnostics/internal/collectors/registry.go\ndeleted file mode 100644\nindex 5611f947466b..000000000000\n--- a/programs/diagnostics/internal/collectors/registry.go\n+++ /dev/null\n@@ -1,75 +0,0 @@\n-package collectors\n-\n-import (\n-\t\"fmt\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/pkg/errors\"\n-\t\"github.com/rs/zerolog/log\"\n-)\n-\n-type Collector interface {\n-\tCollect(config config.Configuration) (*data.DiagnosticBundle, error)\n-\tConfiguration() config.Configuration\n-\tIsDefault() bool\n-\tDescription() string\n-}\n-\n-// Register can be called from init() on a collector in this package\n-// It will automatically be added to the Collectors map to be called externally\n-func Register(name string, collector CollectorFactory) {\n-\tif name == \"diag_trace\" {\n-\t\t// we use this to record errors and warnings\n-\t\tlog.Fatal().Msgf(\"diag_trace is a reserved collector name\")\n-\t}\n-\t// names must be unique\n-\tif _, ok := Collectors[name]; ok {\n-\t\tlog.Fatal().Msgf(\"More than 1 collector is trying to register under the name %s. Names must be unique.\", name)\n-\t}\n-\tCollectors[name] = collector\n-}\n-\n-// CollectorFactory lets us use a closure to get instances of the collector struct\n-type CollectorFactory func() (Collector, error)\n-\n-var Collectors = map[string]CollectorFactory{}\n-\n-func GetCollectorNames(defaultOnly bool) []string {\n-\t// can't pre-allocate as not all maybe default\n-\tvar collectors []string\n-\tfor collectorName := range Collectors {\n-\t\tcollector, err := GetCollectorByName(collectorName)\n-\t\tif err != nil {\n-\t\t\tlog.Fatal().Err(err)\n-\t\t}\n-\t\tif !defaultOnly || (defaultOnly && collector.IsDefault()) {\n-\t\t\tcollectors = append(collectors, collectorName)\n-\t\t}\n-\t}\n-\treturn collectors\n-}\n-\n-func GetCollectorByName(name string) (Collector, error) {\n-\tif collectorFactory, ok := Collectors[name]; ok {\n-\t\t//do something here\n-\t\tcollector, err := collectorFactory()\n-\t\tif err != nil {\n-\t\t\treturn nil, errors.Wrapf(err, \"collector %s could not be initialized\", name)\n-\t\t}\n-\t\treturn collector, nil\n-\t}\n-\treturn nil, fmt.Errorf(\"%s is not a valid collector name\", name)\n-}\n-\n-func BuildConfigurationOptions() (map[string]config.Configuration, error) {\n-\tconfigurations := make(map[string]config.Configuration)\n-\tfor name, collectorFactory := range Collectors {\n-\t\tcollector, err := collectorFactory()\n-\t\tif err != nil {\n-\t\t\treturn nil, errors.Wrapf(err, \"collector %s could not be initialized\", name)\n-\t\t}\n-\t\tconfigurations[name] = collector.Configuration()\n-\t}\n-\treturn configurations, nil\n-}\ndiff --git a/programs/diagnostics/internal/collectors/system/command.go b/programs/diagnostics/internal/collectors/system/command.go\ndeleted file mode 100644\nindex ba4dd1e996c3..000000000000\n--- a/programs/diagnostics/internal/collectors/system/command.go\n+++ /dev/null\n@@ -1,90 +0,0 @@\n-package system\n-\n-import (\n-\t\"bytes\"\n-\t\"os/exec\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/google/shlex\"\n-\t\"github.com/pkg/errors\"\n-)\n-\n-// This collector runs a user specified command and collects it to a file\n-\n-type CommandCollector struct {\n-\tresourceManager *platform.ResourceManager\n-}\n-\n-func NewCommandCollector(m *platform.ResourceManager) *CommandCollector {\n-\treturn &CommandCollector{\n-\t\tresourceManager: m,\n-\t}\n-}\n-\n-func (c *CommandCollector) Collect(conf config.Configuration) (*data.DiagnosticBundle, error) {\n-\tconf, err := conf.ValidateConfig(c.Configuration())\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\tcommand, err := config.ReadStringValue(conf, \"command\")\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\tvar frameErrors []error\n-\t// shlex to split the commands and args\n-\tcmdArgs, err := shlex.Split(command)\n-\tif err != nil || len(cmdArgs) == 0 {\n-\t\treturn &data.DiagnosticBundle{}, errors.Wrap(err, \"Unable to parse command\")\n-\t}\n-\tcmd := exec.Command(cmdArgs[0], cmdArgs[1:]...)\n-\tvar stdout, stderr bytes.Buffer\n-\tcmd.Stdout = &stdout\n-\tcmd.Stderr = &stderr\n-\terr = cmd.Run()\n-\tvar sError string\n-\tif err != nil {\n-\t\tframeErrors = append(frameErrors, errors.Wrap(err, \"Unable to execute command\"))\n-\t\tsError = err.Error()\n-\t}\n-\tmemoryFrame := data.NewMemoryFrame(\"output\", []string{\"command\", \"stdout\", \"stderr\", \"error\"}, [][]interface{}{\n-\t\t{command, stdout.String(), stderr.String(), sError},\n-\t})\n-\treturn &data.DiagnosticBundle{\n-\t\tErrors: data.FrameErrors{Errors: frameErrors},\n-\t\tFrames: map[string]data.Frame{\n-\t\t\t\"output\": memoryFrame,\n-\t\t},\n-\t}, nil\n-}\n-\n-func (c *CommandCollector) Configuration() config.Configuration {\n-\treturn config.Configuration{\n-\t\tParams: []config.ConfigParam{\n-\t\t\tconfig.StringParam{\n-\t\t\t\tValue:      \"\",\n-\t\t\t\tParam:      config.NewParam(\"command\", \"Command to execute\", true),\n-\t\t\t\tAllowEmpty: false,\n-\t\t\t},\n-\t\t},\n-\t}\n-}\n-\n-func (c *CommandCollector) IsDefault() bool {\n-\treturn false\n-}\n-\n-func (c *CommandCollector) Description() string {\n-\treturn \"Allows collection of the output from a user specified command\"\n-}\n-\n-// here we register the collector for use\n-func init() {\n-\tcollectors.Register(\"command\", func() (collectors.Collector, error) {\n-\t\treturn &CommandCollector{\n-\t\t\tresourceManager: platform.GetResourceManager(),\n-\t\t}, nil\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/collectors/system/file.go b/programs/diagnostics/internal/collectors/system/file.go\ndeleted file mode 100644\nindex cda91636c521..000000000000\n--- a/programs/diagnostics/internal/collectors/system/file.go\n+++ /dev/null\n@@ -1,100 +0,0 @@\n-package system\n-\n-import (\n-\t\"os\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/pkg/errors\"\n-\t\"github.com/rs/zerolog/log\"\n-\t\"github.com/yargevad/filepathx\"\n-)\n-\n-// This collector collects arbitrary user files\n-\n-type FileCollector struct {\n-\tresourceManager *platform.ResourceManager\n-}\n-\n-func NewFileCollector(m *platform.ResourceManager) *FileCollector {\n-\treturn &FileCollector{\n-\t\tresourceManager: m,\n-\t}\n-}\n-\n-func (f *FileCollector) Collect(conf config.Configuration) (*data.DiagnosticBundle, error) {\n-\tconf, err := conf.ValidateConfig(f.Configuration())\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\tfilePattern, err := config.ReadStringValue(conf, \"file_pattern\")\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\n-\tvar frameErrors []error\n-\t// this util package supports recursive file matching e.g. /**/*\n-\tmatches, err := filepathx.Glob(filePattern)\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, errors.Wrapf(err, \"Invalid file_pattern \\\"%s\\\"\", filePattern)\n-\t}\n-\n-\tif len(matches) == 0 {\n-\t\tframeErrors = append(frameErrors, errors.New(\"0 files match glob pattern\"))\n-\t\treturn &data.DiagnosticBundle{\n-\t\t\tErrors: data.FrameErrors{Errors: frameErrors},\n-\t\t}, nil\n-\t}\n-\n-\tvar filePaths []string\n-\tfor _, match := range matches {\n-\t\tfi, err := os.Stat(match)\n-\t\tif err != nil {\n-\t\t\tframeErrors = append(frameErrors, errors.Wrapf(err, \"Unable to read file %s\", match))\n-\t\t}\n-\t\tif !fi.IsDir() {\n-\t\t\tlog.Debug().Msgf(\"Collecting file %s\", match)\n-\t\t\tfilePaths = append(filePaths, match)\n-\t\t}\n-\t}\n-\n-\tframe := data.NewFileFrame(\"collection\", filePaths)\n-\n-\treturn &data.DiagnosticBundle{\n-\t\tErrors: data.FrameErrors{Errors: frameErrors},\n-\t\tFrames: map[string]data.Frame{\n-\t\t\t\"collection\": frame,\n-\t\t},\n-\t}, nil\n-}\n-\n-func (f *FileCollector) Configuration() config.Configuration {\n-\treturn config.Configuration{\n-\t\tParams: []config.ConfigParam{\n-\t\t\tconfig.StringParam{\n-\t\t\t\tValue:      \"\",\n-\t\t\t\tParam:      config.NewParam(\"file_pattern\", \"Glob based pattern to specify files for collection\", true),\n-\t\t\t\tAllowEmpty: false,\n-\t\t\t},\n-\t\t},\n-\t}\n-}\n-\n-func (f *FileCollector) IsDefault() bool {\n-\treturn false\n-}\n-\n-func (f *FileCollector) Description() string {\n-\treturn \"Allows collection of user specified files\"\n-}\n-\n-// here we register the collector for use\n-func init() {\n-\tcollectors.Register(\"file\", func() (collectors.Collector, error) {\n-\t\treturn &FileCollector{\n-\t\t\tresourceManager: platform.GetResourceManager(),\n-\t\t}, nil\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/collectors/system/system.go b/programs/diagnostics/internal/collectors/system/system.go\ndeleted file mode 100644\nindex 69d16f36b8b7..000000000000\n--- a/programs/diagnostics/internal/collectors/system/system.go\n+++ /dev/null\n@@ -1,235 +0,0 @@\n-package system\n-\n-import (\n-\t\"strings\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/elastic/gosigar\"\n-\t\"github.com/jaypipes/ghw\"\n-\t\"github.com/matishsiao/goInfo\"\n-\t\"github.com/pkg/errors\"\n-)\n-\n-// This collector collects the system overview\n-\n-type SystemCollector struct {\n-\tresourceManager *platform.ResourceManager\n-}\n-\n-func NewSystemCollector(m *platform.ResourceManager) *SystemCollector {\n-\treturn &SystemCollector{\n-\t\tresourceManager: m,\n-\t}\n-}\n-\n-func (sc *SystemCollector) Collect(conf config.Configuration) (*data.DiagnosticBundle, error) {\n-\t_, err := conf.ValidateConfig(sc.Configuration())\n-\tif err != nil {\n-\t\treturn &data.DiagnosticBundle{}, err\n-\t}\n-\tframes := make(map[string]data.Frame)\n-\tvar frameErrors []error\n-\n-\tframeErrors = addStatsToFrame(frames, frameErrors, \"disks\", getDisk)\n-\tframeErrors = addStatsToFrame(frames, frameErrors, \"disk_usage\", getDiskUsage)\n-\n-\tframeErrors = addStatsToFrame(frames, frameErrors, \"memory\", getMemory)\n-\tframeErrors = addStatsToFrame(frames, frameErrors, \"memory_usage\", getMemoryUsage)\n-\n-\tframeErrors = addStatsToFrame(frames, frameErrors, \"cpu\", getCPU)\n-\t//frameErrors = addStatsToFrame(frames, frameErrors, \"cpu_usage\", getCPUUsage)\n-\n-\tframeErrors = addStatsToFrame(frames, frameErrors, \"processes\", getProcessList)\n-\n-\tframeErrors = addStatsToFrame(frames, frameErrors, \"os\", getHostDetails)\n-\n-\treturn &data.DiagnosticBundle{\n-\t\tFrames: frames,\n-\t\tErrors: data.FrameErrors{\n-\t\t\tErrors: frameErrors,\n-\t\t},\n-\t}, err\n-}\n-\n-func addStatsToFrame(frames map[string]data.Frame, errors []error, name string, statFunc func() (data.MemoryFrame, error)) []error {\n-\tframe, err := statFunc()\n-\tif err != nil {\n-\t\terrors = append(errors, err)\n-\t}\n-\tframes[name] = frame\n-\treturn errors\n-}\n-\n-func (sc *SystemCollector) Configuration() config.Configuration {\n-\treturn config.Configuration{\n-\t\tParams: []config.ConfigParam{},\n-\t}\n-}\n-\n-func (sc *SystemCollector) IsDefault() bool {\n-\treturn true\n-}\n-\n-func getDisk() (data.MemoryFrame, error) {\n-\tblock, err := ghw.Block()\n-\tif err != nil {\n-\t\treturn data.MemoryFrame{}, errors.Wrapf(err, \"unable to list block storage\")\n-\t}\n-\tvar rows [][]interface{}\n-\tcolumns := []string{\"name\", \"size\", \"physicalBlockSize\", \"driveType\", \"controller\", \"vendor\", \"model\", \"partitionName\", \"partitionSize\", \"mountPoint\", \"readOnly\"}\n-\tfor _, disk := range block.Disks {\n-\t\tfor _, part := range disk.Partitions {\n-\t\t\trows = append(rows, []interface{}{disk.Name, disk.SizeBytes, disk.PhysicalBlockSizeBytes, disk.DriveType, disk.StorageController, disk.Vendor, disk.Model, part.Name, part.SizeBytes, part.MountPoint, part.IsReadOnly})\n-\t\t}\n-\t}\n-\treturn data.NewMemoryFrame(\"disk_usage\", columns, rows), nil\n-}\n-\n-func getDiskUsage() (data.MemoryFrame, error) {\n-\tfsList := gosigar.FileSystemList{}\n-\terr := fsList.Get()\n-\tif err != nil {\n-\t\treturn data.MemoryFrame{}, errors.Wrapf(err, \"unable to list filesystems for usage\")\n-\t}\n-\trows := make([][]interface{}, len(fsList.List))\n-\tcolumns := []string{\"filesystem\", \"size\", \"used\", \"avail\", \"use%\", \"mounted on\"}\n-\tfor i, fs := range fsList.List {\n-\t\tdirName := fs.DirName\n-\t\tusage := gosigar.FileSystemUsage{}\n-\t\terr = usage.Get(dirName)\n-\t\tif err == nil {\n-\t\t\trows[i] = []interface{}{fs.DevName, usage.Total, usage.Used, usage.Avail, usage.UsePercent(), dirName}\n-\t\t} else {\n-\t\t\t// we try to output something\n-\t\t\trows[i] = []interface{}{fs.DevName, 0, 0, 0, 0, dirName}\n-\t\t}\n-\t}\n-\treturn data.NewMemoryFrame(\"disk_usage\", columns, rows), nil\n-}\n-\n-func getMemory() (data.MemoryFrame, error) {\n-\tmemory, err := ghw.Memory()\n-\tif err != nil {\n-\t\treturn data.MemoryFrame{}, errors.Wrapf(err, \"unable to read memory\")\n-\t}\n-\tcolumns := []string{\"totalPhysical\", \"totalUsable\", \"supportedPageSizes\"}\n-\trows := make([][]interface{}, 1)\n-\trows[0] = []interface{}{memory.TotalPhysicalBytes, memory.TotalUsableBytes, memory.SupportedPageSizes}\n-\treturn data.NewMemoryFrame(\"memory\", columns, rows), nil\n-}\n-\n-func getMemoryUsage() (data.MemoryFrame, error) {\n-\tmem := gosigar.Mem{}\n-\tswap := gosigar.Swap{}\n-\n-\terr := mem.Get()\n-\tif err != nil {\n-\t\treturn data.MemoryFrame{}, errors.Wrapf(err, \"unable to read memory usage\")\n-\t}\n-\n-\terr = swap.Get()\n-\tif err != nil {\n-\t\treturn data.MemoryFrame{}, errors.Wrapf(err, \"unable to read swap\")\n-\t}\n-\n-\tcolumns := []string{\"type\", \"total\", \"used\", \"free\"}\n-\trows := make([][]interface{}, 3)\n-\n-\trows[0] = []interface{}{\"mem\", mem.Total, mem.Used, mem.Free}\n-\trows[1] = []interface{}{\"buffers/cache\", 0, mem.ActualUsed, mem.ActualFree}\n-\trows[2] = []interface{}{\"swap\", swap.Total, swap.Used, swap.Free}\n-\treturn data.NewMemoryFrame(\"memory_usage\", columns, rows), nil\n-\n-}\n-\n-func getCPU() (data.MemoryFrame, error) {\n-\tcpu, err := ghw.CPU()\n-\tif err != nil {\n-\t\treturn data.MemoryFrame{}, errors.Wrapf(err, \"unable to list cpus\")\n-\t}\n-\tcolumns := []string{\"processor\", \"vendor\", \"model\", \"core\", \"numThreads\", \"logical\", \"capabilities\"}\n-\tvar rows [][]interface{}\n-\tfor _, proc := range cpu.Processors {\n-\t\tfor _, core := range proc.Cores {\n-\t\t\trows = append(rows, []interface{}{proc.ID, proc.Vendor, proc.Model, core.ID, core.NumThreads, core.LogicalProcessors, strings.Join(proc.Capabilities, \" \")})\n-\t\t}\n-\t}\n-\treturn data.NewMemoryFrame(\"cpu\", columns, rows), nil\n-}\n-\n-// this gets cpu usage vs a listing of arch etc - see getCPU(). This needs successive values as its ticks - not currently used\n-// see https://github.com/elastic/beats/blob/master/metricbeat/internal/metrics/cpu/metrics.go#L131 for inspiration\n-//nolint\n-func getCPUUsage() (data.MemoryFrame, error) {\n-\tcpuList := gosigar.CpuList{}\n-\terr := cpuList.Get()\n-\tif err != nil {\n-\t\treturn data.MemoryFrame{}, errors.Wrapf(err, \"unable to list cpus for usage\")\n-\t}\n-\tcolumns := []string{\"sys\", \"nice\", \"stolen\", \"irq\", \"idle\", \"softIrq\", \"user\", \"wait\", \"total\"}\n-\trows := make([][]interface{}, len(cpuList.List), len(cpuList.List))\n-\tfor i, cpu := range cpuList.List {\n-\t\trows[i] = []interface{}{cpu.Sys, cpu.Nice, cpu.Stolen, cpu.Irq, cpu.Idle, cpu.SoftIrq, cpu.User, cpu.Wait, cpu.Total()}\n-\t}\n-\treturn data.NewMemoryFrame(\"cpu_usage\", columns, rows), nil\n-}\n-\n-func getProcessList() (data.MemoryFrame, error) {\n-\tpidList := gosigar.ProcList{}\n-\terr := pidList.Get()\n-\tif err != nil {\n-\t\treturn data.MemoryFrame{}, errors.Wrapf(err, \"unable to list processes\")\n-\t}\n-\tcolumns := []string{\"pid\", \"ppid\", \"stime\", \"time\", \"rss\", \"size\", \"faults\", \"minorFaults\", \"majorFaults\", \"user\", \"state\", \"priority\", \"nice\", \"command\"}\n-\trows := make([][]interface{}, len(pidList.List))\n-\tfor i, pid := range pidList.List {\n-\t\tstate := gosigar.ProcState{}\n-\t\tmem := gosigar.ProcMem{}\n-\t\ttime := gosigar.ProcTime{}\n-\t\targs := gosigar.ProcArgs{}\n-\t\tif err := state.Get(pid); err != nil {\n-\t\t\tcontinue\n-\t\t}\n-\t\tif err := mem.Get(pid); err != nil {\n-\t\t\tcontinue\n-\t\t}\n-\t\tif err := time.Get(pid); err != nil {\n-\t\t\tcontinue\n-\t\t}\n-\t\tif err := args.Get(pid); err != nil {\n-\t\t\tcontinue\n-\t\t}\n-\t\trows[i] = []interface{}{pid, state.Ppid, time.FormatStartTime(), time.FormatTotal(), mem.Resident, mem.Size,\n-\t\t\tmem.PageFaults, mem.MinorFaults, mem.MajorFaults, state.Username, state.State, state.Priority, state.Nice,\n-\t\t\tstrings.Join(args.List, \" \")}\n-\t}\n-\treturn data.NewMemoryFrame(\"process_list\", columns, rows), nil\n-}\n-\n-func getHostDetails() (data.MemoryFrame, error) {\n-\tgi, err := goInfo.GetInfo()\n-\tif err != nil {\n-\t\treturn data.MemoryFrame{}, errors.Wrapf(err, \"unable to get host summary\")\n-\t}\n-\tcolumns := []string{\"hostname\", \"os\", \"goOs\", \"cpus\", \"core\", \"kernel\", \"platform\"}\n-\trows := [][]interface{}{\n-\t\t{gi.Hostname, gi.OS, gi.GoOS, gi.CPUs, gi.Core, gi.Kernel, gi.Platform},\n-\t}\n-\treturn data.NewMemoryFrame(\"os\", columns, rows), nil\n-}\n-\n-func (sc *SystemCollector) Description() string {\n-\treturn \"Collects summary OS and hardware statistics for the host\"\n-}\n-\n-// here we register the collector for use\n-func init() {\n-\tcollectors.Register(\"system\", func() (collectors.Collector, error) {\n-\t\treturn &SystemCollector{\n-\t\t\tresourceManager: platform.GetResourceManager(),\n-\t\t}, nil\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/outputs/file/simple.go b/programs/diagnostics/internal/outputs/file/simple.go\ndeleted file mode 100644\nindex 63847b3addd0..000000000000\n--- a/programs/diagnostics/internal/outputs/file/simple.go\n+++ /dev/null\n@@ -1,344 +0,0 @@\n-package file\n-\n-import (\n-\t\"context\"\n-\t\"encoding/csv\"\n-\t\"fmt\"\n-\t\"os\"\n-\t\"path\"\n-\t\"path/filepath\"\n-\t\"strconv\"\n-\t\"strings\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils\"\n-\t\"github.com/mholt/archiver/v4\"\n-\t\"github.com/pkg/errors\"\n-\t\"github.com/rs/zerolog/log\"\n-)\n-\n-const OutputName = \"simple\"\n-\n-type SubFolderGenerator func() string\n-\n-type SimpleOutput struct {\n-\t// mainly used for testing to make sub folder deterministic - which it won't be by default as it uses a timestamp\n-\tFolderGenerator SubFolderGenerator\n-}\n-\n-func (o SimpleOutput) Write(id string, bundles map[string]*data.DiagnosticBundle, conf config.Configuration) (data.FrameErrors, error) {\n-\tconf, err := conf.ValidateConfig(o.Configuration())\n-\tif err != nil {\n-\t\treturn data.FrameErrors{}, err\n-\t}\n-\tdirectory, err := config.ReadStringValue(conf, \"directory\")\n-\tif err != nil {\n-\t\treturn data.FrameErrors{}, err\n-\t}\n-\tdirectory, err = getWorkingDirectory(directory)\n-\tif err != nil {\n-\t\treturn data.FrameErrors{}, err\n-\t}\n-\tsubFolder := strconv.FormatInt(utils.MakeTimestamp(), 10)\n-\tif o.FolderGenerator != nil {\n-\t\tsubFolder = o.FolderGenerator()\n-\t}\n-\tskipArchive, err := config.ReadBoolValue(conf, \"skip_archive\")\n-\tif err != nil {\n-\t\treturn data.FrameErrors{}, err\n-\t}\n-\n-\toutputDir := filepath.Join(directory, id, subFolder)\n-\tlog.Info().Msgf(\"creating bundle in %s\", outputDir)\n-\tif err := os.MkdirAll(outputDir, os.ModePerm); err != nil {\n-\t\treturn data.FrameErrors{}, err\n-\t}\n-\tframeErrors := data.FrameErrors{}\n-\tvar filePaths []string\n-\tfor name := range bundles {\n-\t\tbundlePaths, frameError := writeDiagnosticBundle(name, bundles[name], outputDir)\n-\t\tfilePaths = append(filePaths, bundlePaths...)\n-\t\tframeErrors.Errors = append(frameErrors.Errors, frameError.Errors...)\n-\t}\n-\tlog.Info().Msg(\"bundle created\")\n-\tif !skipArchive {\n-\t\tarchiveFilename := filepath.Join(directory, id, fmt.Sprintf(\"%s.tar.gz\", subFolder))\n-\t\tlog.Info().Msgf(\"compressing bundle to %s\", archiveFilename)\n-\t\t// produce a map containing the input paths to the archive paths - we preserve the output directory and hierarchy\n-\t\tarchiveMap := createArchiveMap(filePaths, directory)\n-\t\tif err := createArchive(archiveFilename, archiveMap); err != nil {\n-\t\t\treturn frameErrors, err\n-\t\t}\n-\t\t// we delete the original directory leaving just the archive behind\n-\t\tif err := os.RemoveAll(outputDir); err != nil {\n-\t\t\treturn frameErrors, err\n-\t\t}\n-\t\tlog.Info().Msgf(\"archive ready at: %s \", archiveFilename)\n-\t}\n-\treturn frameErrors, nil\n-}\n-\n-func writeDiagnosticBundle(name string, diag *data.DiagnosticBundle, baseDir string) ([]string, data.FrameErrors) {\n-\tdiagDir := filepath.Join(baseDir, name)\n-\tif err := os.MkdirAll(diagDir, os.ModePerm); err != nil {\n-\t\treturn nil, data.FrameErrors{Errors: []error{\n-\t\t\terrors.Wrapf(err, \"unable to create directory for %s\", name),\n-\t\t}}\n-\t}\n-\tframeErrors := data.FrameErrors{}\n-\tvar filePaths []string\n-\tfor frameId, frame := range diag.Frames {\n-\t\tfFilePath, errs := writeFrame(frameId, frame, diagDir)\n-\t\tfilePaths = append(filePaths, fFilePath...)\n-\t\tif len(errs) > 0 {\n-\t\t\t// it would be nice if we could wrap this list of errors into something formal but this logs well\n-\t\t\tframeErrors.Errors = append(frameErrors.Errors, fmt.Errorf(\"unable to write frame %s for %s\", frameId, name))\n-\t\t\tframeErrors.Errors = append(frameErrors.Errors, errs...)\n-\t\t}\n-\t}\n-\treturn filePaths, frameErrors\n-}\n-\n-func writeFrame(frameId string, frame data.Frame, baseDir string) ([]string, []error) {\n-\tswitch f := frame.(type) {\n-\tcase data.DatabaseFrame:\n-\t\treturn writeDatabaseFrame(frameId, f, baseDir)\n-\tcase data.ConfigFileFrame:\n-\t\treturn writeConfigFrame(frameId, f, baseDir)\n-\tcase data.DirectoryFileFrame:\n-\t\treturn processDirectoryFileFrame(frameId, f, baseDir)\n-\tcase data.FileFrame:\n-\t\treturn processFileFrame(frameId, f, baseDir)\n-\tcase data.HierarchicalFrame:\n-\t\treturn writeHierarchicalFrame(frameId, f, baseDir)\n-\tdefault:\n-\t\t// for now our data frame writer supports all frames\n-\t\treturn writeDatabaseFrame(frameId, frame, baseDir)\n-\t}\n-}\n-\n-func writeHierarchicalFrame(frameId string, frame data.HierarchicalFrame, baseDir string) ([]string, []error) {\n-\tfilePaths, errs := writeFrame(frameId, frame.DataFrame, baseDir)\n-\tfor _, subFrame := range frame.SubFrames {\n-\t\tsubDir := filepath.Join(baseDir, subFrame.Name())\n-\t\tif err := os.MkdirAll(subDir, os.ModePerm); err != nil {\n-\t\t\terrs = append(errs, err)\n-\t\t\tcontinue\n-\t\t}\n-\t\tsubPaths, subErrs := writeFrame(subFrame.Name(), subFrame, subDir)\n-\t\tfilePaths = append(filePaths, subPaths...)\n-\t\terrs = append(errs, subErrs...)\n-\t}\n-\treturn filePaths, errs\n-}\n-\n-func writeDatabaseFrame(frameId string, frame data.Frame, baseDir string) ([]string, []error) {\n-\tframeFilePath := filepath.Join(baseDir, fmt.Sprintf(\"%s.csv\", frameId))\n-\tvar errs []error\n-\tf, err := os.Create(frameFilePath)\n-\tif err != nil {\n-\t\terrs = append(errs, errors.Wrapf(err, \"unable to create directory for frame %s\", frameId))\n-\t\treturn []string{}, errs\n-\t}\n-\tdefer f.Close()\n-\tw := csv.NewWriter(f)\n-\tdefer w.Flush()\n-\tif err := w.Write(frame.Columns()); err != nil {\n-\t\terrs = append(errs, errors.Wrapf(err, \"unable to write columns for frame %s\", frameId))\n-\t\treturn []string{}, errs\n-\t}\n-\t// we don't collect an error for every line here like configs and logs - could mean a lot of unnecessary noise\n-\tfor {\n-\t\tvalues, ok, err := frame.Next()\n-\t\tif err != nil {\n-\t\t\terrs = append(errs, errors.Wrapf(err, \"unable to read frame %s\", frameId))\n-\t\t\treturn []string{}, errs\n-\t\t}\n-\t\tif !ok {\n-\t\t\treturn []string{frameFilePath}, errs\n-\t\t}\n-\t\tsValues := make([]string, len(values))\n-\t\tfor i, value := range values {\n-\t\t\tsValues[i] = fmt.Sprintf(\"%v\", value)\n-\t\t}\n-\t\tif err := w.Write(sValues); err != nil {\n-\t\t\terrs = append(errs, errors.Wrapf(err, \"unable to write row for frame %s\", frameId))\n-\t\t\treturn []string{}, errs\n-\t\t}\n-\t}\n-}\n-\n-func writeConfigFrame(frameId string, frame data.ConfigFileFrame, baseDir string) ([]string, []error) {\n-\tvar errs []error\n-\tframeDirectory := filepath.Join(baseDir, frameId)\n-\tif err := os.MkdirAll(frameDirectory, os.ModePerm); err != nil {\n-\t\terrs = append(errs, errors.Wrapf(err, \"unable to create directory for frame %s\", frameId))\n-\t\treturn []string{}, errs\n-\t}\n-\t// this holds our files included\n-\tincludesDirectory := filepath.Join(frameDirectory, \"includes\")\n-\tif err := os.MkdirAll(includesDirectory, os.ModePerm); err != nil {\n-\t\terrs = append(errs, errors.Wrapf(err, \"unable to create includes directory for frame %s\", frameId))\n-\t\treturn []string{}, errs\n-\t}\n-\tfor {\n-\t\tvalues, ok, err := frame.Next()\n-\t\tif err != nil {\n-\t\t\terrs = append(errs, err)\n-\t\t\treturn []string{frameDirectory}, errs\n-\t\t}\n-\t\tif !ok {\n-\t\t\treturn []string{frameDirectory}, errs\n-\t\t}\n-\t\tconfigFile := values[0].(data.ConfigFile)\n-\t\tif !configFile.IsIncluded() {\n-\t\t\trelPath := strings.TrimPrefix(configFile.FilePath(), frame.Directory)\n-\t\t\tdestPath := path.Join(frameDirectory, relPath)\n-\t\t\tif err = configFile.Copy(destPath, true); err != nil {\n-\t\t\t\terrs = append(errs, errors.Wrapf(err, \"Unable to copy file %s\", configFile.FilePath()))\n-\t\t\t}\n-\t\t} else {\n-\t\t\t// include files could be anywhere - potentially multiple with the same name. We thus, recreate the directory\n-\t\t\t// hierarchy under includes to avoid collisions\n-\t\t\tdestPath := path.Join(includesDirectory, configFile.FilePath())\n-\t\t\tif err = configFile.Copy(destPath, true); err != nil {\n-\t\t\t\terrs = append(errs, errors.Wrapf(err, \"Unable to copy file %s\", configFile.FilePath()))\n-\t\t\t}\n-\t\t}\n-\n-\t}\n-}\n-\n-func processDirectoryFileFrame(frameId string, frame data.DirectoryFileFrame, baseDir string) ([]string, []error) {\n-\tvar errs []error\n-\t// each set of files goes under its own directory to preserve grouping\n-\tframeDirectory := filepath.Join(baseDir, frameId)\n-\tif err := os.MkdirAll(frameDirectory, os.ModePerm); err != nil {\n-\t\terrs = append(errs, errors.Wrapf(err, \"unable to create directory for frame %s\", frameId))\n-\t\treturn []string{}, errs\n-\t}\n-\tfor {\n-\t\tvalues, ok, err := frame.Next()\n-\t\tif err != nil {\n-\t\t\terrs = append(errs, err)\n-\t\t\treturn []string{frameDirectory}, errs\n-\t\t}\n-\t\tif !ok {\n-\t\t\treturn []string{frameDirectory}, errs\n-\t\t}\n-\t\tfile := values[0].(data.SimpleFile)\n-\t\trelPath := strings.TrimPrefix(file.FilePath(), frame.Directory)\n-\t\tdestPath := path.Join(frameDirectory, relPath)\n-\n-\t\tif err = file.Copy(destPath, true); err != nil {\n-\t\t\terrs = append(errs, errors.Wrapf(err, \"unable to copy file %s for frame %s\", file, frameId))\n-\t\t}\n-\t}\n-}\n-\n-func processFileFrame(frameId string, frame data.FileFrame, baseDir string) ([]string, []error) {\n-\tvar errs []error\n-\tframeDirectory := filepath.Join(baseDir, frameId)\n-\tif err := os.MkdirAll(frameDirectory, os.ModePerm); err != nil {\n-\t\terrs = append(errs, errors.Wrapf(err, \"unable to create directory for frame %s\", frameId))\n-\t\treturn []string{}, errs\n-\t}\n-\tfor {\n-\t\tvalues, ok, err := frame.Next()\n-\t\tif err != nil {\n-\t\t\terrs = append(errs, err)\n-\t\t}\n-\t\tif !ok {\n-\t\t\treturn []string{frameDirectory}, errs\n-\t\t}\n-\t\tfile := values[0].(data.SimpleFile)\n-\t\t// we need an absolute path to preserve the directory hierarchy\n-\t\tdir, err := filepath.Abs(filepath.Dir(file.FilePath()))\n-\t\tif err != nil {\n-\t\t\terrs = append(errs, errors.Wrapf(err, \"unable to determine dir for %s\", file.FilePath()))\n-\t\t}\n-\t\toutputDir := filepath.Join(frameDirectory, dir)\n-\t\tif _, err := os.Stat(outputDir); os.IsNotExist(err) {\n-\t\t\tif err := os.MkdirAll(outputDir, os.ModePerm); err != nil {\n-\t\t\t\terrs = append(errs, errors.Wrapf(err, \"unable to create directory for %s\", file.FilePath()))\n-\t\t\t} else {\n-\t\t\t\toutputPath := filepath.Join(outputDir, filepath.Base(file.FilePath()))\n-\t\t\t\terr = file.Copy(outputPath, false)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\terrs = append(errs, errors.Wrapf(err, \"unable to copy file %s\", file.FilePath()))\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-}\n-\n-func getWorkingDirectory(path string) (string, error) {\n-\tif !filepath.IsAbs(path) {\n-\t\tworkingPath, err := os.Getwd()\n-\t\tif err != nil {\n-\t\t\treturn \"\", err\n-\t\t}\n-\t\treturn filepath.Join(workingPath, path), nil\n-\t}\n-\treturn path, nil\n-}\n-\n-func createArchiveMap(filePaths []string, prefix string) map[string]string {\n-\tarchiveMap := make(map[string]string)\n-\tfor _, path := range filePaths {\n-\t\tarchiveMap[path] = strings.TrimPrefix(path, prefix)\n-\t}\n-\treturn archiveMap\n-}\n-\n-func createArchive(outputFile string, filePaths map[string]string) error {\n-\tfiles, err := archiver.FilesFromDisk(nil, filePaths)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tout, err := os.Create(outputFile)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer out.Close()\n-\tformat := archiver.CompressedArchive{\n-\t\tCompression: archiver.Gz{},\n-\t\tArchival:    archiver.Tar{},\n-\t}\n-\terr = format.Archive(context.Background(), out, files)\n-\treturn err\n-}\n-\n-func (o SimpleOutput) Configuration() config.Configuration {\n-\treturn config.Configuration{\n-\t\tParams: []config.ConfigParam{\n-\t\t\tconfig.StringParam{\n-\t\t\t\tValue: \"./\",\n-\t\t\t\tParam: config.NewParam(\"directory\", \"Directory in which to create dump. Defaults to the current directory.\", false),\n-\t\t\t},\n-\t\t\tconfig.StringOptions{\n-\t\t\t\tValue: \"csv\",\n-\t\t\t\t// TODO: add tsv and others here later\n-\t\t\t\tOptions: []string{\"csv\"},\n-\t\t\t\tParam:   config.NewParam(\"format\", \"Format of exported files\", false),\n-\t\t\t},\n-\t\t\tconfig.BoolParam{\n-\t\t\t\tValue: false,\n-\t\t\t\tParam: config.NewParam(\"skip_archive\", \"Don't compress output to an archive\", false),\n-\t\t\t},\n-\t\t},\n-\t}\n-}\n-\n-func (o SimpleOutput) Description() string {\n-\treturn \"Writes out the diagnostic bundle as files in a structured directory, optionally producing a compressed archive.\"\n-}\n-\n-// here we register the output for use\n-func init() {\n-\toutputs.Register(OutputName, func() (outputs.Output, error) {\n-\t\treturn SimpleOutput{}, nil\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/outputs/registry.go b/programs/diagnostics/internal/outputs/registry.go\ndeleted file mode 100644\nindex 0187cd9105d7..000000000000\n--- a/programs/diagnostics/internal/outputs/registry.go\n+++ /dev/null\n@@ -1,67 +0,0 @@\n-package outputs\n-\n-import (\n-\t\"fmt\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/pkg/errors\"\n-\t\"github.com/rs/zerolog/log\"\n-)\n-\n-type Output interface {\n-\tWrite(id string, bundles map[string]*data.DiagnosticBundle, config config.Configuration) (data.FrameErrors, error)\n-\tConfiguration() config.Configuration\n-\tDescription() string\n-\t// TODO: we will need to implement this for the convert function\n-\t//Read(config config.Configuration) (data.DiagnosticBundle, error)\n-}\n-\n-// Register can be called from init() on an output in this package\n-// It will automatically be added to the Outputs map to be called externally\n-func Register(name string, output OutputFactory) {\n-\t// names must be unique\n-\tif _, ok := Outputs[name]; ok {\n-\t\tlog.Error().Msgf(\"More than 1 output is trying to register under the name %s. Names must be unique.\", name)\n-\t}\n-\tOutputs[name] = output\n-}\n-\n-// OutputFactory lets us use a closure to get instances of the output struct\n-type OutputFactory func() (Output, error)\n-\n-var Outputs = map[string]OutputFactory{}\n-\n-func GetOutputNames() []string {\n-\toutputs := make([]string, len(Outputs))\n-\ti := 0\n-\tfor k := range Outputs {\n-\t\toutputs[i] = k\n-\t\ti++\n-\t}\n-\treturn outputs\n-}\n-\n-func GetOutputByName(name string) (Output, error) {\n-\tif outputFactory, ok := Outputs[name]; ok {\n-\t\t//do something here\n-\t\toutput, err := outputFactory()\n-\t\tif err != nil {\n-\t\t\treturn nil, errors.Wrapf(err, \"output %s could not be initialized\", name)\n-\t\t}\n-\t\treturn output, nil\n-\t}\n-\treturn nil, fmt.Errorf(\"%s is not a valid output name\", name)\n-}\n-\n-func BuildConfigurationOptions() (map[string]config.Configuration, error) {\n-\tconfigurations := make(map[string]config.Configuration)\n-\tfor name, collectorFactory := range Outputs {\n-\t\toutput, err := collectorFactory()\n-\t\tif err != nil {\n-\t\t\treturn nil, errors.Wrapf(err, \"output %s could not be initialized\", name)\n-\t\t}\n-\t\tconfigurations[name] = output.Configuration()\n-\t}\n-\treturn configurations, nil\n-}\ndiff --git a/programs/diagnostics/internal/outputs/terminal/report.go b/programs/diagnostics/internal/outputs/terminal/report.go\ndeleted file mode 100644\nindex 8337f542457b..000000000000\n--- a/programs/diagnostics/internal/outputs/terminal/report.go\n+++ /dev/null\n@@ -1,284 +0,0 @@\n-package terminal\n-\n-import (\n-\t\"bufio\"\n-\t\"fmt\"\n-\t\"os\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/olekukonko/tablewriter\"\n-\t\"github.com/pkg/errors\"\n-)\n-\n-const OutputName = \"report\"\n-\n-type ReportOutput struct {\n-}\n-\n-func (r ReportOutput) Write(id string, bundles map[string]*data.DiagnosticBundle, conf config.Configuration) (data.FrameErrors, error) {\n-\tconf, err := conf.ValidateConfig(r.Configuration())\n-\tif err != nil {\n-\t\treturn data.FrameErrors{}, err\n-\t}\n-\tformat, err := config.ReadStringOptionsValue(conf, \"format\")\n-\tif err != nil {\n-\t\treturn data.FrameErrors{}, err\n-\t}\n-\tnonInteractive, err := config.ReadBoolValue(conf, \"continue\")\n-\tif err != nil {\n-\t\treturn data.FrameErrors{}, err\n-\t}\n-\tmaxRows, err := config.ReadIntValue(conf, \"row_limit\")\n-\tif err != nil {\n-\t\treturn data.FrameErrors{}, err\n-\t}\n-\tmaxColumns, err := config.ReadIntValue(conf, \"column_limit\")\n-\tif err != nil {\n-\t\treturn data.FrameErrors{}, err\n-\t}\n-\tframeErrors := data.FrameErrors{}\n-\tfor name := range bundles {\n-\t\tframeError := printDiagnosticBundle(name, bundles[name], format, !nonInteractive, int(maxRows), int(maxColumns))\n-\t\tframeErrors.Errors = append(frameErrors.Errors, frameError.Errors...)\n-\t}\n-\treturn data.FrameErrors{}, nil\n-}\n-\n-func printDiagnosticBundle(name string, diag *data.DiagnosticBundle, format string, interactive bool, maxRows, maxColumns int) data.FrameErrors {\n-\tframeErrors := data.FrameErrors{}\n-\tfor frameId, frame := range diag.Frames {\n-\t\tprintFrameHeader(fmt.Sprintf(\"%s.%s\", name, frameId))\n-\t\terr := printFrame(frame, format, maxRows, maxColumns)\n-\t\tif err != nil {\n-\t\t\tframeErrors.Errors = append(frameErrors.Errors, err)\n-\t\t}\n-\t\tif interactive {\n-\t\t\terr := waitForEnter()\n-\t\t\tif err != nil {\n-\t\t\t\tframeErrors.Errors = append(frameErrors.Errors, err)\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn frameErrors\n-}\n-\n-func waitForEnter() error {\n-\tfmt.Println(\"Press the Enter Key to view the next frame report\")\n-\tfor {\n-\t\tconsoleReader := bufio.NewReaderSize(os.Stdin, 1)\n-\t\tinput, err := consoleReader.ReadByte()\n-\t\tif err != nil {\n-\t\t\treturn errors.New(\"Unable to read user input\")\n-\t\t}\n-\t\tif input == 3 {\n-\t\t\t//ctl +c\n-\t\t\tfmt.Println(\"Exiting...\")\n-\t\t\tos.Exit(0)\n-\t\t}\n-\t\tif input == 10 {\n-\t\t\treturn nil\n-\t\t}\n-\t}\n-}\n-\n-func printFrame(frame data.Frame, format string, maxRows, maxColumns int) error {\n-\tswitch f := frame.(type) {\n-\tcase data.DatabaseFrame:\n-\t\treturn printDatabaseFrame(f, format, maxRows, maxColumns)\n-\tcase data.ConfigFileFrame:\n-\t\treturn printConfigFrame(f, format)\n-\tcase data.DirectoryFileFrame:\n-\t\treturn printDirectoryFileFrame(f, format, maxRows)\n-\tcase data.HierarchicalFrame:\n-\t\treturn printHierarchicalFrame(f, format, maxRows, maxColumns)\n-\tdefault:\n-\t\t// for now our data frame writer supports all frames\n-\t\treturn printDatabaseFrame(f, format, maxRows, maxColumns)\n-\t}\n-}\n-\n-func createTable(format string) *tablewriter.Table {\n-\ttable := tablewriter.NewWriter(os.Stdout)\n-\tif format == \"markdown\" {\n-\t\ttable.SetBorders(tablewriter.Border{Left: true, Top: false, Right: true, Bottom: false})\n-\t\ttable.SetCenterSeparator(\"|\")\n-\t}\n-\treturn table\n-}\n-\n-func printFrameHeader(title string) {\n-\ttitleTable := tablewriter.NewWriter(os.Stdout)\n-\ttitleTable.SetHeader([]string{title})\n-\ttitleTable.SetAutoWrapText(false)\n-\ttitleTable.SetAutoFormatHeaders(true)\n-\ttitleTable.SetHeaderAlignment(tablewriter.ALIGN_CENTER)\n-\ttitleTable.SetRowSeparator(\"\\n\")\n-\ttitleTable.SetHeaderLine(false)\n-\ttitleTable.SetBorder(false)\n-\ttitleTable.SetTablePadding(\"\\t\") // pad with tabs\n-\ttitleTable.SetNoWhiteSpace(true)\n-\ttitleTable.Render()\n-}\n-\n-func printHierarchicalFrame(frame data.HierarchicalFrame, format string, maxRows, maxColumns int) error {\n-\terr := printDatabaseFrame(frame, format, maxRows, maxColumns)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tfor _, subFrame := range frame.SubFrames {\n-\t\terr = printHierarchicalFrame(subFrame, format, maxRows, maxColumns)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\treturn nil\n-}\n-\n-func printDatabaseFrame(frame data.Frame, format string, maxRows, maxColumns int) error {\n-\ttable := createTable(format)\n-\ttable.SetAutoWrapText(false)\n-\tcolumns := len(frame.Columns())\n-\tif maxColumns > 0 && maxColumns < columns {\n-\t\tcolumns = maxColumns\n-\t}\n-\ttable.SetHeader(frame.Columns()[:columns])\n-\tr := 0\n-\ttrunColumns := 0\n-\tfor {\n-\t\tvalues, ok, err := frame.Next()\n-\t\tif !ok || r == maxRows {\n-\t\t\ttable.Render()\n-\t\t\tif trunColumns > 0 {\n-\t\t\t\twarning(fmt.Sprintf(\"Truncated %d columns, more available...\", trunColumns))\n-\t\t\t}\n-\t\t\tif r == maxRows {\n-\t\t\t\twarning(\"Truncated rows, more available...\")\n-\t\t\t}\n-\t\t\treturn err\n-\t\t}\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tcolumns := len(values)\n-\t\t// -1 means unlimited\n-\t\tif maxColumns > 0 && maxColumns < columns {\n-\t\t\ttrunColumns = columns - maxColumns\n-\t\t\tcolumns = maxColumns\n-\t\t}\n-\t\trow := make([]string, columns)\n-\t\tfor i, value := range values {\n-\t\t\tif i == columns {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\trow[i] = fmt.Sprintf(\"%v\", value)\n-\t\t}\n-\t\ttable.Append(row)\n-\t\tr++\n-\t}\n-}\n-\n-// currently we dump the whole config - useless in parts\n-func printConfigFrame(frame data.Frame, format string) error {\n-\tfor {\n-\t\tvalues, ok, err := frame.Next()\n-\t\tif !ok {\n-\t\t\treturn err\n-\t\t}\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tconfigFile := values[0].(data.File)\n-\t\tdat, err := os.ReadFile(configFile.FilePath())\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\t// create a table per row - as each will be a file\n-\t\ttable := createTable(format)\n-\t\ttable.SetAutoWrapText(false)\n-\t\ttable.SetAutoFormatHeaders(false)\n-\t\ttable.ClearRows()\n-\t\ttable.SetHeader([]string{configFile.FilePath()})\n-\t\ttable.Append([]string{string(dat)})\n-\t\ttable.Render()\n-\t}\n-}\n-\n-func printDirectoryFileFrame(frame data.Frame, format string, maxRows int) error {\n-\tfor {\n-\t\tvalues, ok, err := frame.Next()\n-\t\tif !ok {\n-\n-\t\t\treturn err\n-\t\t}\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tpath := values[0].(data.SimpleFile)\n-\t\tfile, err := os.Open(path.FilePath())\n-\t\tif err != nil {\n-\t\t\t// failure on one file causes rest to be ignored in frame...we could improve this\n-\t\t\treturn errors.Wrapf(err, \"Unable to read file %s\", path.FilePath())\n-\t\t}\n-\t\tscanner := bufio.NewScanner(file)\n-\t\ti := 0\n-\t\t// create a table per row - as each will be a file\n-\t\ttable := createTable(format)\n-\t\ttable.SetAutoWrapText(false)\n-\t\ttable.SetAutoFormatHeaders(false)\n-\t\ttable.ClearRows()\n-\t\ttable.SetHeader([]string{path.FilePath()})\n-\t\tfor scanner.Scan() {\n-\t\t\tif i == maxRows {\n-\t\t\t\tfmt.Println()\n-\t\t\t\ttable.Render()\n-\t\t\t\twarning(\"Truncated lines, more available...\")\n-\t\t\t\tfmt.Print(\"\\n\")\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\ttable.Append([]string{scanner.Text()})\n-\t\t\ti++\n-\t\t}\n-\t}\n-}\n-\n-// prints a warning\n-func warning(s string) {\n-\tfmt.Printf(\"\\x1b[%dm%v\\x1b[0m%s\\n\", 33, \"WARNING: \", s)\n-}\n-\n-func (r ReportOutput) Configuration() config.Configuration {\n-\treturn config.Configuration{\n-\t\tParams: []config.ConfigParam{\n-\t\t\tconfig.StringOptions{\n-\t\t\t\tValue:   \"default\",\n-\t\t\t\tOptions: []string{\"default\", \"markdown\"},\n-\t\t\t\tParam:   config.NewParam(\"format\", \"Format of tables. Default is terminal friendly.\", false),\n-\t\t\t},\n-\t\t\tconfig.BoolParam{\n-\t\t\t\tValue: false,\n-\t\t\t\tParam: config.NewParam(\"continue\", \"Print report with no interaction\", false),\n-\t\t\t},\n-\t\t\tconfig.IntParam{\n-\t\t\t\tValue: 10,\n-\t\t\t\tParam: config.NewParam(\"row_limit\", \"Max Rows to print per frame.\", false),\n-\t\t\t},\n-\t\t\tconfig.IntParam{\n-\t\t\t\tValue: 8,\n-\t\t\t\tParam: config.NewParam(\"column_limit\", \"Max Columns to print per frame. Negative is unlimited.\", false),\n-\t\t\t},\n-\t\t},\n-\t}\n-}\n-\n-func (r ReportOutput) Description() string {\n-\treturn \"Writes out the diagnostic bundle to the terminal as a simple report.\"\n-}\n-\n-// here we register the output for use\n-func init() {\n-\toutputs.Register(OutputName, func() (outputs.Output, error) {\n-\t\treturn ReportOutput{}, nil\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/platform/config/models.go b/programs/diagnostics/internal/platform/config/models.go\ndeleted file mode 100644\nindex 6c76b8f149b3..000000000000\n--- a/programs/diagnostics/internal/platform/config/models.go\n+++ /dev/null\n@@ -1,129 +0,0 @@\n-package config\n-\n-import (\n-\t\"fmt\"\n-\t\"strings\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils\"\n-)\n-\n-type ConfigParam interface {\n-\tName() string\n-\tRequired() bool\n-\tDescription() string\n-\tvalidate(defaultConfig ConfigParam) error\n-}\n-\n-type Configuration struct {\n-\tParams []ConfigParam\n-}\n-\n-type Param struct {\n-\tname        string\n-\tdescription string\n-\trequired    bool\n-}\n-\n-func NewParam(name string, description string, required bool) Param {\n-\treturn Param{\n-\t\tname:        name,\n-\t\tdescription: description,\n-\t\trequired:    required,\n-\t}\n-}\n-\n-func (bp Param) Name() string {\n-\treturn bp.name\n-}\n-\n-func (bp Param) Required() bool {\n-\treturn bp.required\n-}\n-\n-func (bp Param) Description() string {\n-\treturn bp.description\n-}\n-\n-func (bp Param) validate(defaultConfig ConfigParam) error {\n-\treturn nil\n-}\n-\n-func (c Configuration) GetConfigParam(paramName string) (ConfigParam, error) {\n-\tfor _, param := range c.Params {\n-\t\tif param.Name() == paramName {\n-\t\t\treturn param, nil\n-\t\t}\n-\t}\n-\treturn nil, fmt.Errorf(\"%s does not exist\", paramName)\n-}\n-\n-// ValidateConfig finds the intersection of a config c and a default config. Requires all possible params to be in default.\n-func (c Configuration) ValidateConfig(defaultConfig Configuration) (Configuration, error) {\n-\tvar finalParams []ConfigParam\n-\tfor _, defaultParam := range defaultConfig.Params {\n-\t\tsetParam, err := c.GetConfigParam(defaultParam.Name())\n-\t\tif err == nil {\n-\t\t\t// check the set value is valid\n-\t\t\tif err := setParam.validate(defaultParam); err != nil {\n-\t\t\t\treturn Configuration{}, fmt.Errorf(\"parameter %s is invalid - %s\", defaultParam.Name(), err.Error())\n-\t\t\t}\n-\t\t\tfinalParams = append(finalParams, setParam)\n-\t\t} else if defaultParam.Required() {\n-\t\t\treturn Configuration{}, fmt.Errorf(\"missing required parameter %s - %s\", defaultParam.Name(), err.Error())\n-\t\t} else {\n-\t\t\tfinalParams = append(finalParams, defaultParam)\n-\t\t}\n-\t}\n-\treturn Configuration{\n-\t\tParams: finalParams,\n-\t}, nil\n-}\n-\n-type StringParam struct {\n-\tParam\n-\tValue      string\n-\tAllowEmpty bool\n-}\n-\n-func (sp StringParam) validate(defaultConfig ConfigParam) error {\n-\tdsp := defaultConfig.(StringParam)\n-\tif !dsp.AllowEmpty && strings.TrimSpace(sp.Value) == \"\" {\n-\t\treturn fmt.Errorf(\"%s cannot be empty\", sp.Name())\n-\t}\n-\t// if the parameter is not required it doesn't matter\n-\treturn nil\n-}\n-\n-type StringListParam struct {\n-\tParam\n-\tValues []string\n-}\n-\n-type StringOptions struct {\n-\tParam\n-\tOptions    []string\n-\tValue      string\n-\tAllowEmpty bool\n-}\n-\n-func (so StringOptions) validate(defaultConfig ConfigParam) error {\n-\tdso := defaultConfig.(StringOptions)\n-\tif !dso.AllowEmpty && strings.TrimSpace(so.Value) == \"\" {\n-\t\treturn fmt.Errorf(\"%s cannot be empty\", so.Name())\n-\t}\n-\tif !utils.Contains(dso.Options, so.Value) {\n-\t\treturn fmt.Errorf(\"%s is not a valid value for %s - %v\", so.Value, so.Name(), so.Options)\n-\t}\n-\t// if the parameter is not required it doesn't matter\n-\treturn nil\n-}\n-\n-type IntParam struct {\n-\tParam\n-\tValue int64\n-}\n-\n-type BoolParam struct {\n-\tParam\n-\tValue bool\n-}\ndiff --git a/programs/diagnostics/internal/platform/config/utils.go b/programs/diagnostics/internal/platform/config/utils.go\ndeleted file mode 100644\nindex 5f84c38d4f45..000000000000\n--- a/programs/diagnostics/internal/platform/config/utils.go\n+++ /dev/null\n@@ -1,74 +0,0 @@\n-package config\n-\n-import (\n-\t\"fmt\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils\"\n-)\n-\n-func ReadStringListValues(conf Configuration, paramName string) ([]string, error) {\n-\tparam, err := conf.GetConfigParam(paramName)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tvalue, ok := param.(StringListParam)\n-\tif !ok {\n-\t\tvalue, ok = param.(StringListParam)\n-\t\tif !ok {\n-\t\t\treturn nil, fmt.Errorf(\"%s must be a list of strings\", paramName)\n-\t\t}\n-\t}\n-\n-\treturn value.Values, nil\n-}\n-\n-func ReadStringValue(conf Configuration, paramName string) (string, error) {\n-\tparam, err := conf.GetConfigParam(paramName)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\tvalue, ok := param.(StringParam)\n-\tif !ok {\n-\t\treturn \"\", fmt.Errorf(\"%s must be a list of strings\", paramName)\n-\t}\n-\treturn value.Value, nil\n-}\n-\n-func ReadIntValue(conf Configuration, paramName string) (int64, error) {\n-\tparam, err := conf.GetConfigParam(paramName)\n-\tif err != nil {\n-\t\treturn 0, err\n-\t}\n-\tvalue, ok := param.(IntParam)\n-\tif !ok {\n-\t\treturn 9, fmt.Errorf(\"%s must be an unsigned integer\", paramName)\n-\t}\n-\treturn value.Value, nil\n-}\n-\n-func ReadBoolValue(conf Configuration, paramName string) (bool, error) {\n-\tparam, err := conf.GetConfigParam(paramName)\n-\tif err != nil {\n-\t\treturn false, err\n-\t}\n-\tvalue, ok := param.(BoolParam)\n-\tif !ok {\n-\t\treturn false, fmt.Errorf(\"%s must be a boolean\", paramName)\n-\t}\n-\treturn value.Value, nil\n-}\n-\n-func ReadStringOptionsValue(conf Configuration, paramName string) (string, error) {\n-\tparam, err := conf.GetConfigParam(paramName)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\tvalue, ok := param.(StringOptions)\n-\tif !ok {\n-\t\treturn \"\", fmt.Errorf(\"%s must be a string options\", paramName)\n-\t}\n-\tif !utils.Contains(value.Options, value.Value) {\n-\t\treturn \"\", fmt.Errorf(\"%s is not a valid option in %v for the the parameter %s\", value.Value, value.Options, paramName)\n-\t}\n-\treturn value.Value, nil\n-}\ndiff --git a/programs/diagnostics/internal/platform/data/bundle.go b/programs/diagnostics/internal/platform/data/bundle.go\ndeleted file mode 100644\nindex e4eeede659e3..000000000000\n--- a/programs/diagnostics/internal/platform/data/bundle.go\n+++ /dev/null\n@@ -1,27 +0,0 @@\n-package data\n-\n-import (\n-\t\"strings\"\n-)\n-\n-// DiagnosticBundle contains the results from a Collector\n-// each frame can represent a table or collection of data files. By allowing multiple frames a single DiagnosticBundle\n-// can potentially contain many related tables\n-type DiagnosticBundle struct {\n-\tFrames map[string]Frame\n-\t// Errors is a property to be set if the Collector has an error. This can be used to indicate a partial collection\n-\t// and failed frames\n-\tErrors FrameErrors\n-}\n-\n-type FrameErrors struct {\n-\tErrors []error\n-}\n-\n-func (fe *FrameErrors) Error() string {\n-\terrors := make([]string, len(fe.Errors))\n-\tfor i := range errors {\n-\t\terrors[i] = fe.Errors[i].Error()\n-\t}\n-\treturn strings.Join(errors, \"\\n\")\n-}\ndiff --git a/programs/diagnostics/internal/platform/data/database.go b/programs/diagnostics/internal/platform/data/database.go\ndeleted file mode 100644\nindex d49317d8719e..000000000000\n--- a/programs/diagnostics/internal/platform/data/database.go\n+++ /dev/null\n@@ -1,88 +0,0 @@\n-package data\n-\n-import (\n-\t\"database/sql\"\n-\t\"fmt\"\n-\t\"reflect\"\n-\t\"strings\"\n-)\n-\n-type DatabaseFrame struct {\n-\tname        string\n-\tColumnNames []string\n-\trows        *sql.Rows\n-\tcolumnTypes []*sql.ColumnType\n-\tvars        []interface{}\n-}\n-\n-func NewDatabaseFrame(name string, rows *sql.Rows) (DatabaseFrame, error) {\n-\tdatabaseFrame := DatabaseFrame{}\n-\tcolumnTypes, err := rows.ColumnTypes()\n-\tif err != nil {\n-\t\treturn DatabaseFrame{}, err\n-\t}\n-\tdatabaseFrame.columnTypes = columnTypes\n-\tdatabaseFrame.name = name\n-\tvars := make([]interface{}, len(columnTypes))\n-\tcolumnNames := make([]string, len(columnTypes))\n-\tfor i := range columnTypes {\n-\t\tvalue := reflect.Zero(columnTypes[i].ScanType()).Interface()\n-\t\tvars[i] = &value\n-\t\tcolumnNames[i] = columnTypes[i].Name()\n-\t}\n-\tdatabaseFrame.ColumnNames = columnNames\n-\tdatabaseFrame.vars = vars\n-\tdatabaseFrame.rows = rows\n-\treturn databaseFrame, nil\n-}\n-\n-func (f DatabaseFrame) Next() ([]interface{}, bool, error) {\n-\tvalues := make([]interface{}, len(f.columnTypes))\n-\tfor f.rows.Next() {\n-\t\tif err := f.rows.Scan(f.vars...); err != nil {\n-\t\t\treturn nil, false, err\n-\t\t}\n-\t\tfor i := range f.columnTypes {\n-\t\t\tptr := reflect.ValueOf(f.vars[i])\n-\t\t\tvalues[i] = ptr.Elem().Interface()\n-\t\t}\n-\t\treturn values, true, nil //nolint\n-\t}\n-\t// TODO: raise issue as this seems to always raise an error\n-\t//err := f.rows.Err()\n-\tf.rows.Close()\n-\treturn nil, false, nil\n-}\n-\n-func (f DatabaseFrame) Columns() []string {\n-\treturn f.ColumnNames\n-}\n-\n-func (f DatabaseFrame) Name() string {\n-\treturn f.name\n-}\n-\n-type Order int\n-\n-const (\n-\tAsc  Order = 1\n-\tDesc Order = 2\n-)\n-\n-type OrderBy struct {\n-\tColumn string\n-\tOrder  Order\n-}\n-\n-func (o OrderBy) String() string {\n-\tif strings.TrimSpace(o.Column) == \"\" {\n-\t\treturn \"\"\n-\t}\n-\tswitch o.Order {\n-\tcase Asc:\n-\t\treturn fmt.Sprintf(\" ORDER BY %s ASC\", o.Column)\n-\tcase Desc:\n-\t\treturn fmt.Sprintf(\" ORDER BY %s DESC\", o.Column)\n-\t}\n-\treturn \"\"\n-}\ndiff --git a/programs/diagnostics/internal/platform/data/field.go b/programs/diagnostics/internal/platform/data/field.go\ndeleted file mode 100644\nindex 5e80fc1f467f..000000000000\n--- a/programs/diagnostics/internal/platform/data/field.go\n+++ /dev/null\n@@ -1,8 +0,0 @@\n-package data\n-\n-type Field struct {\n-\t// Name of the field\n-\tName string\n-\t// A list of fields that must implement FieldType interface\n-\tValues []interface{}\n-}\ndiff --git a/programs/diagnostics/internal/platform/data/file.go b/programs/diagnostics/internal/platform/data/file.go\ndeleted file mode 100644\nindex 9760b4b69061..000000000000\n--- a/programs/diagnostics/internal/platform/data/file.go\n+++ /dev/null\n@@ -1,444 +0,0 @@\n-package data\n-\n-import (\n-\t\"bufio\"\n-\t\"encoding/xml\"\n-\t\"io/ioutil\"\n-\t\"os\"\n-\t\"path\"\n-\t\"path/filepath\"\n-\t\"regexp\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils\"\n-\t\"github.com/pkg/errors\"\n-\t\"gopkg.in/yaml.v3\"\n-)\n-\n-type File interface {\n-\tCopy(destPath string, removeSensitive bool) error\n-\tFilePath() string\n-}\n-\n-type SimpleFile struct {\n-\tPath string\n-}\n-\n-// Copy supports removeSensitive for other file types but for a simple file this doesn't do anything\n-func (s SimpleFile) Copy(destPath string, removeSensitive bool) error {\n-\t// simple copy easiest\n-\tif err := utils.CopyFile(s.FilePath(), destPath); err != nil {\n-\t\treturn errors.Wrapf(err, \"unable to copy file %s\", s.FilePath())\n-\t}\n-\treturn nil\n-}\n-\n-func (s SimpleFile) FilePath() string {\n-\treturn s.Path\n-}\n-\n-func NewFileFrame(name string, filePaths []string) FileFrame {\n-\ti := 0\n-\tfiles := make([]File, len(filePaths))\n-\tfor i, path := range filePaths {\n-\t\tfiles[i] = SimpleFile{\n-\t\t\tPath: path,\n-\t\t}\n-\t}\n-\treturn FileFrame{\n-\t\tname:  name,\n-\t\ti:     &i,\n-\t\tfiles: files,\n-\t}\n-}\n-\n-type FileFrame struct {\n-\tname  string\n-\ti     *int\n-\tfiles []File\n-}\n-\n-func (f FileFrame) Next() ([]interface{}, bool, error) {\n-\tif len(f.files) == *(f.i) {\n-\t\treturn nil, false, nil\n-\t}\n-\tfile := f.files[*f.i]\n-\t*f.i++\n-\tvalue := make([]interface{}, 1)\n-\tvalue[0] = file\n-\treturn value, true, nil\n-}\n-\n-func (f FileFrame) Columns() []string {\n-\treturn []string{\"files\"}\n-}\n-\n-func (f FileFrame) Name() string {\n-\treturn f.name\n-}\n-\n-// DirectoryFileFrame represents a set of files under a directory\n-type DirectoryFileFrame struct {\n-\tFileFrame\n-\tDirectory string\n-}\n-\n-func NewFileDirectoryFrame(directory string, exts []string) (DirectoryFileFrame, []error) {\n-\tfilePaths, errs := utils.ListFilesInDirectory(directory, exts)\n-\tfiles := make([]File, len(filePaths))\n-\tfor i, path := range filePaths {\n-\t\tfiles[i] = SimpleFile{\n-\t\t\tPath: path,\n-\t\t}\n-\t}\n-\ti := 0\n-\treturn DirectoryFileFrame{\n-\t\tDirectory: directory,\n-\t\tFileFrame: FileFrame{\n-\t\t\tfiles: files,\n-\t\t\ti:     &i,\n-\t\t},\n-\t}, errs\n-}\n-\n-func (f DirectoryFileFrame) Next() ([]interface{}, bool, error) {\n-\tif len(f.files) == *(f.i) {\n-\t\treturn nil, false, nil\n-\t}\n-\tfile := f.files[*f.i]\n-\t*f.i++\n-\tvalue := make([]interface{}, 1)\n-\tvalue[0] = file\n-\treturn value, true, nil\n-}\n-\n-func (f DirectoryFileFrame) Columns() []string {\n-\treturn []string{\"files\"}\n-}\n-\n-func (f DirectoryFileFrame) Name() string {\n-\treturn f.Directory\n-}\n-\n-type ConfigFile interface {\n-\tFile\n-\tFindLogPaths() ([]string, error)\n-\tFindIncludedConfig() (ConfigFile, error)\n-\tIsIncluded() bool\n-}\n-\n-type ConfigFileFrame struct {\n-\ti         *int\n-\tDirectory string\n-\tfiles     []ConfigFile\n-}\n-\n-func (f ConfigFileFrame) Next() ([]interface{}, bool, error) {\n-\tif len(f.files) == *(f.i) {\n-\t\treturn nil, false, nil\n-\t}\n-\tfile := f.files[*f.i]\n-\t*f.i++\n-\tvalue := make([]interface{}, 1)\n-\tvalue[0] = file\n-\treturn value, true, nil\n-}\n-\n-func (f ConfigFileFrame) Name() string {\n-\treturn f.Directory\n-}\n-\n-func NewConfigFileFrame(directory string) (ConfigFileFrame, []error) {\n-\tfiles, errs := utils.ListFilesInDirectory(directory, []string{\"*.xml\", \"*.yaml\", \"*.yml\"})\n-\t// we can't predict the length because of include files\n-\tvar configs []ConfigFile\n-\n-\tfor _, path := range files {\n-\t\tvar configFile ConfigFile\n-\t\tswitch ext := filepath.Ext(path); ext {\n-\t\tcase \".xml\":\n-\t\t\tconfigFile = XmlConfigFile{\n-\t\t\t\tPath:     path,\n-\t\t\t\tIncluded: false,\n-\t\t\t}\n-\t\tcase \".yml\":\n-\t\t\tconfigFile = YamlConfigFile{\n-\t\t\t\tPath:     path,\n-\t\t\t\tIncluded: false,\n-\t\t\t}\n-\t\tcase \".yaml\":\n-\t\t\tconfigFile = YamlConfigFile{\n-\t\t\t\tPath: path,\n-\t\t\t}\n-\t\t}\n-\t\tif configFile != nil {\n-\t\t\tconfigs = append(configs, configFile)\n-\t\t\t// add any included configs\n-\t\t\tiConf, err := configFile.FindIncludedConfig()\n-\t\t\tif err != nil {\n-\t\t\t\terrs = append(errs, err)\n-\t\t\t} else {\n-\t\t\t\tif iConf.FilePath() != \"\" {\n-\t\t\t\t\tconfigs = append(configs, iConf)\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\ti := 0\n-\n-\treturn ConfigFileFrame{\n-\t\ti:         &i,\n-\t\tDirectory: directory,\n-\t\tfiles:     configs,\n-\t}, errs\n-}\n-\n-func (f ConfigFileFrame) Columns() []string {\n-\treturn []string{\"config\"}\n-}\n-\n-func (f ConfigFileFrame) FindLogPaths() (logPaths []string, errors []error) {\n-\tfor _, configFile := range f.files {\n-\t\tpaths, err := configFile.FindLogPaths()\n-\t\tif err != nil {\n-\t\t\terrors = append(errors, err)\n-\t\t} else {\n-\t\t\tlogPaths = append(logPaths, paths...)\n-\t\t}\n-\t}\n-\treturn logPaths, errors\n-}\n-\n-type XmlConfigFile struct {\n-\tPath     string\n-\tIncluded bool\n-}\n-\n-// these patterns will be used to remove sensitive content - matches of the pattern will be replaced with the key\n-var xmlSensitivePatterns = map[string]*regexp.Regexp{\n-\t\"<password>Replaced</password>\":                       regexp.MustCompile(`<password>(.*)</password>`),\n-\t\"<password_sha256_hex>Replaced</password_sha256_hex>\": regexp.MustCompile(`<password_sha256_hex>(.*)</password_sha256_hex>`),\n-\t\"<secret_access_key>Replaced</secret_access_key>\":     regexp.MustCompile(`<secret_access_key>(.*)</secret_access_key>`),\n-\t\"<access_key_id>Replaced</access_key_id>\":             regexp.MustCompile(`<access_key_id>(.*)</access_key_id>`),\n-\t\"<secret>Replaced</secret>\":                           regexp.MustCompile(`<secret>(.*)</secret>`),\n-}\n-\n-func (x XmlConfigFile) Copy(destPath string, removeSensitive bool) error {\n-\tif !removeSensitive {\n-\t\t// simple copy easiest\n-\t\tif err := utils.CopyFile(x.FilePath(), destPath); err != nil {\n-\t\t\treturn errors.Wrapf(err, \"unable to copy file %s\", x.FilePath())\n-\t\t}\n-\t\treturn nil\n-\t}\n-\treturn sensitiveFileCopy(x.FilePath(), destPath, xmlSensitivePatterns)\n-}\n-\n-func (x XmlConfigFile) FilePath() string {\n-\treturn x.Path\n-}\n-\n-func (x XmlConfigFile) IsIncluded() bool {\n-\treturn x.Included\n-}\n-\n-type XmlLoggerConfig struct {\n-\tXMLName  xml.Name `xml:\"logger\"`\n-\tErrorLog string   `xml:\"errorlog\"`\n-\tLog      string   `xml:\"log\"`\n-}\n-\n-type YandexXMLConfig struct {\n-\tXMLName     xml.Name        `xml:\"yandex\"`\n-\tClickhouse  XmlLoggerConfig `xml:\"logger\"`\n-\tIncludeFrom string          `xml:\"include_from\"`\n-}\n-\n-type XmlConfig struct {\n-\tXMLName     xml.Name        `xml:\"clickhouse\"`\n-\tClickhouse  XmlLoggerConfig `xml:\"logger\"`\n-\tIncludeFrom string          `xml:\"include_from\"`\n-}\n-\n-func (x XmlConfigFile) UnmarshallConfig() (XmlConfig, error) {\n-\tinputFile, err := ioutil.ReadFile(x.Path)\n-\n-\tif err != nil {\n-\t\treturn XmlConfig{}, err\n-\t}\n-\tvar cConfig XmlConfig\n-\terr = xml.Unmarshal(inputFile, &cConfig)\n-\tif err == nil {\n-\t\treturn XmlConfig{\n-\t\t\tClickhouse:  cConfig.Clickhouse,\n-\t\t\tIncludeFrom: cConfig.IncludeFrom,\n-\t\t}, nil\n-\t}\n-\t// attempt to marshall as yandex file\n-\tvar yConfig YandexXMLConfig\n-\terr = xml.Unmarshal(inputFile, &yConfig)\n-\tif err != nil {\n-\t\treturn XmlConfig{}, err\n-\t}\n-\treturn XmlConfig{\n-\t\tClickhouse:  yConfig.Clickhouse,\n-\t\tIncludeFrom: yConfig.IncludeFrom,\n-\t}, nil\n-}\n-\n-func (x XmlConfigFile) FindLogPaths() ([]string, error) {\n-\tvar paths []string\n-\tconfig, err := x.UnmarshallConfig()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif config.Clickhouse.Log != \"\" {\n-\t\tpaths = append(paths, config.Clickhouse.Log)\n-\t}\n-\tif config.Clickhouse.ErrorLog != \"\" {\n-\t\tpaths = append(paths, config.Clickhouse.ErrorLog)\n-\t}\n-\n-\treturn paths, nil\n-}\n-\n-func (x XmlConfigFile) FindIncludedConfig() (ConfigFile, error) {\n-\tif x.Included {\n-\t\t//can't recurse\n-\t\treturn XmlConfigFile{}, nil\n-\t}\n-\tconfig, err := x.UnmarshallConfig()\n-\tif err != nil {\n-\t\treturn XmlConfigFile{}, err\n-\t}\n-\t// we need to convert this\n-\tif config.IncludeFrom != \"\" {\n-\t\tif filepath.IsAbs(config.IncludeFrom) {\n-\t\t\treturn XmlConfigFile{Path: config.IncludeFrom, Included: true}, nil\n-\t\t}\n-\t\tconfDir := filepath.Dir(x.FilePath())\n-\t\treturn XmlConfigFile{Path: path.Join(confDir, config.IncludeFrom), Included: true}, nil\n-\t}\n-\treturn XmlConfigFile{}, nil\n-}\n-\n-type YamlConfigFile struct {\n-\tPath     string\n-\tIncluded bool\n-}\n-\n-var ymlSensitivePatterns = map[string]*regexp.Regexp{\n-\t\"password: 'Replaced'\":            regexp.MustCompile(`password:\\s*.*$`),\n-\t\"password_sha256_hex: 'Replaced'\": regexp.MustCompile(`password_sha256_hex:\\s*.*$`),\n-\t\"access_key_id: 'Replaced'\":       regexp.MustCompile(`access_key_id:\\s*.*$`),\n-\t\"secret_access_key: 'Replaced'\":   regexp.MustCompile(`secret_access_key:\\s*.*$`),\n-\t\"secret: 'Replaced'\":              regexp.MustCompile(`secret:\\s*.*$`),\n-}\n-\n-func (y YamlConfigFile) Copy(destPath string, removeSensitive bool) error {\n-\tif !removeSensitive {\n-\t\t// simple copy easiest\n-\t\tif err := utils.CopyFile(y.FilePath(), destPath); err != nil {\n-\t\t\treturn errors.Wrapf(err, \"unable to copy file %s\", y.FilePath())\n-\t\t}\n-\t\treturn nil\n-\t}\n-\treturn sensitiveFileCopy(y.FilePath(), destPath, ymlSensitivePatterns)\n-}\n-\n-func (y YamlConfigFile) FilePath() string {\n-\treturn y.Path\n-}\n-\n-func (y YamlConfigFile) IsIncluded() bool {\n-\treturn y.Included\n-}\n-\n-type YamlLoggerConfig struct {\n-\tLog      string\n-\tErrorLog string\n-}\n-\n-type YamlConfig struct {\n-\tLogger       YamlLoggerConfig\n-\tInclude_From string\n-}\n-\n-func (y YamlConfigFile) FindLogPaths() ([]string, error) {\n-\tvar paths []string\n-\tinputFile, err := ioutil.ReadFile(y.Path)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tvar config YamlConfig\n-\terr = yaml.Unmarshal(inputFile, &config)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif config.Logger.Log != \"\" {\n-\t\tpaths = append(paths, config.Logger.Log)\n-\t}\n-\tif config.Logger.ErrorLog != \"\" {\n-\t\tpaths = append(paths, config.Logger.ErrorLog)\n-\t}\n-\treturn paths, nil\n-}\n-\n-func (y YamlConfigFile) FindIncludedConfig() (ConfigFile, error) {\n-\tif y.Included {\n-\t\t//can't recurse\n-\t\treturn YamlConfigFile{}, nil\n-\t}\n-\tinputFile, err := ioutil.ReadFile(y.Path)\n-\tif err != nil {\n-\t\treturn YamlConfigFile{}, err\n-\t}\n-\tvar config YamlConfig\n-\terr = yaml.Unmarshal(inputFile, &config)\n-\tif err != nil {\n-\t\treturn YamlConfigFile{}, err\n-\t}\n-\tif config.Include_From != \"\" {\n-\t\tif filepath.IsAbs(config.Include_From) {\n-\t\t\treturn YamlConfigFile{Path: config.Include_From, Included: true}, nil\n-\t\t}\n-\t\tconfDir := filepath.Dir(y.FilePath())\n-\t\treturn YamlConfigFile{Path: path.Join(confDir, config.Include_From), Included: true}, nil\n-\t}\n-\treturn YamlConfigFile{}, nil\n-}\n-\n-func sensitiveFileCopy(sourcePath string, destPath string, patterns map[string]*regexp.Regexp) error {\n-\tdestDir := filepath.Dir(destPath)\n-\tif err := os.MkdirAll(destDir, os.ModePerm); err != nil {\n-\t\treturn errors.Wrapf(err, \"unable to create directory %s\", destDir)\n-\t}\n-\t// currently, we don't unmarshall into a struct - we want to preserve structure and comments. Possibly could\n-\t// be handled but for simplicity we do a line parse for now\n-\tinputFile, err := os.Open(sourcePath)\n-\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer inputFile.Close()\n-\toutputFile, err := os.Create(destPath)\n-\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer outputFile.Close()\n-\twriter := bufio.NewWriter(outputFile)\n-\tscanner := bufio.NewScanner(inputFile)\n-\n-\tfor scanner.Scan() {\n-\t\tline := scanner.Text()\n-\t\tfor repl, pattern := range patterns {\n-\t\t\tline = pattern.ReplaceAllString(line, repl)\n-\t\t}\n-\t\t_, err = writer.WriteString(line + \"\\n\")\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\twriter.Flush()\n-\treturn nil\n-}\ndiff --git a/programs/diagnostics/internal/platform/data/frame.go b/programs/diagnostics/internal/platform/data/frame.go\ndeleted file mode 100644\nindex 659784301095..000000000000\n--- a/programs/diagnostics/internal/platform/data/frame.go\n+++ /dev/null\n@@ -1,11 +0,0 @@\n-package data\n-\n-type BaseFrame struct {\n-\tName string\n-}\n-\n-type Frame interface {\n-\tNext() ([]interface{}, bool, error)\n-\tColumns() []string\n-\tName() string\n-}\ndiff --git a/programs/diagnostics/internal/platform/data/memory.go b/programs/diagnostics/internal/platform/data/memory.go\ndeleted file mode 100644\nindex 25da25cf2510..000000000000\n--- a/programs/diagnostics/internal/platform/data/memory.go\n+++ /dev/null\n@@ -1,35 +0,0 @@\n-package data\n-\n-type MemoryFrame struct {\n-\ti           *int\n-\tColumnNames []string\n-\tRows        [][]interface{}\n-\tname        string\n-}\n-\n-func NewMemoryFrame(name string, columns []string, rows [][]interface{}) MemoryFrame {\n-\ti := 0\n-\treturn MemoryFrame{\n-\t\ti:           &i,\n-\t\tRows:        rows,\n-\t\tColumnNames: columns,\n-\t\tname:        name,\n-\t}\n-}\n-\n-func (f MemoryFrame) Next() ([]interface{}, bool, error) {\n-\tif f.Rows == nil || len(f.Rows) == *(f.i) {\n-\t\treturn nil, false, nil\n-\t}\n-\tvalue := f.Rows[*f.i]\n-\t*f.i++\n-\treturn value, true, nil\n-}\n-\n-func (f MemoryFrame) Columns() []string {\n-\treturn f.ColumnNames\n-}\n-\n-func (f MemoryFrame) Name() string {\n-\treturn f.name\n-}\ndiff --git a/programs/diagnostics/internal/platform/data/misc.go b/programs/diagnostics/internal/platform/data/misc.go\ndeleted file mode 100644\nindex a03213c4f46d..000000000000\n--- a/programs/diagnostics/internal/platform/data/misc.go\n+++ /dev/null\n@@ -1,27 +0,0 @@\n-package data\n-\n-func NewHierarchicalFrame(name string, frame Frame, subFrames []HierarchicalFrame) HierarchicalFrame {\n-\treturn HierarchicalFrame{\n-\t\tname:      name,\n-\t\tDataFrame: frame,\n-\t\tSubFrames: subFrames,\n-\t}\n-}\n-\n-type HierarchicalFrame struct {\n-\tname      string\n-\tDataFrame Frame\n-\tSubFrames []HierarchicalFrame\n-}\n-\n-func (hf HierarchicalFrame) Name() string {\n-\treturn hf.name\n-}\n-\n-func (hf HierarchicalFrame) Columns() []string {\n-\treturn hf.DataFrame.Columns()\n-}\n-\n-func (hf HierarchicalFrame) Next() ([]interface{}, bool, error) {\n-\treturn hf.DataFrame.Next()\n-}\ndiff --git a/programs/diagnostics/internal/platform/database/native.go b/programs/diagnostics/internal/platform/database/native.go\ndeleted file mode 100644\nindex 45b9af0349e2..000000000000\n--- a/programs/diagnostics/internal/platform/database/native.go\n+++ /dev/null\n@@ -1,95 +0,0 @@\n-package database\n-\n-import (\n-\t\"database/sql\"\n-\t\"fmt\"\n-\t\"net/url\"\n-\t\"strings\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t_ \"github.com/ClickHouse/clickhouse-go/v2\"\n-\t\"github.com/pkg/errors\"\n-)\n-\n-type ClickhouseNativeClient struct {\n-\thost       string\n-\tconnection *sql.DB\n-}\n-\n-func NewNativeClient(host string, port uint16, username string, password string) (*ClickhouseNativeClient, error) {\n-\t// debug output ?debug=true\n-\tconnection, err := sql.Open(\"clickhouse\", fmt.Sprintf(\"clickhouse://%s:%s@%s:%d/\", url.QueryEscape(username), url.QueryEscape(password), host, port))\n-\tif err != nil {\n-\t\treturn &ClickhouseNativeClient{}, err\n-\t}\n-\tif err := connection.Ping(); err != nil {\n-\t\treturn &ClickhouseNativeClient{}, err\n-\t}\n-\treturn &ClickhouseNativeClient{\n-\t\thost:       host,\n-\t\tconnection: connection,\n-\t}, nil\n-}\n-\n-func (c *ClickhouseNativeClient) Ping() error {\n-\treturn c.connection.Ping()\n-}\n-\n-func (c *ClickhouseNativeClient) ReadTable(databaseName string, tableName string, excludeColumns []string, orderBy data.OrderBy, limit int64) (data.Frame, error) {\n-\texceptClause := \"\"\n-\tif len(excludeColumns) > 0 {\n-\t\texceptClause = fmt.Sprintf(\"EXCEPT(%s) \", strings.Join(excludeColumns, \",\"))\n-\t}\n-\tlimitClause := \"\"\n-\tif limit >= 0 {\n-\t\tlimitClause = fmt.Sprintf(\" LIMIT %d\", limit)\n-\t}\n-\trows, err := c.connection.Query(fmt.Sprintf(\"SELECT * %sFROM %s.%s%s%s\", exceptClause, databaseName, tableName, orderBy.String(), limitClause))\n-\tif err != nil {\n-\t\treturn data.DatabaseFrame{}, err\n-\t}\n-\treturn data.NewDatabaseFrame(fmt.Sprintf(\"%s.%s\", databaseName, tableName), rows)\n-}\n-\n-func (c *ClickhouseNativeClient) ReadTableNamesForDatabase(databaseName string) ([]string, error) {\n-\trows, err := c.connection.Query(fmt.Sprintf(\"SHOW TABLES FROM %s\", databaseName))\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tdefer rows.Close()\n-\tvar tableNames []string\n-\tvar name string\n-\tfor rows.Next() {\n-\t\tif err := rows.Scan(&name); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\ttableNames = append(tableNames, name)\n-\t}\n-\treturn tableNames, nil\n-}\n-\n-func (c *ClickhouseNativeClient) ExecuteStatement(id string, statement string) (data.Frame, error) {\n-\trows, err := c.connection.Query(statement)\n-\tif err != nil {\n-\t\treturn data.DatabaseFrame{}, err\n-\t}\n-\treturn data.NewDatabaseFrame(id, rows)\n-}\n-\n-func (c *ClickhouseNativeClient) Version() (string, error) {\n-\tframe, err := c.ExecuteStatement(\"version\", \"SELECT version() as version\")\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\tvalues, ok, err := frame.Next()\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\tif !ok {\n-\t\treturn \"\", errors.New(\"unable to read ClickHouse version\")\n-\t}\n-\tif len(values) != 1 {\n-\t\treturn \"\", errors.New(\"unable to read ClickHouse version - no rows returned\")\n-\t}\n-\treturn values[0].(string), nil\n-}\ndiff --git a/programs/diagnostics/internal/platform/manager.go b/programs/diagnostics/internal/platform/manager.go\ndeleted file mode 100644\nindex b4435b62ea2f..000000000000\n--- a/programs/diagnostics/internal/platform/manager.go\n+++ /dev/null\n@@ -1,49 +0,0 @@\n-package platform\n-\n-import (\n-\t\"errors\"\n-\t\"sync\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/database\"\n-)\n-\n-var once sync.Once\n-var dbInit sync.Once\n-\n-// manages all resources that collectors and outputs may wish to ensure inc. db connections\n-\n-type DBClient interface {\n-\tReadTableNamesForDatabase(databaseName string) ([]string, error)\n-\tReadTable(databaseName string, tableName string, excludeColumns []string, orderBy data.OrderBy, limit int64) (data.Frame, error)\n-\tExecuteStatement(id string, statement string) (data.Frame, error)\n-\tVersion() (string, error)\n-}\n-\n-var manager *ResourceManager\n-\n-type ResourceManager struct {\n-\tDbClient DBClient\n-}\n-\n-func GetResourceManager() *ResourceManager {\n-\tonce.Do(func() {\n-\t\tmanager = &ResourceManager{}\n-\t})\n-\treturn manager\n-}\n-\n-func (m *ResourceManager) Connect(host string, port uint16, username string, password string) error {\n-\tvar err error\n-\tvar clientInstance DBClient\n-\tinit := false\n-\tdbInit.Do(func() {\n-\t\tclientInstance, err = database.NewNativeClient(host, port, username, password)\n-\t\tmanager.DbClient = clientInstance\n-\t\tinit = true\n-\t})\n-\tif !init {\n-\t\treturn errors.New(\"connect can only be called once\")\n-\t}\n-\treturn err\n-}\ndiff --git a/programs/diagnostics/internal/platform/utils/file.go b/programs/diagnostics/internal/platform/utils/file.go\ndeleted file mode 100644\nindex 71af4b32658b..000000000000\n--- a/programs/diagnostics/internal/platform/utils/file.go\n+++ /dev/null\n@@ -1,95 +0,0 @@\n-package utils\n-\n-import (\n-\t\"fmt\"\n-\t\"io\"\n-\t\"io/fs\"\n-\t\"os\"\n-\t\"path/filepath\"\n-\n-\t\"github.com/pkg/errors\"\n-)\n-\n-func FileExists(name string) (bool, error) {\n-\tf, err := os.Stat(name)\n-\tif err == nil {\n-\t\tif !f.IsDir() {\n-\t\t\treturn true, nil\n-\t\t}\n-\t\treturn false, fmt.Errorf(\"%s is a directory\", name)\n-\t}\n-\tif errors.Is(err, os.ErrNotExist) {\n-\t\treturn false, nil\n-\t}\n-\treturn false, err\n-}\n-\n-func DirExists(name string) (bool, error) {\n-\tf, err := os.Stat(name)\n-\tif err == nil {\n-\t\tif f.IsDir() {\n-\t\t\treturn true, nil\n-\t\t}\n-\t\treturn false, fmt.Errorf(\"%s is a file\", name)\n-\t}\n-\tif errors.Is(err, os.ErrNotExist) {\n-\t\treturn false, nil\n-\t}\n-\treturn false, err\n-}\n-\n-func CopyFile(sourceFilename string, destFilename string) error {\n-\texists, err := FileExists(sourceFilename)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tif !exists {\n-\t\treturn fmt.Errorf(\"%s does not exist\", sourceFilename)\n-\t}\n-\tsource, err := os.Open(sourceFilename)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer source.Close()\n-\tdestDir := filepath.Dir(destFilename)\n-\tif err := os.MkdirAll(destDir, os.ModePerm); err != nil {\n-\t\treturn errors.Wrapf(err, \"unable to create directory %s\", destDir)\n-\t}\n-\n-\tdestination, err := os.Create(destFilename)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer destination.Close()\n-\t_, err = io.Copy(destination, source)\n-\treturn err\n-}\n-\n-// patterns passed are an OR - any can be satisfied and the file will be listed\n-\n-func ListFilesInDirectory(directory string, patterns []string) ([]string, []error) {\n-\tvar files []string\n-\texists, err := DirExists(directory)\n-\tif err != nil {\n-\t\treturn files, []error{err}\n-\t}\n-\tif !exists {\n-\t\treturn files, []error{fmt.Errorf(\"directory %s does not exist\", directory)}\n-\t}\n-\tvar pathErrors []error\n-\t_ = filepath.Walk(directory, func(path string, info fs.FileInfo, err error) error {\n-\t\tif err != nil {\n-\t\t\tpathErrors = append(pathErrors, err)\n-\t\t} else if !info.IsDir() {\n-\t\t\tfor _, pattern := range patterns {\n-\t\t\t\tif matched, err := filepath.Match(pattern, filepath.Base(path)); err != nil {\n-\t\t\t\t\tpathErrors = append(pathErrors, err)\n-\t\t\t\t} else if matched {\n-\t\t\t\t\tfiles = append(files, path)\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\treturn nil\n-\t})\n-\treturn files, pathErrors\n-}\ndiff --git a/programs/diagnostics/internal/platform/utils/process.go b/programs/diagnostics/internal/platform/utils/process.go\ndeleted file mode 100644\nindex 7b27c215eeaa..000000000000\n--- a/programs/diagnostics/internal/platform/utils/process.go\n+++ /dev/null\n@@ -1,49 +0,0 @@\n-package utils\n-\n-import (\n-\t\"github.com/elastic/gosigar\"\n-\t\"strings\"\n-)\n-\n-func FindClickHouseProcesses() ([]gosigar.ProcArgs, error) {\n-\tpids := gosigar.ProcList{}\n-\terr := pids.Get()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tvar clickhousePs []gosigar.ProcArgs\n-\tfor _, pid := range pids.List {\n-\t\targs := gosigar.ProcArgs{}\n-\t\tif err := args.Get(pid); err != nil {\n-\t\t\tcontinue\n-\t\t}\n-\t\tif len(args.List) > 0 {\n-\t\t\tif strings.Contains(args.List[0], \"clickhouse-server\") {\n-\t\t\t\tclickhousePs = append(clickhousePs, args)\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn clickhousePs, nil\n-}\n-\n-func FindConfigsFromClickHouseProcesses() ([]string, error) {\n-\tclickhouseProcesses, err := FindClickHouseProcesses()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tvar configs []string\n-\tif len(clickhouseProcesses) > 0 {\n-\t\t// we have candidate matches\n-\t\tfor _, ps := range clickhouseProcesses {\n-\t\t\tfor _, arg := range ps.List {\n-\t\t\t\tif strings.Contains(arg, \"--config\") {\n-\t\t\t\t\tconfigFile := strings.ReplaceAll(arg, \"--config-file=\", \"\")\n-\t\t\t\t\t// containers receive config with --config\n-\t\t\t\t\tconfigFile = strings.ReplaceAll(configFile, \"--config=\", \"\")\n-\t\t\t\t\tconfigs = append(configs, configFile)\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn configs, err\n-}\ndiff --git a/programs/diagnostics/internal/platform/utils/slices.go b/programs/diagnostics/internal/platform/utils/slices.go\ndeleted file mode 100644\nindex cf5a5f97ce88..000000000000\n--- a/programs/diagnostics/internal/platform/utils/slices.go\n+++ /dev/null\n@@ -1,68 +0,0 @@\n-package utils\n-\n-// Intersection of elements in s1 and s2\n-func Intersection(s1, s2 []string) (inter []string) {\n-\thash := make(map[string]bool)\n-\tfor _, e := range s1 {\n-\t\thash[e] = false\n-\t}\n-\tfor _, e := range s2 {\n-\t\t// If elements present in the hashmap then append intersection list.\n-\t\tif val, ok := hash[e]; ok {\n-\t\t\tif !val {\n-\t\t\t\t// only add once\n-\t\t\t\tinter = append(inter, e)\n-\t\t\t\thash[e] = true\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn inter\n-}\n-\n-// Distinct returns elements in s1, not in s2\n-func Distinct(s1, s2 []string) (distinct []string) {\n-\thash := make(map[string]bool)\n-\tfor _, e := range s2 {\n-\t\thash[e] = true\n-\t}\n-\tfor _, e := range s1 {\n-\t\tif _, ok := hash[e]; !ok {\n-\t\t\tdistinct = append(distinct, e)\n-\t\t}\n-\t}\n-\treturn distinct\n-}\n-\n-// Unique func Unique(s1 []string) (unique []string) returns unique elements in s1\n-func Unique(s1 []string) (unique []string) {\n-\thash := make(map[string]bool)\n-\tfor _, e := range s1 {\n-\t\tif _, ok := hash[e]; !ok {\n-\t\t\tunique = append(unique, e)\n-\t\t}\n-\t\thash[e] = true\n-\t}\n-\treturn unique\n-}\n-\n-func Contains(s []string, e string) bool {\n-\tfor _, a := range s {\n-\t\tif a == e {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n-}\n-\n-func IndexOf(s []string, e string) int {\n-\tfor i, a := range s {\n-\t\tif a == e {\n-\t\t\treturn i\n-\t\t}\n-\t}\n-\treturn -1\n-}\n-\n-func Remove(slice []interface{}, s int) []interface{} {\n-\treturn append(slice[:s], slice[s+1:]...)\n-}\ndiff --git a/programs/diagnostics/internal/platform/utils/time.go b/programs/diagnostics/internal/platform/utils/time.go\ndeleted file mode 100644\nindex 622e92b873ae..000000000000\n--- a/programs/diagnostics/internal/platform/utils/time.go\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-package utils\n-\n-import \"time\"\n-\n-func MakeTimestamp() int64 {\n-\treturn time.Now().UnixNano() / int64(time.Millisecond)\n-}\ndiff --git a/programs/diagnostics/internal/runner.go b/programs/diagnostics/internal/runner.go\ndeleted file mode 100644\nindex 9386a1d178ba..000000000000\n--- a/programs/diagnostics/internal/runner.go\n+++ /dev/null\n@@ -1,115 +0,0 @@\n-package internal\n-\n-import (\n-\tc \"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors\"\n-\to \"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/pkg/errors\"\n-\t\"github.com/rs/zerolog/log\"\n-)\n-\n-type runConfiguration struct {\n-\tid               string\n-\thost             string\n-\tport             uint16\n-\tusername         string\n-\tpassword         string\n-\toutput           string\n-\tcollectors       []string\n-\tcollectorConfigs map[string]config.Configuration\n-\toutputConfig     config.Configuration\n-}\n-\n-func NewRunConfiguration(id string, host string, port uint16, username string, password string, output string, outputConfig config.Configuration,\n-\tcollectors []string, collectorConfigs map[string]config.Configuration) *runConfiguration {\n-\tconfig := runConfiguration{\n-\t\tid:               id,\n-\t\thost:             host,\n-\t\tport:             port,\n-\t\tusername:         username,\n-\t\tpassword:         password,\n-\t\tcollectors:       collectors,\n-\t\toutput:           output,\n-\t\tcollectorConfigs: collectorConfigs,\n-\t\toutputConfig:     outputConfig,\n-\t}\n-\treturn &config\n-}\n-\n-func Capture(config *runConfiguration) {\n-\tbundles, err := collect(config)\n-\tif err != nil {\n-\t\tlog.Fatal().Err(err).Msg(\"unable to perform collection\")\n-\t}\n-\tlog.Info().Msgf(\"collectors initialized\")\n-\tif err = output(config, bundles); err != nil {\n-\t\tlog.Fatal().Err(err).Msg(\"unable to create output\")\n-\t}\n-\tlog.Info().Msgf(\"bundle export complete\")\n-}\n-\n-func collect(config *runConfiguration) (map[string]*data.DiagnosticBundle, error) {\n-\tresourceManager := platform.GetResourceManager()\n-\terr := resourceManager.Connect(config.host, config.port, config.username, config.password)\n-\tif err != nil {\n-\t\t// if we can't connect this is fatal\n-\t\tlog.Fatal().Err(err).Msg(\"Unable to connect to database\")\n-\t}\n-\t//grab the required connectors - we pass what we can\n-\tbundles := make(map[string]*data.DiagnosticBundle)\n-\tlog.Info().Msgf(\"connection established\")\n-\t//these store our collection errors and will be output in the bundle\n-\tvar collectorErrors [][]interface{}\n-\tfor _, collectorName := range config.collectors {\n-\t\tcollectorConfig := config.collectorConfigs[collectorName]\n-\t\tlog.Info().Msgf(\"initializing %s collector\", collectorName)\n-\t\tcollector, err := c.GetCollectorByName(collectorName)\n-\t\tif err != nil {\n-\t\t\tlog.Error().Err(err).Msgf(\"Unable to fetch collector %s\", collectorName)\n-\t\t\tcollectorErrors = append(collectorErrors, []interface{}{err.Error()})\n-\t\t\tcontinue\n-\t\t}\n-\t\tbundle, err := collector.Collect(collectorConfig)\n-\t\tif err != nil {\n-\t\t\tlog.Error().Err(err).Msgf(\"Error in collector %s\", collectorName)\n-\t\t\tcollectorErrors = append(collectorErrors, []interface{}{err.Error()})\n-\t\t\t// this indicates a fatal error in the collector\n-\t\t\tcontinue\n-\t\t}\n-\t\tfor _, fError := range bundle.Errors.Errors {\n-\t\t\terr = errors.Wrapf(fError, \"Failure to collect frame in collector %s\", collectorName)\n-\t\t\tcollectorErrors = append(collectorErrors, []interface{}{err.Error()})\n-\t\t\tlog.Warn().Msg(err.Error())\n-\t\t}\n-\t\tbundles[collectorName] = bundle\n-\t}\n-\tbundles[\"diag_trace\"] = buildTraceBundle(collectorErrors)\n-\treturn bundles, nil\n-}\n-\n-func output(config *runConfiguration, bundles map[string]*data.DiagnosticBundle) error {\n-\tlog.Info().Msgf(\"attempting to export bundle using %s output...\", config.output)\n-\toutput, err := o.GetOutputByName(config.output)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tframeErrors, err := output.Write(config.id, bundles, config.outputConfig)\n-\t// we report over failing hard on frame errors - up to the output to determine what is fatal via error\n-\tfor _, fError := range frameErrors.Errors {\n-\t\tlog.Warn().Msgf(\"failure to write frame in output %s - %s\", config.output, fError)\n-\t}\n-\treturn err\n-}\n-\n-func buildTraceBundle(collectorErrors [][]interface{}) *data.DiagnosticBundle {\n-\terrorBundle := data.DiagnosticBundle{\n-\t\tFrames: map[string]data.Frame{\n-\t\t\t\"errors\": data.NewMemoryFrame(\"errors\", []string{\"errors\"}, collectorErrors),\n-\t\t},\n-\t\tErrors: data.FrameErrors{},\n-\t}\n-\t// add any other metrics from collection\n-\treturn &errorBundle\n-}\n",
  "test_patch": "diff --git a/programs/diagnostics/cmd/params/params_test.go b/programs/diagnostics/cmd/params/params_test.go\ndeleted file mode 100644\nindex 7671506ba596..000000000000\n--- a/programs/diagnostics/cmd/params/params_test.go\n+++ /dev/null\n@@ -1,247 +0,0 @@\n-package params_test\n-\n-import (\n-\t\"os\"\n-\t\"sort\"\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/cmd/params\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/spf13/cobra\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-var conf = map[string]config.Configuration{\n-\t\"config\": {\n-\t\tParams: []config.ConfigParam{\n-\t\t\tconfig.StringParam{\n-\t\t\t\tValue:      \"\",\n-\t\t\t\tParam:      config.NewParam(\"directory\", \"A directory\", false),\n-\t\t\t\tAllowEmpty: true,\n-\t\t\t},\n-\t\t},\n-\t},\n-\t\"system\": {\n-\t\tParams: []config.ConfigParam{\n-\t\t\tconfig.StringListParam{\n-\t\t\t\t// nil means include everything\n-\t\t\t\tValues: nil,\n-\t\t\t\tParam:  config.NewParam(\"include_tables\", \"Include tables\", false),\n-\t\t\t},\n-\t\t\tconfig.StringListParam{\n-\t\t\t\tValues: []string{\"distributed_ddl_queue\", \"query_thread_log\", \"query_log\", \"asynchronous_metric_log\", \"zookeeper\"},\n-\t\t\t\tParam:  config.NewParam(\"exclude_tables\", \"Excluded tables\", false),\n-\t\t\t},\n-\t\t\tconfig.IntParam{\n-\t\t\t\tValue: 100000,\n-\t\t\t\tParam: config.NewParam(\"row_limit\", \"Max rows\", false),\n-\t\t\t},\n-\t\t},\n-\t},\n-\t\"reader\": {\n-\t\tParams: []config.ConfigParam{\n-\t\t\tconfig.StringOptions{\n-\t\t\t\tValue:   \"csv\",\n-\t\t\t\tOptions: []string{\"csv\"},\n-\t\t\t\tParam:   config.NewParam(\"format\", \"Format of imported files\", false),\n-\t\t\t},\n-\t\t\tconfig.BoolParam{\n-\t\t\t\tValue: true,\n-\t\t\t\tParam: config.NewParam(\"collect_archives\", \"Collect archives\", false),\n-\t\t\t},\n-\t\t},\n-\t},\n-}\n-\n-func TestNewParamMap(t *testing.T) {\n-\t// test each of the types via NewParamMap - one with each type. the keys here can represent anything e.g. a collector name\n-\tt.Run(\"test param map correctly converts types\", func(t *testing.T) {\n-\t\tparamMap := params.NewParamMap(conf)\n-\t\trequire.Len(t, paramMap, 3)\n-\t\t// check config\n-\t\trequire.Contains(t, paramMap, \"config\")\n-\t\trequire.Len(t, paramMap[\"config\"], 1)\n-\t\trequire.Contains(t, paramMap[\"config\"], \"directory\")\n-\t\trequire.IsType(t, params.CliParam{}, paramMap[\"config\"][\"directory\"])\n-\t\trequire.Equal(t, \"A directory\", paramMap[\"config\"][\"directory\"].Description)\n-\t\trequire.Equal(t, \"\", *(paramMap[\"config\"][\"directory\"].Value.(*string)))\n-\t\trequire.Equal(t, \"\", paramMap[\"config\"][\"directory\"].Default)\n-\t\trequire.Equal(t, params.String, paramMap[\"config\"][\"directory\"].Type)\n-\t\t// check system\n-\t\trequire.Contains(t, paramMap, \"system\")\n-\t\trequire.Len(t, paramMap[\"system\"], 3)\n-\t\trequire.IsType(t, params.CliParam{}, paramMap[\"system\"][\"include_tables\"])\n-\n-\t\trequire.Equal(t, \"Include tables\", paramMap[\"system\"][\"include_tables\"].Description)\n-\t\tvar value []string\n-\t\trequire.Equal(t, &value, paramMap[\"system\"][\"include_tables\"].Value)\n-\t\trequire.Equal(t, value, paramMap[\"system\"][\"include_tables\"].Default)\n-\t\trequire.Equal(t, params.StringList, paramMap[\"system\"][\"include_tables\"].Type)\n-\n-\t\trequire.Equal(t, \"Excluded tables\", paramMap[\"system\"][\"exclude_tables\"].Description)\n-\t\trequire.IsType(t, params.CliParam{}, paramMap[\"system\"][\"exclude_tables\"])\n-\t\trequire.Equal(t, &value, paramMap[\"system\"][\"exclude_tables\"].Value)\n-\t\trequire.Equal(t, []string{\"distributed_ddl_queue\", \"query_thread_log\", \"query_log\", \"asynchronous_metric_log\", \"zookeeper\"}, paramMap[\"system\"][\"exclude_tables\"].Default)\n-\t\trequire.Equal(t, params.StringList, paramMap[\"system\"][\"exclude_tables\"].Type)\n-\n-\t\trequire.Equal(t, \"Max rows\", paramMap[\"system\"][\"row_limit\"].Description)\n-\t\trequire.IsType(t, params.CliParam{}, paramMap[\"system\"][\"row_limit\"])\n-\t\tvar iValue int64\n-\t\trequire.Equal(t, &iValue, paramMap[\"system\"][\"row_limit\"].Value)\n-\t\trequire.Equal(t, int64(100000), paramMap[\"system\"][\"row_limit\"].Default)\n-\t\trequire.Equal(t, params.Integer, paramMap[\"system\"][\"row_limit\"].Type)\n-\n-\t\t// check reader\n-\t\trequire.Contains(t, paramMap, \"reader\")\n-\t\trequire.Len(t, paramMap[\"reader\"], 2)\n-\t\trequire.IsType(t, params.CliParam{}, paramMap[\"reader\"][\"format\"])\n-\t\trequire.Equal(t, \"Format of imported files\", paramMap[\"reader\"][\"format\"].Description)\n-\t\trequire.IsType(t, params.CliParam{}, paramMap[\"reader\"][\"format\"])\n-\t\toValue := params.StringOptionsVar{\n-\t\t\tOptions: []string{\"csv\"},\n-\t\t\tValue:   \"csv\",\n-\t\t}\n-\t\trequire.Equal(t, &oValue, paramMap[\"reader\"][\"format\"].Value)\n-\t\trequire.Equal(t, \"csv\", paramMap[\"reader\"][\"format\"].Default)\n-\t\trequire.Equal(t, params.StringOptionsList, paramMap[\"reader\"][\"format\"].Type)\n-\n-\t\trequire.IsType(t, params.CliParam{}, paramMap[\"reader\"][\"collect_archives\"])\n-\t\trequire.Equal(t, \"Collect archives\", paramMap[\"reader\"][\"collect_archives\"].Description)\n-\t\trequire.IsType(t, params.CliParam{}, paramMap[\"reader\"][\"collect_archives\"])\n-\t\tvar bVar bool\n-\t\trequire.Equal(t, &bVar, paramMap[\"reader\"][\"collect_archives\"].Value)\n-\t\trequire.Equal(t, true, paramMap[\"reader\"][\"collect_archives\"].Default)\n-\t\trequire.Equal(t, params.Boolean, paramMap[\"reader\"][\"collect_archives\"].Type)\n-\n-\t})\n-\n-}\n-\n-//  test GetConfigParam\n-func TestConvertParamsToConfig(t *testing.T) {\n-\tparamMap := params.NewParamMap(conf)\n-\tt.Run(\"test we can convert a param map back to a config\", func(t *testing.T) {\n-\t\tcParam := params.ConvertParamsToConfig(paramMap)\n-\t\t// these will not be equal as we have some information loss e.g. allowEmpty\n-\t\t//require.Equal(t, conf, cParam)\n-\t\t// deep equality\n-\t\tfor name := range conf {\n-\t\t\trequire.Equal(t, len(conf[name].Params), len(cParam[name].Params))\n-\t\t\t// sort both consistently\n-\t\t\tsort.Slice(conf[name].Params, func(i, j int) bool {\n-\t\t\t\treturn conf[name].Params[i].Name() < conf[name].Params[j].Name()\n-\t\t\t})\n-\t\t\tsort.Slice(cParam[name].Params, func(i, j int) bool {\n-\t\t\t\treturn cParam[name].Params[i].Name() < cParam[name].Params[j].Name()\n-\t\t\t})\n-\t\t\tfor i, param := range conf[name].Params {\n-\t\t\t\trequire.Equal(t, param.Required(), cParam[name].Params[i].Required())\n-\t\t\t\trequire.Equal(t, param.Name(), cParam[name].Params[i].Name())\n-\t\t\t\trequire.Equal(t, param.Description(), cParam[name].Params[i].Description())\n-\t\t\t}\n-\t\t}\n-\t})\n-}\n-\n-// create via NewParamMap and add to command AddParamMapToCmd - check contents\n-func TestAddParamMapToCmd(t *testing.T) {\n-\tparamMap := params.NewParamMap(conf)\n-\tt.Run(\"test we can add hidden params to a command\", func(t *testing.T) {\n-\t\ttestComand := &cobra.Command{\n-\t\t\tUse:   \"test\",\n-\t\t\tShort: \"Run a test\",\n-\t\t\tLong:  `Longer description`,\n-\t\t\tRun: func(cmd *cobra.Command, args []string) {\n-\t\t\t\tos.Exit(0)\n-\t\t\t},\n-\t\t}\n-\t\tparams.AddParamMapToCmd(paramMap, testComand, \"collector\", true)\n-\t\t// check we get an error on one which doesn't exist\n-\t\t_, err := testComand.Flags().GetString(\"collector.config.random\")\n-\t\trequire.NotNil(t, err)\n-\t\t// check getting incorrect type\n-\t\t_, err = testComand.Flags().GetString(\"collector.system.include_tables\")\n-\t\trequire.NotNil(t, err)\n-\n-\t\t// check existence of all flags\n-\t\tdirectory, err := testComand.Flags().GetString(\"collector.config.directory\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, \"\", directory)\n-\n-\t\tincludeTables, err := testComand.Flags().GetStringSlice(\"collector.system.include_tables\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, []string{}, includeTables)\n-\n-\t\texcludeTables, err := testComand.Flags().GetStringSlice(\"collector.system.exclude_tables\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, []string{\"distributed_ddl_queue\", \"query_thread_log\", \"query_log\", \"asynchronous_metric_log\", \"zookeeper\"}, excludeTables)\n-\n-\t\trowLimit, err := testComand.Flags().GetInt64(\"collector.system.row_limit\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, int64(100000), rowLimit)\n-\n-\t\tformat, err := testComand.Flags().GetString(\"collector.reader.format\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, \"csv\", format)\n-\n-\t\tcollectArchives, err := testComand.Flags().GetBool(\"collector.reader.collect_archives\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, true, collectArchives)\n-\t})\n-}\n-\n-// test StringOptionsVar\n-func TestStringOptionsVar(t *testing.T) {\n-\n-\tt.Run(\"test we can set\", func(t *testing.T) {\n-\t\tformat := params.StringOptionsVar{\n-\t\t\tOptions: []string{\"csv\", \"tsv\", \"native\"},\n-\t\t\tValue:   \"csv\",\n-\t\t}\n-\t\trequire.Equal(t, \"csv\", format.String())\n-\t\terr := format.Set(\"tsv\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, \"tsv\", format.String())\n-\t})\n-\n-\tt.Run(\"test set invalid\", func(t *testing.T) {\n-\t\tformat := params.StringOptionsVar{\n-\t\t\tOptions: []string{\"csv\", \"tsv\", \"native\"},\n-\t\t\tValue:   \"csv\",\n-\t\t}\n-\t\trequire.Equal(t, \"csv\", format.String())\n-\t\terr := format.Set(\"random\")\n-\t\trequire.NotNil(t, err)\n-\t\trequire.Equal(t, \"random is not included in options: [csv tsv native]\", err.Error())\n-\t})\n-}\n-\n-// test StringSliceOptionsVar\n-func TestStringSliceOptionsVar(t *testing.T) {\n-\n-\tt.Run(\"test we can set\", func(t *testing.T) {\n-\t\tformats := params.StringSliceOptionsVar{\n-\t\t\tOptions: []string{\"csv\", \"tsv\", \"native\", \"qsv\"},\n-\t\t\tValues:  []string{\"csv\", \"tsv\"},\n-\t\t}\n-\t\trequire.Equal(t, \"[csv,tsv]\", formats.String())\n-\t\terr := formats.Set(\"tsv,native\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, \"[tsv,native]\", formats.String())\n-\t})\n-\n-\tt.Run(\"test set invalid\", func(t *testing.T) {\n-\t\tformats := params.StringSliceOptionsVar{\n-\t\t\tOptions: []string{\"csv\", \"tsv\", \"native\", \"qsv\"},\n-\t\t\tValues:  []string{\"csv\", \"tsv\"},\n-\t\t}\n-\t\trequire.Equal(t, \"[csv,tsv]\", formats.String())\n-\t\terr := formats.Set(\"tsv,random\")\n-\t\trequire.NotNil(t, err)\n-\t\trequire.Equal(t, \"[random] are not included in options: [csv tsv native qsv]\", err.Error())\n-\t\terr = formats.Set(\"msv,random\")\n-\t\trequire.NotNil(t, err)\n-\t\trequire.Equal(t, \"[msv random] are not included in options: [csv tsv native qsv]\", err.Error())\n-\t})\n-\n-}\ndiff --git a/programs/diagnostics/internal/collectors/clickhouse/config_test.go b/programs/diagnostics/internal/collectors/clickhouse/config_test.go\ndeleted file mode 100644\nindex 355cbb65620b..000000000000\n--- a/programs/diagnostics/internal/collectors/clickhouse/config_test.go\n+++ /dev/null\n@@ -1,128 +0,0 @@\n-package clickhouse_test\n-\n-import (\n-\t\"encoding/xml\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"os\"\n-\t\"path\"\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestConfigConfiguration(t *testing.T) {\n-\tt.Run(\"correct configuration is returned for config collector\", func(t *testing.T) {\n-\t\tconfigCollector := clickhouse.NewConfigCollector(&platform.ResourceManager{})\n-\t\tconf := configCollector.Configuration()\n-\t\trequire.Len(t, conf.Params, 1)\n-\t\t// check first param\n-\t\trequire.IsType(t, config.StringParam{}, conf.Params[0])\n-\t\tdirectory, ok := conf.Params[0].(config.StringParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, directory.Required())\n-\t\trequire.Equal(t, directory.Name(), \"directory\")\n-\t\trequire.Equal(t, \"\", directory.Value)\n-\t})\n-}\n-\n-func TestConfigCollect(t *testing.T) {\n-\tconfigCollector := clickhouse.NewConfigCollector(&platform.ResourceManager{})\n-\n-\tt.Run(\"test default file collector configuration\", func(t *testing.T) {\n-\t\tdiagSet, err := configCollector.Collect(config.Configuration{})\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, diagSet)\n-\t\t// we won't be able to collect the default configs preprocessed and default - even if clickhouse is installed\n-\t\t// these directories should not be readable under any permissions these tests are unrealistically executed!\n-\t\t// note: we may also pick up configs from a local clickhouse process - we thus allow a len >=2 but don't check this\n-\t\t// as its non-deterministic\n-\t\trequire.GreaterOrEqual(t, len(diagSet.Frames), 2)\n-\t\t// check default key\n-\t\trequire.Contains(t, diagSet.Frames, \"default\")\n-\t\trequire.Equal(t, diagSet.Frames[\"default\"].Name(), \"/etc/clickhouse-server/\")\n-\t\trequire.Equal(t, diagSet.Frames[\"default\"].Columns(), []string{\"config\"})\n-\t\t// collection will have failed\n-\t\tcheckFrame(t, diagSet.Frames[\"default\"], nil)\n-\t\t// check preprocessed key\n-\t\trequire.Contains(t, diagSet.Frames, \"preprocessed\")\n-\t\trequire.Equal(t, diagSet.Frames[\"preprocessed\"].Name(), \"/var/lib/clickhouse/preprocessed_configs\")\n-\t\trequire.Equal(t, diagSet.Frames[\"preprocessed\"].Columns(), []string{\"config\"})\n-\t\t// min of 2 - might be more if a local installation of clickhouse is running\n-\t\trequire.GreaterOrEqual(t, len(diagSet.Errors.Errors), 2)\n-\t})\n-\n-\tt.Run(\"test configuration when specified\", func(t *testing.T) {\n-\t\t// create some test files\n-\t\ttempDir := t.TempDir()\n-\t\tconfDir := path.Join(tempDir, \"conf\")\n-\t\t// create an includes file\n-\t\tincludesDir := path.Join(tempDir, \"includes\")\n-\t\terr := os.MkdirAll(includesDir, os.ModePerm)\n-\t\trequire.Nil(t, err)\n-\t\tincludesPath := path.Join(includesDir, \"random.xml\")\n-\t\tincludesFile, err := os.Create(includesPath)\n-\t\trequire.Nil(t, err)\n-\t\txmlWriter := io.Writer(includesFile)\n-\t\tenc := xml.NewEncoder(xmlWriter)\n-\t\tenc.Indent(\"  \", \"    \")\n-\t\txmlConfig := data.XmlConfig{\n-\t\t\tXMLName: xml.Name{},\n-\t\t\tClickhouse: data.XmlLoggerConfig{\n-\t\t\t\tXMLName:  xml.Name{},\n-\t\t\t\tErrorLog: \"/var/log/clickhouse-server/clickhouse-server.err.log\",\n-\t\t\t\tLog:      \"/var/log/clickhouse-server/clickhouse-server.log\",\n-\t\t\t},\n-\t\t\tIncludeFrom: \"\",\n-\t\t}\n-\t\terr = enc.Encode(xmlConfig)\n-\t\trequire.Nil(t, err)\n-\t\t// create 5 temporary config files - length is 6 for the included file\n-\t\trows := make([][]interface{}, 6)\n-\t\tfor i := 0; i < 5; i++ {\n-\t\t\tif i == 4 {\n-\t\t\t\t// set the includes for the last doc\n-\t\t\t\txmlConfig.IncludeFrom = includesPath\n-\t\t\t}\n-\t\t\t// we want to check hierarchies are walked so create a simple folder for each file\n-\t\t\tfileDir := path.Join(confDir, fmt.Sprintf(\"%d\", i))\n-\t\t\terr := os.MkdirAll(fileDir, os.ModePerm)\n-\t\t\trequire.Nil(t, err)\n-\t\t\tfilepath := path.Join(fileDir, fmt.Sprintf(\"random-%d.xml\", i))\n-\t\t\trow := make([]interface{}, 1)\n-\t\t\trow[0] = data.XmlConfigFile{Path: filepath}\n-\t\t\trows[i] = row\n-\t\t\txmlFile, err := os.Create(filepath)\n-\t\t\trequire.Nil(t, err)\n-\t\t\t// write a little xml so its valid\n-\t\t\txmlConfig := xmlConfig\n-\t\t\txmlWriter := io.Writer(xmlFile)\n-\t\t\tenc := xml.NewEncoder(xmlWriter)\n-\t\t\tenc.Indent(\"  \", \"    \")\n-\t\t\terr = enc.Encode(xmlConfig)\n-\t\t\trequire.Nil(t, err)\n-\t\t}\n-\t\tdiagSet, err := configCollector.Collect(config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue: confDir,\n-\t\t\t\t\tParam: config.NewParam(\"directory\", \"File locations\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t})\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, diagSet)\n-\t\trequire.Len(t, diagSet.Frames, 1)\n-\t\trequire.Contains(t, diagSet.Frames, \"user_specified\")\n-\t\trequire.Equal(t, diagSet.Frames[\"user_specified\"].Name(), confDir)\n-\t\trequire.Equal(t, diagSet.Frames[\"user_specified\"].Columns(), []string{\"config\"})\n-\t\tiConf := make([]interface{}, 1)\n-\t\tiConf[0] = data.XmlConfigFile{Path: includesPath, Included: true}\n-\t\trows[5] = iConf\n-\t\tcheckFrame(t, diagSet.Frames[\"user_specified\"], rows)\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/collectors/clickhouse/db_logs_test.go b/programs/diagnostics/internal/collectors/clickhouse/db_logs_test.go\ndeleted file mode 100644\nindex 3fc585f3352d..000000000000\n--- a/programs/diagnostics/internal/collectors/clickhouse/db_logs_test.go\n+++ /dev/null\n@@ -1,119 +0,0 @@\n-package clickhouse_test\n-\n-import (\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestDbLogsConfiguration(t *testing.T) {\n-\tt.Run(\"correct configuration is returned for summary collector\", func(t *testing.T) {\n-\t\tclient := test.NewFakeClickhouseClient(make(map[string][]string))\n-\t\tdbLogsCollector := clickhouse.NewDBLogsCollector(&platform.ResourceManager{\n-\t\t\tDbClient: client,\n-\t\t})\n-\t\tconf := dbLogsCollector.Configuration()\n-\t\trequire.Len(t, conf.Params, 1)\n-\t\trequire.IsType(t, config.IntParam{}, conf.Params[0])\n-\t\trowLimit, ok := conf.Params[0].(config.IntParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, rowLimit.Required())\n-\t\trequire.Equal(t, rowLimit.Name(), \"row_limit\")\n-\t\trequire.Equal(t, int64(100000), rowLimit.Value)\n-\t})\n-}\n-\n-func TestDbLogsCollect(t *testing.T) {\n-\tclient := test.NewFakeClickhouseClient(make(map[string][]string))\n-\tdbLogsCollector := clickhouse.NewDBLogsCollector(&platform.ResourceManager{\n-\t\tDbClient: client,\n-\t})\n-\tqueryLogColumns := []string{\"type\", \"event_date\", \"event_time\", \"event_time_microseconds\",\n-\t\t\"query_start_time\", \"query_start_time_microseconds\", \"query_duration_ms\", \"read_rows\", \"read_bytes\", \"written_rows\", \"written_bytes\",\n-\t\t\"result_rows\", \"result_bytes\", \"memory_usage\", \"current_database\", \"query\", \"formatted_query\", \"normalized_query_hash\",\n-\t\t\"query_kind\", \"databases\", \"tables\", \"columns\", \"projections\", \"views\", \"exception_code\", \"exception\", \"stack_trace\",\n-\t\t\"is_initial_query\", \"user\", \"query_id\", \"address\", \"port\", \"initial_user\", \"initial_query_id\", \"initial_address\", \"initial_port\",\n-\t\t\"initial_query_start_time\", \"initial_query_start_time_microseconds\", \"interface\", \"os_user\", \"client_hostname\", \"client_name\",\n-\t\t\"client_revision\", \"client_version_major\", \"client_version_minor\", \"client_version_patch\", \"http_method\", \"http_user_agent\",\n-\t\t\"http_referer\", \"forwarded_for\", \"quota_key\", \"revision\", \"log_comment\", \"thread_ids\", \"ProfileEvents\", \"Settings\",\n-\t\t\"used_aggregate_functions\", \"used_aggregate_function_combinators\", \"used_database_engines\", \"used_data_type_families\",\n-\t\t\"used_dictionaries\", \"used_formats\", \"used_functions\", \"used_storages\", \"used_table_functions\"}\n-\tqueryLogFrame := test.NewFakeDataFrame(\"queryLog\", queryLogColumns,\n-\t\t[][]interface{}{\n-\t\t\t{\"QueryStart\", \"2021-12-13\", \"2021-12-13 12:53:20\", \"2021-12-13 12:53:20.590579\", \"2021-12-13 12:53:20\", \"2021-12-13 12:53:20.590579\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"default\", \"SELECT DISTINCT arrayJoin(extractAll(name, '[\\\\w_]{2,}')) AS res FROM (SELECT name FROM system.functions UNION ALL SELECT name FROM system.table_engines UNION ALL SELECT name FROM system.formats UNION ALL SELECT name FROM system.table_functions UNION ALL SELECT name FROM system.data_type_families UNION ALL SELECT name FROM system.merge_tree_settings UNION ALL SELECT name FROM system.settings UNION ALL SELECT cluster FROM system.clusters UNION ALL SELECT macro FROM system.macros UNION ALL SELECT policy_name FROM system.storage_policies UNION ALL SELECT concat(func.name, comb.name) FROM system.functions AS func CROSS JOIN system.aggregate_function_combinators AS comb WHERE is_aggregate UNION ALL SELECT name FROM system.databases LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.tables LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.dictionaries LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.columns LIMIT 10000) WHERE notEmpty(res)\", \"\", \"6666026786019643712\", \"Select\", \"['system']\", \"['system.aggregate_function_combinators','system.clusters','system.columns','system.data_type_families','system.databases','system.dictionaries','system.formats','system.functions','system.macros','system.merge_tree_settings','system.settings','system.storage_policies','system.table_engines','system.table_functions','system.tables']\", \"['system.aggregate_function_combinators.name','system.clusters.cluster','system.columns.name','system.data_type_families.name','system.databases.name','system.dictionaries.name','system.formats.name','system.functions.is_aggregate','system.functions.name','system.macros.macro','system.merge_tree_settings.name','system.settings.name','system.storage_policies.policy_name','system.table_engines.name','system.table_functions.name','system.tables.name']\", \"[]\", \"[]\", \"0\", \"\", \"\", \"1\", \"default\", \"3b5feb6d-3086-4718-adb2-17464988ff12\", \"::ffff:127.0.0.1\", \"50920\", \"default\", \"3b5feb6d-3086-4718-adb2-17464988ff12\", \"::ffff:127.0.0.1\", \"50920\", \"2021-12-13 12:53:30\", \"2021-12-13 12:53:30.590579\", \"1\", \"\", \"\", \"ClickHouse client\", \"54450\", \"21\", \"11\", \"0\", \"0\", \"\", \"\", \"\", \"\", \"54456\", \"\", \"[]\", \"{}\", \"{'load_balancing':'random','max_memory_usage':'10000000000'}\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\"},\n-\t\t\t{\"QueryFinish\", \"2021-12-13\", \"2021-12-13 12:53:30\", \"2021-12-13 12:53:30.607292\", \"2021-12-13 12:53:30\", \"2021-12-13 12:53:30.590579\", \"15\", \"4512\", \"255694\", \"0\", \"0\", \"4358\", \"173248\", \"4415230\", \"default\", \"SELECT DISTINCT arrayJoin(extractAll(name, '[\\\\w_]{2,}')) AS res FROM (SELECT name FROM system.functions UNION ALL SELECT name FROM system.table_engines UNION ALL SELECT name FROM system.formats UNION ALL SELECT name FROM system.table_functions UNION ALL SELECT name FROM system.data_type_families UNION ALL SELECT name FROM system.merge_tree_settings UNION ALL SELECT name FROM system.settings UNION ALL SELECT cluster FROM system.clusters UNION ALL SELECT macro FROM system.macros UNION ALL SELECT policy_name FROM system.storage_policies UNION ALL SELECT concat(func.name, comb.name) FROM system.functions AS func CROSS JOIN system.aggregate_function_combinators AS comb WHERE is_aggregate UNION ALL SELECT name FROM system.databases LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.tables LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.dictionaries LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.columns LIMIT 10000) WHERE notEmpty(res)\", \"\", \"6666026786019643712\", \"Select\", \"['system']\", \"['system.aggregate_function_combinators','system.clusters','system.columns','system.data_type_families','system.databases','system.dictionaries','system.formats','system.functions','system.macros','system.merge_tree_settings','system.settings','system.storage_policies','system.table_engines','system.table_functions','system.tables']\", \"['system.aggregate_function_combinators.name','system.clusters.cluster','system.columns.name','system.data_type_families.name','system.databases.name','system.dictionaries.name','system.formats.name','system.functions.is_aggregate','system.functions.name','system.macros.macro','system.merge_tree_settings.name','system.settings.name','system.storage_policies.policy_name','system.table_engines.name','system.table_functions.name','system.tables.name']\", \"[]\", \"[]\", \"0\", \"\", \"\", \"1\", \"default\", \"3b5feb6d-3086-4718-adb2-17464988ff12\", \"::ffff:127.0.0.1\", \"50920\", \"default\", \"3b5feb6d-3086-4718-adb2-17464988ff12\", \"::ffff:127.0.0.1\", \"50920\", \"2021-12-13 12:53:30\", \"2021-12-13 12:53:30.590579\", \"1\", \"\", \"\", \"ClickHouse client\", \"54450\", \"21\", \"11\", \"0\", \"0\", \"\", \"\", \"\", \"\", \"54456\", \"\", \"[95298,95315,95587,95316,95312,95589,95318,95586,95588,95585]\", \"{'Query':1,'SelectQuery':1,'ArenaAllocChunks':41,'ArenaAllocBytes':401408,'FunctionExecute':62,'NetworkSendElapsedMicroseconds':463,'NetworkSendBytes':88452,'SelectedRows':4512,'SelectedBytes':255694,'RegexpCreated':6,'ContextLock':411,'RWLockAcquiredReadLocks':190,'RealTimeMicroseconds':49221,'UserTimeMicroseconds':19811,'SystemTimeMicroseconds':2817,'SoftPageFaults':1128,'OSCPUWaitMicroseconds':127,'OSCPUVirtualTimeMicroseconds':22624,'OSWriteBytes':12288,'OSWriteChars':13312}\", \"{'load_balancing':'random','max_memory_usage':'10000000000'}\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\", \"['concat','notEmpty','extractAll']\", \"[]\", \"[]\"},\n-\t\t\t{\"QueryStart\", \"2021-12-13\", \"2021-12-13 13:02:53\", \"2021-12-13 13:02:53.419528\", \"2021-12-13 13:02:53\", \"2021-12-13 13:02:53.419528\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"default\", \"SELECT DISTINCT arrayJoin(extractAll(name, '[\\\\w_]{2,}')) AS res FROM (SELECT name FROM system.functions UNION ALL SELECT name FROM system.table_engines UNION ALL SELECT name FROM system.formats UNION ALL SELECT name FROM system.table_functions UNION ALL SELECT name FROM system.data_type_families UNION ALL SELECT name FROM system.merge_tree_settings UNION ALL SELECT name FROM system.settings UNION ALL SELECT cluster FROM system.clusters UNION ALL SELECT macro FROM system.macros UNION ALL SELECT policy_name FROM system.storage_policies UNION ALL SELECT concat(func.name, comb.name) FROM system.functions AS func CROSS JOIN system.aggregate_function_combinators AS comb WHERE is_aggregate UNION ALL SELECT name FROM system.databases LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.tables LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.dictionaries LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.columns LIMIT 10000) WHERE notEmpty(res)\", \"\", \"6666026786019643712\", \"Select\", \"['system']\", \"['system.aggregate_function_combinators','system.clusters','system.columns','system.data_type_families','system.databases','system.dictionaries','system.formats','system.functions','system.macros','system.merge_tree_settings','system.settings','system.storage_policies','system.table_engines','system.table_functions','system.tables']\", \"['system.aggregate_function_combinators.name','system.clusters.cluster','system.columns.name','system.data_type_families.name','system.databases.name','system.dictionaries.name','system.formats.name','system.functions.is_aggregate','system.functions.name','system.macros.macro','system.merge_tree_settings.name','system.settings.name','system.storage_policies.policy_name','system.table_engines.name','system.table_functions.name','system.tables.name']\", \"[]\", \"[]\", \"0\", \"\", \"\", \"1\", \"default\", \"351b58e4-6128-47d4-a7b8-03d78c1f84c6\", \"::ffff:127.0.0.1\", \"50968\", \"default\", \"351b58e4-6128-47d4-a7b8-03d78c1f84c6\", \"::ffff:127.0.0.1\", \"50968\", \"2021-12-13 13:02:53\", \"2021-12-13 13:02:53.419528\", \"1\", \"\", \"\", \"ClickHouse client\", \"54450\", \"21\", \"11\", \"0\", \"0\", \"\", \"\", \"\", \"\", \"54456\", \"\", \"[]\", \"{}\", \"{'load_balancing':'random','max_memory_usage':'10000000000'}\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\"},\n-\t\t\t{\"QueryFinish\", \"2021-12-13\", \"2021-12-13 13:02:56\", \"2021-12-13 13:02:56.437115\", \"2021-12-13 13:02:56\", \"2021-12-13 13:02:56.419528\", \"16\", \"4629\", \"258376\", \"0\", \"0\", \"4377\", \"174272\", \"4404694\", \"default\", \"SELECT DISTINCT arrayJoin(extractAll(name, '[\\\\w_]{2,}')) AS res FROM (SELECT name FROM system.functions UNION ALL SELECT name FROM system.table_engines UNION ALL SELECT name FROM system.formats UNION ALL SELECT name FROM system.table_functions UNION ALL SELECT name FROM system.data_type_families UNION ALL SELECT name FROM system.merge_tree_settings UNION ALL SELECT name FROM system.settings UNION ALL SELECT cluster FROM system.clusters UNION ALL SELECT macro FROM system.macros UNION ALL SELECT policy_name FROM system.storage_policies UNION ALL SELECT concat(func.name, comb.name) FROM system.functions AS func CROSS JOIN system.aggregate_function_combinators AS comb WHERE is_aggregate UNION ALL SELECT name FROM system.databases LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.tables LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.dictionaries LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.columns LIMIT 10000) WHERE notEmpty(res)\", \"\", \"6666026786019643712\", \"Select\", \"['system']\", \"['system.aggregate_function_combinators','system.clusters','system.columns','system.data_type_families','system.databases','system.dictionaries','system.formats','system.functions','system.macros','system.merge_tree_settings','system.settings','system.storage_policies','system.table_engines','system.table_functions','system.tables']\", \"['system.aggregate_function_combinators.name','system.clusters.cluster','system.columns.name','system.data_type_families.name','system.databases.name','system.dictionaries.name','system.formats.name','system.functions.is_aggregate','system.functions.name','system.macros.macro','system.merge_tree_settings.name','system.settings.name','system.storage_policies.policy_name','system.table_engines.name','system.table_functions.name','system.tables.name']\", \"[]\", \"[]\", \"0\", \"\", \"\", \"1\", \"default\", \"351b58e4-6128-47d4-a7b8-03d78c1f84c6\", \"::ffff:127.0.0.1\", \"50968\", \"default\", \"351b58e4-6128-47d4-a7b8-03d78c1f84c6\", \"::ffff:127.0.0.1\", \"50968\", \"2021-12-13 13:02:53\", \"2021-12-13 13:02:53.419528\", \"1\", \"\", \"\", \"ClickHouse client\", \"54450\", \"21\", \"11\", \"0\", \"0\", \"\", \"\", \"\", \"\", \"54456\", \"\", \"[95298,95318,95315,95316,95312,95588,95589,95586,95585,95587]\", \"{'Query':1,'SelectQuery':1,'ArenaAllocChunks':41,'ArenaAllocBytes':401408,'FunctionExecute':62,'NetworkSendElapsedMicroseconds':740,'NetworkSendBytes':88794,'SelectedRows':4629,'SelectedBytes':258376,'ContextLock':411,'RWLockAcquiredReadLocks':194,'RealTimeMicroseconds':52469,'UserTimeMicroseconds':17179,'SystemTimeMicroseconds':4218,'SoftPageFaults':569,'OSCPUWaitMicroseconds':303,'OSCPUVirtualTimeMicroseconds':25087,'OSWriteBytes':12288,'OSWriteChars':12288}\", \"{'load_balancing':'random','max_memory_usage':'10000000000'}\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\", \"['concat','notEmpty','extractAll']\", \"[]\", \"[]\"},\n-\t\t})\n-\n-\tclient.QueryResponses[\"SELECT * FROM system.query_log ORDER BY event_time_microseconds ASC LIMIT 100000\"] = &queryLogFrame\n-\n-\ttextLogColumns := []string{\"event_date\", \"event_time\", \"event_time_microseconds\", \"microseconds\", \"thread_name\", \"thread_id\", \"level\", \"query_id\", \"logger_name\", \"message\", \"revision\", \"source_file\", \"source_line\"}\n-\ttextLogFrame := test.NewFakeDataFrame(\"textLog\", textLogColumns,\n-\t\t[][]interface{}{\n-\t\t\t{\"2022-02-03\", \"2022-02-03 16:17:47\", \"2022-02-03 16:37:17.056950\", \"56950\", \"clickhouse-serv\", \"68947\", \"Information\", \"\", \"DNSCacheUpdater\", \"Update period 15 seconds\", \"54458\", \"../src/Interpreters/DNSCacheUpdater.cpp; void DB::DNSCacheUpdater::start()\", \"46\"},\n-\t\t\t{\"2022-02-03\", \"2022-02-03 16:27:47\", \"2022-02-03 16:37:27.057022\", \"57022\", \"clickhouse-serv\", \"68947\", \"Information\", \"\", \"Application\", \"Available RAM: 62.24 GiB; physical cores: 8; logical cores: 16.\", \"54458\", \"../programs/server/Server.cpp; virtual int DB::Server::main(const std::vector<std::string> &)\", \"1380\"},\n-\t\t\t{\"2022-02-03\", \"2022-02-03 16:37:47\", \"2022-02-03 16:37:37.057484\", \"57484\", \"clickhouse-serv\", \"68947\", \"Information\", \"\", \"Application\", \"Listening for http://[::1]:8123\", \"54458\", \"../programs/server/Server.cpp; virtual int DB::Server::main(const std::vector<std::string> &)\", \"1444\"},\n-\t\t\t{\"2022-02-03\", \"2022-02-03 16:47:47\", \"2022-02-03 16:37:47.057527\", \"57527\", \"clickhouse-serv\", \"68947\", \"Information\", \"\", \"Application\", \"Listening for native protocol (tcp): [::1]:9000\", \"54458\", \"../programs/server/Server.cpp; virtual int DB::Server::main(const std::vector<std::string> &)\", \"1444\"},\n-\t\t})\n-\n-\tclient.QueryResponses[\"SELECT * FROM system.text_log ORDER BY event_time_microseconds ASC LIMIT 100000\"] = &textLogFrame\n-\n-\t// skip query_thread_log frame - often it doesn't exist anyway unless enabled\n-\tt.Run(\"test default db logs collection\", func(t *testing.T) {\n-\t\tbundle, errs := dbLogsCollector.Collect(config.Configuration{})\n-\t\trequire.Empty(t, errs)\n-\t\trequire.NotNil(t, bundle)\n-\t\trequire.Len(t, bundle.Frames, 2)\n-\t\trequire.Contains(t, bundle.Frames, \"text_log\")\n-\t\trequire.Contains(t, bundle.Frames, \"query_log\")\n-\t\trequire.Len(t, bundle.Errors.Errors, 1)\n-\t\t// check query_log frame\n-\t\trequire.Contains(t, bundle.Frames, \"query_log\")\n-\t\trequire.Equal(t, queryLogColumns, bundle.Frames[\"query_log\"].Columns())\n-\t\tcheckFrame(t, bundle.Frames[\"query_log\"], queryLogFrame.Rows)\n-\t\t//check text_log frame\n-\t\trequire.Contains(t, bundle.Frames, \"text_log\")\n-\t\trequire.Equal(t, textLogColumns, bundle.Frames[\"text_log\"].Columns())\n-\t\tcheckFrame(t, bundle.Frames[\"text_log\"], textLogFrame.Rows)\n-\t\tclient.Reset()\n-\t})\n-\n-\tt.Run(\"test db logs collection with limit\", func(t *testing.T) {\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.IntParam{\n-\t\t\t\t\tValue: 1,\n-\t\t\t\t\tParam: config.NewParam(\"row_limit\", \"Maximum number of log rows to collect. Negative values mean unlimited\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tbundle, err := dbLogsCollector.Collect(conf)\n-\t\trequire.Empty(t, err)\n-\t\trequire.NotNil(t, bundle)\n-\t\trequire.Len(t, bundle.Frames, 0)\n-\t\trequire.Len(t, bundle.Errors.Errors, 3)\n-\t\t// populate client\n-\t\tclient.QueryResponses[\"SELECT * FROM system.query_log ORDER BY event_time_microseconds ASC LIMIT 1\"] = &queryLogFrame\n-\t\tclient.QueryResponses[\"SELECT * FROM system.text_log ORDER BY event_time_microseconds ASC LIMIT 1\"] = &textLogFrame\n-\t\tbundle, err = dbLogsCollector.Collect(conf)\n-\t\trequire.Empty(t, err)\n-\t\trequire.Len(t, bundle.Frames, 2)\n-\t\trequire.Len(t, bundle.Errors.Errors, 1)\n-\t\trequire.Contains(t, bundle.Frames, \"text_log\")\n-\t\trequire.Contains(t, bundle.Frames, \"query_log\")\n-\t\t// check query_log frame\n-\t\trequire.Contains(t, bundle.Frames, \"query_log\")\n-\t\trequire.Equal(t, queryLogColumns, bundle.Frames[\"query_log\"].Columns())\n-\t\tcheckFrame(t, bundle.Frames[\"query_log\"], queryLogFrame.Rows[:1])\n-\t\t//check text_log frame\n-\t\trequire.Contains(t, bundle.Frames, \"text_log\")\n-\t\trequire.Equal(t, textLogColumns, bundle.Frames[\"text_log\"].Columns())\n-\t\tcheckFrame(t, bundle.Frames[\"text_log\"], textLogFrame.Rows[:1])\n-\t\tclient.Reset()\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/collectors/clickhouse/logs_test.go b/programs/diagnostics/internal/collectors/clickhouse/logs_test.go\ndeleted file mode 100644\nindex 5f0be7344453..000000000000\n--- a/programs/diagnostics/internal/collectors/clickhouse/logs_test.go\n+++ /dev/null\n@@ -1,147 +0,0 @@\n-package clickhouse_test\n-\n-import (\n-\t\"fmt\"\n-\t\"os\"\n-\t\"path\"\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestLogsConfiguration(t *testing.T) {\n-\tt.Run(\"correct configuration is returned for logs collector\", func(t *testing.T) {\n-\t\tclient := test.NewFakeClickhouseClient(make(map[string][]string))\n-\t\tlogsCollector := clickhouse.NewLogsCollector(&platform.ResourceManager{\n-\t\t\tDbClient: client,\n-\t\t})\n-\t\tconf := logsCollector.Configuration()\n-\t\trequire.Len(t, conf.Params, 2)\n-\t\t// check directory\n-\t\trequire.IsType(t, config.StringParam{}, conf.Params[0])\n-\t\tdirectory, ok := conf.Params[0].(config.StringParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, directory.Required())\n-\t\trequire.Equal(t, directory.Name(), \"directory\")\n-\t\trequire.Empty(t, directory.Value)\n-\t\t// check collect_archives\n-\t\trequire.IsType(t, config.BoolParam{}, conf.Params[1])\n-\t\tcollectArchives, ok := conf.Params[1].(config.BoolParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, collectArchives.Required())\n-\t\trequire.Equal(t, collectArchives.Name(), \"collect_archives\")\n-\t\trequire.False(t, collectArchives.Value)\n-\t})\n-}\n-\n-func TestLogsCollect(t *testing.T) {\n-\n-\tlogsCollector := clickhouse.NewLogsCollector(&platform.ResourceManager{})\n-\n-\tt.Run(\"test default logs collection\", func(t *testing.T) {\n-\t\t// we can't rely on a local installation of clickhouse being present for tests - if it is present (and running)\n-\t\t// results maybe variable e.g. we may find a config. For now, we allow flexibility and test only default.\n-\t\t// TODO: we may want to test this within a container\n-\t\tbundle, err := logsCollector.Collect(config.Configuration{})\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, bundle)\n-\t\t// we will have some errors if clickhouse is installed or not. If former, permission issues - if latter missing folders.\n-\t\trequire.Greater(t, len(bundle.Errors.Errors), 0)\n-\t\trequire.Len(t, bundle.Frames, 1)\n-\t\trequire.Contains(t, bundle.Frames, \"default\")\n-\t\t_, ok := bundle.Frames[\"default\"].(data.DirectoryFileFrame)\n-\t\trequire.True(t, ok)\n-\t\t// no guarantees clickhouse is installed so this bundle could have no frames\n-\t})\n-\n-\tt.Run(\"test logs collection when directory is specified\", func(t *testing.T) {\n-\t\tcwd, err := os.Getwd()\n-\t\trequire.Nil(t, err)\n-\t\tlogsPath := path.Join(cwd, \"../../../testdata\", \"logs\", \"var\", \"logs\")\n-\t\tbundle, err := logsCollector.Collect(config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue:      logsPath,\n-\t\t\t\t\tParam:      config.NewParam(\"directory\", \"Specify the location of the log files for ClickHouse Server e.g. /var/log/clickhouse-server/\", false),\n-\t\t\t\t\tAllowEmpty: true,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t})\n-\t\trequire.Nil(t, err)\n-\t\tcheckDirectoryBundle(t, bundle, logsPath, []string{\"clickhouse-server.log\", \"clickhouse-server.err.log\"})\n-\n-\t})\n-\n-\tt.Run(\"test logs collection of archives\", func(t *testing.T) {\n-\t\tcwd, err := os.Getwd()\n-\t\trequire.Nil(t, err)\n-\t\tlogsPath := path.Join(cwd, \"../../../testdata\", \"logs\", \"var\", \"logs\")\n-\t\tbundle, err := logsCollector.Collect(config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue:      logsPath,\n-\t\t\t\t\tParam:      config.NewParam(\"directory\", \"Specify the location of the log files for ClickHouse Server e.g. /var/log/clickhouse-server/\", false),\n-\t\t\t\t\tAllowEmpty: true,\n-\t\t\t\t},\n-\t\t\t\tconfig.BoolParam{\n-\t\t\t\t\tValue: true,\n-\t\t\t\t\tParam: config.NewParam(\"collect_archives\", \"Collect compressed log archive files\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t})\n-\t\trequire.Nil(t, err)\n-\t\tcheckDirectoryBundle(t, bundle, logsPath, []string{\"clickhouse-server.log\", \"clickhouse-server.err.log\", \"clickhouse-server.log.gz\"})\n-\t})\n-\n-\tt.Run(\"test when directory does not exist\", func(t *testing.T) {\n-\t\ttmpDir := t.TempDir()\n-\t\tlogsPath := path.Join(tmpDir, \"random\")\n-\t\tbundle, err := logsCollector.Collect(config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue:      logsPath,\n-\t\t\t\t\tParam:      config.NewParam(\"directory\", \"Specify the location of the log files for ClickHouse Server e.g. /var/log/clickhouse-server/\", false),\n-\t\t\t\t\tAllowEmpty: true,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t})\n-\t\t// not a fatal error currently\n-\t\trequire.Nil(t, err)\n-\t\trequire.Len(t, bundle.Errors.Errors, 1)\n-\t\trequire.Equal(t, fmt.Sprintf(\"directory %s does not exist\", logsPath), bundle.Errors.Errors[0].Error())\n-\t})\n-}\n-\n-func checkDirectoryBundle(t *testing.T, bundle *data.DiagnosticBundle, logsPath string, expectedFiles []string) {\n-\trequire.NotNil(t, bundle)\n-\trequire.Nil(t, bundle.Errors.Errors)\n-\trequire.Len(t, bundle.Frames, 1)\n-\trequire.Contains(t, bundle.Frames, \"user_specified\")\n-\tdirFrame, ok := bundle.Frames[\"user_specified\"].(data.DirectoryFileFrame)\n-\trequire.True(t, ok)\n-\trequire.Equal(t, logsPath, dirFrame.Directory)\n-\trequire.Equal(t, []string{\"files\"}, dirFrame.Columns())\n-\ti := 0\n-\tfullPaths := make([]string, len(expectedFiles))\n-\tfor i, filePath := range expectedFiles {\n-\t\tfullPaths[i] = path.Join(logsPath, filePath)\n-\t}\n-\tfor {\n-\t\tvalues, ok, err := dirFrame.Next()\n-\t\trequire.Nil(t, err)\n-\t\tif !ok {\n-\t\t\tbreak\n-\t\t}\n-\t\trequire.Len(t, values, 1)\n-\t\tfile, ok := values[0].(data.SimpleFile)\n-\t\trequire.True(t, ok)\n-\t\trequire.Contains(t, fullPaths, file.FilePath())\n-\t\ti += 1\n-\t}\n-\trequire.Equal(t, len(fullPaths), i)\n-}\ndiff --git a/programs/diagnostics/internal/collectors/clickhouse/summary_test.go b/programs/diagnostics/internal/collectors/clickhouse/summary_test.go\ndeleted file mode 100644\nindex 92945d987ed5..000000000000\n--- a/programs/diagnostics/internal/collectors/clickhouse/summary_test.go\n+++ /dev/null\n@@ -1,111 +0,0 @@\n-package clickhouse_test\n-\n-import (\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestSummaryConfiguration(t *testing.T) {\n-\tt.Run(\"correct configuration is returned for summary collector\", func(t *testing.T) {\n-\t\tclient := test.NewFakeClickhouseClient(make(map[string][]string))\n-\t\tsummaryCollector := clickhouse.NewSummaryCollector(&platform.ResourceManager{\n-\t\t\tDbClient: client,\n-\t\t})\n-\t\tconf := summaryCollector.Configuration()\n-\t\trequire.Len(t, conf.Params, 1)\n-\t\trequire.IsType(t, config.IntParam{}, conf.Params[0])\n-\t\tlimit, ok := conf.Params[0].(config.IntParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, limit.Required())\n-\t\trequire.Equal(t, limit.Name(), \"row_limit\")\n-\t\trequire.Equal(t, int64(20), limit.Value)\n-\t})\n-}\n-\n-func TestSummaryCollection(t *testing.T) {\n-\n-\tclient := test.NewFakeClickhouseClient(make(map[string][]string))\n-\tversionFrame := test.NewFakeDataFrame(\"version\", []string{\"version()\"},\n-\t\t[][]interface{}{\n-\t\t\t{\"22.1.3.7\"},\n-\t\t},\n-\t)\n-\tclient.QueryResponses[\"SELECT version()\"] = &versionFrame\n-\tdatabasesFrame := test.NewFakeDataFrame(\"databases\", []string{\"name\", \"engine\", \"tables\", \"partitions\", \"parts\", \"disk_size\"},\n-\t\t[][]interface{}{\n-\t\t\t{\"tutorial\", \"Atomic\", 2, 2, 2, \"1.70 GiB\"},\n-\t\t\t{\"default\", \"Atomic\", 5, 5, 6, \"1.08 GiB\"},\n-\t\t\t{\"system\", \"Atomic\", 11, 24, 70, \"1.05 GiB\"},\n-\t\t\t{\"INFORMATION_SCHEMA\", \"Memory\", 0, 0, 0, \"0.00 B\"},\n-\t\t\t{\"covid19db\", \"Atomic\", 0, 0, 0, \"0.00 B\"},\n-\t\t\t{\"information_schema\", \"Memory\", 0, 0, 0, \"0.00 B\"}})\n-\n-\tclient.QueryResponses[\"SELECT name, engine, tables, partitions, parts, formatReadableSize(bytes_on_disk) \\\"disk_size\\\" \"+\n-\t\t\"FROM system.databases db LEFT JOIN ( SELECT database, uniq(table) \\\"tables\\\", uniq(table, partition) \\\"partitions\\\", \"+\n-\t\t\"count() AS parts, sum(bytes_on_disk) \\\"bytes_on_disk\\\" FROM system.parts WHERE active GROUP BY database ) AS db_stats \"+\n-\t\t\"ON db.name = db_stats.database ORDER BY bytes_on_disk DESC LIMIT 20\"] = &databasesFrame\n-\n-\tsummaryCollector := clickhouse.NewSummaryCollector(&platform.ResourceManager{\n-\t\tDbClient: client,\n-\t})\n-\n-\tt.Run(\"test default summary collection\", func(t *testing.T) {\n-\t\tbundle, errs := summaryCollector.Collect(config.Configuration{})\n-\t\trequire.Empty(t, errs)\n-\t\trequire.Len(t, bundle.Errors.Errors, 30)\n-\t\trequire.NotNil(t, bundle)\n-\t\trequire.Len(t, bundle.Frames, 2)\n-\t\t// check version frame\n-\t\trequire.Contains(t, bundle.Frames, \"version\")\n-\t\trequire.Equal(t, []string{\"version()\"}, bundle.Frames[\"version\"].Columns())\n-\t\tcheckFrame(t, bundle.Frames[\"version\"], versionFrame.Rows)\n-\t\t//check databases frame\n-\t\trequire.Contains(t, bundle.Frames, \"databases\")\n-\t\trequire.Equal(t, []string{\"name\", \"engine\", \"tables\", \"partitions\", \"parts\", \"disk_size\"}, bundle.Frames[\"databases\"].Columns())\n-\t\tcheckFrame(t, bundle.Frames[\"databases\"], databasesFrame.Rows)\n-\t\tclient.Reset()\n-\t})\n-\n-\tt.Run(\"test summary collection with limit\", func(t *testing.T) {\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.IntParam{\n-\t\t\t\t\tValue: 1,\n-\t\t\t\t\tParam: config.NewParam(\"row_limit\", \"Limit rows on supported queries.\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tbundle, errs := summaryCollector.Collect(conf)\n-\n-\t\trequire.Empty(t, errs)\n-\t\trequire.Len(t, bundle.Errors.Errors, 31)\n-\t\trequire.NotNil(t, bundle)\n-\t\t// databases will be absent due to limit\n-\t\trequire.Len(t, bundle.Frames, 1)\n-\t\t// check version frame\n-\t\trequire.Contains(t, bundle.Frames, \"version\")\n-\t\trequire.Equal(t, []string{\"version()\"}, bundle.Frames[\"version\"].Columns())\n-\t\tcheckFrame(t, bundle.Frames[\"version\"], versionFrame.Rows)\n-\n-\t\tclient.QueryResponses[\"SELECT name, engine, tables, partitions, parts, formatReadableSize(bytes_on_disk) \\\"disk_size\\\" \"+\n-\t\t\t\"FROM system.databases db LEFT JOIN ( SELECT database, uniq(table) \\\"tables\\\", uniq(table, partition) \\\"partitions\\\", \"+\n-\t\t\t\"count() AS parts, sum(bytes_on_disk) \\\"bytes_on_disk\\\" FROM system.parts WHERE active GROUP BY database ) AS db_stats \"+\n-\t\t\t\"ON db.name = db_stats.database ORDER BY bytes_on_disk DESC LIMIT 1\"] = &databasesFrame\n-\t\tbundle, errs = summaryCollector.Collect(conf)\n-\t\trequire.Empty(t, errs)\n-\t\trequire.Len(t, bundle.Errors.Errors, 30)\n-\t\trequire.NotNil(t, bundle)\n-\t\trequire.Len(t, bundle.Frames, 2)\n-\t\trequire.Contains(t, bundle.Frames, \"version\")\n-\t\t//check databases frame\n-\t\trequire.Contains(t, bundle.Frames, \"databases\")\n-\t\trequire.Equal(t, []string{\"name\", \"engine\", \"tables\", \"partitions\", \"parts\", \"disk_size\"}, bundle.Frames[\"databases\"].Columns())\n-\t\t// this will parse as our mock client does not read statement (specifically the limit clause) when called with execute\n-\t\tcheckFrame(t, bundle.Frames[\"databases\"], databasesFrame.Rows)\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/collectors/clickhouse/system_test.go b/programs/diagnostics/internal/collectors/clickhouse/system_test.go\ndeleted file mode 100644\nindex d1b9a6e78591..000000000000\n--- a/programs/diagnostics/internal/collectors/clickhouse/system_test.go\n+++ /dev/null\n@@ -1,366 +0,0 @@\n-package clickhouse_test\n-\n-import (\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestSystemConfiguration(t *testing.T) {\n-\tt.Run(\"correct configuration is returned for system db collector\", func(t *testing.T) {\n-\t\tclient := test.NewFakeClickhouseClient(make(map[string][]string))\n-\t\tsystemDbCollector := clickhouse.NewSystemDatabaseCollector(&platform.ResourceManager{\n-\t\t\tDbClient: client,\n-\t\t})\n-\t\tconf := systemDbCollector.Configuration()\n-\t\trequire.Len(t, conf.Params, 3)\n-\t\t// check first param\n-\t\trequire.IsType(t, config.StringListParam{}, conf.Params[0])\n-\t\tincludeTables, ok := conf.Params[0].(config.StringListParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, includeTables.Required())\n-\t\trequire.Equal(t, includeTables.Name(), \"include_tables\")\n-\t\trequire.Nil(t, includeTables.Values)\n-\t\t// check second param\n-\t\trequire.IsType(t, config.StringListParam{}, conf.Params[1])\n-\t\texcludeTables, ok := conf.Params[1].(config.StringListParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, excludeTables.Required())\n-\t\trequire.Equal(t, \"exclude_tables\", excludeTables.Name())\n-\t\trequire.Equal(t, []string{\"licenses\", \"distributed_ddl_queue\", \"query_thread_log\", \"query_log\", \"asynchronous_metric_log\", \"zookeeper\", \"aggregate_function_combinators\", \"collations\", \"contributors\", \"data_type_families\", \"formats\", \"graphite_retentions\", \"numbers\", \"numbers_mt\", \"one\", \"parts_columns\", \"projection_parts\", \"projection_parts_columns\", \"table_engines\", \"time_zones\", \"zeros\", \"zeros_mt\"}, excludeTables.Values)\n-\t\t// check third param\n-\t\trequire.IsType(t, config.IntParam{}, conf.Params[2])\n-\t\trowLimit, ok := conf.Params[2].(config.IntParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, rowLimit.Required())\n-\t\trequire.Equal(t, \"row_limit\", rowLimit.Name())\n-\t\trequire.Equal(t, int64(100000), rowLimit.Value)\n-\t})\n-}\n-\n-func TestSystemDbCollect(t *testing.T) {\n-\n-\tdiskFrame := test.NewFakeDataFrame(\"disks\", []string{\"name\", \"path\", \"free_space\", \"total_space\", \"keep_free_space\", \"type\"},\n-\t\t[][]interface{}{\n-\t\t\t{\"default\", \"/var/lib/clickhouse\", 1729659346944, 1938213220352, \"\", \"local\"},\n-\t\t},\n-\t)\n-\tclusterFrame := test.NewFakeDataFrame(\"clusters\", []string{\"cluster\", \"shard_num\", \"shard_weight\", \"replica_num\", \"host_name\", \"host_address\", \"port\", \"is_local\", \"user\", \"default_database\", \"errors_count\", \"slowdowns_count\", \"estimated_recovery_time\"},\n-\t\t[][]interface{}{\n-\t\t\t{\"events\", 1, 1, 1, \"dalem-local-clickhouse-blue-1\", \"192.168.144.2\", 9000, 1, \"default\", \"\", 0, 0, 0},\n-\t\t\t{\"events\", 2, 1, 1, \"dalem-local-clickhouse-blue-2\", \"192.168.144.4\", 9000, 1, \"default\", \"\", 0, 0, 0},\n-\t\t\t{\"events\", 3, 1, 1, \"dalem-local-clickhouse-blue-3\", \"192.168.144.3\", 9000, 1, \"default\", \"\", 0, 0, 0},\n-\t\t},\n-\t)\n-\tuserFrame := test.NewFakeDataFrame(\"users\", []string{\"name\", \"id\", \"storage\", \"auth_type\", \"auth_params\", \"host_ip\", \"host_names\", \"host_names_regexp\", \"host_names_like\"},\n-\t\t[][]interface{}{\n-\t\t\t{\"default\", \"94309d50-4f52-5250-31bd-74fecac179db,users.xml,plaintext_password\", \"sha256_password\", []string{\"::0\"}, []string{}, []string{}, []string{}},\n-\t\t},\n-\t)\n-\n-\tdbTables := map[string][]string{\n-\t\tclickhouse.SystemDatabase: {\"disks\", \"clusters\", \"users\"},\n-\t}\n-\tclient := test.NewFakeClickhouseClient(dbTables)\n-\n-\tclient.QueryResponses[\"SELECT * FROM system.disks LIMIT 100000\"] = &diskFrame\n-\tclient.QueryResponses[\"SELECT * FROM system.clusters LIMIT 100000\"] = &clusterFrame\n-\tclient.QueryResponses[\"SELECT * FROM system.users LIMIT 100000\"] = &userFrame\n-\tsystemDbCollector := clickhouse.NewSystemDatabaseCollector(&platform.ResourceManager{\n-\t\tDbClient: client,\n-\t})\n-\n-\tt.Run(\"test default system db collection\", func(t *testing.T) {\n-\t\tdiagSet, err := systemDbCollector.Collect(config.Configuration{})\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, diagSet)\n-\t\trequire.Len(t, diagSet.Errors.Errors, 0)\n-\t\trequire.Len(t, diagSet.Frames, 3)\n-\t\t// disks frame\n-\t\trequire.Equal(t, \"disks\", diagSet.Frames[\"disks\"].Name())\n-\t\trequire.Equal(t, diskFrame.ColumnNames, diagSet.Frames[\"disks\"].Columns())\n-\t\tcheckFrame(t, diagSet.Frames[\"disks\"], diskFrame.Rows)\n-\t\t// clusters frame\n-\t\trequire.Equal(t, \"clusters\", diagSet.Frames[\"clusters\"].Name())\n-\t\trequire.Equal(t, clusterFrame.ColumnNames, diagSet.Frames[\"clusters\"].Columns())\n-\t\tcheckFrame(t, diagSet.Frames[\"clusters\"], clusterFrame.Rows)\n-\t\t// users frame\n-\t\trequire.Equal(t, \"users\", diagSet.Frames[\"users\"].Name())\n-\t\trequire.Equal(t, userFrame.ColumnNames, diagSet.Frames[\"users\"].Columns())\n-\t\tcheckFrame(t, diagSet.Frames[\"users\"], userFrame.Rows)\n-\t\tclient.Reset()\n-\t})\n-\n-\tt.Run(\"test when we pass an includes\", func(t *testing.T) {\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringListParam{\n-\t\t\t\t\t// nil means include everything\n-\t\t\t\t\tValues: []string{\"disks\"},\n-\t\t\t\t\tParam:  config.NewParam(\"include_tables\", \"Exclusion\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tdiagSet, err := systemDbCollector.Collect(conf)\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, diagSet)\n-\t\trequire.Len(t, diagSet.Errors.Errors, 0)\n-\t\trequire.Len(t, diagSet.Frames, 1)\n-\t\t// disks frame\n-\t\trequire.Equal(t, \"disks\", diagSet.Frames[\"disks\"].Name())\n-\t\trequire.Equal(t, diskFrame.ColumnNames, diagSet.Frames[\"disks\"].Columns())\n-\t\tcheckFrame(t, diagSet.Frames[\"disks\"], diskFrame.Rows)\n-\t\tclient.Reset()\n-\t})\n-\n-\t// test excludes\n-\tt.Run(\"test when we pass an excludes\", func(t *testing.T) {\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringListParam{\n-\t\t\t\t\tValues: []string{\"disks\"},\n-\t\t\t\t\tParam:  config.NewParam(\"exclude_tables\", \"Exclusion\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tdiagSet, err := systemDbCollector.Collect(conf)\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, diagSet)\n-\t\trequire.Len(t, diagSet.Errors.Errors, 0)\n-\t\trequire.Len(t, diagSet.Frames, 2)\n-\t\t// clusters frame\n-\t\trequire.Equal(t, \"clusters\", diagSet.Frames[\"clusters\"].Name())\n-\t\trequire.Equal(t, clusterFrame.ColumnNames, diagSet.Frames[\"clusters\"].Columns())\n-\t\tcheckFrame(t, diagSet.Frames[\"clusters\"], clusterFrame.Rows)\n-\t\t// users frame\n-\t\trequire.Equal(t, \"users\", diagSet.Frames[\"users\"].Name())\n-\t\trequire.Equal(t, userFrame.ColumnNames, diagSet.Frames[\"users\"].Columns())\n-\t\tcheckFrame(t, diagSet.Frames[\"users\"], userFrame.Rows)\n-\t\tclient.Reset()\n-\t})\n-\n-\t// test includes which isn't in the list\n-\tt.Run(\"test when we pass an invalid includes\", func(t *testing.T) {\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringListParam{\n-\t\t\t\t\t// nil means include everything\n-\t\t\t\t\tValues: []string{\"disks\", \"invalid\"},\n-\t\t\t\t\tParam:  config.NewParam(\"include_tables\", \"Exclusion\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tdiagSet, err := systemDbCollector.Collect(conf)\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, diagSet)\n-\t\trequire.Len(t, diagSet.Errors.Errors, 1)\n-\t\trequire.Equal(t, diagSet.Errors.Error(), \"some tables specified in the include_tables are not in the \"+\n-\t\t\t\"system database and will not be exported: [invalid]\")\n-\t\trequire.Len(t, diagSet.Frames, 1)\n-\t\t// disks frame\n-\t\trequire.Equal(t, \"disks\", diagSet.Frames[\"disks\"].Name())\n-\t\trequire.Equal(t, diskFrame.ColumnNames, diagSet.Frames[\"disks\"].Columns())\n-\t\tcheckFrame(t, diagSet.Frames[\"disks\"], diskFrame.Rows)\n-\t\tclient.Reset()\n-\t})\n-\n-\tt.Run(\"test when we use a table with excluded fields\", func(t *testing.T) {\n-\t\texcludeDefault := clickhouse.ExcludeColumns\n-\t\tclient.QueryResponses[\"SELECT * EXCEPT(keep_free_space,type) FROM system.disks LIMIT 100000\"] = &diskFrame\n-\t\tclickhouse.ExcludeColumns = map[string][]string{\n-\t\t\t\"disks\": {\"keep_free_space\", \"type\"},\n-\t\t}\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringListParam{\n-\t\t\t\t\t// nil means include everything\n-\t\t\t\t\tValues: []string{\"disks\"},\n-\t\t\t\t\tParam:  config.NewParam(\"include_tables\", \"Exclusion\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tdiagSet, err := systemDbCollector.Collect(conf)\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, diagSet)\n-\t\trequire.Len(t, diagSet.Errors.Errors, 0)\n-\t\trequire.Len(t, diagSet.Frames, 1)\n-\t\t// disks frame\n-\t\trequire.Equal(t, \"disks\", diagSet.Frames[\"disks\"].Name())\n-\t\trequire.Equal(t, []string{\"name\", \"path\", \"free_space\", \"total_space\"}, diagSet.Frames[\"disks\"].Columns())\n-\t\teDiskFrame := test.NewFakeDataFrame(\"disks\", []string{\"name\", \"path\", \"free_space\", \"total_space\"},\n-\t\t\t[][]interface{}{\n-\t\t\t\t{\"default\", \"/var/lib/clickhouse\", 1729659346944, 1938213220352},\n-\t\t\t},\n-\t\t)\n-\t\tcheckFrame(t, diagSet.Frames[\"disks\"], eDiskFrame.Rows)\n-\t\tclickhouse.ExcludeColumns = excludeDefault\n-\t\tclient.Reset()\n-\t})\n-\n-\tt.Run(\"test with a low row limit\", func(t *testing.T) {\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.IntParam{\n-\t\t\t\t\tValue: 1,\n-\t\t\t\t\tParam: config.NewParam(\"row_limit\", \"Maximum number of rows to collect from any table. Negative values mean unlimited.\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tclient.QueryResponses[\"SELECT * FROM system.disks LIMIT 1\"] = &diskFrame\n-\t\tclient.QueryResponses[\"SELECT * FROM system.clusters LIMIT 1\"] = &clusterFrame\n-\t\tclient.QueryResponses[\"SELECT * FROM system.users LIMIT 1\"] = &userFrame\n-\t\tdiagSet, err := systemDbCollector.Collect(conf)\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, diagSet)\n-\t\trequire.Len(t, diagSet.Errors.Errors, 0)\n-\t\trequire.Len(t, diagSet.Frames, 3)\n-\t\t// clusters frame\n-\t\trequire.Equal(t, \"clusters\", diagSet.Frames[\"clusters\"].Name())\n-\t\trequire.Equal(t, clusterFrame.ColumnNames, diagSet.Frames[\"clusters\"].Columns())\n-\t\tlClusterFrame := test.NewFakeDataFrame(\"clusters\", []string{\"cluster\", \"shard_num\", \"shard_weight\", \"replica_num\", \"host_name\", \"host_address\", \"port\", \"is_local\", \"user\", \"default_database\", \"errors_count\", \"slowdowns_count\", \"estimated_recovery_time\"},\n-\t\t\t[][]interface{}{\n-\t\t\t\t{\"events\", 1, 1, 1, \"dalem-local-clickhouse-blue-1\", \"192.168.144.2\", 9000, 1, \"default\", \"\", 0, 0, 0},\n-\t\t\t})\n-\t\tcheckFrame(t, diagSet.Frames[\"clusters\"], lClusterFrame.Rows)\n-\t\tclient.Reset()\n-\t})\n-\n-\tt.Run(\"test with a negative low row limit\", func(t *testing.T) {\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.IntParam{\n-\t\t\t\t\tValue: -23,\n-\t\t\t\t\tParam: config.NewParam(\"row_limit\", \"Maximum number of rows to collect from any table. Negative values mean unlimited.\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tclient.QueryResponses[\"SELECT * FROM system.clusters\"] = &clusterFrame\n-\t\tclient.QueryResponses[\"SELECT * FROM system.disks\"] = &diskFrame\n-\t\tclient.QueryResponses[\"SELECT * FROM system.users\"] = &userFrame\n-\t\tdiagSet, err := systemDbCollector.Collect(conf)\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, diagSet)\n-\t\trequire.Len(t, diagSet.Errors.Errors, 0)\n-\t\trequire.Len(t, diagSet.Frames, 3)\n-\t\t// disks frame\n-\t\trequire.Equal(t, \"disks\", diagSet.Frames[\"disks\"].Name())\n-\t\trequire.Equal(t, diskFrame.ColumnNames, diagSet.Frames[\"disks\"].Columns())\n-\t\tcheckFrame(t, diagSet.Frames[\"disks\"], diskFrame.Rows)\n-\t\t// clusters frame\n-\t\trequire.Equal(t, \"clusters\", diagSet.Frames[\"clusters\"].Name())\n-\t\trequire.Equal(t, clusterFrame.ColumnNames, diagSet.Frames[\"clusters\"].Columns())\n-\t\tcheckFrame(t, diagSet.Frames[\"clusters\"], clusterFrame.Rows)\n-\t\t// users frame\n-\t\trequire.Equal(t, \"users\", diagSet.Frames[\"users\"].Name())\n-\t\trequire.Equal(t, userFrame.ColumnNames, diagSet.Frames[\"users\"].Columns())\n-\t\tcheckFrame(t, diagSet.Frames[\"users\"], userFrame.Rows)\n-\t\tclient.Reset()\n-\t})\n-\n-\tt.Run(\"test that includes overrides excludes\", func(t *testing.T) {\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringListParam{\n-\t\t\t\t\t// nil means include everything\n-\t\t\t\t\tValues: []string{\"disks\"},\n-\t\t\t\t\tParam:  config.NewParam(\"exclude_tables\", \"Excluded\", false),\n-\t\t\t\t},\n-\t\t\t\tconfig.StringListParam{\n-\t\t\t\t\t// nil means include everything\n-\t\t\t\t\tValues: []string{\"disks\", \"clusters\", \"users\"},\n-\t\t\t\t\tParam:  config.NewParam(\"include_tables\", \"Included\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tdiagSet, err := systemDbCollector.Collect(conf)\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, diagSet)\n-\t\trequire.Len(t, diagSet.Errors.Errors, 0)\n-\t\trequire.Len(t, diagSet.Frames, 3)\n-\t\tclient.Reset()\n-\t})\n-\n-\tt.Run(\"test banned\", func(t *testing.T) {\n-\t\tbannedDefault := clickhouse.BannedTables\n-\t\tclickhouse.BannedTables = []string{\"disks\"}\n-\t\tdiagSet, err := systemDbCollector.Collect(config.Configuration{})\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, diagSet)\n-\t\trequire.Len(t, diagSet.Errors.Errors, 0)\n-\t\trequire.Len(t, diagSet.Frames, 2)\n-\t\trequire.Contains(t, diagSet.Frames, \"users\")\n-\t\trequire.Contains(t, diagSet.Frames, \"clusters\")\n-\t\tclickhouse.BannedTables = bannedDefault\n-\t\tclient.Reset()\n-\t})\n-\n-\tt.Run(\"test banned unless included\", func(t *testing.T) {\n-\t\tbannedDefault := clickhouse.BannedTables\n-\t\tclickhouse.BannedTables = []string{\"disks\"}\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringListParam{\n-\t\t\t\t\t// nil means include everything\n-\t\t\t\t\tValues: []string{\"disks\", \"clusters\", \"users\"},\n-\t\t\t\t\tParam:  config.NewParam(\"include_tables\", \"Included\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tdiagSet, err := systemDbCollector.Collect(conf)\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, diagSet)\n-\t\trequire.Len(t, diagSet.Errors.Errors, 0)\n-\t\trequire.Len(t, diagSet.Frames, 3)\n-\t\trequire.Contains(t, diagSet.Frames, \"disks\")\n-\t\trequire.Contains(t, diagSet.Frames, \"users\")\n-\t\trequire.Contains(t, diagSet.Frames, \"clusters\")\n-\t\tclickhouse.BannedTables = bannedDefault\n-\t\tclient.Reset()\n-\t})\n-\n-\tt.Run(\"tables are ordered if configured\", func(t *testing.T) {\n-\t\tdefaultOrderBy := clickhouse.OrderBy\n-\t\tclickhouse.OrderBy = map[string]data.OrderBy{\n-\t\t\t\"clusters\": {\n-\t\t\t\tColumn: \"shard_num\",\n-\t\t\t\tOrder:  data.Desc,\n-\t\t\t},\n-\t\t}\n-\t\tclient.QueryResponses[\"SELECT * FROM system.clusters ORDER BY shard_num DESC LIMIT 100000\"] = &clusterFrame\n-\t\tdiagSet, err := systemDbCollector.Collect(config.Configuration{})\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, diagSet)\n-\t\trequire.Len(t, diagSet.Errors.Errors, 0)\n-\t\trequire.Len(t, diagSet.Frames, 3)\n-\t\tclickhouse.OrderBy = defaultOrderBy\n-\t\toClusterFrame := test.NewFakeDataFrame(\"clusters\", []string{\"cluster\", \"shard_num\", \"shard_weight\", \"replica_num\", \"host_name\", \"host_address\", \"port\", \"is_local\", \"user\", \"default_database\", \"errors_count\", \"slowdowns_count\", \"estimated_recovery_time\"},\n-\t\t\t[][]interface{}{\n-\t\t\t\t{\"events\", 3, 1, 1, \"dalem-local-clickhouse-blue-3\", \"192.168.144.3\", 9000, 1, \"default\", \"\", 0, 0, 0},\n-\t\t\t\t{\"events\", 2, 1, 1, \"dalem-local-clickhouse-blue-2\", \"192.168.144.4\", 9000, 1, \"default\", \"\", 0, 0, 0},\n-\t\t\t\t{\"events\", 1, 1, 1, \"dalem-local-clickhouse-blue-1\", \"192.168.144.2\", 9000, 1, \"default\", \"\", 0, 0, 0},\n-\t\t\t},\n-\t\t)\n-\t\tcheckFrame(t, diagSet.Frames[\"clusters\"], oClusterFrame.Rows)\n-\t\tclient.Reset()\n-\t})\n-\n-}\n-\n-func checkFrame(t *testing.T, frame data.Frame, rows [][]interface{}) {\n-\ti := 0\n-\tfor {\n-\t\tvalues, ok, err := frame.Next()\n-\t\trequire.Nil(t, err)\n-\t\tif !ok {\n-\t\t\tbreak\n-\t\t}\n-\t\trequire.ElementsMatch(t, rows[i], values)\n-\t\ti += 1\n-\t}\n-\trequire.Equal(t, i, len(rows))\n-}\ndiff --git a/programs/diagnostics/internal/collectors/clickhouse/zookeeper_test.go b/programs/diagnostics/internal/collectors/clickhouse/zookeeper_test.go\ndeleted file mode 100644\nindex 3e56f6200f07..000000000000\n--- a/programs/diagnostics/internal/collectors/clickhouse/zookeeper_test.go\n+++ /dev/null\n@@ -1,102 +0,0 @@\n-package clickhouse_test\n-\n-import (\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestZookeeperConfiguration(t *testing.T) {\n-\tt.Run(\"correct configuration is returned for system zookeeper collector\", func(t *testing.T) {\n-\t\tclient := test.NewFakeClickhouseClient(make(map[string][]string))\n-\t\tzkCollector := clickhouse.NewZookeeperCollector(&platform.ResourceManager{\n-\t\t\tDbClient: client,\n-\t\t})\n-\t\tconf := zkCollector.Configuration()\n-\t\trequire.Len(t, conf.Params, 3)\n-\t\t// check first param\n-\t\trequire.IsType(t, config.StringParam{}, conf.Params[0])\n-\t\tpathPattern, ok := conf.Params[0].(config.StringParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, pathPattern.Required())\n-\t\trequire.Equal(t, pathPattern.Name(), \"path_pattern\")\n-\t\trequire.Equal(t, \"/clickhouse/{task_queue}/**\", pathPattern.Value)\n-\t\t// check second param\n-\t\trequire.IsType(t, config.IntParam{}, conf.Params[1])\n-\t\tmaxDepth, ok := conf.Params[1].(config.IntParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, maxDepth.Required())\n-\t\trequire.Equal(t, \"max_depth\", maxDepth.Name())\n-\t\trequire.Equal(t, int64(8), maxDepth.Value)\n-\t\t// check third param\n-\t\trequire.IsType(t, config.IntParam{}, conf.Params[2])\n-\t\trowLimit, ok := conf.Params[2].(config.IntParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, rowLimit.Required())\n-\t\trequire.Equal(t, \"row_limit\", rowLimit.Name())\n-\t\trequire.Equal(t, int64(10), rowLimit.Value)\n-\t})\n-}\n-\n-func TestZookeeperCollect(t *testing.T) {\n-\tlevel1 := test.NewFakeDataFrame(\"level_1\", []string{\"name\", \"value\", \"czxid\", \"mzxid\", \"ctime\", \"mtime\", \"version\", \"cversion\", \"aversion\", \"ephemeralOwner\", \"dataLength\", \"numChildren\", \"pzxid\", \"path\"},\n-\t\t[][]interface{}{\n-\t\t\t{\"name\", \"value\", \"czxid\", \"mzxid\", \"ctime\", \"mtime\", \"version\", \"cversion\", \"aversion\", \"ephemeralOwner\", \"dataLength\", \"numChildren\", \"pzxid\", \"path\"},\n-\t\t\t{\"task_queue\", \"\", \"4\", \"4\", \"2022-02-22 13:30:15\", \"2022-02-22 13:30:15\", \"0\", \"1\", \"0\", \"0\", \"0\", \"1\", \"5\", \"/clickhouse\"},\n-\t\t\t{\"copytasks\", \"\", \"525608\", \"525608\", \"2022-03-09 13:47:39\", \"2022-03-09 13:47:39\", \"0\", \"7\", \"0\", \"0\", \"0\", \"7\", \"526100\", \"/clickhouse\"},\n-\t\t},\n-\t)\n-\tlevel2 := test.NewFakeDataFrame(\"level_2\", []string{\"name\", \"value\", \"czxid\", \"mzxid\", \"ctime\", \"mtime\", \"version\", \"cversion\", \"aversion\", \"ephemeralOwner\", \"dataLength\", \"numChildren\", \"pzxid\", \"path\"},\n-\t\t[][]interface{}{\n-\t\t\t{\"ddl\", \"\", \"5\", \"5\", \"2022-02-22 13:30:15\", \"2022-02-22 13:30:15\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"5\", \"/clickhouse/task_queue\"},\n-\t\t},\n-\t)\n-\tlevel3 := test.NewFakeDataFrame(\"level_2\", []string{\"name\", \"value\", \"czxid\", \"mzxid\", \"ctime\", \"mtime\", \"version\", \"cversion\", \"aversion\", \"ephemeralOwner\", \"dataLength\", \"numChildren\", \"pzxid\", \"path\"},\n-\t\t[][]interface{}{},\n-\t)\n-\tdbTables := map[string][]string{\n-\t\tclickhouse.SystemDatabase: {\"zookeeper\"},\n-\t}\n-\tclient := test.NewFakeClickhouseClient(dbTables)\n-\n-\tclient.QueryResponses[\"SELECT name FROM system.zookeeper WHERE path='/clickhouse' LIMIT 10\"] = &level1\n-\t// can't reuse the frame as the first frame will be iterated as part of the recursive zookeeper search performed by the collector\n-\tcLevel1 := test.NewFakeDataFrame(\"level_1\", level1.Columns(), level1.Rows)\n-\tclient.QueryResponses[\"SELECT * FROM system.zookeeper WHERE path='/clickhouse' LIMIT 10\"] = &cLevel1\n-\tclient.QueryResponses[\"SELECT name FROM system.zookeeper WHERE path='/clickhouse/task_queue' LIMIT 10\"] = &level2\n-\tcLevel2 := test.NewFakeDataFrame(\"level_2\", level2.Columns(), level2.Rows)\n-\tclient.QueryResponses[\"SELECT * FROM system.zookeeper WHERE path='/clickhouse/task_queue' LIMIT 10\"] = &cLevel2\n-\tclient.QueryResponses[\"SELECT name FROM system.zookeeper WHERE path='/clickhouse/task_queue/ddl' LIMIT 10\"] = &level3\n-\tcLevel3 := test.NewFakeDataFrame(\"level_3\", level3.Columns(), level3.Rows)\n-\tclient.QueryResponses[\"SELECT * FROM system.zookeeper WHERE path='/clickhouse/task_queue/ddl' LIMIT 10\"] = &cLevel3\n-\n-\tzKCollector := clickhouse.NewZookeeperCollector(&platform.ResourceManager{\n-\t\tDbClient: client,\n-\t})\n-\n-\tt.Run(\"test default zookeeper collection\", func(t *testing.T) {\n-\t\tdiagSet, err := zKCollector.Collect(config.Configuration{})\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, diagSet)\n-\t\trequire.Len(t, diagSet.Errors.Errors, 0)\n-\t\trequire.Len(t, diagSet.Frames, 1)\n-\t\trequire.Contains(t, diagSet.Frames, \"zookeeper_db\")\n-\t\trequire.Equal(t, \"clickhouse\", diagSet.Frames[\"zookeeper_db\"].Name())\n-\t\trequire.IsType(t, data.HierarchicalFrame{}, diagSet.Frames[\"zookeeper_db\"])\n-\t\tcheckFrame(t, diagSet.Frames[\"zookeeper_db\"], level1.Rows)\n-\t\trequire.Equal(t, level1.Columns(), diagSet.Frames[\"zookeeper_db\"].Columns())\n-\t\thierarchicalFrame := diagSet.Frames[\"zookeeper_db\"].(data.HierarchicalFrame)\n-\t\trequire.Len(t, hierarchicalFrame.SubFrames, 1)\n-\t\tcheckFrame(t, hierarchicalFrame.SubFrames[0], cLevel2.Rows)\n-\t\trequire.Equal(t, cLevel2.Columns(), hierarchicalFrame.SubFrames[0].Columns())\n-\t\thierarchicalFrame = hierarchicalFrame.SubFrames[0]\n-\t\trequire.Len(t, hierarchicalFrame.SubFrames, 1)\n-\t\tcheckFrame(t, hierarchicalFrame.SubFrames[0], cLevel3.Rows)\n-\t\trequire.Equal(t, cLevel3.Columns(), hierarchicalFrame.SubFrames[0].Columns())\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/collectors/registry_test.go b/programs/diagnostics/internal/collectors/registry_test.go\ndeleted file mode 100644\nindex eccc5f2265dc..000000000000\n--- a/programs/diagnostics/internal/collectors/registry_test.go\n+++ /dev/null\n@@ -1,57 +0,0 @@\n-package collectors_test\n-\n-import (\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse\"\n-\t_ \"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/system\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestGetCollectorNames(t *testing.T) {\n-\tt.Run(\"can get all collector names\", func(t *testing.T) {\n-\t\tcollectorNames := collectors.GetCollectorNames(false)\n-\t\trequire.ElementsMatch(t, []string{\"system_db\", \"config\", \"summary\", \"system\", \"logs\", \"db_logs\", \"file\", \"command\", \"zookeeper_db\"}, collectorNames)\n-\t})\n-\n-\tt.Run(\"can get default collector names\", func(t *testing.T) {\n-\t\tcollectorNames := collectors.GetCollectorNames(true)\n-\t\trequire.ElementsMatch(t, []string{\"system_db\", \"config\", \"summary\", \"system\", \"logs\", \"db_logs\"}, collectorNames)\n-\t})\n-}\n-\n-func TestGetCollectorByName(t *testing.T) {\n-\n-\tt.Run(\"can get collector by name\", func(t *testing.T) {\n-\t\tcollector, err := collectors.GetCollectorByName(\"system_db\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, clickhouse.NewSystemDatabaseCollector(platform.GetResourceManager()), collector)\n-\t})\n-\n-\tt.Run(\"fails on non existing collector\", func(t *testing.T) {\n-\t\tcollector, err := collectors.GetCollectorByName(\"random\")\n-\t\trequire.NotNil(t, err)\n-\t\trequire.Equal(t, \"random is not a valid collector name\", err.Error())\n-\t\trequire.Nil(t, collector)\n-\t})\n-}\n-\n-func TestBuildConfigurationOptions(t *testing.T) {\n-\n-\tt.Run(\"can get all collector configurations\", func(t *testing.T) {\n-\t\tconfigs, err := collectors.BuildConfigurationOptions()\n-\t\trequire.Nil(t, err)\n-\t\trequire.Len(t, configs, 9)\n-\t\trequire.Contains(t, configs, \"system_db\")\n-\t\trequire.Contains(t, configs, \"config\")\n-\t\trequire.Contains(t, configs, \"summary\")\n-\t\trequire.Contains(t, configs, \"system\")\n-\t\trequire.Contains(t, configs, \"logs\")\n-\t\trequire.Contains(t, configs, \"db_logs\")\n-\t\trequire.Contains(t, configs, \"file\")\n-\t\trequire.Contains(t, configs, \"command\")\n-\t\trequire.Contains(t, configs, \"zookeeper_db\")\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/collectors/system/command_test.go b/programs/diagnostics/internal/collectors/system/command_test.go\ndeleted file mode 100644\nindex 7de00cdabf46..000000000000\n--- a/programs/diagnostics/internal/collectors/system/command_test.go\n+++ /dev/null\n@@ -1,107 +0,0 @@\n-package system_test\n-\n-import (\n-\t\"fmt\"\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/system\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestCommandConfiguration(t *testing.T) {\n-\tt.Run(\"correct configuration is returned for file collector\", func(t *testing.T) {\n-\t\tcommandCollector := system.NewCommandCollector(&platform.ResourceManager{})\n-\t\tconf := commandCollector.Configuration()\n-\t\trequire.Len(t, conf.Params, 1)\n-\t\trequire.IsType(t, config.StringParam{}, conf.Params[0])\n-\t\tcommand, ok := conf.Params[0].(config.StringParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.True(t, command.Required())\n-\t\trequire.Equal(t, command.Name(), \"command\")\n-\t\trequire.Equal(t, \"\", command.Value)\n-\t})\n-}\n-\n-func TestCommandCollect(t *testing.T) {\n-\tt.Run(\"test simple command with args\", func(t *testing.T) {\n-\t\tcommandCollector := system.NewCommandCollector(&platform.ResourceManager{})\n-\t\tbundle, err := commandCollector.Collect(config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue:      \"ls -l ../../../testdata\",\n-\t\t\t\t\tParam:      config.NewParam(\"command\", \"Command to execute\", true),\n-\t\t\t\t\tAllowEmpty: false,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t})\n-\t\trequire.Nil(t, err)\n-\t\trequire.Nil(t, bundle.Errors.Errors)\n-\t\trequire.Len(t, bundle.Frames, 1)\n-\t\trequire.Contains(t, bundle.Frames, \"output\")\n-\t\trequire.Equal(t, bundle.Frames[\"output\"].Columns(), []string{\"command\", \"stdout\", \"stderr\", \"error\"})\n-\t\tmemFrame := bundle.Frames[\"output\"].(data.MemoryFrame)\n-\t\tvalues, ok, err := memFrame.Next()\n-\t\trequire.True(t, ok)\n-\t\trequire.Nil(t, err)\n-\t\tfmt.Println(values)\n-\t\trequire.Len(t, values, 4)\n-\t\trequire.Equal(t, \"ls -l ../../../testdata\", values[0])\n-\t\trequire.Contains(t, values[1], \"configs\")\n-\t\trequire.Contains(t, values[1], \"docker\")\n-\t\trequire.Contains(t, values[1], \"log\")\n-\t\trequire.Equal(t, \"\", values[2])\n-\t\trequire.Equal(t, \"\", values[3])\n-\t\tvalues, ok, err = memFrame.Next()\n-\t\trequire.False(t, ok)\n-\t\trequire.Nil(t, err)\n-\t\trequire.Nil(t, values)\n-\t})\n-\n-\tt.Run(\"test empty command\", func(t *testing.T) {\n-\t\tcommandCollector := system.NewCommandCollector(&platform.ResourceManager{})\n-\t\tbundle, err := commandCollector.Collect(config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue:      \"\",\n-\t\t\t\t\tParam:      config.NewParam(\"command\", \"Command to execute\", true),\n-\t\t\t\t\tAllowEmpty: false,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t})\n-\t\trequire.Equal(t, \"parameter command is invalid - command cannot be empty\", err.Error())\n-\t\trequire.Equal(t, &data.DiagnosticBundle{}, bundle)\n-\t})\n-\n-\tt.Run(\"test invalid command\", func(t *testing.T) {\n-\t\tcommandCollector := system.NewCommandCollector(&platform.ResourceManager{})\n-\t\tbundle, err := commandCollector.Collect(config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue:      \"ls --invalid ../../../testdata\",\n-\t\t\t\t\tParam:      config.NewParam(\"command\", \"Command to execute\", true),\n-\t\t\t\t\tAllowEmpty: false,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t})\n-\t\t// commands may error with output - we still capture on stderr\n-\t\trequire.Nil(t, err)\n-\t\trequire.Len(t, bundle.Errors.Errors, 1)\n-\t\trequire.Equal(t, \"Unable to execute command: exit status 2\", bundle.Errors.Errors[0].Error())\n-\t\trequire.Len(t, bundle.Frames, 1)\n-\t\trequire.Contains(t, bundle.Frames, \"output\")\n-\t\trequire.Equal(t, bundle.Frames[\"output\"].Columns(), []string{\"command\", \"stdout\", \"stderr\", \"error\"})\n-\t\tmemFrame := bundle.Frames[\"output\"].(data.MemoryFrame)\n-\t\tvalues, ok, err := memFrame.Next()\n-\t\trequire.True(t, ok)\n-\t\trequire.Nil(t, err)\n-\t\trequire.Len(t, values, 4)\n-\t\trequire.Equal(t, \"ls --invalid ../../../testdata\", values[0])\n-\t\trequire.Equal(t, \"\", values[1])\n-\t\t// exact values here may vary on platform\n-\t\trequire.NotEmpty(t, values[2])\n-\t\trequire.NotEmpty(t, values[3])\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/collectors/system/file_test.go b/programs/diagnostics/internal/collectors/system/file_test.go\ndeleted file mode 100644\nindex 5b1d5b3a92f0..000000000000\n--- a/programs/diagnostics/internal/collectors/system/file_test.go\n+++ /dev/null\n@@ -1,110 +0,0 @@\n-package system_test\n-\n-import (\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/system\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestFileConfiguration(t *testing.T) {\n-\tt.Run(\"correct configuration is returned for file collector\", func(t *testing.T) {\n-\t\tfileCollector := system.NewFileCollector(&platform.ResourceManager{})\n-\t\tconf := fileCollector.Configuration()\n-\t\trequire.Len(t, conf.Params, 1)\n-\t\trequire.IsType(t, config.StringParam{}, conf.Params[0])\n-\t\tfilePattern, ok := conf.Params[0].(config.StringParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.True(t, filePattern.Required())\n-\t\trequire.Equal(t, filePattern.Name(), \"file_pattern\")\n-\t\trequire.Equal(t, \"\", filePattern.Value)\n-\t})\n-}\n-\n-func TestFileCollect(t *testing.T) {\n-\n-\tt.Run(\"test filter patterns work\", func(t *testing.T) {\n-\t\tfileCollector := system.NewFileCollector(&platform.ResourceManager{})\n-\t\tbundle, err := fileCollector.Collect(config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue:      \"../../../testdata/**/*.xml\",\n-\t\t\t\t\tParam:      config.NewParam(\"file_pattern\", \"Glob based pattern to specify files for collection\", true),\n-\t\t\t\t\tAllowEmpty: false,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t})\n-\t\trequire.Nil(t, err)\n-\t\trequire.Nil(t, bundle.Errors.Errors)\n-\t\tcheckFileBundle(t, bundle,\n-\t\t\t[]string{\"../../../testdata/configs/include/xml/server-include.xml\",\n-\t\t\t\t\"../../../testdata/configs/include/xml/user-include.xml\",\n-\t\t\t\t\"../../../testdata/configs/xml/config.xml\",\n-\t\t\t\t\"../../../testdata/configs/xml/users.xml\",\n-\t\t\t\t\"../../../testdata/configs/xml/users.d/default-password.xml\",\n-\t\t\t\t\"../../../testdata/configs/yandex_xml/config.xml\",\n-\t\t\t\t\"../../../testdata/docker/admin.xml\",\n-\t\t\t\t\"../../../testdata/docker/custom.xml\"})\n-\t})\n-\n-\tt.Run(\"invalid file patterns are detected\", func(t *testing.T) {\n-\t\tfileCollector := system.NewFileCollector(&platform.ResourceManager{})\n-\t\tbundle, err := fileCollector.Collect(config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue:      \"\",\n-\t\t\t\t\tParam:      config.NewParam(\"file_pattern\", \"Glob based pattern to specify files for collection\", true),\n-\t\t\t\t\tAllowEmpty: false,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t})\n-\t\trequire.NotNil(t, err)\n-\t\trequire.Equal(t, \"parameter file_pattern is invalid - file_pattern cannot be empty\", err.Error())\n-\t\trequire.Equal(t, &data.DiagnosticBundle{}, bundle)\n-\t})\n-\n-\tt.Run(\"check empty matches are reported\", func(t *testing.T) {\n-\t\tfileCollector := system.NewFileCollector(&platform.ResourceManager{})\n-\t\tbundle, err := fileCollector.Collect(config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue:      \"../../../testdata/**/*.random\",\n-\t\t\t\t\tParam:      config.NewParam(\"file_pattern\", \"Glob based pattern to specify files for collection\", true),\n-\t\t\t\t\tAllowEmpty: false,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t})\n-\t\trequire.Nil(t, err)\n-\t\trequire.Nil(t, bundle.Frames)\n-\t\trequire.Len(t, bundle.Errors.Errors, 1)\n-\t\trequire.Equal(t, \"0 files match glob pattern\", bundle.Errors.Errors[0].Error())\n-\t})\n-\n-}\n-\n-func checkFileBundle(t *testing.T, bundle *data.DiagnosticBundle, expectedFiles []string) {\n-\trequire.NotNil(t, bundle)\n-\trequire.Nil(t, bundle.Errors.Errors)\n-\trequire.Len(t, bundle.Frames, 1)\n-\trequire.Contains(t, bundle.Frames, \"collection\")\n-\tdirFrame, ok := bundle.Frames[\"collection\"].(data.FileFrame)\n-\trequire.True(t, ok)\n-\trequire.Equal(t, []string{\"files\"}, dirFrame.Columns())\n-\ti := 0\n-\tfor {\n-\t\tvalues, ok, err := dirFrame.Next()\n-\t\trequire.Nil(t, err)\n-\t\tif !ok {\n-\t\t\tbreak\n-\t\t}\n-\t\trequire.Len(t, values, 1)\n-\t\tfile, ok := values[0].(data.SimpleFile)\n-\t\trequire.True(t, ok)\n-\t\trequire.Contains(t, expectedFiles, file.FilePath())\n-\t\ti += 1\n-\t}\n-\trequire.Equal(t, len(expectedFiles), i)\n-}\ndiff --git a/programs/diagnostics/internal/collectors/system/system_test.go b/programs/diagnostics/internal/collectors/system/system_test.go\ndeleted file mode 100644\nindex fb1e16bd1ed3..000000000000\n--- a/programs/diagnostics/internal/collectors/system/system_test.go\n+++ /dev/null\n@@ -1,89 +0,0 @@\n-package system_test\n-\n-import (\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/system\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestSystemConfiguration(t *testing.T) {\n-\tt.Run(\"correct configuration is returned for system collector\", func(t *testing.T) {\n-\t\tsystemCollector := system.NewSystemCollector(&platform.ResourceManager{})\n-\t\tconf := systemCollector.Configuration()\n-\t\trequire.Len(t, conf.Params, 0)\n-\t\trequire.Equal(t, []config.ConfigParam{}, conf.Params)\n-\t})\n-}\n-\n-func TestSystemCollect(t *testing.T) {\n-\tt.Run(\"test default system collection\", func(t *testing.T) {\n-\t\tsystemCollector := system.NewSystemCollector(&platform.ResourceManager{})\n-\t\tdiagSet, err := systemCollector.Collect(config.Configuration{})\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, diagSet)\n-\t\trequire.Len(t, diagSet.Errors.Errors, 0)\n-\t\trequire.Len(t, diagSet.Frames, 7)\n-\t\trequire.Contains(t, diagSet.Frames, \"disks\")\n-\t\trequire.Contains(t, diagSet.Frames, \"disk_usage\")\n-\t\trequire.Contains(t, diagSet.Frames, \"memory\")\n-\t\trequire.Contains(t, diagSet.Frames, \"memory_usage\")\n-\t\trequire.Contains(t, diagSet.Frames, \"cpu\")\n-\t\trequire.Contains(t, diagSet.Frames, \"processes\")\n-\t\trequire.Contains(t, diagSet.Frames, \"os\")\n-\t\t// responses here will vary depending on platform - mocking seems excessive so we test we have some data\n-\t\t// disks\n-\t\trequire.Equal(t, []string{\"name\", \"size\", \"physicalBlockSize\", \"driveType\", \"controller\", \"vendor\", \"model\", \"partitionName\", \"partitionSize\", \"mountPoint\", \"readOnly\"}, diagSet.Frames[\"disks\"].Columns())\n-\t\tdiskFrames, err := countFrameRows(diagSet, \"disks\")\n-\t\trequire.Greater(t, diskFrames, 0)\n-\t\trequire.Nil(t, err)\n-\t\t// disk usage\n-\t\trequire.Equal(t, []string{\"filesystem\", \"size\", \"used\", \"avail\", \"use%\", \"mounted on\"}, diagSet.Frames[\"disk_usage\"].Columns())\n-\t\tdiskUsageFrames, err := countFrameRows(diagSet, \"disk_usage\")\n-\t\trequire.Greater(t, diskUsageFrames, 0)\n-\t\trequire.Nil(t, err)\n-\t\t// memory\n-\t\trequire.Equal(t, []string{\"totalPhysical\", \"totalUsable\", \"supportedPageSizes\"}, diagSet.Frames[\"memory\"].Columns())\n-\t\tmemoryFrames, err := countFrameRows(diagSet, \"memory\")\n-\t\trequire.Greater(t, memoryFrames, 0)\n-\t\trequire.Nil(t, err)\n-\t\t// memory_usage\n-\t\trequire.Equal(t, []string{\"type\", \"total\", \"used\", \"free\"}, diagSet.Frames[\"memory_usage\"].Columns())\n-\t\tmemoryUsageFrames, err := countFrameRows(diagSet, \"memory_usage\")\n-\t\trequire.Greater(t, memoryUsageFrames, 0)\n-\t\trequire.Nil(t, err)\n-\t\t// cpu\n-\t\trequire.Equal(t, []string{\"processor\", \"vendor\", \"model\", \"core\", \"numThreads\", \"logical\", \"capabilities\"}, diagSet.Frames[\"cpu\"].Columns())\n-\t\tcpuFrames, err := countFrameRows(diagSet, \"cpu\")\n-\t\trequire.Greater(t, cpuFrames, 0)\n-\t\trequire.Nil(t, err)\n-\t\t// processes\n-\t\trequire.Equal(t, []string{\"pid\", \"ppid\", \"stime\", \"time\", \"rss\", \"size\", \"faults\", \"minorFaults\", \"majorFaults\", \"user\", \"state\", \"priority\", \"nice\", \"command\"}, diagSet.Frames[\"processes\"].Columns())\n-\t\tprocessesFrames, err := countFrameRows(diagSet, \"processes\")\n-\t\trequire.Greater(t, processesFrames, 0)\n-\t\trequire.Nil(t, err)\n-\t\t// os\n-\t\trequire.Equal(t, []string{\"hostname\", \"os\", \"goOs\", \"cpus\", \"core\", \"kernel\", \"platform\"}, diagSet.Frames[\"os\"].Columns())\n-\t\tosFrames, err := countFrameRows(diagSet, \"os\")\n-\t\trequire.Greater(t, osFrames, 0)\n-\t\trequire.Nil(t, err)\n-\t})\n-}\n-\n-func countFrameRows(diagSet *data.DiagnosticBundle, frameName string) (int, error) {\n-\tframe := diagSet.Frames[frameName]\n-\ti := 0\n-\tfor {\n-\t\t_, ok, err := frame.Next()\n-\t\tif !ok {\n-\t\t\treturn i, err\n-\t\t}\n-\t\tif err != nil {\n-\t\t\treturn i, err\n-\t\t}\n-\t\ti++\n-\t}\n-}\ndiff --git a/programs/diagnostics/internal/outputs/file/simple_test.go b/programs/diagnostics/internal/outputs/file/simple_test.go\ndeleted file mode 100644\nindex 471a1c70cc1b..000000000000\n--- a/programs/diagnostics/internal/outputs/file/simple_test.go\n+++ /dev/null\n@@ -1,468 +0,0 @@\n-package file_test\n-\n-import (\n-\t\"bufio\"\n-\t\"encoding/xml\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"os\"\n-\t\"path\"\n-\t\"strings\"\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs/file\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-var clusterFrame = test.NewFakeDataFrame(\"clusters\", []string{\"cluster\", \"shard_num\", \"shard_weight\", \"replica_num\", \"host_name\", \"host_address\", \"port\", \"is_local\", \"user\", \"default_database\", \"errors_count\", \"slowdowns_count\", \"estimated_recovery_time\"},\n-\t[][]interface{}{\n-\t\t{\"events\", 1, 1, 1, \"dalem-local-clickhouse-blue-1\", \"192.168.144.2\", 9000, 1, \"default\", \"\", 0, 0, 0},\n-\t\t{\"events\", 2, 1, 1, \"dalem-local-clickhouse-blue-2\", \"192.168.144.4\", 9001, 1, \"default\", \"\", 0, 0, 0},\n-\t\t{\"events\", 3, 1, 1, \"dalem-local-clickhouse-blue-3\", \"192.168.144.3\", 9002, 1, \"default\", \"\", 0, 0, 0},\n-\t},\n-)\n-\n-var diskFrame = test.NewFakeDataFrame(\"disks\", []string{\"name\", \"path\", \"free_space\", \"total_space\", \"keep_free_space\", \"type\"},\n-\t[][]interface{}{\n-\t\t{\"default\", \"/var/lib/clickhouse\", 1729659346944, 1938213220352, \"\", \"local\"},\n-\t},\n-)\n-\n-var userFrame = test.NewFakeDataFrame(\"users\", []string{\"name\", \"id\", \"storage\", \"auth_type\", \"auth_params\", \"host_ip\", \"host_names\", \"host_names_regexp\", \"host_names_like\"},\n-\t[][]interface{}{\n-\t\t{\"default\", \"94309d50-4f52-5250-31bd-74fecac179db,users.xml,plaintext_password\", \"sha256_password\", []string{\"::0\"}, []string{}, []string{}, []string{}},\n-\t},\n-)\n-\n-func TestConfiguration(t *testing.T) {\n-\tt.Run(\"correct configuration is returned\", func(t *testing.T) {\n-\t\toutput := file.SimpleOutput{}\n-\t\tconf := output.Configuration()\n-\t\trequire.Len(t, conf.Params, 3)\n-\t\t// check first directory param\n-\t\trequire.IsType(t, config.StringParam{}, conf.Params[0])\n-\t\tdirectory, ok := conf.Params[0].(config.StringParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, directory.Required())\n-\t\trequire.Equal(t, \"directory\", directory.Name())\n-\t\trequire.Equal(t, \"./\", directory.Value)\n-\t\t// check second format param\n-\t\trequire.IsType(t, config.StringOptions{}, conf.Params[1])\n-\t\tformat, ok := conf.Params[1].(config.StringOptions)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, format.Required())\n-\t\trequire.Equal(t, \"format\", format.Name())\n-\t\trequire.Equal(t, \"csv\", format.Value)\n-\t\trequire.Equal(t, []string{\"csv\"}, format.Options)\n-\t\t// check third format compress\n-\t\trequire.IsType(t, config.BoolParam{}, conf.Params[2])\n-\t\tskipArchive, ok := conf.Params[2].(config.BoolParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, format.Required())\n-\t\trequire.False(t, skipArchive.Value)\n-\t})\n-}\n-\n-func TestWrite(t *testing.T) {\n-\tbundles := map[string]*data.DiagnosticBundle{\n-\t\t\"systemA\": {\n-\t\t\tFrames: map[string]data.Frame{\n-\t\t\t\t\"disk\":    diskFrame,\n-\t\t\t\t\"cluster\": clusterFrame,\n-\t\t\t},\n-\t\t},\n-\t\t\"systemB\": {\n-\t\t\tFrames: map[string]data.Frame{\n-\t\t\t\t\"user\": userFrame,\n-\t\t\t},\n-\t\t},\n-\t}\n-\tt.Run(\"test we can write simple diagnostic sets\", func(t *testing.T) {\n-\t\ttempDir := t.TempDir()\n-\t\tconfiguration := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tParam: config.NewParam(\"directory\", \"A directory\", true),\n-\t\t\t\t\tValue: tempDir,\n-\t\t\t\t},\n-\t\t\t\t// turn compression off as the folder will be deleted by default\n-\t\t\t\tconfig.BoolParam{\n-\t\t\t\t\tValue: true,\n-\t\t\t\t\tParam: config.NewParam(\"skip_archive\", \"Skip archive\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\toutput := file.SimpleOutput{FolderGenerator: staticFolderName}\n-\t\tframeErrors, err := output.Write(\"test\", bundles, configuration)\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, data.FrameErrors{}, frameErrors)\n-\t\tclusterFile := path.Join(tempDir, \"test\", \"test\", \"systemA\", \"cluster.csv\")\n-\t\tdiskFile := path.Join(tempDir, \"test\", \"test\", \"systemA\", \"disk.csv\")\n-\t\tuserFile := path.Join(tempDir, \"test\", \"test\", \"systemB\", \"user.csv\")\n-\t\trequire.FileExists(t, clusterFile)\n-\t\trequire.FileExists(t, diskFile)\n-\t\trequire.FileExists(t, userFile)\n-\t\tdiskLines, err := readFileLines(diskFile)\n-\t\trequire.Nil(t, err)\n-\t\trequire.Len(t, diskLines, 2)\n-\t\tusersLines, err := readFileLines(userFile)\n-\t\trequire.Nil(t, err)\n-\t\trequire.Len(t, usersLines, 2)\n-\t\tclusterLines, err := readFileLines(clusterFile)\n-\t\trequire.Nil(t, err)\n-\t\trequire.Len(t, clusterLines, 4)\n-\t\trequire.Equal(t, strings.Join(clusterFrame.ColumnNames, \",\"), clusterLines[0])\n-\t\trequire.Equal(t, \"events,1,1,1,dalem-local-clickhouse-blue-1,192.168.144.2,9000,1,default,,0,0,0\", clusterLines[1])\n-\t\trequire.Equal(t, \"events,2,1,1,dalem-local-clickhouse-blue-2,192.168.144.4,9001,1,default,,0,0,0\", clusterLines[2])\n-\t\trequire.Equal(t, \"events,3,1,1,dalem-local-clickhouse-blue-3,192.168.144.3,9002,1,default,,0,0,0\", clusterLines[3])\n-\t\tresetFrames()\n-\t})\n-\n-\tt.Run(\"test invalid parameter\", func(t *testing.T) {\n-\t\ttempDir := t.TempDir()\n-\t\tconfiguration := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tParam: config.NewParam(\"directory\", \"A directory\", true),\n-\t\t\t\t\tValue: tempDir,\n-\t\t\t\t},\n-\t\t\t\tconfig.StringOptions{\n-\t\t\t\t\tValue:   \"random\",\n-\t\t\t\t\tOptions: []string{\"csv\"},\n-\t\t\t\t\t// TODO: add tsv and others here later\n-\t\t\t\t\tParam: config.NewParam(\"format\", \"Format of exported files\", false),\n-\t\t\t\t},\n-\t\t\t\tconfig.BoolParam{\n-\t\t\t\t\tValue: true,\n-\t\t\t\t\tParam: config.NewParam(\"skip_archive\", \"Skip compressed archive\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\toutput := file.SimpleOutput{FolderGenerator: staticFolderName}\n-\t\tframeErrors, err := output.Write(\"test\", bundles, configuration)\n-\t\trequire.Equal(t, data.FrameErrors{}, frameErrors)\n-\t\trequire.NotNil(t, err)\n-\t\trequire.Equal(t, \"parameter format is invalid - random is not a valid value for format - [csv]\", err.Error())\n-\t\tresetFrames()\n-\t})\n-\n-\tt.Run(\"test compression\", func(t *testing.T) {\n-\t\ttempDir := t.TempDir()\n-\t\tconfiguration := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tParam: config.NewParam(\"directory\", \"A directory\", true),\n-\t\t\t\t\tValue: tempDir,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\toutput := file.SimpleOutput{FolderGenerator: staticFolderName}\n-\t\tframeErrors, err := output.Write(\"test\", bundles, configuration)\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, data.FrameErrors{}, frameErrors)\n-\t\tarchiveFileName := path.Join(tempDir, \"test\", \"test.tar.gz\")\n-\t\tfi, err := os.Stat(archiveFileName)\n-\t\trequire.Nil(t, err)\n-\t\trequire.FileExists(t, archiveFileName)\n-\t\t// compression will vary so lets test range\n-\t\trequire.Greater(t, int64(600), fi.Size())\n-\t\trequire.Less(t, int64(200), fi.Size())\n-\t\toutputFolder := path.Join(tempDir, \"test\", \"test\")\n-\t\t// check the folder doesn't exist and is cleaned up\n-\t\trequire.NoFileExists(t, outputFolder)\n-\t\tresetFrames()\n-\t})\n-\n-\tt.Run(\"test support for directory frames\", func(t *testing.T) {\n-\t\t// create 5 temporary files\n-\t\ttempDir := t.TempDir()\n-\t\tfiles := createRandomFiles(tempDir, 5)\n-\t\tdirFrame, errs := data.NewFileDirectoryFrame(tempDir, []string{\"*.log\"})\n-\t\trequire.Empty(t, errs)\n-\t\tfileBundles := map[string]*data.DiagnosticBundle{\n-\t\t\t\"systemA\": {\n-\t\t\t\tFrames: map[string]data.Frame{\n-\t\t\t\t\t\"disk\":    diskFrame,\n-\t\t\t\t\t\"cluster\": clusterFrame,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\t\"config\": {\n-\t\t\t\tFrames: map[string]data.Frame{\n-\t\t\t\t\t\"logs\": dirFrame,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tdestDir := t.TempDir()\n-\t\tconfiguration := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tParam: config.NewParam(\"directory\", \"A directory\", true),\n-\t\t\t\t\tValue: destDir,\n-\t\t\t\t},\n-\t\t\t\t// turn compression off as the folder will be deleted by default\n-\t\t\t\tconfig.BoolParam{\n-\t\t\t\t\tValue: true,\n-\t\t\t\t\tParam: config.NewParam(\"skip_archive\", \"Skip archive\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\toutput := file.SimpleOutput{FolderGenerator: staticFolderName}\n-\t\tframeErrors, err := output.Write(\"test\", fileBundles, configuration)\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, frameErrors)\n-\n-\t\t// test the usual frames still work\n-\t\tclusterFile := path.Join(destDir, \"test\", \"test\", \"systemA\", \"cluster.csv\")\n-\t\tdiskFile := path.Join(destDir, \"test\", \"test\", \"systemA\", \"disk.csv\")\n-\t\trequire.FileExists(t, clusterFile)\n-\t\trequire.FileExists(t, diskFile)\n-\t\tdiskLines, err := readFileLines(diskFile)\n-\t\trequire.Nil(t, err)\n-\t\trequire.Len(t, diskLines, 2)\n-\t\tclusterLines, err := readFileLines(clusterFile)\n-\t\trequire.Nil(t, err)\n-\t\trequire.Len(t, clusterLines, 4)\n-\t\trequire.Equal(t, strings.Join(clusterFrame.ColumnNames, \",\"), clusterLines[0])\n-\t\trequire.Equal(t, \"events,1,1,1,dalem-local-clickhouse-blue-1,192.168.144.2,9000,1,default,,0,0,0\", clusterLines[1])\n-\t\trequire.Equal(t, \"events,2,1,1,dalem-local-clickhouse-blue-2,192.168.144.4,9001,1,default,,0,0,0\", clusterLines[2])\n-\t\trequire.Equal(t, \"events,3,1,1,dalem-local-clickhouse-blue-3,192.168.144.3,9002,1,default,,0,0,0\", clusterLines[3])\n-\t\t//test our directory frame\n-\t\tfor _, filepath := range files {\n-\t\t\t// check they were copied\n-\t\t\tsubPath := strings.TrimPrefix(filepath, tempDir)\n-\t\t\t// path here will be <destDir>/<id>/test>/config/logs/<sub path>\n-\t\t\tnewPath := path.Join(destDir, \"test\", \"test\", \"config\", \"logs\", subPath)\n-\t\t\trequire.FileExists(t, newPath)\n-\t\t}\n-\t\tresetFrames()\n-\t})\n-\n-\tt.Run(\"test support for config frames\", func(t *testing.T) {\n-\t\txmlConfig := data.XmlConfig{\n-\t\t\tXMLName: xml.Name{},\n-\t\t\tClickhouse: data.XmlLoggerConfig{\n-\t\t\t\tXMLName:  xml.Name{},\n-\t\t\t\tErrorLog: \"/var/log/clickhouse-server/clickhouse-server.err.log\",\n-\t\t\t\tLog:      \"/var/log/clickhouse-server/clickhouse-server.log\",\n-\t\t\t},\n-\t\t\tIncludeFrom: \"\",\n-\t\t}\n-\t\ttempDir := t.TempDir()\n-\t\tconfDir := path.Join(tempDir, \"conf\")\n-\t\t// create an includes file\n-\t\tincludesDir := path.Join(tempDir, \"includes\")\n-\t\terr := os.MkdirAll(includesDir, os.ModePerm)\n-\t\trequire.Nil(t, err)\n-\t\tincludesPath := path.Join(includesDir, \"random.xml\")\n-\t\tincludesFile, err := os.Create(includesPath)\n-\t\trequire.Nil(t, err)\n-\t\txmlWriter := io.Writer(includesFile)\n-\t\tenc := xml.NewEncoder(xmlWriter)\n-\t\tenc.Indent(\"  \", \"    \")\n-\t\terr = enc.Encode(xmlConfig)\n-\t\trequire.Nil(t, err)\n-\t\t// create 5 temporary config files\n-\t\tfiles := make([]string, 5)\n-\t\t// set the includes\n-\t\txmlConfig.IncludeFrom = includesPath\n-\t\tfor i := 0; i < 5; i++ {\n-\t\t\t// we want to check hierarchies are preserved so create a simple folder for each file\n-\t\t\tfileDir := path.Join(confDir, fmt.Sprintf(\"%d\", i))\n-\t\t\terr := os.MkdirAll(fileDir, os.ModePerm)\n-\t\t\trequire.Nil(t, err)\n-\t\t\tfilepath := path.Join(fileDir, fmt.Sprintf(\"random-%d.xml\", i))\n-\t\t\tfiles[i] = filepath\n-\t\t\txmlFile, err := os.Create(filepath)\n-\t\t\trequire.Nil(t, err)\n-\t\t\t// write a little xml so its valid\n-\t\t\txmlWriter := io.Writer(xmlFile)\n-\t\t\tenc := xml.NewEncoder(xmlWriter)\n-\t\t\tenc.Indent(\"  \", \"    \")\n-\t\t\terr = enc.Encode(xmlConfig)\n-\t\t\trequire.Nil(t, err)\n-\t\t}\n-\t\tconfigFrame, errs := data.NewConfigFileFrame(confDir)\n-\t\trequire.Empty(t, errs)\n-\t\tfileBundles := map[string]*data.DiagnosticBundle{\n-\t\t\t\"systemA\": {\n-\t\t\t\tFrames: map[string]data.Frame{\n-\t\t\t\t\t\"disk\":    diskFrame,\n-\t\t\t\t\t\"cluster\": clusterFrame,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\t\"config\": {\n-\t\t\t\tFrames: map[string]data.Frame{\n-\t\t\t\t\t\"user_specified\": configFrame,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tdestDir := t.TempDir()\n-\t\tconfiguration := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tParam: config.NewParam(\"directory\", \"A directory\", true),\n-\t\t\t\t\tValue: destDir,\n-\t\t\t\t},\n-\t\t\t\t// turn compression off as the folder will be deleted by default\n-\t\t\t\tconfig.BoolParam{\n-\t\t\t\t\tValue: true,\n-\t\t\t\t\tParam: config.NewParam(\"skip_archive\", \"Skip archive\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\toutput := file.SimpleOutput{FolderGenerator: staticFolderName}\n-\t\tframeErrors, err := output.Write(\"test\", fileBundles, configuration)\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, frameErrors)\n-\t\trequire.Empty(t, frameErrors.Errors)\n-\t\t//test our config frame\n-\t\tfor _, filepath := range files {\n-\t\t\t// check they were copied\n-\t\t\tsubPath := strings.TrimPrefix(filepath, confDir)\n-\t\t\t// path here will be <destDir>/<id>/test>/config/user_specified/file\n-\t\t\tnewPath := path.Join(destDir, \"test\", \"test\", \"config\", \"user_specified\", subPath)\n-\t\t\trequire.FileExists(t, newPath)\n-\t\t}\n-\t\t// check our includes file exits\n-\t\t// path here will be <destDir>/<id>/test>/config/user_specified/file/includes\n-\t\trequire.FileExists(t, path.Join(destDir, \"test\", \"test\", \"config\", \"user_specified\", \"includes\", includesPath))\n-\t\tresetFrames()\n-\t})\n-\n-\tt.Run(\"test support for file frames\", func(t *testing.T) {\n-\t\t// create 5 temporary files\n-\t\ttempDir := t.TempDir()\n-\t\tfiles := createRandomFiles(tempDir, 5)\n-\t\tfileFrame := data.NewFileFrame(\"collection\", files)\n-\t\tfileBundles := map[string]*data.DiagnosticBundle{\n-\t\t\t\"systemA\": {\n-\t\t\t\tFrames: map[string]data.Frame{\n-\t\t\t\t\t\"disk\":    diskFrame,\n-\t\t\t\t\t\"cluster\": clusterFrame,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\t\"file\": {\n-\t\t\t\tFrames: map[string]data.Frame{\n-\t\t\t\t\t\"collection\": fileFrame,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tdestDir := t.TempDir()\n-\t\tconfiguration := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tParam: config.NewParam(\"directory\", \"A directory\", true),\n-\t\t\t\t\tValue: destDir,\n-\t\t\t\t},\n-\t\t\t\t// turn compression off as the folder will be deleted by default\n-\t\t\t\tconfig.BoolParam{\n-\t\t\t\t\tValue: true,\n-\t\t\t\t\tParam: config.NewParam(\"skip_archive\", \"Skip archive\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\toutput := file.SimpleOutput{FolderGenerator: staticFolderName}\n-\t\tframeErrors, err := output.Write(\"test\", fileBundles, configuration)\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, frameErrors)\n-\t\t//test our directory frame\n-\t\tfor _, filepath := range files {\n-\t\t\t// path here will be <destDir>/<id>/test>/file/collection/<sub path>\n-\t\t\tnewPath := path.Join(destDir, \"test\", \"test\", \"file\", \"collection\", filepath)\n-\t\t\trequire.FileExists(t, newPath)\n-\t\t}\n-\t\tresetFrames()\n-\t})\n-\n-\tt.Run(\"test support for hierarchical frames\", func(t *testing.T) {\n-\t\tbottomFrame := data.NewHierarchicalFrame(\"bottomLevel\", userFrame, []data.HierarchicalFrame{})\n-\t\tmiddleFrame := data.NewHierarchicalFrame(\"middleLevel\", diskFrame, []data.HierarchicalFrame{bottomFrame})\n-\t\ttopFrame := data.NewHierarchicalFrame(\"topLevel\", clusterFrame, []data.HierarchicalFrame{middleFrame})\n-\t\ttempDir := t.TempDir()\n-\t\tconfiguration := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tParam: config.NewParam(\"directory\", \"A directory\", true),\n-\t\t\t\t\tValue: tempDir,\n-\t\t\t\t},\n-\t\t\t\t// turn compression off as the folder will be deleted by default\n-\t\t\t\tconfig.BoolParam{\n-\t\t\t\t\tValue: true,\n-\t\t\t\t\tParam: config.NewParam(\"skip_archive\", \"Skip archive\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\toutput := file.SimpleOutput{FolderGenerator: staticFolderName}\n-\t\thierarchicalBundle := map[string]*data.DiagnosticBundle{\n-\t\t\t\"systemA\": {\n-\t\t\t\tFrames: map[string]data.Frame{\n-\t\t\t\t\t\"topLevel\": topFrame,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tframeErrors, err := output.Write(\"test\", hierarchicalBundle, configuration)\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, data.FrameErrors{}, frameErrors)\n-\t\ttopFile := path.Join(tempDir, \"test\", \"test\", \"systemA\", \"topLevel.csv\")\n-\t\tmiddleFile := path.Join(tempDir, \"test\", \"test\", \"systemA\", \"middleLevel\", \"middleLevel.csv\")\n-\t\tbottomFile := path.Join(tempDir, \"test\", \"test\", \"systemA\", \"middleLevel\", \"bottomLevel\", \"bottomLevel.csv\")\n-\t\trequire.FileExists(t, topFile)\n-\t\trequire.FileExists(t, middleFile)\n-\t\trequire.FileExists(t, bottomFile)\n-\t\ttopLines, err := readFileLines(topFile)\n-\t\trequire.Nil(t, err)\n-\t\trequire.Len(t, topLines, 4)\n-\t\tmiddleLines, err := readFileLines(middleFile)\n-\t\trequire.Nil(t, err)\n-\t\trequire.Len(t, middleLines, 2)\n-\t\tbottomLines, err := readFileLines(bottomFile)\n-\t\trequire.Nil(t, err)\n-\t\trequire.Len(t, bottomLines, 2)\n-\t\trequire.Equal(t, strings.Join(clusterFrame.ColumnNames, \",\"), topLines[0])\n-\t\trequire.Equal(t, \"events,1,1,1,dalem-local-clickhouse-blue-1,192.168.144.2,9000,1,default,,0,0,0\", topLines[1])\n-\t\trequire.Equal(t, \"events,2,1,1,dalem-local-clickhouse-blue-2,192.168.144.4,9001,1,default,,0,0,0\", topLines[2])\n-\t\trequire.Equal(t, \"events,3,1,1,dalem-local-clickhouse-blue-3,192.168.144.3,9002,1,default,,0,0,0\", topLines[3])\n-\t\tresetFrames()\n-\t})\n-}\n-\n-func createRandomFiles(tempDir string, num int) []string {\n-\tfiles := make([]string, num)\n-\tfor i := 0; i < 5; i++ {\n-\t\t// we want to check hierarchies are preserved so create a simple folder for each file\n-\t\tfileDir := path.Join(tempDir, fmt.Sprintf(\"%d\", i))\n-\t\tos.MkdirAll(fileDir, os.ModePerm) //nolint:errcheck\n-\t\tfilepath := path.Join(fileDir, fmt.Sprintf(\"random-%d.log\", i))\n-\t\tfiles[i] = filepath\n-\t\tos.Create(filepath) //nolint:errcheck\n-\t}\n-\treturn files\n-}\n-\n-func resetFrames() {\n-\tclusterFrame.Reset()\n-\tuserFrame.Reset()\n-\tdiskFrame.Reset()\n-}\n-\n-func readFileLines(filename string) ([]string, error) {\n-\tfile, err := os.Open(filename)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tdefer file.Close()\n-\n-\tvar lines []string\n-\tscanner := bufio.NewScanner(file)\n-\tfor scanner.Scan() {\n-\t\tlines = append(lines, scanner.Text())\n-\t}\n-\treturn lines, scanner.Err()\n-}\n-\n-func staticFolderName() string {\n-\treturn \"test\"\n-}\ndiff --git a/programs/diagnostics/internal/outputs/registry_test.go b/programs/diagnostics/internal/outputs/registry_test.go\ndeleted file mode 100644\nindex ba8408e5a594..000000000000\n--- a/programs/diagnostics/internal/outputs/registry_test.go\n+++ /dev/null\n@@ -1,45 +0,0 @@\n-package outputs_test\n-\n-import (\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs/file\"\n-\t_ \"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs/terminal\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestGetOutputNames(t *testing.T) {\n-\tt.Run(\"can get all output names\", func(t *testing.T) {\n-\t\toutputNames := outputs.GetOutputNames()\n-\t\trequire.ElementsMatch(t, []string{\"simple\", \"report\"}, outputNames)\n-\t})\n-\n-}\n-\n-func TestGetOutputByName(t *testing.T) {\n-\n-\tt.Run(\"can get output by name\", func(t *testing.T) {\n-\t\toutput, err := outputs.GetOutputByName(\"simple\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, file.SimpleOutput{}, output)\n-\t})\n-\n-\tt.Run(\"fails on non existing output\", func(t *testing.T) {\n-\t\toutput, err := outputs.GetOutputByName(\"random\")\n-\t\trequire.NotNil(t, err)\n-\t\trequire.Equal(t, \"random is not a valid output name\", err.Error())\n-\t\trequire.Nil(t, output)\n-\t})\n-}\n-\n-func TestBuildConfigurationOptions(t *testing.T) {\n-\n-\tt.Run(\"can get all output configurations\", func(t *testing.T) {\n-\t\toutputs, err := outputs.BuildConfigurationOptions()\n-\t\trequire.Nil(t, err)\n-\t\trequire.Len(t, outputs, 2)\n-\t\trequire.Contains(t, outputs, \"simple\")\n-\t\trequire.Contains(t, outputs, \"report\")\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/platform/config/models_test.go b/programs/diagnostics/internal/platform/config/models_test.go\ndeleted file mode 100644\nindex 916d20ec28b1..000000000000\n--- a/programs/diagnostics/internal/platform/config/models_test.go\n+++ /dev/null\n@@ -1,182 +0,0 @@\n-package config_test\n-\n-import (\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-var conf = config.Configuration{\n-\tParams: []config.ConfigParam{\n-\t\tconfig.StringListParam{\n-\t\t\tValues: []string{\"some\", \"values\"},\n-\t\t\tParam:  config.NewParam(\"paramA\", \"\", false),\n-\t\t},\n-\t\tconfig.StringParam{\n-\t\t\tValue: \"random\",\n-\t\t\tParam: config.NewParam(\"paramB\", \"\", true),\n-\t\t},\n-\t\tconfig.StringParam{\n-\t\t\tValue:      \"\",\n-\t\t\tAllowEmpty: true,\n-\t\t\tParam:      config.NewParam(\"paramC\", \"\", false),\n-\t\t},\n-\t\tconfig.StringOptions{\n-\t\t\tValue:      \"random\",\n-\t\t\tOptions:    []string{\"random\", \"very_random\", \"very_very_random\"},\n-\t\t\tParam:      config.NewParam(\"paramD\", \"\", false),\n-\t\t\tAllowEmpty: true,\n-\t\t},\n-\t},\n-}\n-\n-func TestGetConfigParam(t *testing.T) {\n-\n-\tt.Run(\"can find get config param by name\", func(t *testing.T) {\n-\t\tparamA, err := conf.GetConfigParam(\"paramA\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, paramA)\n-\t\trequire.IsType(t, config.StringListParam{}, paramA)\n-\t\tstringListParam, ok := paramA.(config.StringListParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, stringListParam.Required())\n-\t\trequire.Equal(t, stringListParam.Name(), \"paramA\")\n-\t\trequire.ElementsMatch(t, stringListParam.Values, []string{\"some\", \"values\"})\n-\t})\n-\n-\tt.Run(\"throws error on missing element\", func(t *testing.T) {\n-\t\tparamZ, err := conf.GetConfigParam(\"paramZ\")\n-\t\trequire.Nil(t, paramZ)\n-\t\trequire.NotNil(t, err)\n-\t\trequire.Equal(t, err.Error(), \"paramZ does not exist\")\n-\t})\n-}\n-\n-func TestValidateConfig(t *testing.T) {\n-\n-\tt.Run(\"validate adds the default and allows override\", func(t *testing.T) {\n-\t\tcustomConf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue: \"custom\",\n-\t\t\t\t\tParam: config.NewParam(\"paramB\", \"\", true),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tnewConf, err := customConf.ValidateConfig(conf)\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, newConf)\n-\t\trequire.Len(t, newConf.Params, 4)\n-\t\t// check first param\n-\t\trequire.IsType(t, config.StringListParam{}, newConf.Params[0])\n-\t\tstringListParam, ok := newConf.Params[0].(config.StringListParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.False(t, stringListParam.Required())\n-\t\trequire.Equal(t, stringListParam.Name(), \"paramA\")\n-\t\trequire.ElementsMatch(t, stringListParam.Values, []string{\"some\", \"values\"})\n-\t\t// check second param\n-\t\trequire.IsType(t, config.StringParam{}, newConf.Params[1])\n-\t\tstringParam, ok := newConf.Params[1].(config.StringParam)\n-\t\trequire.True(t, ok)\n-\t\trequire.True(t, stringParam.Required())\n-\t\trequire.Equal(t, \"paramB\", stringParam.Name())\n-\t\trequire.Equal(t, \"custom\", stringParam.Value)\n-\t})\n-\n-\tt.Run(\"validate errors if missing param\", func(t *testing.T) {\n-\t\t//missing required paramB\n-\t\tcustomConf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringListParam{\n-\t\t\t\t\tValues: []string{\"some\", \"values\"},\n-\t\t\t\t\tParam:  config.NewParam(\"paramA\", \"\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tnewConf, err := customConf.ValidateConfig(conf)\n-\t\trequire.Nil(t, newConf.Params)\n-\t\trequire.NotNil(t, err)\n-\t\trequire.Equal(t, \"missing required parameter paramB - paramB does not exist\", err.Error())\n-\t})\n-\n-\tt.Run(\"validate errors if invalid string value\", func(t *testing.T) {\n-\t\t//missing required paramB\n-\t\tcustomConf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue: \"\",\n-\t\t\t\t\tParam: config.NewParam(\"paramB\", \"\", true),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tnewConf, err := customConf.ValidateConfig(conf)\n-\t\trequire.Nil(t, newConf.Params)\n-\t\trequire.NotNil(t, err)\n-\t\trequire.Equal(t, \"parameter paramB is invalid - paramB cannot be empty\", err.Error())\n-\t})\n-\n-\tt.Run(\"allow empty string value if specified\", func(t *testing.T) {\n-\t\t//missing required paramB\n-\t\tcustomConf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue: \"\",\n-\t\t\t\t\tParam: config.NewParam(\"paramC\", \"\", true),\n-\t\t\t\t},\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue: \"custom\",\n-\t\t\t\t\tParam: config.NewParam(\"paramB\", \"\", true),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tnewConf, err := customConf.ValidateConfig(conf)\n-\t\trequire.NotNil(t, newConf.Params)\n-\t\trequire.Nil(t, err)\n-\t})\n-\n-\tt.Run(\"validate errors if invalid string options value\", func(t *testing.T) {\n-\t\t//missing required paramB\n-\t\tcustomConf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue: \"not_random\",\n-\t\t\t\t\tParam: config.NewParam(\"paramB\", \"\", true),\n-\t\t\t\t},\n-\t\t\t\tconfig.StringOptions{\n-\t\t\t\t\tValue: \"custom\",\n-\t\t\t\t\t// this isn't ideal we need to ensure options are set for this to validate correctly\n-\t\t\t\t\tOptions: []string{\"random\", \"very_random\", \"very_very_random\"},\n-\t\t\t\t\tParam:   config.NewParam(\"paramD\", \"\", true),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tnewConf, err := customConf.ValidateConfig(conf)\n-\t\trequire.Nil(t, newConf.Params)\n-\t\trequire.NotNil(t, err)\n-\t\trequire.Equal(t, \"parameter paramD is invalid - custom is not a valid value for paramD - [random very_random very_very_random]\", err.Error())\n-\t})\n-\n-\tt.Run(\"allow empty string value for StringOptions if specified\", func(t *testing.T) {\n-\t\t//missing required paramB\n-\t\tcustomConf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue: \"custom\",\n-\t\t\t\t\tParam: config.NewParam(\"paramB\", \"\", true),\n-\t\t\t\t},\n-\t\t\t\tconfig.StringOptions{\n-\t\t\t\t\tParam: config.Param{},\n-\t\t\t\t\t// this isn't ideal we need to ensure options are set for this to validate correctly\n-\t\t\t\t\tOptions: []string{\"random\", \"very_random\", \"very_very_random\"},\n-\t\t\t\t\tValue:   \"\",\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tnewConf, err := customConf.ValidateConfig(conf)\n-\t\trequire.NotNil(t, newConf.Params)\n-\t\trequire.Nil(t, err)\n-\t})\n-\n-\t//TODO: Do we need to test if parameters of the same name but wrong type are passed??\n-}\ndiff --git a/programs/diagnostics/internal/platform/config/utils_test.go b/programs/diagnostics/internal/platform/config/utils_test.go\ndeleted file mode 100644\nindex 9e03e5e69d2c..000000000000\n--- a/programs/diagnostics/internal/platform/config/utils_test.go\n+++ /dev/null\n@@ -1,142 +0,0 @@\n-package config_test\n-\n-import (\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestReadStringListValues(t *testing.T) {\n-\n-\tt.Run(\"can find a string list param\", func(t *testing.T) {\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringListParam{\n-\t\t\t\t\t// nil means include everything\n-\t\t\t\t\tValues: nil,\n-\t\t\t\t\tParam:  config.NewParam(\"include_tables\", \"Specify list of tables to collect\", false),\n-\t\t\t\t},\n-\t\t\t\tconfig.StringListParam{\n-\t\t\t\t\tValues: []string{\"licenses\", \"settings\"},\n-\t\t\t\t\tParam:  config.NewParam(\"exclude_tables\", \"Specify list of tables not to collect\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\texcludeTables, err := config.ReadStringListValues(conf, \"exclude_tables\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, []string{\"licenses\", \"settings\"}, excludeTables)\n-\t})\n-\n-}\n-\n-func TestReadStringValue(t *testing.T) {\n-\n-\tt.Run(\"can find a string param\", func(t *testing.T) {\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringListParam{\n-\t\t\t\t\t// nil means include everything\n-\t\t\t\t\tValues: nil,\n-\t\t\t\t\tParam:  config.NewParam(\"include_tables\", \"Specify list of tables to collect\", false),\n-\t\t\t\t},\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue: \"/tmp/dump\",\n-\t\t\t\t\tParam: config.NewParam(\"directory\", \"Specify a directory\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tdirectory, err := config.ReadStringValue(conf, \"directory\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, \"/tmp/dump\", directory)\n-\t})\n-\n-}\n-\n-func TestReadIntValue(t *testing.T) {\n-\tt.Run(\"can find an integer param\", func(t *testing.T) {\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.IntParam{\n-\t\t\t\t\t// nil means include everything\n-\t\t\t\t\tValue: 10000,\n-\t\t\t\t\tParam: config.NewParam(\"row_limit\", \"Max Rows to collect\", false),\n-\t\t\t\t},\n-\t\t\t\tconfig.StringListParam{\n-\t\t\t\t\t// nil means include everything\n-\t\t\t\t\tValues: nil,\n-\t\t\t\t\tParam:  config.NewParam(\"include_tables\", \"Specify list of tables to collect\", false),\n-\t\t\t\t},\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue: \"/tmp/dump\",\n-\t\t\t\t\tParam: config.NewParam(\"directory\", \"Specify a directory\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\trowLimit, err := config.ReadIntValue(conf, \"row_limit\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, int64(10000), rowLimit)\n-\t})\n-\n-}\n-\n-func TestReadBoolValue(t *testing.T) {\n-\tt.Run(\"can find a boolean param\", func(t *testing.T) {\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.BoolParam{\n-\t\t\t\t\t// nil means include everything\n-\t\t\t\t\tValue: true,\n-\t\t\t\t\tParam: config.NewParam(\"compress\", \"Compress data\", false),\n-\t\t\t\t},\n-\t\t\t\tconfig.StringListParam{\n-\t\t\t\t\t// nil means include everything\n-\t\t\t\t\tValues: nil,\n-\t\t\t\t\tParam:  config.NewParam(\"include_tables\", \"Specify list of tables to collect\", false),\n-\t\t\t\t},\n-\t\t\t\tconfig.StringParam{\n-\t\t\t\t\tValue: \"/tmp/dump\",\n-\t\t\t\t\tParam: config.NewParam(\"directory\", \"Specify a directory\", false),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\n-\t\tcompress, err := config.ReadBoolValue(conf, \"compress\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.True(t, compress)\n-\t})\n-}\n-\n-func TestReadStringOptionsValue(t *testing.T) {\n-\tt.Run(\"can find a string value in a list of options\", func(t *testing.T) {\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringOptions{\n-\t\t\t\t\tParam:      config.NewParam(\"format\", \"List of formats\", false),\n-\t\t\t\t\tOptions:    []string{\"csv\", \"tsv\", \"binary\", \"json\", \"ndjson\"},\n-\t\t\t\t\tValue:      \"csv\",\n-\t\t\t\t\tAllowEmpty: false,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tformat, err := config.ReadStringOptionsValue(conf, \"format\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.Equal(t, \"csv\", format)\n-\t})\n-\n-\tt.Run(\"errors on invalid value\", func(t *testing.T) {\n-\t\tconf := config.Configuration{\n-\t\t\tParams: []config.ConfigParam{\n-\t\t\t\tconfig.StringOptions{\n-\t\t\t\t\tParam:      config.NewParam(\"format\", \"List of formats\", false),\n-\t\t\t\t\tOptions:    []string{\"csv\", \"tsv\", \"binary\", \"json\", \"ndjson\"},\n-\t\t\t\t\tValue:      \"random\",\n-\t\t\t\t\tAllowEmpty: false,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tformat, err := config.ReadStringOptionsValue(conf, \"format\")\n-\t\trequire.Equal(t, \"random is not a valid option in [csv tsv binary json ndjson] for the the parameter format\", err.Error())\n-\t\trequire.Equal(t, \"\", format)\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/platform/data/bundle_test.go b/programs/diagnostics/internal/platform/data/bundle_test.go\ndeleted file mode 100644\nindex ff9cfc2cf567..000000000000\n--- a/programs/diagnostics/internal/platform/data/bundle_test.go\n+++ /dev/null\n@@ -1,26 +0,0 @@\n-package data_test\n-\n-import (\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/pkg/errors\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestBundleError(t *testing.T) {\n-\n-\tt.Run(\"can get a bundle error\", func(t *testing.T) {\n-\t\terrs := make([]error, 3)\n-\t\terrs[0] = errors.New(\"Error 1\")\n-\t\terrs[1] = errors.New(\"Error 2\")\n-\t\terrs[2] = errors.New(\"Error 3\")\n-\t\tfErrors := data.FrameErrors{\n-\t\t\tErrors: errs,\n-\t\t}\n-\t\trequire.Equal(t, `Error 1\n-Error 2\n-Error 3`, fErrors.Error())\n-\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/platform/data/database_test.go b/programs/diagnostics/internal/platform/data/database_test.go\ndeleted file mode 100644\nindex 57d89e78efc2..000000000000\n--- a/programs/diagnostics/internal/platform/data/database_test.go\n+++ /dev/null\n@@ -1,86 +0,0 @@\n-package data_test\n-\n-import (\n-\t\"database/sql\"\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/DATA-DOG/go-sqlmock\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestString(t *testing.T) {\n-\tt.Run(\"can order by asc\", func(t *testing.T) {\n-\t\torderBy := data.OrderBy{\n-\t\t\tColumn: \"created_at\",\n-\t\t\tOrder:  data.Asc,\n-\t\t}\n-\t\trequire.Equal(t, \" ORDER BY created_at ASC\", orderBy.String())\n-\t})\n-\n-\tt.Run(\"can order by desc\", func(t *testing.T) {\n-\t\torderBy := data.OrderBy{\n-\t\t\tColumn: \"created_at\",\n-\t\t\tOrder:  data.Desc,\n-\t\t}\n-\t\trequire.Equal(t, \" ORDER BY created_at DESC\", orderBy.String())\n-\t})\n-\n-}\n-\n-func TestNextDatabaseFrame(t *testing.T) {\n-\n-\tt.Run(\"can iterate sql rows\", func(t *testing.T) {\n-\t\trowValues := [][]interface{}{\n-\t\t\t{int64(1), \"post_1\", \"hello\"},\n-\t\t\t{int64(2), \"post_2\", \"world\"},\n-\t\t\t{int64(3), \"post_3\", \"goodbye\"},\n-\t\t\t{int64(4), \"post_4\", \"world\"},\n-\t\t}\n-\t\tmockRows := sqlmock.NewRows([]string{\"id\", \"title\", \"body\"})\n-\t\tfor i := range rowValues {\n-\t\t\tmockRows.AddRow(rowValues[i][0], rowValues[i][1], rowValues[i][2])\n-\t\t}\n-\t\trows := mockRowsToSqlRows(mockRows)\n-\t\tdbFrame, err := data.NewDatabaseFrame(\"test\", rows)\n-\t\trequire.ElementsMatch(t, dbFrame.Columns(), []string{\"id\", \"title\", \"body\"})\n-\t\trequire.Nil(t, err)\n-\t\ti := 0\n-\t\tfor {\n-\t\t\tvalues, ok, err := dbFrame.Next()\n-\t\t\trequire.Nil(t, err)\n-\t\t\tif !ok {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\trequire.Len(t, values, 3)\n-\t\t\trequire.ElementsMatch(t, values, rowValues[i])\n-\t\t\ti++\n-\t\t}\n-\t\trequire.Equal(t, 4, i)\n-\t})\n-\n-\tt.Run(\"can iterate empty sql rows\", func(t *testing.T) {\n-\t\tmockRows := sqlmock.NewRows([]string{\"id\", \"title\", \"body\"})\n-\t\trows := mockRowsToSqlRows(mockRows)\n-\t\tdbFrame, err := data.NewDatabaseFrame(\"test\", rows)\n-\t\trequire.ElementsMatch(t, dbFrame.Columns(), []string{\"id\", \"title\", \"body\"})\n-\t\trequire.Nil(t, err)\n-\t\ti := 0\n-\t\tfor {\n-\t\t\t_, ok, err := dbFrame.Next()\n-\t\t\trequire.Nil(t, err)\n-\t\t\tif !ok {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\ti++\n-\t\t}\n-\t\trequire.Equal(t, 0, i)\n-\t})\n-}\n-\n-func mockRowsToSqlRows(mockRows *sqlmock.Rows) *sql.Rows {\n-\tdb, mock, _ := sqlmock.New()\n-\tmock.ExpectQuery(\"select\").WillReturnRows(mockRows)\n-\trows, _ := db.Query(\"select\")\n-\treturn rows\n-}\ndiff --git a/programs/diagnostics/internal/platform/data/file_test.go b/programs/diagnostics/internal/platform/data/file_test.go\ndeleted file mode 100644\nindex 9e305b1a5daa..000000000000\n--- a/programs/diagnostics/internal/platform/data/file_test.go\n+++ /dev/null\n@@ -1,263 +0,0 @@\n-package data_test\n-\n-import (\n-\t\"fmt\"\n-\t\"io/ioutil\"\n-\t\"os\"\n-\t\"path\"\n-\t\"path/filepath\"\n-\t\"strings\"\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestNextFileDirectoryFrame(t *testing.T) {\n-\tt.Run(\"can iterate file frame\", func(t *testing.T) {\n-\t\ttempDir := t.TempDir()\n-\t\tfiles := make([]string, 5)\n-\t\tfor i := 0; i < 5; i++ {\n-\t\t\tfileDir := path.Join(tempDir, fmt.Sprintf(\"%d\", i))\n-\t\t\terr := os.MkdirAll(fileDir, os.ModePerm)\n-\t\t\trequire.Nil(t, err)\n-\t\t\tfilepath := path.Join(fileDir, fmt.Sprintf(\"random-%d.txt\", i))\n-\t\t\tfiles[i] = filepath\n-\t\t\t_, err = os.Create(filepath)\n-\t\t\trequire.Nil(t, err)\n-\t\t}\n-\t\tfileFrame, errs := data.NewFileDirectoryFrame(tempDir, []string{\"*.txt\"})\n-\t\trequire.Empty(t, errs)\n-\t\ti := 0\n-\t\tfor {\n-\t\t\tvalues, ok, err := fileFrame.Next()\n-\t\t\trequire.Nil(t, err)\n-\t\t\tif !ok {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\trequire.Len(t, values, 1)\n-\t\t\trequire.Equal(t, files[i], values[0].(data.SimpleFile).Path)\n-\t\t\ti += 1\n-\t\t}\n-\t\trequire.Equal(t, 5, i)\n-\t})\n-\n-\tt.Run(\"can iterate file frame when empty\", func(t *testing.T) {\n-\t\t// create 5 temporary files\n-\t\ttempDir := t.TempDir()\n-\t\tfileFrame, errs := data.NewFileDirectoryFrame(tempDir, []string{\"*\"})\n-\t\trequire.Empty(t, errs)\n-\t\ti := 0\n-\t\tfor {\n-\t\t\t_, ok, err := fileFrame.Next()\n-\t\t\trequire.Nil(t, err)\n-\t\t\tif !ok {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t\trequire.Equal(t, 0, i)\n-\t})\n-}\n-\n-func TestNewConfigFileFrame(t *testing.T) {\n-\tt.Run(\"can iterate config file frame\", func(t *testing.T) {\n-\t\tcwd, err := os.Getwd()\n-\t\trequire.Nil(t, err)\n-\n-\t\tconfigFrame, errs := data.NewConfigFileFrame(path.Join(cwd, \"../../../testdata\", \"configs\", \"xml\"))\n-\t\trequire.Empty(t, errs)\n-\t\ti := 0\n-\t\tfor {\n-\t\t\tvalues, ok, err := configFrame.Next()\n-\t\t\trequire.Nil(t, err)\n-\t\t\tif !ok {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\trequire.Len(t, values, 1)\n-\t\t\tfilePath := values[0].(data.XmlConfigFile).FilePath()\n-\t\t\trequire.True(t, strings.Contains(filePath, \".xml\"))\n-\t\t\ti += 1\n-\t\t}\n-\t\t// 5 not 3 due to the includes\n-\t\trequire.Equal(t, 5, i)\n-\t})\n-\n-\tt.Run(\"can iterate file frame when empty\", func(t *testing.T) {\n-\t\t// create 5 temporary files\n-\t\ttempDir := t.TempDir()\n-\t\tconfigFrame, errs := data.NewConfigFileFrame(tempDir)\n-\t\trequire.Empty(t, errs)\n-\t\ti := 0\n-\t\tfor {\n-\t\t\t_, ok, err := configFrame.Next()\n-\t\t\trequire.Nil(t, err)\n-\t\t\tif !ok {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t\trequire.Equal(t, 0, i)\n-\t})\n-}\n-\n-func TestConfigFileFrameCopy(t *testing.T) {\n-\tt.Run(\"can copy non-sensitive xml config files\", func(t *testing.T) {\n-\t\ttmrDir := t.TempDir()\n-\t\tcwd, err := os.Getwd()\n-\t\trequire.Nil(t, err)\n-\t\tconfigFrame, errs := data.NewConfigFileFrame(path.Join(cwd, \"../../../testdata\", \"configs\", \"xml\"))\n-\t\trequire.Empty(t, errs)\n-\t\tfor {\n-\t\t\tvalues, ok, err := configFrame.Next()\n-\t\t\trequire.Nil(t, err)\n-\t\t\tif !ok {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\trequire.Nil(t, err)\n-\t\t\trequire.True(t, ok)\n-\t\t\tconfigFile := values[0].(data.XmlConfigFile)\n-\t\t\tnewPath := path.Join(tmrDir, filepath.Base(configFile.FilePath()))\n-\t\t\terr = configFile.Copy(newPath, false)\n-\t\t\trequire.FileExists(t, newPath)\n-\t\t\tsourceInfo, _ := os.Stat(configFile.FilePath())\n-\t\t\tdestInfo, _ := os.Stat(newPath)\n-\t\t\trequire.Equal(t, sourceInfo.Size(), destInfo.Size())\n-\t\t\trequire.Nil(t, err)\n-\t\t}\n-\t})\n-\n-\tt.Run(\"can copy sensitive xml config files\", func(t *testing.T) {\n-\t\ttmrDir := t.TempDir()\n-\t\tcwd, err := os.Getwd()\n-\t\trequire.Nil(t, err)\n-\t\tconfigFrame, errs := data.NewConfigFileFrame(path.Join(cwd, \"../../../testdata\", \"configs\", \"xml\"))\n-\t\trequire.Empty(t, errs)\n-\t\ti := 0\n-\t\tvar checkedFiles []string\n-\t\tfor {\n-\t\t\tvalues, ok, err := configFrame.Next()\n-\t\t\trequire.Nil(t, err)\n-\t\t\tif !ok {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\trequire.Nil(t, err)\n-\t\t\trequire.True(t, ok)\n-\t\t\tconfigFile := values[0].(data.XmlConfigFile)\n-\t\t\tfileName := filepath.Base(configFile.FilePath())\n-\t\t\tnewPath := path.Join(tmrDir, fileName)\n-\t\t\terr = configFile.Copy(newPath, true)\n-\t\t\trequire.FileExists(t, newPath)\n-\t\t\trequire.Nil(t, err)\n-\t\t\tbytes, err := ioutil.ReadFile(newPath)\n-\t\t\trequire.Nil(t, err)\n-\t\t\ts := string(bytes)\n-\t\t\tcheckedFiles = append(checkedFiles, fileName)\n-\t\t\tif fileName == \"users.xml\" || fileName == \"default-password.xml\" || fileName == \"user-include.xml\" {\n-\t\t\t\trequire.True(t, strings.Contains(s, \"<password>Replaced</password>\") ||\n-\t\t\t\t\tstrings.Contains(s, \"<password_sha256_hex>Replaced</password_sha256_hex>\"))\n-\t\t\t\trequire.NotContains(t, s, \"<password>REPLACE_ME</password>\")\n-\t\t\t\trequire.NotContains(t, s, \"<password_sha256_hex>REPLACE_ME</password_sha256_hex>\")\n-\t\t\t} else if fileName == \"config.xml\" {\n-\t\t\t\trequire.True(t, strings.Contains(s, \"<access_key_id>Replaced</access_key_id>\"))\n-\t\t\t\trequire.True(t, strings.Contains(s, \"<secret_access_key>Replaced</secret_access_key>\"))\n-\t\t\t\trequire.True(t, strings.Contains(s, \"<secret>Replaced</secret>\"))\n-\t\t\t\trequire.NotContains(t, s, \"<access_key_id>REPLACE_ME</access_key_id>\")\n-\t\t\t\trequire.NotContains(t, s, \"<secret_access_key>REPLACE_ME</secret_access_key>\")\n-\t\t\t\trequire.NotContains(t, s, \"<secret>REPLACE_ME</secret>\")\n-\t\t\t}\n-\t\t\ti++\n-\t\t}\n-\t\trequire.ElementsMatch(t, []string{\"users.xml\", \"default-password.xml\", \"user-include.xml\", \"config.xml\", \"server-include.xml\"}, checkedFiles)\n-\t\trequire.Equal(t, 5, i)\n-\t})\n-\n-\tt.Run(\"can copy sensitive yaml config files\", func(t *testing.T) {\n-\t\ttmrDir := t.TempDir()\n-\t\tcwd, err := os.Getwd()\n-\t\trequire.Nil(t, err)\n-\t\tconfigFrame, errs := data.NewConfigFileFrame(path.Join(cwd, \"../../../testdata\", \"configs\", \"yaml\"))\n-\t\trequire.Empty(t, errs)\n-\t\ti := 0\n-\t\tvar checkedFiles []string\n-\t\tfor {\n-\t\t\tvalues, ok, err := configFrame.Next()\n-\t\t\trequire.Nil(t, err)\n-\t\t\tif !ok {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\trequire.Nil(t, err)\n-\t\t\trequire.True(t, ok)\n-\t\t\tconfigFile := values[0].(data.YamlConfigFile)\n-\t\t\tfileName := filepath.Base(configFile.FilePath())\n-\t\t\tnewPath := path.Join(tmrDir, fileName)\n-\t\t\terr = configFile.Copy(newPath, true)\n-\t\t\trequire.FileExists(t, newPath)\n-\t\t\trequire.Nil(t, err)\n-\t\t\tbytes, err := ioutil.ReadFile(newPath)\n-\t\t\trequire.Nil(t, err)\n-\t\t\ts := string(bytes)\n-\t\t\tcheckedFiles = append(checkedFiles, fileName)\n-\t\t\tif fileName == \"users.yaml\" || fileName == \"default-password.yaml\" || fileName == \"user-include.yaml\" {\n-\t\t\t\trequire.True(t, strings.Contains(s, \"password: 'Replaced'\") ||\n-\t\t\t\t\tstrings.Contains(s, \"password_sha256_hex: 'Replaced'\"))\n-\t\t\t\trequire.NotContains(t, s, \"password: 'REPLACE_ME'\")\n-\t\t\t\trequire.NotContains(t, s, \"password_sha256_hex: \\\"REPLACE_ME\\\"\")\n-\t\t\t} else if fileName == \"config.yaml\" {\n-\t\t\t\trequire.True(t, strings.Contains(s, \"access_key_id: 'Replaced'\"))\n-\t\t\t\trequire.True(t, strings.Contains(s, \"secret_access_key: 'Replaced'\"))\n-\t\t\t\trequire.True(t, strings.Contains(s, \"secret: 'Replaced'\"))\n-\t\t\t\trequire.NotContains(t, s, \"access_key_id: 'REPLACE_ME'\")\n-\t\t\t\trequire.NotContains(t, s, \"secret_access_key: REPLACE_ME\")\n-\t\t\t\trequire.NotContains(t, s, \"secret: REPLACE_ME\")\n-\t\t\t}\n-\t\t\ti++\n-\t\t}\n-\t\trequire.ElementsMatch(t, []string{\"users.yaml\", \"default-password.yaml\", \"user-include.yaml\", \"config.yaml\", \"server-include.yaml\"}, checkedFiles)\n-\t\trequire.Equal(t, 5, i)\n-\t})\n-}\n-\n-func TestConfigFileFrameFindLogPaths(t *testing.T) {\n-\tt.Run(\"can find xml log paths\", func(t *testing.T) {\n-\t\tcwd, err := os.Getwd()\n-\t\trequire.Nil(t, err)\n-\t\tconfigFrame, errs := data.NewConfigFileFrame(path.Join(cwd, \"../../../testdata\", \"configs\", \"xml\"))\n-\t\trequire.Empty(t, errs)\n-\t\tpaths, errs := configFrame.FindLogPaths()\n-\t\trequire.Empty(t, errs)\n-\t\trequire.ElementsMatch(t, []string{\"/var/log/clickhouse-server/clickhouse-server.log\",\n-\t\t\t\"/var/log/clickhouse-server/clickhouse-server.err.log\"}, paths)\n-\t})\n-\n-\tt.Run(\"can handle empty log paths\", func(t *testing.T) {\n-\t\tconfigFrame, errs := data.NewConfigFileFrame(t.TempDir())\n-\t\trequire.Empty(t, errs)\n-\t\tpaths, errs := configFrame.FindLogPaths()\n-\t\trequire.Empty(t, errs)\n-\t\trequire.Empty(t, paths)\n-\t})\n-\n-\tt.Run(\"can find yaml log paths\", func(t *testing.T) {\n-\t\tcwd, err := os.Getwd()\n-\t\trequire.Nil(t, err)\n-\t\tconfigFrame, errs := data.NewConfigFileFrame(path.Join(cwd, \"../../../testdata\", \"configs\", \"yaml\"))\n-\t\trequire.Empty(t, errs)\n-\t\tpaths, errs := configFrame.FindLogPaths()\n-\t\trequire.Empty(t, errs)\n-\t\trequire.ElementsMatch(t, []string{\"/var/log/clickhouse-server/clickhouse-server.log\",\n-\t\t\t\"/var/log/clickhouse-server/clickhouse-server.err.log\"}, paths)\n-\t})\n-}\n-\n-// test the legacy format for ClickHouse xml config files with a yandex root tag\n-func TestYandexConfigFile(t *testing.T) {\n-\tt.Run(\"can find xml log paths with yandex root\", func(t *testing.T) {\n-\t\tcwd, err := os.Getwd()\n-\t\trequire.Nil(t, err)\n-\t\tconfigFrame, errs := data.NewConfigFileFrame(path.Join(cwd, \"../../../testdata\", \"configs\", \"yandex_xml\"))\n-\t\trequire.Empty(t, errs)\n-\t\tpaths, errs := configFrame.FindLogPaths()\n-\t\trequire.Empty(t, errs)\n-\t\trequire.ElementsMatch(t, []string{\"/var/log/clickhouse-server/clickhouse-server.log\",\n-\t\t\t\"/var/log/clickhouse-server/clickhouse-server.err.log\"}, paths)\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/platform/data/memory_test.go b/programs/diagnostics/internal/platform/data/memory_test.go\ndeleted file mode 100644\nindex fcc02e37d324..000000000000\n--- a/programs/diagnostics/internal/platform/data/memory_test.go\n+++ /dev/null\n@@ -1,61 +0,0 @@\n-package data_test\n-\n-import (\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestNextMemoryFrame(t *testing.T) {\n-\tt.Run(\"can iterate memory frame\", func(t *testing.T) {\n-\t\tcolumns := []string{\"Filesystem\", \"Size\", \"Used\", \"Avail\", \"Use%\", \"Mounted on\"}\n-\t\trows := [][]interface{}{\n-\t\t\t{\"sysfs\", 0, 0, 0, 0, \"/sys\"},\n-\t\t\t{\"proc\", 0, 0, 0, 0, \"/proc\"},\n-\t\t\t{\"udev\", 33357840384, 0, 33357840384, 0, \"/dev\"},\n-\t\t\t{\"devpts\", 0, 0, 0, 0, \"/dev/pts\"},\n-\t\t\t{\"tmpfs\", 6682607616, 2228224, 6680379392, 1, \"/run\"},\n-\t\t\t{\"/dev/mapper/system-root\", 1938213220352, 118136926208, 1721548947456, 7.000000000000001, \"/\"},\n-\t\t}\n-\t\tmemoryFrame := data.NewMemoryFrame(\"disks\", columns, rows)\n-\t\ti := 0\n-\t\tfor {\n-\t\t\tvalues, ok, err := memoryFrame.Next()\n-\t\t\trequire.Nil(t, err)\n-\t\t\tif !ok {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\trequire.ElementsMatch(t, values, rows[i])\n-\t\t\trequire.Len(t, values, 6)\n-\t\t\ti += 1\n-\t\t}\n-\t\trequire.Equal(t, 6, i)\n-\t})\n-\n-\tt.Run(\"can iterate memory frame when empty\", func(t *testing.T) {\n-\t\tmemoryFrame := data.NewMemoryFrame(\"test\", []string{}, [][]interface{}{})\n-\t\ti := 0\n-\t\tfor {\n-\t\t\t_, ok, err := memoryFrame.Next()\n-\t\t\trequire.Nil(t, err)\n-\t\t\tif !ok {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t\trequire.Equal(t, 0, i)\n-\t})\n-\n-\tt.Run(\"can iterate memory frame when empty\", func(t *testing.T) {\n-\t\tmemoryFrame := data.MemoryFrame{}\n-\t\ti := 0\n-\t\tfor {\n-\t\t\t_, ok, err := memoryFrame.Next()\n-\t\t\trequire.Nil(t, err)\n-\t\t\tif !ok {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t\trequire.Equal(t, 0, i)\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/platform/database/native_test.go b/programs/diagnostics/internal/platform/database/native_test.go\ndeleted file mode 100644\nindex 7028a4b4800b..000000000000\n--- a/programs/diagnostics/internal/platform/database/native_test.go\n+++ /dev/null\n@@ -1,289 +0,0 @@\n-//go:build !no_docker\n-\n-package database_test\n-\n-import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"os\"\n-\t\"path\"\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/database\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test\"\n-\t\"github.com/docker/go-connections/nat\"\n-\t\"github.com/stretchr/testify/require\"\n-\t\"github.com/testcontainers/testcontainers-go\"\n-\t\"github.com/testcontainers/testcontainers-go/wait\"\n-)\n-\n-func createClickHouseContainer(t *testing.T, ctx context.Context) (testcontainers.Container, nat.Port) {\n-\t// create a ClickHouse container\n-\tcwd, err := os.Getwd()\n-\tif err != nil {\n-\t\t// can't test without current directory\n-\t\tpanic(err)\n-\t}\n-\n-\t// for now, we test against a hardcoded database-server version but we should make this a property\n-\treq := testcontainers.ContainerRequest{\n-\t\tImage:        fmt.Sprintf(\"clickhouse/clickhouse-server:%s\", test.GetClickHouseTestVersion()),\n-\t\tExposedPorts: []string{\"9000/tcp\"},\n-\t\tWaitingFor:   wait.ForLog(\"Ready for connections\"),\n-\t\tMounts: testcontainers.ContainerMounts{\n-\t\t\t{\n-\t\t\t\tSource: testcontainers.GenericBindMountSource{\n-\t\t\t\t\tHostPath: path.Join(cwd, \"../../../testdata/docker/custom.xml\"),\n-\t\t\t\t},\n-\t\t\t\tTarget: \"/etc/clickhouse-server/config.d/custom.xml\",\n-\t\t\t},\n-\t\t\t{\n-\t\t\t\tSource: testcontainers.GenericBindMountSource{\n-\t\t\t\t\tHostPath: path.Join(cwd, \"../../../testdata/docker/admin.xml\"),\n-\t\t\t\t},\n-\t\t\t\tTarget: \"/etc/clickhouse-server/users.d/admin.xml\",\n-\t\t\t},\n-\t\t},\n-\t}\n-\tclickhouseContainer, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{\n-\t\tContainerRequest: req,\n-\t\tStarted:          true,\n-\t})\n-\tif err != nil {\n-\t\t// can't test without container\n-\t\tpanic(err)\n-\t}\n-\n-\tp, _ := clickhouseContainer.MappedPort(ctx, \"9000\")\n-\tif err != nil {\n-\t\t// can't test without container's port\n-\t\tpanic(err)\n-\t}\n-\n-\tt.Setenv(\"CLICKHOUSE_DB_PORT\", p.Port())\n-\n-\treturn clickhouseContainer, p\n-}\n-\n-func getClient(t *testing.T, mappedPort int) *database.ClickhouseNativeClient {\n-\tclickhouseClient, err := database.NewNativeClient(\"localhost\", uint16(mappedPort), \"\", \"\")\n-\tif err != nil {\n-\t\tt.Fatalf(\"unable to build client : %v\", err)\n-\t}\n-\treturn clickhouseClient\n-}\n-\n-func TestReadTableNamesForDatabase(t *testing.T) {\n-\tctx := context.Background()\n-\tclickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)\n-\tdefer clickhouseContainer.Terminate(ctx) //nolint\n-\n-\tclickhouseClient := getClient(t, mappedPort.Int())\n-\tt.Run(\"client can read tables for a database\", func(t *testing.T) {\n-\t\ttables, err := clickhouseClient.ReadTableNamesForDatabase(\"system\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.GreaterOrEqual(t, len(tables), 70)\n-\t\trequire.Contains(t, tables, \"merge_tree_settings\")\n-\t})\n-}\n-\n-func TestReadTable(t *testing.T) {\n-\tt.Run(\"client can get all rows for system.disks table\", func(t *testing.T) {\n-\t\tctx := context.Background()\n-\t\tclickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)\n-\t\tdefer clickhouseContainer.Terminate(ctx) //nolint\n-\n-\t\tclickhouseClient := getClient(t, mappedPort.Int())\n-\n-\t\t// we read the table system.disks as this should contain only 1 row\n-\t\tframe, err := clickhouseClient.ReadTable(\"system\", \"disks\", []string{}, data.OrderBy{}, 10)\n-\t\trequire.Nil(t, err)\n-\t\trequire.ElementsMatch(t, frame.Columns(), [9]string{\"name\", \"path\", \"free_space\", \"total_space\", \"unreserved_space\", \"keep_free_space\", \"type\", \"is_encrypted\", \"cache_path\"})\n-\t\ti := 0\n-\t\tfor {\n-\t\t\tvalues, ok, err := frame.Next()\n-\t\t\tif i == 0 {\n-\t\t\t\trequire.Nil(t, err)\n-\t\t\t\trequire.True(t, ok)\n-\t\t\t\trequire.Equal(t, \"default\", values[0])\n-\t\t\t\trequire.Equal(t, \"/var/lib/clickhouse/\", values[1])\n-\t\t\t\trequire.Greater(t, values[2], uint64(0))\n-\t\t\t\trequire.Greater(t, values[3], uint64(0))\n-\t\t\t\trequire.Greater(t, values[4], uint64(0))\n-\t\t\t\trequire.Equal(t, values[5], uint64(0))\n-\t\t\t\trequire.Equal(t, \"local\", values[6])\n-\t\t\t\trequire.Equal(t, values[7], uint8(0))\n-\t\t\t\trequire.Equal(t, values[8], \"\")\n-\t\t\t} else {\n-\t\t\t\trequire.False(t, ok)\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\ti += 1\n-\t\t}\n-\t})\n-\n-\tt.Run(\"client can get all rows for system.databases table\", func(t *testing.T) {\n-\t\tctx := context.Background()\n-\t\tclickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)\n-\t\tdefer clickhouseContainer.Terminate(ctx) //nolint\n-\n-\t\tclickhouseClient := getClient(t, mappedPort.Int())\n-\n-\t\t// we read the table system.databases as this should be small and consistent on fresh db instances\n-\t\tframe, err := clickhouseClient.ReadTable(\"system\", \"databases\", []string{}, data.OrderBy{}, 10)\n-\t\trequire.Nil(t, err)\n-\t\trequire.ElementsMatch(t, frame.Columns(), [6]string{\"name\", \"engine\", \"data_path\", \"metadata_path\", \"uuid\", \"comment\"})\n-\t\texpectedRows := [4][3]string{{\"INFORMATION_SCHEMA\", \"Memory\", \"/var/lib/clickhouse/\"},\n-\t\t\t{\"default\", \"Atomic\", \"/var/lib/clickhouse/store/\"},\n-\t\t\t{\"information_schema\", \"Memory\", \"/var/lib/clickhouse/\"},\n-\t\t\t{\"system\", \"Atomic\", \"/var/lib/clickhouse/store/\"}}\n-\t\ti := 0\n-\t\tfor {\n-\t\t\tvalues, ok, err := frame.Next()\n-\n-\t\t\tif i < 4 {\n-\t\t\t\trequire.Nil(t, err)\n-\t\t\t\trequire.True(t, ok)\n-\t\t\t\trequire.Equal(t, expectedRows[i][0], values[0])\n-\t\t\t\trequire.Equal(t, expectedRows[i][1], values[1])\n-\t\t\t\trequire.Equal(t, expectedRows[i][2], values[2])\n-\t\t\t\trequire.NotNil(t, values[3])\n-\t\t\t\trequire.NotNil(t, values[4])\n-\t\t\t\trequire.Equal(t, \"\", values[5])\n-\t\t\t} else {\n-\t\t\t\trequire.False(t, ok)\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\ti += 1\n-\t\t}\n-\t})\n-\n-\tt.Run(\"client can get all rows for system.databases table with except\", func(t *testing.T) {\n-\t\tctx := context.Background()\n-\t\tclickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)\n-\t\tdefer clickhouseContainer.Terminate(ctx) //nolint\n-\n-\t\tclickhouseClient := getClient(t, mappedPort.Int())\n-\n-\t\tframe, err := clickhouseClient.ReadTable(\"system\", \"databases\", []string{\"data_path\", \"comment\"}, data.OrderBy{}, 10)\n-\t\trequire.Nil(t, err)\n-\t\trequire.ElementsMatch(t, frame.Columns(), [4]string{\"name\", \"engine\", \"metadata_path\", \"uuid\"})\n-\t})\n-\n-\tt.Run(\"client can limit rows for system.databases\", func(t *testing.T) {\n-\t\tctx := context.Background()\n-\t\tclickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)\n-\t\tdefer clickhouseContainer.Terminate(ctx) //nolint\n-\n-\t\tclickhouseClient := getClient(t, mappedPort.Int())\n-\n-\t\tframe, err := clickhouseClient.ReadTable(\"system\", \"databases\", []string{}, data.OrderBy{}, 1)\n-\t\trequire.Nil(t, err)\n-\t\trequire.ElementsMatch(t, frame.Columns(), [6]string{\"name\", \"engine\", \"data_path\", \"metadata_path\", \"uuid\", \"comment\"})\n-\t\texpectedRows := [1][3]string{{\"INFORMATION_SCHEMA\", \"Memory\", \"/var/lib/clickhouse/\"}}\n-\t\ti := 0\n-\t\tfor {\n-\t\t\tvalues, ok, err := frame.Next()\n-\t\t\tif i == 0 {\n-\t\t\t\trequire.Nil(t, err)\n-\t\t\t\trequire.True(t, ok)\n-\t\t\t\trequire.Equal(t, expectedRows[i][0], values[0])\n-\t\t\t\trequire.Equal(t, expectedRows[i][1], values[1])\n-\t\t\t\trequire.Equal(t, expectedRows[i][2], values[2])\n-\t\t\t\trequire.NotNil(t, values[3])\n-\t\t\t\trequire.NotNil(t, values[4])\n-\t\t\t\trequire.Equal(t, \"\", values[5])\n-\t\t\t} else {\n-\t\t\t\trequire.False(t, ok)\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\ti += 1\n-\t\t}\n-\t})\n-\n-\tt.Run(\"client can order rows for system.databases\", func(t *testing.T) {\n-\t\tctx := context.Background()\n-\t\tclickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)\n-\t\tdefer clickhouseContainer.Terminate(ctx) //nolint\n-\n-\t\tclickhouseClient := getClient(t, mappedPort.Int())\n-\n-\t\tframe, err := clickhouseClient.ReadTable(\"system\", \"databases\", []string{}, data.OrderBy{\n-\t\t\tColumn: \"engine\",\n-\t\t\tOrder:  data.Asc,\n-\t\t}, 10)\n-\t\trequire.Nil(t, err)\n-\t\trequire.ElementsMatch(t, frame.Columns(), [6]string{\"name\", \"engine\", \"data_path\", \"metadata_path\", \"uuid\", \"comment\"})\n-\t\texpectedRows := [4][3]string{\n-\t\t\t{\"default\", \"Atomic\", \"/var/lib/clickhouse/store/\"},\n-\t\t\t{\"system\", \"Atomic\", \"/var/lib/clickhouse/store/\"},\n-\t\t\t{\"INFORMATION_SCHEMA\", \"Memory\", \"/var/lib/clickhouse/\"},\n-\t\t\t{\"information_schema\", \"Memory\", \"/var/lib/clickhouse/\"},\n-\t\t}\n-\t\ti := 0\n-\t\tfor {\n-\t\t\tvalues, ok, err := frame.Next()\n-\n-\t\t\tif i < 4 {\n-\t\t\t\trequire.Nil(t, err)\n-\t\t\t\trequire.True(t, ok)\n-\t\t\t\trequire.Equal(t, expectedRows[i][0], values[0])\n-\t\t\t\trequire.Equal(t, expectedRows[i][1], values[1])\n-\t\t\t\trequire.Equal(t, expectedRows[i][2], values[2])\n-\t\t\t\trequire.NotNil(t, values[3])\n-\t\t\t\trequire.NotNil(t, values[4])\n-\t\t\t\trequire.Equal(t, \"\", values[5])\n-\t\t\t} else {\n-\t\t\t\trequire.False(t, ok)\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\ti += 1\n-\t\t}\n-\t})\n-}\n-\n-func TestExecuteStatement(t *testing.T) {\n-\tt.Run(\"client can execute any statement\", func(t *testing.T) {\n-\t\tctx := context.Background()\n-\t\tclickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)\n-\t\tdefer clickhouseContainer.Terminate(ctx) //nolint\n-\n-\t\tclickhouseClient := getClient(t, mappedPort.Int())\n-\n-\t\tstatement := \"SELECT path, count(*) as count FROM system.disks GROUP BY path;\"\n-\t\tframe, err := clickhouseClient.ExecuteStatement(\"engines\", statement)\n-\t\trequire.Nil(t, err)\n-\t\trequire.ElementsMatch(t, frame.Columns(), [2]string{\"path\", \"count\"})\n-\t\texpectedRows := [1][2]interface{}{\n-\t\t\t{\"/var/lib/clickhouse/\", uint64(1)},\n-\t\t}\n-\t\ti := 0\n-\t\tfor {\n-\t\t\tvalues, ok, err := frame.Next()\n-\t\t\tif !ok {\n-\t\t\t\trequire.Nil(t, err)\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\trequire.Nil(t, err)\n-\t\t\trequire.Equal(t, expectedRows[i][0], values[0])\n-\t\t\trequire.Equal(t, expectedRows[i][1], values[1])\n-\t\t\ti++\n-\t\t}\n-\t\tfmt.Println(i)\n-\t})\n-}\n-\n-func TestVersion(t *testing.T) {\n-\tt.Run(\"client can read version\", func(t *testing.T) {\n-\t\tctx := context.Background()\n-\t\tclickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)\n-\t\tdefer clickhouseContainer.Terminate(ctx) //nolint\n-\n-\t\tclickhouseClient := getClient(t, mappedPort.Int())\n-\n-\t\tversion, err := clickhouseClient.Version()\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotEmpty(t, version)\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/platform/manager_test.go b/programs/diagnostics/internal/platform/manager_test.go\ndeleted file mode 100644\nindex e6c50c6e505a..000000000000\n--- a/programs/diagnostics/internal/platform/manager_test.go\n+++ /dev/null\n@@ -1,100 +0,0 @@\n-//go:build !no_docker\n-\n-package platform_test\n-\n-import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"os\"\n-\t\"path\"\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test\"\n-\t\"github.com/docker/go-connections/nat\"\n-\t\"github.com/stretchr/testify/require\"\n-\t\"github.com/testcontainers/testcontainers-go\"\n-\t\"github.com/testcontainers/testcontainers-go/wait\"\n-)\n-\n-// create a ClickHouse container\n-func createClickHouseContainer(t *testing.T, ctx context.Context) (testcontainers.Container, nat.Port) {\n-\tcwd, err := os.Getwd()\n-\tif err != nil {\n-\t\tfmt.Println(\"unable to read current directory\", err)\n-\t\tos.Exit(1)\n-\t}\n-\t// for now, we test against a hardcoded database-server version but we should make this a property\n-\treq := testcontainers.ContainerRequest{\n-\t\tImage:        fmt.Sprintf(\"clickhouse/clickhouse-server:%s\", test.GetClickHouseTestVersion()),\n-\t\tExposedPorts: []string{\"9000/tcp\"},\n-\t\tWaitingFor:   wait.ForLog(\"Ready for connections\"),\n-\t\tMounts: testcontainers.ContainerMounts{\n-\t\t\t{\n-\t\t\t\tSource: testcontainers.GenericBindMountSource{\n-\t\t\t\t\tHostPath: path.Join(cwd, \"../../testdata/docker/custom.xml\"),\n-\t\t\t\t},\n-\t\t\t\tTarget: \"/etc/clickhouse-server/config.d/custom.xml\",\n-\t\t\t},\n-\t\t\t{\n-\t\t\t\tSource: testcontainers.GenericBindMountSource{\n-\t\t\t\t\tHostPath: path.Join(cwd, \"../../testdata/docker/admin.xml\"),\n-\t\t\t\t},\n-\t\t\t\tTarget: \"/etc/clickhouse-server/users.d/admin.xml\",\n-\t\t\t},\n-\t\t},\n-\t}\n-\tclickhouseContainer, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{\n-\t\tContainerRequest: req,\n-\t\tStarted:          true,\n-\t})\n-\tif err != nil {\n-\t\t// can't test without container\n-\t\tpanic(err)\n-\t}\n-\n-\tp, err := clickhouseContainer.MappedPort(ctx, \"9000\")\n-\tif err != nil {\n-\t\t// can't test without a port\n-\t\tpanic(err)\n-\t}\n-\n-\treturn clickhouseContainer, p\n-}\n-\n-func TestConnect(t *testing.T) {\n-\tt.Run(\"can only connect once\", func(t *testing.T) {\n-\t\tctx := context.Background()\n-\n-\t\tclickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)\n-\t\tdefer clickhouseContainer.Terminate(ctx) //nolint\n-\n-\t\tt.Setenv(\"CLICKHOUSE_DB_PORT\", mappedPort.Port())\n-\n-\t\tport := mappedPort.Int()\n-\n-\t\t// get before connection\n-\t\tmanager := platform.GetResourceManager()\n-\t\trequire.Nil(t, manager.DbClient)\n-\t\t// init connection\n-\t\terr := manager.Connect(\"localhost\", uint16(port), \"\", \"\")\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotNil(t, manager.DbClient)\n-\t\t// try and re-fetch connection\n-\t\terr = manager.Connect(\"localhost\", uint16(port), \"\", \"\")\n-\t\trequire.NotNil(t, err)\n-\t\trequire.Equal(t, \"connect can only be called once\", err.Error())\n-\t})\n-\n-}\n-\n-func TestGetResourceManager(t *testing.T) {\n-\tt.Run(\"get resource manager\", func(t *testing.T) {\n-\t\tmanager := platform.GetResourceManager()\n-\t\trequire.NotNil(t, manager)\n-\t\tmanager2 := platform.GetResourceManager()\n-\t\trequire.NotNil(t, manager2)\n-\t\trequire.Equal(t, &manager, &manager2)\n-\t})\n-\n-}\ndiff --git a/programs/diagnostics/internal/platform/test/data.go b/programs/diagnostics/internal/platform/test/data.go\ndeleted file mode 100644\nindex 7710e9a69a1f..000000000000\n--- a/programs/diagnostics/internal/platform/test/data.go\n+++ /dev/null\n@@ -1,166 +0,0 @@\n-package test\n-\n-import (\n-\t\"fmt\"\n-\t\"sort\"\n-\t\"strings\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils\"\n-\t\"github.com/pkg/errors\"\n-)\n-\n-type fakeClickhouseClient struct {\n-\ttables         map[string][]string\n-\tQueryResponses map[string]*FakeDataFrame\n-}\n-\n-func NewFakeClickhouseClient(tables map[string][]string) fakeClickhouseClient {\n-\tqueryResponses := make(map[string]*FakeDataFrame)\n-\treturn fakeClickhouseClient{\n-\t\ttables:         tables,\n-\t\tQueryResponses: queryResponses,\n-\t}\n-}\n-\n-func (f fakeClickhouseClient) ReadTableNamesForDatabase(databaseName string) ([]string, error) {\n-\tif _, ok := f.tables[databaseName]; ok {\n-\t\treturn f.tables[databaseName], nil\n-\t}\n-\treturn nil, fmt.Errorf(\"database %s does not exist\", databaseName)\n-}\n-\n-func (f fakeClickhouseClient) ReadTable(databaseName string, tableName string, excludeColumns []string, orderBy data.OrderBy, limit int64) (data.Frame, error) {\n-\n-\texceptClause := \"\"\n-\tif len(excludeColumns) > 0 {\n-\t\texceptClause = fmt.Sprintf(\"EXCEPT(%s) \", strings.Join(excludeColumns, \",\"))\n-\t}\n-\tlimitClause := \"\"\n-\tif limit >= 0 {\n-\t\tlimitClause = fmt.Sprintf(\" LIMIT %d\", limit)\n-\t}\n-\tquery := fmt.Sprintf(\"SELECT * %sFROM %s.%s%s%s\", exceptClause, databaseName, tableName, orderBy.String(), limitClause)\n-\tframe, error := f.ExecuteStatement(fmt.Sprintf(\"read_table_%s.%s\", databaseName, tableName), query)\n-\tif error != nil {\n-\t\treturn frame, error\n-\t}\n-\tfFrame := *(frame.(*FakeDataFrame))\n-\tfFrame = fFrame.FilterColumns(excludeColumns)\n-\tfFrame = fFrame.Order(orderBy)\n-\tfFrame = fFrame.Limit(limit)\n-\treturn fFrame, nil\n-}\n-\n-func (f fakeClickhouseClient) ExecuteStatement(id string, statement string) (data.Frame, error) {\n-\tif frame, ok := f.QueryResponses[statement]; ok {\n-\t\treturn frame, nil\n-\t}\n-\treturn FakeDataFrame{}, errors.New(fmt.Sprintf(\"No recorded response for %s\", statement))\n-}\n-\n-func (f fakeClickhouseClient) Version() (string, error) {\n-\treturn \"21.12.3\", nil\n-}\n-\n-func (f fakeClickhouseClient) Reset() {\n-\tfor key, frame := range f.QueryResponses {\n-\t\tframe.Reset()\n-\t\tf.QueryResponses[key] = frame\n-\t}\n-}\n-\n-type FakeDataFrame struct {\n-\ti           *int\n-\tRows        [][]interface{}\n-\tColumnNames []string\n-\tname        string\n-}\n-\n-func NewFakeDataFrame(name string, columns []string, rows [][]interface{}) FakeDataFrame {\n-\ti := 0\n-\treturn FakeDataFrame{\n-\t\ti:           &i,\n-\t\tRows:        rows,\n-\t\tColumnNames: columns,\n-\t\tname:        name,\n-\t}\n-}\n-\n-func (f FakeDataFrame) Next() ([]interface{}, bool, error) {\n-\tif len(f.Rows) == *(f.i) {\n-\t\treturn nil, false, nil\n-\t}\n-\tvalue := f.Rows[*f.i]\n-\t*f.i++\n-\treturn value, true, nil\n-}\n-\n-func (f FakeDataFrame) Columns() []string {\n-\treturn f.ColumnNames\n-}\n-\n-func (f FakeDataFrame) Name() string {\n-\treturn f.name\n-}\n-\n-func (f *FakeDataFrame) Reset() {\n-\ti := 0\n-\tf.i = &i\n-}\n-\n-func (f FakeDataFrame) FilterColumns(excludeColumns []string) FakeDataFrame {\n-\t// get columns we can remove\n-\trColumns := utils.Intersection(f.ColumnNames, excludeColumns)\n-\trIndexes := make([]int, len(rColumns))\n-\t// find the indexes of the columns to remove\n-\tfor i, column := range rColumns {\n-\t\trIndexes[i] = utils.IndexOf(f.ColumnNames, column)\n-\t}\n-\tnewRows := make([][]interface{}, len(f.Rows))\n-\tfor r, row := range f.Rows {\n-\t\tnewRow := row\n-\t\tfor i, index := range rIndexes {\n-\t\t\tnewRow = utils.Remove(newRow, index-i)\n-\t\t}\n-\t\tnewRows[r] = newRow\n-\t}\n-\tf.Rows = newRows\n-\tf.ColumnNames = utils.Distinct(f.ColumnNames, excludeColumns)\n-\treturn f\n-}\n-\n-func (f FakeDataFrame) Limit(rowLimit int64) FakeDataFrame {\n-\tif rowLimit >= 0 {\n-\t\tif int64(len(f.Rows)) > rowLimit {\n-\t\t\tf.Rows = f.Rows[:rowLimit]\n-\t\t}\n-\t}\n-\treturn f\n-}\n-\n-func (f FakeDataFrame) Order(orderBy data.OrderBy) FakeDataFrame {\n-\tif orderBy.Column == \"\" {\n-\t\treturn f\n-\t}\n-\tcIndex := utils.IndexOf(f.ColumnNames, orderBy.Column)\n-\tsort.Slice(f.Rows, func(i, j int) bool {\n-\t\tleft := f.Rows[i][cIndex]\n-\t\tright := f.Rows[j][cIndex]\n-\t\tif iLeft, ok := left.(int); ok {\n-\t\t\tif orderBy.Order == data.Asc {\n-\t\t\t\treturn iLeft < right.(int)\n-\t\t\t}\n-\t\t\treturn iLeft > right.(int)\n-\t\t} else {\n-\t\t\t// we aren't a full db - revert to string order\n-\t\t\tsLeft := left.(string)\n-\t\t\tsRight := right.(string)\n-\t\t\tif orderBy.Order == data.Asc {\n-\t\t\t\treturn sLeft < sRight\n-\t\t\t}\n-\t\t\treturn sLeft > sRight\n-\t\t}\n-\t})\n-\treturn f\n-}\ndiff --git a/programs/diagnostics/internal/platform/test/env.go b/programs/diagnostics/internal/platform/test/env.go\ndeleted file mode 100644\nindex 36b03772ab08..000000000000\n--- a/programs/diagnostics/internal/platform/test/env.go\n+++ /dev/null\n@@ -1,16 +0,0 @@\n-package test\n-\n-import \"os\"\n-\n-const defaultClickHouseVersion = \"latest\"\n-\n-func GetClickHouseTestVersion() string {\n-\treturn GetEnv(\"CLICKHOUSE_VERSION\", defaultClickHouseVersion)\n-}\n-\n-func GetEnv(key, fallback string) string {\n-\tif value, ok := os.LookupEnv(key); ok {\n-\t\treturn value\n-\t}\n-\treturn fallback\n-}\ndiff --git a/programs/diagnostics/internal/platform/utils/file_test.go b/programs/diagnostics/internal/platform/utils/file_test.go\ndeleted file mode 100644\nindex 8d0430090c96..000000000000\n--- a/programs/diagnostics/internal/platform/utils/file_test.go\n+++ /dev/null\n@@ -1,134 +0,0 @@\n-package utils_test\n-\n-import (\n-\t\"fmt\"\n-\t\"os\"\n-\t\"path\"\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestFileExists(t *testing.T) {\n-\tt.Run(\"returns true for file\", func(t *testing.T) {\n-\t\ttempDir := t.TempDir()\n-\t\tfilepath := path.Join(tempDir, \"random.txt\")\n-\t\t_, err := os.Create(filepath)\n-\t\trequire.Nil(t, err)\n-\t\texists, err := utils.FileExists(filepath)\n-\t\trequire.True(t, exists)\n-\t\trequire.Nil(t, err)\n-\t})\n-\n-\tt.Run(\"doesn't return true for not existence file\", func(t *testing.T) {\n-\t\ttempDir := t.TempDir()\n-\t\tfile := path.Join(tempDir, \"random.txt\")\n-\t\texists, err := utils.FileExists(file)\n-\t\trequire.False(t, exists)\n-\t\trequire.Nil(t, err)\n-\t})\n-\n-\tt.Run(\"doesn't return true for directory\", func(t *testing.T) {\n-\t\ttempDir := t.TempDir()\n-\t\texists, err := utils.FileExists(tempDir)\n-\t\trequire.False(t, exists)\n-\t\trequire.NotNil(t, err)\n-\t\trequire.Equal(t, fmt.Sprintf(\"%s is a directory\", tempDir), err.Error())\n-\t})\n-}\n-\n-func TestDirExists(t *testing.T) {\n-\tt.Run(\"doesn't return true for file\", func(t *testing.T) {\n-\t\ttempDir := t.TempDir()\n-\t\tfilepath := path.Join(tempDir, \"random.txt\")\n-\t\t_, err := os.Create(filepath)\n-\t\trequire.Nil(t, err)\n-\t\texists, err := utils.DirExists(filepath)\n-\t\trequire.False(t, exists)\n-\t\trequire.NotNil(t, err)\n-\t\trequire.Equal(t, fmt.Sprintf(\"%s is a file\", filepath), err.Error())\n-\t})\n-\n-\tt.Run(\"returns true for directory\", func(t *testing.T) {\n-\t\ttempDir := t.TempDir()\n-\t\texists, err := utils.DirExists(tempDir)\n-\t\trequire.True(t, exists)\n-\t\trequire.Nil(t, err)\n-\t})\n-\n-\tt.Run(\"doesn't return true random directory\", func(t *testing.T) {\n-\t\texists, err := utils.FileExists(fmt.Sprintf(\"%d\", utils.MakeTimestamp()))\n-\t\trequire.False(t, exists)\n-\t\trequire.Nil(t, err)\n-\t})\n-}\n-\n-func TestCopyFile(t *testing.T) {\n-\tt.Run(\"can copy file\", func(t *testing.T) {\n-\t\ttempDir := t.TempDir()\n-\t\tsourcePath := path.Join(tempDir, \"random.txt\")\n-\t\t_, err := os.Create(sourcePath)\n-\t\trequire.Nil(t, err)\n-\t\tdestPath := path.Join(tempDir, \"random-2.txt\")\n-\t\terr = utils.CopyFile(sourcePath, destPath)\n-\t\trequire.Nil(t, err)\n-\t})\n-\n-\tt.Run(\"can copy nested file\", func(t *testing.T) {\n-\t\ttempDir := t.TempDir()\n-\t\tsourcePath := path.Join(tempDir, \"random.txt\")\n-\t\t_, err := os.Create(sourcePath)\n-\t\trequire.Nil(t, err)\n-\t\tdestPath := path.Join(tempDir, \"sub_dir\", \"random-2.txt\")\n-\t\terr = utils.CopyFile(sourcePath, destPath)\n-\t\trequire.Nil(t, err)\n-\t})\n-\n-\tt.Run(\"fails when file does not exist\", func(t *testing.T) {\n-\t\ttempDir := t.TempDir()\n-\t\tsourcePath := path.Join(tempDir, \"random.txt\")\n-\t\tdestPath := path.Join(tempDir, \"random-2.txt\")\n-\t\terr := utils.CopyFile(sourcePath, destPath)\n-\t\trequire.NotNil(t, err)\n-\t\trequire.Equal(t, fmt.Sprintf(\"%s does not exist\", sourcePath), err.Error())\n-\t})\n-}\n-\n-func TestListFilesInDirectory(t *testing.T) {\n-\ttempDir := t.TempDir()\n-\tfiles := make([]string, 5)\n-\tfor i := 0; i < 5; i++ {\n-\t\tfileDir := path.Join(tempDir, fmt.Sprintf(\"%d\", i))\n-\t\terr := os.MkdirAll(fileDir, os.ModePerm)\n-\t\trequire.Nil(t, err)\n-\t\text := \".txt\"\n-\t\tif i%2 == 0 {\n-\t\t\text = \".csv\"\n-\t\t}\n-\t\tfilepath := path.Join(fileDir, fmt.Sprintf(\"random-%d%s\", i, ext))\n-\t\tfiles[i] = filepath\n-\t\t_, err = os.Create(filepath)\n-\t\trequire.Nil(t, err)\n-\t}\n-\n-\tt.Run(\"can list all files\", func(t *testing.T) {\n-\t\tmFiles, errs := utils.ListFilesInDirectory(tempDir, []string{\"*\"})\n-\t\trequire.Len(t, mFiles, 5)\n-\t\trequire.Empty(t, errs)\n-\t})\n-\n-\tt.Run(\"can list by extension\", func(t *testing.T) {\n-\t\tmFiles, errs := utils.ListFilesInDirectory(tempDir, []string{\"*.csv\"})\n-\t\trequire.Len(t, mFiles, 3)\n-\t\trequire.Empty(t, errs)\n-\t\trequire.ElementsMatch(t, []string{files[0], files[2], files[4]}, mFiles)\n-\t})\n-\n-\tt.Run(\"can list on multiple extensions files\", func(t *testing.T) {\n-\t\tmFiles, errs := utils.ListFilesInDirectory(tempDir, []string{\"*.csv\", \"*.txt\"})\n-\t\trequire.Len(t, mFiles, 5)\n-\t\trequire.Empty(t, errs)\n-\t})\n-\n-}\ndiff --git a/programs/diagnostics/internal/platform/utils/process_test.go b/programs/diagnostics/internal/platform/utils/process_test.go\ndeleted file mode 100644\nindex 9baaa5597522..000000000000\n--- a/programs/diagnostics/internal/platform/utils/process_test.go\n+++ /dev/null\n@@ -1,97 +0,0 @@\n-//go:build !no_docker\n-\n-package utils_test\n-\n-import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"os\"\n-\t\"path\"\n-\t\"strings\"\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test\"\n-\t\"github.com/stretchr/testify/require\"\n-\t\"github.com/testcontainers/testcontainers-go\"\n-\t\"github.com/testcontainers/testcontainers-go/wait\"\n-)\n-\n-func getProcessesInContainer(t *testing.T, container testcontainers.Container) ([]string, error) {\n-\tresult, reader, err := container.Exec(context.Background(), []string{\"ps\", \"-aux\"})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\trequire.Zero(t, result)\n-\trequire.NotNil(t, reader)\n-\n-\tb, err := io.ReadAll(reader)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\trequire.NotNil(t, b)\n-\n-\tlines := strings.Split(string(b), \"\\n\")\n-\n-\t// discard PS header\n-\treturn lines[1:], nil\n-}\n-\n-func TestFindClickHouseProcessesAndConfigs(t *testing.T) {\n-\n-\tt.Run(\"can find ClickHouse processes and configs\", func(t *testing.T) {\n-\t\t// create a ClickHouse container\n-\t\tctx := context.Background()\n-\t\tcwd, err := os.Getwd()\n-\t\tif err != nil {\n-\t\t\tfmt.Println(\"unable to read current directory\", err)\n-\t\t\tos.Exit(1)\n-\t\t}\n-\n-\t\t// run a ClickHouse container that guarantees that it runs only for the duration of the test\n-\t\treq := testcontainers.ContainerRequest{\n-\t\t\tImage:        fmt.Sprintf(\"clickhouse/clickhouse-server:%s\", test.GetClickHouseTestVersion()),\n-\t\t\tExposedPorts: []string{\"9000/tcp\"},\n-\t\t\tWaitingFor:   wait.ForLog(\"Ready for connections\"),\n-\t\t\tMounts: testcontainers.ContainerMounts{\n-\t\t\t\t{\n-\t\t\t\t\tSource: testcontainers.GenericBindMountSource{\n-\t\t\t\t\t\tHostPath: path.Join(cwd, \"../../../testdata/docker/custom.xml\"),\n-\t\t\t\t\t},\n-\t\t\t\t\tTarget: \"/etc/clickhouse-server/config.d/custom.xml\",\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t\tclickhouseContainer, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{\n-\t\t\tContainerRequest: req,\n-\t\t\tStarted:          true,\n-\t\t})\n-\t\tif err != nil {\n-\t\t\t// can't test without container\n-\t\t\tpanic(err)\n-\t\t}\n-\n-\t\tp, _ := clickhouseContainer.MappedPort(ctx, \"9000\")\n-\n-\t\tt.Setenv(\"CLICKHOUSE_DB_PORT\", p.Port())\n-\n-\t\tdefer clickhouseContainer.Terminate(ctx) //nolint\n-\n-\t\tlines, err := getProcessesInContainer(t, clickhouseContainer)\n-\t\trequire.Nil(t, err)\n-\t\trequire.NotEmpty(t, lines)\n-\n-\t\tfor _, line := range lines {\n-\t\t\tparts := strings.Fields(line)\n-\t\t\tif len(parts) < 11 {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\tif !strings.Contains(parts[10], \"clickhouse-server\") {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\n-\t\t\trequire.Equal(t, \"/usr/bin/clickhouse-server\", parts[10])\n-\t\t\trequire.Equal(t, \"--config-file=/etc/clickhouse-server/config.xml\", parts[11])\n-\t\t}\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/platform/utils/slices_test.go b/programs/diagnostics/internal/platform/utils/slices_test.go\ndeleted file mode 100644\nindex ea5c1c81dcc4..000000000000\n--- a/programs/diagnostics/internal/platform/utils/slices_test.go\n+++ /dev/null\n@@ -1,64 +0,0 @@\n-package utils_test\n-\n-import (\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils\"\n-\t\"github.com/stretchr/testify/require\"\n-)\n-\n-func TestIntersection(t *testing.T) {\n-\tt.Run(\"can perform intersection\", func(t *testing.T) {\n-\t\tsetA := []string{\"A\", \"b\", \"C\", \"D\", \"E\"}\n-\t\tsetB := []string{\"A\", \"B\", \"F\", \"C\", \"G\"}\n-\t\tsetC := utils.Intersection(setA, setB)\n-\t\trequire.Len(t, setC, 2)\n-\t\trequire.ElementsMatch(t, []string{\"A\", \"C\"}, setC)\n-\t})\n-}\n-\n-func TestDistinct(t *testing.T) {\n-\tt.Run(\"can perform distinct\", func(t *testing.T) {\n-\t\tsetA := []string{\"A\", \"b\", \"C\", \"D\", \"E\"}\n-\t\tsetB := []string{\"A\", \"B\", \"F\", \"C\", \"G\"}\n-\t\tsetC := utils.Distinct(setA, setB)\n-\t\trequire.Len(t, setC, 3)\n-\t\trequire.ElementsMatch(t, []string{\"b\", \"D\", \"E\"}, setC)\n-\t})\n-\n-\tt.Run(\"can perform distinct on empty\", func(t *testing.T) {\n-\t\tsetA := []string{\"A\", \"b\", \"C\", \"D\", \"E\"}\n-\t\tvar setB []string\n-\t\tsetC := utils.Distinct(setA, setB)\n-\t\trequire.Len(t, setC, 5)\n-\t\trequire.ElementsMatch(t, []string{\"A\", \"b\", \"C\", \"D\", \"E\"}, setC)\n-\t})\n-}\n-\n-func TestContains(t *testing.T) {\n-\tt.Run(\"can perform contains\", func(t *testing.T) {\n-\t\tsetA := []string{\"A\", \"b\", \"C\", \"D\", \"E\"}\n-\t\trequire.True(t, utils.Contains(setA, \"A\"))\n-\t\trequire.True(t, utils.Contains(setA, \"b\"))\n-\t\trequire.True(t, utils.Contains(setA, \"C\"))\n-\t\trequire.True(t, utils.Contains(setA, \"D\"))\n-\t\trequire.True(t, utils.Contains(setA, \"E\"))\n-\t\trequire.False(t, utils.Contains(setA, \"B\"))\n-\t})\n-}\n-\n-func TestUnique(t *testing.T) {\n-\n-\tt.Run(\"can perform unique\", func(t *testing.T) {\n-\t\tsetA := []string{\"A\", \"b\", \"D\", \"D\", \"E\", \"E\", \"A\"}\n-\t\tsetC := utils.Unique(setA)\n-\t\trequire.Len(t, setC, 4)\n-\t\trequire.ElementsMatch(t, []string{\"A\", \"b\", \"D\", \"E\"}, setC)\n-\t})\n-\n-\tt.Run(\"can perform unique on empty\", func(t *testing.T) {\n-\t\tvar setA []string\n-\t\tsetC := utils.Unique(setA)\n-\t\trequire.Len(t, setC, 0)\n-\t})\n-}\ndiff --git a/programs/diagnostics/internal/runner_test.go b/programs/diagnostics/internal/runner_test.go\ndeleted file mode 100644\nindex 2369f8b3007d..000000000000\n--- a/programs/diagnostics/internal/runner_test.go\n+++ /dev/null\n@@ -1,130 +0,0 @@\n-//go:build !no_docker\n-\n-package internal_test\n-\n-import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"io/ioutil\"\n-\t\"os\"\n-\t\"path\"\n-\t\"testing\"\n-\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors\"\n-\t_ \"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse\"\n-\t_ \"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/system\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs\"\n-\t_ \"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs/file\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test\"\n-\t\"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils\"\n-\t\"github.com/stretchr/testify/require\"\n-\t\"github.com/testcontainers/testcontainers-go\"\n-\t\"github.com/testcontainers/testcontainers-go/wait\"\n-)\n-\n-// Execute a full default capture, with simple output, and check if a bundle is produced and it's not empty\n-func TestCapture(t *testing.T) {\n-\t// create a ClickHouse container\n-\tctx := context.Background()\n-\tcwd, err := os.Getwd()\n-\n-\tif err != nil {\n-\t\t// can't test without container\n-\t\tpanic(err)\n-\t}\n-\t// for now, we test against a hardcoded database-server version but we should make this a property\n-\treq := testcontainers.ContainerRequest{\n-\t\tImage:        fmt.Sprintf(\"clickhouse/clickhouse-server:%s\", test.GetClickHouseTestVersion()),\n-\t\tExposedPorts: []string{\"9000/tcp\"},\n-\t\tWaitingFor:   wait.ForLog(\"Ready for connections\"),\n-\t\tMounts: testcontainers.ContainerMounts{\n-\t\t\t{\n-\t\t\t\tSource: testcontainers.GenericBindMountSource{\n-\t\t\t\t\tHostPath: path.Join(cwd, \"../testdata/docker/custom.xml\"),\n-\t\t\t\t},\n-\t\t\t\tTarget: \"/etc/clickhouse-server/config.d/custom.xml\",\n-\t\t\t},\n-\t\t\t{\n-\t\t\t\tSource: testcontainers.GenericBindMountSource{\n-\t\t\t\t\tHostPath: path.Join(cwd, \"../testdata/docker/admin.xml\"),\n-\t\t\t\t},\n-\t\t\t\tTarget: \"/etc/clickhouse-server/users.d/admin.xml\",\n-\t\t\t},\n-\t\t},\n-\t}\n-\tclickhouseContainer, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{\n-\t\tContainerRequest: req,\n-\t\tStarted:          true,\n-\t})\n-\tif err != nil {\n-\t\t// can't test without container\n-\t\tpanic(err)\n-\t}\n-\n-\tp, _ := clickhouseContainer.MappedPort(ctx, \"9000\")\n-\n-\tt.Setenv(\"CLICKHOUSE_DB_PORT\", p.Port())\n-\tdefer clickhouseContainer.Terminate(ctx) //nolint\n-\n-\ttmrDir := t.TempDir()\n-\tport := p.Int()\n-\n-\t// test a simple output exists\n-\t_, err = outputs.GetOutputByName(\"simple\")\n-\trequire.Nil(t, err)\n-\t// this relies on the simple out not changing its params - test will likely fail if so\n-\toutputConfig := config.Configuration{\n-\t\tParams: []config.ConfigParam{\n-\t\t\tconfig.StringParam{\n-\t\t\t\tValue: tmrDir,\n-\t\t\t\tParam: config.NewParam(\"directory\", \"Directory in which to create dump. Defaults to the current directory.\", false),\n-\t\t\t},\n-\t\t\tconfig.StringOptions{\n-\t\t\t\tValue:   \"csv\",\n-\t\t\t\tOptions: []string{\"csv\"},\n-\t\t\t\tParam:   config.NewParam(\"format\", \"Format of exported files\", false),\n-\t\t\t},\n-\t\t\tconfig.BoolParam{\n-\t\t\t\tValue: true,\n-\t\t\t\tParam: config.NewParam(\"skip_archive\", \"Don't compress output to an archive\", false),\n-\t\t\t},\n-\t\t},\n-\t}\n-\t// test default collectors\n-\tcollectorNames := collectors.GetCollectorNames(true)\n-\t// grab all configs - only default will be used because of collectorNames\n-\tcollectorConfigs, err := collectors.BuildConfigurationOptions()\n-\trequire.Nil(t, err)\n-\tconf := internal.NewRunConfiguration(\"random\", \"localhost\", uint16(port), \"\", \"\", \"simple\", outputConfig, collectorNames, collectorConfigs)\n-\tinternal.Capture(conf)\n-\toutputDir := path.Join(tmrDir, \"random\")\n-\t_, err = os.Stat(outputDir)\n-\trequire.Nil(t, err)\n-\trequire.True(t, !os.IsNotExist(err))\n-\tfiles, err := ioutil.ReadDir(outputDir)\n-\trequire.Nil(t, err)\n-\trequire.Len(t, files, 1)\n-\toutputDir = path.Join(outputDir, files[0].Name())\n-\t// check we have a folder per collector i.e. collectorNames + diag_trace\n-\tfiles, err = ioutil.ReadDir(outputDir)\n-\trequire.Nil(t, err)\n-\trequire.Len(t, files, len(collectorNames)+1)\n-\texpectedFolders := append(collectorNames, \"diag_trace\")\n-\tfor _, file := range files {\n-\t\trequire.True(t, file.IsDir())\n-\t\tutils.Contains(expectedFolders, file.Name())\n-\t}\n-\t// we don't test the specific collector outputs but make sure something was written to system\n-\tsystemFolder := path.Join(outputDir, \"system\")\n-\tfiles, err = ioutil.ReadDir(systemFolder)\n-\trequire.Nil(t, err)\n-\trequire.Greater(t, len(files), 0)\n-\t// test diag_trace\n-\tdiagFolder := path.Join(outputDir, \"diag_trace\")\n-\tfiles, err = ioutil.ReadDir(diagFolder)\n-\trequire.Nil(t, err)\n-\trequire.Equal(t, 1, len(files))\n-\trequire.FileExists(t, path.Join(diagFolder, \"errors.csv\"))\n-}\ndiff --git a/programs/diagnostics/testdata/configs/include/xml/server-include.xml b/programs/diagnostics/testdata/configs/include/xml/server-include.xml\ndeleted file mode 100644\nindex 30e6587c935e..000000000000\n--- a/programs/diagnostics/testdata/configs/include/xml/server-include.xml\n+++ /dev/null\n@@ -1,8 +0,0 @@\n-<clickhouse>\n-    <network_max>5000000</network_max>\n-    <test_profile>\n-        <test_p>\n-        </test_p>\n-    </test_profile>\n-    <pg_port>9008</pg_port>\n-</clickhouse>\n\\ No newline at end of file\ndiff --git a/programs/diagnostics/testdata/configs/include/xml/user-include.xml b/programs/diagnostics/testdata/configs/include/xml/user-include.xml\ndeleted file mode 100644\nindex b12b34a56bb9..000000000000\n--- a/programs/diagnostics/testdata/configs/include/xml/user-include.xml\n+++ /dev/null\n@@ -1,20 +0,0 @@\n-<clickhouse>\n-    <test_user>\n-        <networks>\n-            <ip>::/0</ip>\n-        </networks>\n-        <profile>default</profile>\n-        <quota>default</quota>\n-        <password_sha256_hex>REPLACE_ME</password_sha256_hex>\n-        <access_management>1</access_management>\n-    </test_user>\n-    <another_user>\n-        <networks>\n-            <ip>::/0</ip>\n-        </networks>\n-        <profile>default</profile>\n-        <quota>default</quota>\n-        <passwird>REPLACE_ME</passwird>\n-        <access_management>1</access_management>\n-    </another_user>\n-</clickhouse>\ndiff --git a/programs/diagnostics/testdata/configs/include/yaml/server-include.yaml b/programs/diagnostics/testdata/configs/include/yaml/server-include.yaml\ndeleted file mode 100644\nindex 903d7b6f733f..000000000000\n--- a/programs/diagnostics/testdata/configs/include/yaml/server-include.yaml\n+++ /dev/null\n@@ -1,1 +0,0 @@\n-network_max: 5000000\ndiff --git a/programs/diagnostics/testdata/configs/include/yaml/user-include.yaml b/programs/diagnostics/testdata/configs/include/yaml/user-include.yaml\ndeleted file mode 100644\nindex 23b592507fab..000000000000\n--- a/programs/diagnostics/testdata/configs/include/yaml/user-include.yaml\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-test_user:\n-  password: 'REPLACE_ME'\n-  networks:\n-    ip: '::/0'\n-  profile: default\n-  quota: default\n-  access_management: 1\ndiff --git a/programs/diagnostics/testdata/configs/xml/config.xml b/programs/diagnostics/testdata/configs/xml/config.xml\ndeleted file mode 100644\nindex eb7c70cf4981..000000000000\n--- a/programs/diagnostics/testdata/configs/xml/config.xml\n+++ /dev/null\n@@ -1,1195 +0,0 @@\n-<!--\n-  NOTE: User and query level settings are set up in \"users.xml\" file.\n-  If you have accidentally specified user-level settings here, server won't start.\n-  You can either move the settings to the right place inside \"users.xml\" file\n-   or add <skip_check_for_incorrect_settings>1</skip_check_for_incorrect_settings> here.\n--->\n-<clickhouse>\n-    <include_from>../include/xml/server-include.xml</include_from>\n-    <logger>\n-        <!-- Possible levels [1]:\n-\n-          - none (turns off logging)\n-          - fatal\n-          - critical\n-          - error\n-          - warning\n-          - notice\n-          - information\n-          - debug\n-          - trace\n-          - test (not for production usage)\n-\n-            [1]: https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/Logger.h#L105-L114\n-        -->\n-        <level>trace</level>\n-        <log>/var/log/clickhouse-server/clickhouse-server.log</log>\n-        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>\n-        <!-- Rotation policy\n-             See https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/FileChannel.h#L54-L85\n-          -->\n-        <size>1000M</size>\n-        <count>10</count>\n-        <!-- <console>1</console> --> <!-- Default behavior is autodetection (log to console if not daemon mode and is tty) -->\n-\n-        <!-- Per level overrides (legacy):\n-\n-        For example to suppress logging of the ConfigReloader you can use:\n-        NOTE: levels.logger is reserved, see below.\n-        -->\n-        <!--\n-        <levels>\n-          <ConfigReloader>none</ConfigReloader>\n-        </levels>\n-        -->\n-\n-        <!-- Per level overrides:\n-\n-        For example to suppress logging of the RBAC for default user you can use:\n-        (But please note that the logger name maybe changed from version to version, even after minor upgrade)\n-        -->\n-        <!--\n-        <levels>\n-          <logger>\n-            <name>ContextAccess (default)</name>\n-            <level>none</level>\n-          </logger>\n-          <logger>\n-            <name>DatabaseOrdinary (test)</name>\n-            <level>none</level>\n-          </logger>\n-        </levels>\n-        -->\n-    </logger>\n-    <!-- Add headers to response in options request. OPTIONS method is used in CORS preflight requests. -->\n-    <!-- It is off by default. Next headers are obligate for CORS.-->\n-    <!-- http_options_response>\n-        <header>\n-            <name>Access-Control-Allow-Origin</name>\n-            <value>*</value>\n-        </header>\n-        <header>\n-            <name>Access-Control-Allow-Headers</name>\n-            <value>origin, x-requested-with</value>\n-        </header>\n-        <header>\n-            <name>Access-Control-Allow-Methods</name>\n-            <value>POST, GET, OPTIONS</value>\n-        </header>\n-        <header>\n-            <name>Access-Control-Max-Age</name>\n-            <value>86400</value>\n-        </header>\n-    </http_options_response -->\n-\n-    <!-- It is the name that will be shown in the clickhouse-client.\n-         By default, anything with \"production\" will be highlighted in red in query prompt.\n-    -->\n-    <!--display_name>production</display_name-->\n-\n-    <!-- Port for HTTP API. See also 'https_port' for secure connections.\n-         This interface is also used by ODBC and JDBC drivers (DataGrip, Dbeaver, ...)\n-         and by most of web interfaces (embedded UI, Grafana, Redash, ...).\n-      -->\n-    <http_port>8123</http_port>\n-\n-    <!-- Port for interaction by native protocol with:\n-         - clickhouse-client and other native ClickHouse tools (clickhouse-benchmark);\n-         - clickhouse-server with other clickhouse-servers for distributed query processing;\n-         - ClickHouse drivers and applications supporting native protocol\n-         (this protocol is also informally called as \"the TCP protocol\");\n-         See also 'tcp_port_secure' for secure connections.\n-    -->\n-    <tcp_port>9000</tcp_port>\n-\n-    <!-- Compatibility with MySQL protocol.\n-         ClickHouse will pretend to be MySQL for applications connecting to this port.\n-    -->\n-    <mysql_port>9004</mysql_port>\n-\n-    <!-- Compatibility with PostgreSQL protocol.\n-         ClickHouse will pretend to be PostgreSQL for applications connecting to this port.\n-    -->\n-    <postgresql_port>9005</postgresql_port>\n-\n-    <!-- HTTP API with TLS (HTTPS).\n-         You have to configure certificate to enable this interface.\n-         See the openSSL section below.\n-    -->\n-    <!-- <https_port>8443</https_port> -->\n-\n-    <!-- Native interface with TLS.\n-         You have to configure certificate to enable this interface.\n-         See the openSSL section below.\n-    -->\n-    <!-- <tcp_port_secure>9440</tcp_port_secure> -->\n-\n-    <!-- Native interface wrapped with PROXYv1 protocol\n-         PROXYv1 header sent for every connection.\n-         ClickHouse will extract information about proxy-forwarded client address from the header.\n-    -->\n-    <!-- <tcp_with_proxy_port>9011</tcp_with_proxy_port> -->\n-\n-    <!-- Port for communication between replicas. Used for data exchange.\n-         It provides low-level data access between servers.\n-         This port should not be accessible from untrusted networks.\n-         See also 'interserver_http_credentials'.\n-         Data transferred over connections to this port should not go through untrusted networks.\n-         See also 'interserver_https_port'.\n-      -->\n-    <interserver_http_port>9009</interserver_http_port>\n-\n-    <!-- Port for communication between replicas with TLS.\n-         You have to configure certificate to enable this interface.\n-         See the openSSL section below.\n-         See also 'interserver_http_credentials'.\n-      -->\n-    <!-- <interserver_https_port>9010</interserver_https_port> -->\n-\n-    <!-- Hostname that is used by other replicas to request this server.\n-         If not specified, than it is determined analogous to 'hostname -f' command.\n-         This setting could be used to switch replication to another network interface\n-         (the server may be connected to multiple networks via multiple addresses)\n-      -->\n-    <!--\n-    <interserver_http_host>example.yandex.ru</interserver_http_host>\n-    -->\n-\n-    <!-- You can specify credentials for authenthication between replicas.\n-         This is required when interserver_https_port is accessible from untrusted networks,\n-         and also recommended to avoid SSRF attacks from possibly compromised services in your network.\n-      -->\n-    <!--<interserver_http_credentials>\n-        <user>interserver</user>\n-        <password></password>\n-    </interserver_http_credentials>-->\n-\n-    <!-- Listen specified address.\n-         Use :: (wildcard IPv6 address), if you want to accept connections both with IPv4 and IPv6 from everywhere.\n-         Notes:\n-         If you open connections from wildcard address, make sure that at least one of the following measures applied:\n-         - server is protected by firewall and not accessible from untrusted networks;\n-         - all users are restricted to subset of network addresses (see users.xml);\n-         - all users have strong passwords, only secure (TLS) interfaces are accessible, or connections are only made via TLS interfaces.\n-         - users without password have readonly access.\n-         See also: https://www.shodan.io/search?query=clickhouse\n-      -->\n-    <!-- <listen_host>::</listen_host> -->\n-\n-    <!-- Same for hosts without support for IPv6: -->\n-    <!-- <listen_host>0.0.0.0</listen_host> -->\n-\n-    <!-- Default values - try listen localhost on IPv4 and IPv6. -->\n-    <!--\n-    <listen_host>::1</listen_host>\n-    <listen_host>127.0.0.1</listen_host>\n-    -->\n-\n-    <!-- Don't exit if IPv6 or IPv4 networks are unavailable while trying to listen. -->\n-    <!-- <listen_try>0</listen_try> -->\n-\n-    <!-- Allow multiple servers to listen on the same address:port. This is not recommended.\n-      -->\n-    <!-- <listen_reuse_port>0</listen_reuse_port> -->\n-\n-    <!-- <listen_backlog>4096</listen_backlog> -->\n-\n-    <max_connections>4096</max_connections>\n-\n-    <!-- For 'Connection: keep-alive' in HTTP 1.1 -->\n-    <keep_alive_timeout>3</keep_alive_timeout>\n-\n-    <!-- gRPC protocol (see src/Server/grpc_protos/clickhouse_grpc.proto for the API) -->\n-    <!-- <grpc_port>9100</grpc_port> -->\n-    <grpc>\n-        <enable_ssl>false</enable_ssl>\n-\n-        <!-- The following two files are used only if enable_ssl=1 -->\n-        <ssl_cert_file>/path/to/ssl_cert_file</ssl_cert_file>\n-        <ssl_key_file>/path/to/ssl_key_file</ssl_key_file>\n-\n-        <!-- Whether server will request client for a certificate -->\n-        <ssl_require_client_auth>false</ssl_require_client_auth>\n-\n-        <!-- The following file is used only if ssl_require_client_auth=1 -->\n-        <ssl_ca_cert_file>/path/to/ssl_ca_cert_file</ssl_ca_cert_file>\n-\n-        <!-- Default compression algorithm (applied if client doesn't specify another algorithm, see result_compression in QueryInfo).\n-             Supported algorithms: none, deflate, gzip, stream_gzip -->\n-        <compression>deflate</compression>\n-\n-        <!-- Default compression level (applied if client doesn't specify another level, see result_compression in QueryInfo).\n-             Supported levels: none, low, medium, high -->\n-        <compression_level>medium</compression_level>\n-\n-        <!-- Send/receive message size limits in bytes. -1 means unlimited -->\n-        <max_send_message_size>-1</max_send_message_size>\n-        <max_receive_message_size>-1</max_receive_message_size>\n-\n-        <!-- Enable if you want very detailed logs -->\n-        <verbose_logs>false</verbose_logs>\n-    </grpc>\n-\n-    <!-- Used with https_port and tcp_port_secure. Full ssl options list: https://github.com/ClickHouse-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/SSLManager.h#L71 -->\n-    <openSSL>\n-        <server> <!-- Used for https server AND secure tcp port -->\n-            <!-- openssl req -subj \"/CN=localhost\" -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/clickhouse-server/server.key -out /etc/clickhouse-server/server.crt -->\n-            <certificateFile>/etc/clickhouse-server/server.crt</certificateFile>\n-            <privateKeyFile>/etc/clickhouse-server/server.key</privateKeyFile>\n-            <!-- dhparams are optional. You can delete the <dhParamsFile> element.\n-                 To generate dhparams, use the following command:\n-                  openssl dhparam -out /etc/clickhouse-server/dhparam.pem 4096\n-                 Only file format with BEGIN DH PARAMETERS is supported.\n-              -->\n-            <dhParamsFile>/etc/clickhouse-server/dhparam.pem</dhParamsFile>\n-            <verificationMode>none</verificationMode>\n-            <loadDefaultCAFile>true</loadDefaultCAFile>\n-            <cacheSessions>true</cacheSessions>\n-            <disableProtocols>sslv2,sslv3</disableProtocols>\n-            <preferServerCiphers>true</preferServerCiphers>\n-        </server>\n-\n-        <client> <!-- Used for connecting to https dictionary source and secured Zookeeper communication -->\n-            <loadDefaultCAFile>true</loadDefaultCAFile>\n-            <cacheSessions>true</cacheSessions>\n-            <disableProtocols>sslv2,sslv3</disableProtocols>\n-            <preferServerCiphers>true</preferServerCiphers>\n-            <!-- Use for self-signed: <verificationMode>none</verificationMode> -->\n-            <invalidCertificateHandler>\n-                <!-- Use for self-signed: <name>AcceptCertificateHandler</name> -->\n-                <name>RejectCertificateHandler</name>\n-            </invalidCertificateHandler>\n-        </client>\n-    </openSSL>\n-\n-    <!-- Default root page on http[s] server. For example load UI from https://tabix.io/ when opening http://localhost:8123 -->\n-    <!--\n-    <http_server_default_response><![CDATA[<html ng-app=\"SMI2\"><head><base href=\"http://ui.tabix.io/\"></head><body><div ui-view=\"\" class=\"content-ui\"></div><script src=\"http://loader.tabix.io/master.js\"></script></body></html>]]></http_server_default_response>\n-    -->\n-\n-    <!-- Maximum number of concurrent queries. -->\n-    <max_concurrent_queries>100</max_concurrent_queries>\n-\n-    <!-- Maximum memory usage (resident set size) for server process.\n-         Zero value or unset means default. Default is \"max_server_memory_usage_to_ram_ratio\" of available physical RAM.\n-         If the value is larger than \"max_server_memory_usage_to_ram_ratio\" of available physical RAM, it will be cut down.\n-\n-         The constraint is checked on query execution time.\n-         If a query tries to allocate memory and the current memory usage plus allocation is greater\n-          than specified threshold, exception will be thrown.\n-\n-         It is not practical to set this constraint to small values like just a few gigabytes,\n-          because memory allocator will keep this amount of memory in caches and the server will deny service of queries.\n-      -->\n-    <max_server_memory_usage>0</max_server_memory_usage>\n-\n-    <!-- Maximum number of threads in the Global thread pool.\n-    This will default to a maximum of 10000 threads if not specified.\n-    This setting will be useful in scenarios where there are a large number\n-    of distributed queries that are running concurrently but are idling most\n-    of the time, in which case a higher number of threads might be required.\n-    -->\n-\n-    <max_thread_pool_size>10000</max_thread_pool_size>\n-\n-    <!-- On memory constrained environments you may have to set this to value larger than 1.\n-      -->\n-    <max_server_memory_usage_to_ram_ratio>0.9</max_server_memory_usage_to_ram_ratio>\n-\n-    <!-- Simple server-wide memory profiler. Collect a stack trace at every peak allocation step (in bytes).\n-         Data will be stored in system.trace_log table with query_id = empty string.\n-         Zero means disabled.\n-      -->\n-    <total_memory_profiler_step>4194304</total_memory_profiler_step>\n-\n-    <!-- Collect random allocations and deallocations and write them into system.trace_log with 'MemorySample' trace_type.\n-         The probability is for every alloc/free regardless to the size of the allocation.\n-         Note that sampling happens only when the amount of untracked memory exceeds the untracked memory limit,\n-          which is 4 MiB by default but can be lowered if 'total_memory_profiler_step' is lowered.\n-         You may want to set 'total_memory_profiler_step' to 1 for extra fine grained sampling.\n-      -->\n-    <total_memory_tracker_sample_probability>0</total_memory_tracker_sample_probability>\n-\n-    <!-- Set limit on number of open files (default: maximum). This setting makes sense on Mac OS X because getrlimit() fails to retrieve\n-         correct maximum value. -->\n-    <!-- <max_open_files>262144</max_open_files> -->\n-\n-    <!-- Size of cache of uncompressed blocks of data, used in tables of MergeTree family.\n-         In bytes. Cache is single for server. Memory is allocated only on demand.\n-         Cache is used when 'use_uncompressed_cache' user setting turned on (off by default).\n-         Uncompressed cache is advantageous only for very short queries and in rare cases.\n-\n-         Note: uncompressed cache can be pointless for lz4, because memory bandwidth\n-         is slower than multi-core decompression on some server configurations.\n-         Enabling it can sometimes paradoxically make queries slower.\n-      -->\n-    <uncompressed_cache_size>8589934592</uncompressed_cache_size>\n-\n-    <!-- Approximate size of mark cache, used in tables of MergeTree family.\n-         In bytes. Cache is single for server. Memory is allocated only on demand.\n-         You should not lower this value.\n-      -->\n-    <mark_cache_size>5368709120</mark_cache_size>\n-\n-\n-    <!-- If you enable the `min_bytes_to_use_mmap_io` setting,\n-         the data in MergeTree tables can be read with mmap to avoid copying from kernel to userspace.\n-         It makes sense only for large files and helps only if data reside in page cache.\n-         To avoid frequent open/mmap/munmap/close calls (which are very expensive due to consequent page faults)\n-         and to reuse mappings from several threads and queries,\n-         the cache of mapped files is maintained. Its size is the number of mapped regions (usually equal to the number of mapped files).\n-         The amount of data in mapped files can be monitored\n-         in system.metrics, system.metric_log by the MMappedFiles, MMappedFileBytes metrics\n-         and in system.asynchronous_metrics, system.asynchronous_metrics_log by the MMapCacheCells metric,\n-         and also in system.events, system.processes, system.query_log, system.query_thread_log, system.query_views_log by the\n-         CreatedReadBufferMMap, CreatedReadBufferMMapFailed, MMappedFileCacheHits, MMappedFileCacheMisses events.\n-         Note that the amount of data in mapped files does not consume memory directly and is not accounted\n-         in query or server memory usage - because this memory can be discarded similar to OS page cache.\n-         The cache is dropped (the files are closed) automatically on removal of old parts in MergeTree,\n-         also it can be dropped manually by the SYSTEM DROP MMAP CACHE query.\n-      -->\n-    <mmap_cache_size>1000</mmap_cache_size>\n-\n-    <!-- Cache size in bytes for compiled expressions.-->\n-    <compiled_expression_cache_size>134217728</compiled_expression_cache_size>\n-\n-    <!-- Cache size in elements for compiled expressions.-->\n-    <compiled_expression_cache_elements_size>10000</compiled_expression_cache_elements_size>\n-\n-    <!-- Path to data directory, with trailing slash. -->\n-    <path>/var/lib/clickhouse/</path>\n-\n-    <!-- Path to temporary data for processing hard queries. -->\n-    <tmp_path>/var/lib/clickhouse/tmp/</tmp_path>\n-\n-    <!-- Policy from the <storage_configuration> for the temporary files.\n-         If not set <tmp_path> is used, otherwise <tmp_path> is ignored.\n-\n-         Notes:\n-         - move_factor              is ignored\n-         - keep_free_space_bytes    is ignored\n-         - max_data_part_size_bytes is ignored\n-         - you must have exactly one volume in that policy\n-    -->\n-    <!-- <tmp_policy>tmp</tmp_policy> -->\n-\n-    <!-- Directory with user provided files that are accessible by 'file' table function. -->\n-    <user_files_path>/var/lib/clickhouse/user_files/</user_files_path>\n-\n-    <!-- LDAP server definitions. -->\n-    <ldap_servers>\n-        <!-- List LDAP servers with their connection parameters here to later 1) use them as authenticators for dedicated local users,\n-              who have 'ldap' authentication mechanism specified instead of 'password', or to 2) use them as remote user directories.\n-             Parameters:\n-                host - LDAP server hostname or IP, this parameter is mandatory and cannot be empty.\n-                port - LDAP server port, default is 636 if enable_tls is set to true, 389 otherwise.\n-                bind_dn - template used to construct the DN to bind to.\n-                        The resulting DN will be constructed by replacing all '{user_name}' substrings of the template with the actual\n-                         user name during each authentication attempt.\n-                user_dn_detection - section with LDAP search parameters for detecting the actual user DN of the bound user.\n-                        This is mainly used in search filters for further role mapping when the server is Active Directory. The\n-                         resulting user DN will be used when replacing '{user_dn}' substrings wherever they are allowed. By default,\n-                         user DN is set equal to bind DN, but once search is performed, it will be updated with to the actual detected\n-                         user DN value.\n-                    base_dn - template used to construct the base DN for the LDAP search.\n-                            The resulting DN will be constructed by replacing all '{user_name}' and '{bind_dn}' substrings\n-                             of the template with the actual user name and bind DN during the LDAP search.\n-                    scope - scope of the LDAP search.\n-                            Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).\n-                    search_filter - template used to construct the search filter for the LDAP search.\n-                            The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', and '{base_dn}'\n-                             substrings of the template with the actual user name, bind DN, and base DN during the LDAP search.\n-                            Note, that the special characters must be escaped properly in XML.\n-                verification_cooldown - a period of time, in seconds, after a successful bind attempt, during which a user will be assumed\n-                         to be successfully authenticated for all consecutive requests without contacting the LDAP server.\n-                        Specify 0 (the default) to disable caching and force contacting the LDAP server for each authentication request.\n-                enable_tls - flag to trigger use of secure connection to the LDAP server.\n-                        Specify 'no' for plain text (ldap://) protocol (not recommended).\n-                        Specify 'yes' for LDAP over SSL/TLS (ldaps://) protocol (recommended, the default).\n-                        Specify 'starttls' for legacy StartTLS protocol (plain text (ldap://) protocol, upgraded to TLS).\n-                tls_minimum_protocol_version - the minimum protocol version of SSL/TLS.\n-                        Accepted values are: 'ssl2', 'ssl3', 'tls1.0', 'tls1.1', 'tls1.2' (the default).\n-                tls_require_cert - SSL/TLS peer certificate verification behavior.\n-                        Accepted values are: 'never', 'allow', 'try', 'demand' (the default).\n-                tls_cert_file - path to certificate file.\n-                tls_key_file - path to certificate key file.\n-                tls_ca_cert_file - path to CA certificate file.\n-                tls_ca_cert_dir - path to the directory containing CA certificates.\n-                tls_cipher_suite - allowed cipher suite (in OpenSSL notation).\n-             Example:\n-                <my_ldap_server>\n-                    <host>localhost</host>\n-                    <port>636</port>\n-                    <bind_dn>uid={user_name},ou=users,dc=example,dc=com</bind_dn>\n-                    <verification_cooldown>300</verification_cooldown>\n-                    <enable_tls>yes</enable_tls>\n-                    <tls_minimum_protocol_version>tls1.2</tls_minimum_protocol_version>\n-                    <tls_require_cert>demand</tls_require_cert>\n-                    <tls_cert_file>/path/to/tls_cert_file</tls_cert_file>\n-                    <tls_key_file>/path/to/tls_key_file</tls_key_file>\n-                    <tls_ca_cert_file>/path/to/tls_ca_cert_file</tls_ca_cert_file>\n-                    <tls_ca_cert_dir>/path/to/tls_ca_cert_dir</tls_ca_cert_dir>\n-                    <tls_cipher_suite>ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:AES256-GCM-SHA384</tls_cipher_suite>\n-                </my_ldap_server>\n-             Example (typical Active Directory with configured user DN detection for further role mapping):\n-                <my_ad_server>\n-                    <host>localhost</host>\n-                    <port>389</port>\n-                    <bind_dn>EXAMPLE\\{user_name}</bind_dn>\n-                    <user_dn_detection>\n-                        <base_dn>CN=Users,DC=example,DC=com</base_dn>\n-                        <search_filter>(&amp;(objectClass=user)(sAMAccountName={user_name}))</search_filter>\n-                    </user_dn_detection>\n-                    <enable_tls>no</enable_tls>\n-                </my_ad_server>\n-        -->\n-    </ldap_servers>\n-\n-    <!-- To enable Kerberos authentication support for HTTP requests (GSS-SPNEGO), for those users who are explicitly configured\n-          to authenticate via Kerberos, define a single 'kerberos' section here.\n-         Parameters:\n-            principal - canonical service principal name, that will be acquired and used when accepting security contexts.\n-                    This parameter is optional, if omitted, the default principal will be used.\n-                    This parameter cannot be specified together with 'realm' parameter.\n-            realm - a realm, that will be used to restrict authentication to only those requests whose initiator's realm matches it.\n-                    This parameter is optional, if omitted, no additional filtering by realm will be applied.\n-                    This parameter cannot be specified together with 'principal' parameter.\n-         Example:\n-            <kerberos />\n-         Example:\n-            <kerberos>\n-                <principal>HTTP/clickhouse.example.com@EXAMPLE.COM</principal>\n-            </kerberos>\n-         Example:\n-            <kerberos>\n-                <realm>EXAMPLE.COM</realm>\n-            </kerberos>\n-    -->\n-\n-    <!-- Sources to read users, roles, access rights, profiles of settings, quotas. -->\n-    <user_directories>\n-        <users_xml>\n-            <!-- Path to configuration file with predefined users. -->\n-            <path>users.xml</path>\n-        </users_xml>\n-        <local_directory>\n-            <!-- Path to folder where users created by SQL commands are stored. -->\n-            <path>/var/lib/clickhouse/access/</path>\n-        </local_directory>\n-\n-        <!-- To add an LDAP server as a remote user directory of users that are not defined locally, define a single 'ldap' section\n-              with the following parameters:\n-                server - one of LDAP server names defined in 'ldap_servers' config section above.\n-                        This parameter is mandatory and cannot be empty.\n-                roles - section with a list of locally defined roles that will be assigned to each user retrieved from the LDAP server.\n-                        If no roles are specified here or assigned during role mapping (below), user will not be able to perform any\n-                         actions after authentication.\n-                role_mapping - section with LDAP search parameters and mapping rules.\n-                        When a user authenticates, while still bound to LDAP, an LDAP search is performed using search_filter and the\n-                         name of the logged in user. For each entry found during that search, the value of the specified attribute is\n-                         extracted. For each attribute value that has the specified prefix, the prefix is removed, and the rest of the\n-                         value becomes the name of a local role defined in ClickHouse, which is expected to be created beforehand by\n-                         CREATE ROLE command.\n-                        There can be multiple 'role_mapping' sections defined inside the same 'ldap' section. All of them will be\n-                         applied.\n-                    base_dn - template used to construct the base DN for the LDAP search.\n-                            The resulting DN will be constructed by replacing all '{user_name}', '{bind_dn}', and '{user_dn}'\n-                             substrings of the template with the actual user name, bind DN, and user DN during each LDAP search.\n-                    scope - scope of the LDAP search.\n-                            Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).\n-                    search_filter - template used to construct the search filter for the LDAP search.\n-                            The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', '{user_dn}', and\n-                             '{base_dn}' substrings of the template with the actual user name, bind DN, user DN, and base DN during\n-                             each LDAP search.\n-                            Note, that the special characters must be escaped properly in XML.\n-                    attribute - attribute name whose values will be returned by the LDAP search. 'cn', by default.\n-                    prefix - prefix, that will be expected to be in front of each string in the original list of strings returned by\n-                             the LDAP search. Prefix will be removed from the original strings and resulting strings will be treated\n-                             as local role names. Empty, by default.\n-             Example:\n-                <ldap>\n-                    <server>my_ldap_server</server>\n-                    <roles>\n-                        <my_local_role1 />\n-                        <my_local_role2 />\n-                    </roles>\n-                    <role_mapping>\n-                        <base_dn>ou=groups,dc=example,dc=com</base_dn>\n-                        <scope>subtree</scope>\n-                        <search_filter>(&amp;(objectClass=groupOfNames)(member={bind_dn}))</search_filter>\n-                        <attribute>cn</attribute>\n-                        <prefix>clickhouse_</prefix>\n-                    </role_mapping>\n-                </ldap>\n-             Example (typical Active Directory with role mapping that relies on the detected user DN):\n-                <ldap>\n-                    <server>my_ad_server</server>\n-                    <role_mapping>\n-                        <base_dn>CN=Users,DC=example,DC=com</base_dn>\n-                        <attribute>CN</attribute>\n-                        <scope>subtree</scope>\n-                        <search_filter>(&amp;(objectClass=group)(member={user_dn}))</search_filter>\n-                        <prefix>clickhouse_</prefix>\n-                    </role_mapping>\n-                </ldap>\n-        -->\n-    </user_directories>\n-\n-    <!-- Default profile of settings. -->\n-    <default_profile>default</default_profile>\n-\n-    <!-- Comma-separated list of prefixes for user-defined settings. -->\n-    <custom_settings_prefixes></custom_settings_prefixes>\n-\n-    <!-- System profile of settings. This settings are used by internal processes (Distributed DDL worker and so on). -->\n-    <!-- <system_profile>default</system_profile> -->\n-\n-    <!-- Buffer profile of settings.\n-         This settings are used by Buffer storage to flush data to the underlying table.\n-         Default: used from system_profile directive.\n-    -->\n-    <!-- <buffer_profile>default</buffer_profile> -->\n-\n-    <!-- Default database. -->\n-    <default_database>default</default_database>\n-\n-    <!-- Server time zone could be set here.\n-\n-         Time zone is used when converting between String and DateTime types,\n-          when printing DateTime in text formats and parsing DateTime from text,\n-          it is used in date and time related functions, if specific time zone was not passed as an argument.\n-\n-         Time zone is specified as identifier from IANA time zone database, like UTC or Africa/Abidjan.\n-         If not specified, system time zone at server startup is used.\n-\n-         Please note, that server could display time zone alias instead of specified name.\n-         Example: W-SU is an alias for Europe/Moscow and Zulu is an alias for UTC.\n-    -->\n-    <!-- <timezone>Europe/Moscow</timezone> -->\n-\n-    <!-- You can specify umask here (see \"man umask\"). Server will apply it on startup.\n-         Number is always parsed as octal. Default umask is 027 (other users cannot read logs, data files, etc; group can only read).\n-    -->\n-    <!-- <umask>022</umask> -->\n-\n-    <!-- Perform mlockall after startup to lower first queries latency\n-          and to prevent clickhouse executable from being paged out under high IO load.\n-         Enabling this option is recommended but will lead to increased startup time for up to a few seconds.\n-    -->\n-    <mlock_executable>true</mlock_executable>\n-\n-    <!-- Reallocate memory for machine code (\"text\") using huge pages. Highly experimental. -->\n-    <remap_executable>false</remap_executable>\n-\n-    <![CDATA[\n-         Uncomment below in order to use JDBC table engine and function.\n-\n-         To install and run JDBC bridge in background:\n-         * [Debian/Ubuntu]\n-           export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge\n-           export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\\(.*\\)<.*|\\1|')\n-           wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge_$PKG_VER-1_all.deb\n-           apt install --no-install-recommends -f ./clickhouse-jdbc-bridge_$PKG_VER-1_all.deb\n-           clickhouse-jdbc-bridge &\n-\n-         * [CentOS/RHEL]\n-           export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge\n-           export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\\(.*\\)<.*|\\1|')\n-           wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm\n-           yum localinstall -y clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm\n-           clickhouse-jdbc-bridge &\n-\n-         Please refer to https://github.com/ClickHouse/clickhouse-jdbc-bridge#usage for more information.\n-    ]]>\n-    <!--\n-    <jdbc_bridge>\n-        <host>127.0.0.1</host>\n-        <port>9019</port>\n-    </jdbc_bridge>\n-    -->\n-\n-    <!-- Configuration of clusters that could be used in Distributed tables.\n-         https://clickhouse.com/docs/en/operations/table_engines/distributed/\n-      -->\n-    <remote_servers>\n-        <!-- Test only shard config for testing distributed storage -->\n-        <test_shard_localhost>\n-            <!-- Inter-server per-cluster secret for Distributed queries\n-                 default: no secret (no authentication will be performed)\n-\n-                 If set, then Distributed queries will be validated on shards, so at least:\n-                 - such cluster should exist on the shard,\n-                 - such cluster should have the same secret.\n-\n-                 And also (and which is more important), the initial_user will\n-                 be used as current user for the query.\n-\n-                 Right now the protocol is pretty simple and it only takes into account:\n-                 - cluster name\n-                 - query\n-\n-                 Also it will be nice if the following will be implemented:\n-                 - source hostname (see interserver_http_host), but then it will depends from DNS,\n-                   it can use IP address instead, but then the you need to get correct on the initiator node.\n-                 - target hostname / ip address (same notes as for source hostname)\n-                 - time-based security tokens\n-            -->\n-            <secret>REPLACE_ME</secret>\n-\n-            <shard>\n-                <!-- Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas). -->\n-                <!-- <internal_replication>false</internal_replication> -->\n-                <!-- Optional. Shard weight when writing data. Default: 1. -->\n-                <!-- <weight>1</weight> -->\n-                <replica>\n-                    <host>localhost</host>\n-                    <port>9000</port>\n-                    <!-- Optional. Priority of the replica for load_balancing. Default: 1 (less value has more priority). -->\n-                    <!-- <priority>1</priority> -->\n-                </replica>\n-            </shard>\n-        </test_shard_localhost>\n-    </remote_servers>\n-\n-    <!-- The list of hosts allowed to use in URL-related storage engines and table functions.\n-        If this section is not present in configuration, all hosts are allowed.\n-    -->\n-    <!--<remote_url_allow_hosts>-->\n-    <!-- Host should be specified exactly as in URL. The name is checked before DNS resolution.\n-        Example: \"yandex.ru\", \"yandex.ru.\" and \"www.yandex.ru\" are different hosts.\n-                If port is explicitly specified in URL, the host:port is checked as a whole.\n-                If host specified here without port, any port with this host allowed.\n-                \"yandex.ru\" -> \"yandex.ru:443\", \"yandex.ru:80\" etc. is allowed, but \"yandex.ru:80\" -> only \"yandex.ru:80\" is allowed.\n-        If the host is specified as IP address, it is checked as specified in URL. Example: \"[2a02:6b8:a::a]\".\n-        If there are redirects and support for redirects is enabled, every redirect (the Location field) is checked.\n-        Host should be specified using the host xml tag:\n-                <host>yandex.ru</host>\n-    -->\n-\n-    <!-- Regular expression can be specified. RE2 engine is used for regexps.\n-        Regexps are not aligned: don't forget to add ^ and $. Also don't forget to escape dot (.) metacharacter\n-        (forgetting to do so is a common source of error).\n-    -->\n-    <!--</remote_url_allow_hosts>-->\n-\n-    <!-- If element has 'incl' attribute, then for it's value will be used corresponding substitution from another file.\n-         By default, path to file with substitutions is /etc/metrika.xml. It could be changed in config in 'include_from' element.\n-         Values for substitutions are specified in /clickhouse/name_of_substitution elements in that file.\n-      -->\n-\n-    <!-- ZooKeeper is used to store metadata about replicas, when using Replicated tables.\n-         Optional. If you don't use replicated tables, you could omit that.\n-\n-         See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/\n-      -->\n-\n-    <!--\n-    <zookeeper>\n-        <node>\n-            <host>example1</host>\n-            <port>2181</port>\n-        </node>\n-        <node>\n-            <host>example2</host>\n-            <port>2181</port>\n-        </node>\n-        <node>\n-            <host>example3</host>\n-            <port>2181</port>\n-        </node>\n-    </zookeeper>\n-    -->\n-\n-    <!-- Substitutions for parameters of replicated tables.\n-          Optional. If you don't use replicated tables, you could omit that.\n-\n-         See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/#creating-replicated-tables\n-      -->\n-    <!--\n-    <macros>\n-        <shard>01</shard>\n-        <replica>example01-01-1</replica>\n-    </macros>\n-    -->\n-\n-\n-    <!-- Reloading interval for embedded dictionaries, in seconds. Default: 3600. -->\n-    <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>\n-\n-\n-    <!-- Maximum session timeout, in seconds. Default: 3600. -->\n-    <max_session_timeout>3600</max_session_timeout>\n-\n-    <!-- Default session timeout, in seconds. Default: 60. -->\n-    <default_session_timeout>60</default_session_timeout>\n-\n-    <!-- Sending data to Graphite for monitoring. Several sections can be defined. -->\n-    <!--\n-        interval - send every X second\n-        root_path - prefix for keys\n-        hostname_in_path - append hostname to root_path (default = true)\n-        metrics - send data from table system.metrics\n-        events - send data from table system.events\n-        asynchronous_metrics - send data from table system.asynchronous_metrics\n-    -->\n-    <!--\n-    <graphite>\n-        <host>localhost</host>\n-        <port>42000</port>\n-        <timeout>0.1</timeout>\n-        <interval>60</interval>\n-        <root_path>one_min</root_path>\n-        <hostname_in_path>true</hostname_in_path>\n-\n-        <metrics>true</metrics>\n-        <events>true</events>\n-        <events_cumulative>false</events_cumulative>\n-        <asynchronous_metrics>true</asynchronous_metrics>\n-    </graphite>\n-    <graphite>\n-        <host>localhost</host>\n-        <port>42000</port>\n-        <timeout>0.1</timeout>\n-        <interval>1</interval>\n-        <root_path>one_sec</root_path>\n-\n-        <metrics>true</metrics>\n-        <events>true</events>\n-        <events_cumulative>false</events_cumulative>\n-        <asynchronous_metrics>false</asynchronous_metrics>\n-    </graphite>\n-    -->\n-\n-    <!-- Query log. Used only for queries with setting log_queries = 1. -->\n-    <query_log>\n-        <!-- What table to insert data. If table is not exist, it will be created.\n-             When query log structure is changed after system update,\n-              then old table will be renamed and new table will be created automatically.\n-        -->\n-        <database>system</database>\n-        <table>query_log</table>\n-        <!--\n-            PARTITION BY expr: https://clickhouse.com/docs/en/table_engines/mergetree-family/custom_partitioning_key/\n-            Example:\n-                event_date\n-                toMonday(event_date)\n-                toYYYYMM(event_date)\n-                toStartOfHour(event_time)\n-        -->\n-        <partition_by>toYYYYMM(event_date)</partition_by>\n-        <!--\n-            Table TTL specification: https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#mergetree-table-ttl\n-            Example:\n-                event_date + INTERVAL 1 WEEK\n-                event_date + INTERVAL 7 DAY DELETE\n-                event_date + INTERVAL 2 WEEK TO DISK 'bbb'\n-\n-        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>\n-        -->\n-\n-        <!-- Instead of partition_by, you can provide full engine expression (starting with ENGINE = ) with parameters,\n-             Example: <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024</engine>\n-          -->\n-\n-        <!-- Interval of flushing data. -->\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-    </query_log>\n-\n-    <!-- Trace log. Stores stack traces collected by query profilers.\n-         See query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings. -->\n-    <trace_log>\n-        <database>system</database>\n-        <table>trace_log</table>\n-\n-        <partition_by>toYYYYMM(event_date)</partition_by>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-    </trace_log>\n-\n-    <!-- Query thread log. Has information about all threads participated in query execution.\n-         Used only for queries with setting log_query_threads = 1. -->\n-    <query_thread_log>\n-        <database>system</database>\n-        <table>query_thread_log</table>\n-        <partition_by>toYYYYMM(event_date)</partition_by>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-    </query_thread_log>\n-\n-    <!-- Query views log. Has information about all dependent views associated with a query.\n-         Used only for queries with setting log_query_views = 1. -->\n-    <query_views_log>\n-        <database>system</database>\n-        <table>query_views_log</table>\n-        <partition_by>toYYYYMM(event_date)</partition_by>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-    </query_views_log>\n-\n-    <!-- Uncomment if use part log.\n-         Part log contains information about all actions with parts in MergeTree tables (creation, deletion, merges, downloads).-->\n-    <part_log>\n-        <database>system</database>\n-        <table>part_log</table>\n-        <partition_by>toYYYYMM(event_date)</partition_by>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-    </part_log>\n-\n-    <!-- Uncomment to write text log into table.\n-         Text log contains all information from usual server log but stores it in structured and efficient way.\n-         The level of the messages that goes to the table can be limited (<level>), if not specified all messages will go to the table.\n-    <text_log>\n-        <database>system</database>\n-        <table>text_log</table>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-        <level></level>\n-    </text_log>\n-    -->\n-\n-    <!-- Metric log contains rows with current values of ProfileEvents, CurrentMetrics collected with \"collect_interval_milliseconds\" interval. -->\n-    <metric_log>\n-        <database>system</database>\n-        <table>metric_log</table>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-        <collect_interval_milliseconds>1000</collect_interval_milliseconds>\n-    </metric_log>\n-\n-    <!--\n-        Asynchronous metric log contains values of metrics from\n-        system.asynchronous_metrics.\n-    -->\n-    <asynchronous_metric_log>\n-        <database>system</database>\n-        <table>asynchronous_metric_log</table>\n-        <!--\n-            Asynchronous metrics are updated once a minute, so there is\n-            no need to flush more often.\n-        -->\n-        <flush_interval_milliseconds>7000</flush_interval_milliseconds>\n-    </asynchronous_metric_log>\n-\n-    <!--\n-        OpenTelemetry log contains OpenTelemetry trace spans.\n-    -->\n-    <opentelemetry_span_log>\n-        <!--\n-            The default table creation code is insufficient, this <engine> spec\n-            is a workaround. There is no 'event_time' for this log, but two times,\n-            start and finish. It is sorted by finish time, to avoid inserting\n-            data too far away in the past (probably we can sometimes insert a span\n-            that is seconds earlier than the last span in the table, due to a race\n-            between several spans inserted in parallel). This gives the spans a\n-            global order that we can use to e.g. retry insertion into some external\n-            system.\n-        -->\n-        <engine>\n-            engine MergeTree\n-            partition by toYYYYMM(finish_date)\n-            order by (finish_date, finish_time_us, trace_id)\n-        </engine>\n-        <database>system</database>\n-        <table>opentelemetry_span_log</table>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-    </opentelemetry_span_log>\n-\n-\n-    <!-- Crash log. Stores stack traces for fatal errors.\n-         This table is normally empty. -->\n-    <crash_log>\n-        <database>system</database>\n-        <table>crash_log</table>\n-\n-        <partition_by/>\n-        <flush_interval_milliseconds>1000</flush_interval_milliseconds>\n-    </crash_log>\n-\n-    <!-- Session log. Stores user log in (successful or not) and log out events. -->\n-    <session_log>\n-        <database>system</database>\n-        <table>session_log</table>\n-\n-        <partition_by>toYYYYMM(event_date)</partition_by>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-    </session_log>\n-\n-    <!-- Parameters for embedded dictionaries, used in Yandex.Metrica.\n-         See https://clickhouse.com/docs/en/dicts/internal_dicts/\n-    -->\n-\n-    <!-- Path to file with region hierarchy. -->\n-    <!-- <path_to_regions_hierarchy_file>/opt/geo/regions_hierarchy.txt</path_to_regions_hierarchy_file> -->\n-\n-    <!-- Path to directory with files containing names of regions -->\n-    <!-- <path_to_regions_names_files>/opt/geo/</path_to_regions_names_files> -->\n-\n-\n-    <!-- <top_level_domains_path>/var/lib/clickhouse/top_level_domains/</top_level_domains_path> -->\n-    <!-- Custom TLD lists.\n-         Format: <name>/path/to/file</name>\n-\n-         Changes will not be applied w/o server restart.\n-         Path to the list is under top_level_domains_path (see above).\n-    -->\n-    <top_level_domains_lists>\n-        <!--\n-        <public_suffix_list>/path/to/public_suffix_list.dat</public_suffix_list>\n-        -->\n-    </top_level_domains_lists>\n-\n-    <!-- Configuration of external dictionaries. See:\n-         https://clickhouse.com/docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts\n-    -->\n-    <dictionaries_config>*_dictionary.xml</dictionaries_config>\n-\n-    <!-- Configuration of user defined executable functions -->\n-    <user_defined_executable_functions_config>*_function.xml</user_defined_executable_functions_config>\n-\n-    <!-- Uncomment if you want data to be compressed 30-100% better.\n-         Don't do that if you just started using ClickHouse.\n-      -->\n-    <!--\n-    <compression>\n-        <!- - Set of variants. Checked in order. Last matching case wins. If nothing matches, lz4 will be used. - ->\n-        <case>\n-\n-            <!- - Conditions. All must be satisfied. Some conditions may be omitted. - ->\n-            <min_part_size>10000000000</min_part_size>        <!- - Min part size in bytes. - ->\n-            <min_part_size_ratio>0.01</min_part_size_ratio>   <!- - Min size of part relative to whole table size. - ->\n-\n-            <!- - What compression method to use. - ->\n-            <method>zstd</method>\n-        </case>\n-    </compression>\n-    -->\n-\n-    <!-- Configuration of encryption. The server executes a command to\n-         obtain an encryption key at startup if such a command is\n-         defined, or encryption codecs will be disabled otherwise. The\n-         command is executed through /bin/sh and is expected to write\n-         a Base64-encoded key to the stdout. -->\n-    <encryption_codecs>\n-        <!-- aes_128_gcm_siv -->\n-        <!-- Example of getting hex key from env -->\n-        <!-- the code should use this key and throw an exception if its length is not 16 bytes -->\n-        <!--key_hex from_env=\"...\"></key_hex -->\n-\n-        <!-- Example of multiple hex keys. They can be imported from env or be written down in config-->\n-        <!-- the code should use these keys and throw an exception if their length is not 16 bytes -->\n-        <!-- key_hex id=\"0\">...</key_hex -->\n-        <!-- key_hex id=\"1\" from_env=\"..\"></key_hex -->\n-        <!-- key_hex id=\"2\">...</key_hex -->\n-        <!-- current_key_id>2</current_key_id -->\n-\n-        <!-- Example of getting hex key from config -->\n-        <!-- the code should use this key and throw an exception if its length is not 16 bytes -->\n-        <!-- key>...</key -->\n-\n-        <!-- example of adding nonce -->\n-        <!-- nonce>...</nonce -->\n-\n-        <!-- /aes_128_gcm_siv -->\n-    </encryption_codecs>\n-\n-    <!-- Allow to execute distributed DDL queries (CREATE, DROP, ALTER, RENAME) on cluster.\n-         Works only if ZooKeeper is enabled. Comment it if such functionality isn't required. -->\n-    <distributed_ddl>\n-        <!-- Path in ZooKeeper to queue with DDL queries -->\n-        <path>/clickhouse/task_queue/ddl</path>\n-\n-        <!-- Settings from this profile will be used to execute DDL queries -->\n-        <!-- <profile>default</profile> -->\n-\n-        <!-- Controls how much ON CLUSTER queries can be run simultaneously. -->\n-        <!-- <pool_size>1</pool_size> -->\n-\n-        <!--\n-             Cleanup settings (active tasks will not be removed)\n-        -->\n-\n-        <!-- Controls task TTL (default 1 week) -->\n-        <!-- <task_max_lifetime>604800</task_max_lifetime> -->\n-\n-        <!-- Controls how often cleanup should be performed (in seconds) -->\n-        <!-- <cleanup_delay_period>60</cleanup_delay_period> -->\n-\n-        <!-- Controls how many tasks could be in the queue -->\n-        <!-- <max_tasks_in_queue>1000</max_tasks_in_queue> -->\n-    </distributed_ddl>\n-\n-    <!-- Settings to fine tune MergeTree tables. See documentation in source code, in MergeTreeSettings.h -->\n-    <!--\n-    <merge_tree>\n-        <max_suspicious_broken_parts>5</max_suspicious_broken_parts>\n-    </merge_tree>\n-    -->\n-\n-    <!-- Protection from accidental DROP.\n-         If size of a MergeTree table is greater than max_table_size_to_drop (in bytes) than table could not be dropped with any DROP query.\n-         If you want do delete one table and don't want to change clickhouse-server config, you could create special file <clickhouse-path>/flags/force_drop_table and make DROP once.\n-         By default max_table_size_to_drop is 50GB; max_table_size_to_drop=0 allows to DROP any tables.\n-         The same for max_partition_size_to_drop.\n-         Uncomment to disable protection.\n-    -->\n-    <!-- <max_table_size_to_drop>0</max_table_size_to_drop> -->\n-    <!-- <max_partition_size_to_drop>0</max_partition_size_to_drop> -->\n-\n-    <!-- Example of parameters for GraphiteMergeTree table engine -->\n-    <graphite_rollup_example>\n-        <pattern>\n-            <regexp>click_cost</regexp>\n-            <function>any</function>\n-            <retention>\n-                <age>0</age>\n-                <precision>3600</precision>\n-            </retention>\n-            <retention>\n-                <age>86400</age>\n-                <precision>60</precision>\n-            </retention>\n-        </pattern>\n-        <default>\n-            <function>max</function>\n-            <retention>\n-                <age>0</age>\n-                <precision>60</precision>\n-            </retention>\n-            <retention>\n-                <age>3600</age>\n-                <precision>300</precision>\n-            </retention>\n-            <retention>\n-                <age>86400</age>\n-                <precision>3600</precision>\n-            </retention>\n-        </default>\n-    </graphite_rollup_example>\n-\n-    <!-- Directory in <clickhouse-path> containing schema files for various input formats.\n-         The directory will be created if it doesn't exist.\n-      -->\n-    <format_schema_path>/var/lib/clickhouse/format_schemas/</format_schema_path>\n-\n-    <!-- Default query masking rules, matching lines would be replaced with something else in the logs\n-        (both text logs and system.query_log).\n-        name - name for the rule (optional)\n-        regexp - RE2 compatible regular expression (mandatory)\n-        replace - substitution string for sensitive data (optional, by default - six asterisks)\n-    -->\n-    <query_masking_rules>\n-        <rule>\n-            <name>hide encrypt/decrypt arguments</name>\n-            <regexp>((?:aes_)?(?:encrypt|decrypt)(?:_mysql)?)\\s*\\(\\s*(?:'(?:\\\\'|.)+'|.*?)\\s*\\)</regexp>\n-            <!-- or more secure, but also more invasive:\n-                (aes_\\w+)\\s*\\(.*\\)\n-            -->\n-            <replace>\\1(???)</replace>\n-        </rule>\n-    </query_masking_rules>\n-\n-    <!-- Uncomment to use custom http handlers.\n-        rules are checked from top to bottom, first match runs the handler\n-            url - to match request URL, you can use 'regex:' prefix to use regex match(optional)\n-            methods - to match request method, you can use commas to separate multiple method matches(optional)\n-            headers - to match request headers, match each child element(child element name is header name), you can use 'regex:' prefix to use regex match(optional)\n-        handler is request handler\n-            type - supported types: static, dynamic_query_handler, predefined_query_handler\n-            query - use with predefined_query_handler type, executes query when the handler is called\n-            query_param_name - use with dynamic_query_handler type, extracts and executes the value corresponding to the <query_param_name> value in HTTP request params\n-            status - use with static type, response status code\n-            content_type - use with static type, response content-type\n-            response_content - use with static type, Response content sent to client, when using the prefix 'file://' or 'config://', find the content from the file or configuration send to client.\n-\n-    <http_handlers>\n-        <rule>\n-            <url>/</url>\n-            <methods>POST,GET</methods>\n-            <headers><pragma>no-cache</pragma></headers>\n-            <handler>\n-                <type>dynamic_query_handler</type>\n-                <query_param_name>query</query_param_name>\n-            </handler>\n-        </rule>\n-\n-        <rule>\n-            <url>/predefined_query</url>\n-            <methods>POST,GET</methods>\n-            <handler>\n-                <type>predefined_query_handler</type>\n-                <query>SELECT * FROM system.settings</query>\n-            </handler>\n-        </rule>\n-\n-        <rule>\n-            <handler>\n-                <type>static</type>\n-                <status>200</status>\n-                <content_type>text/plain; charset=UTF-8</content_type>\n-                <response_content>config://http_server_default_response</response_content>\n-            </handler>\n-        </rule>\n-    </http_handlers>\n-    -->\n-\n-    <send_crash_reports>\n-        <!-- Changing <enabled> to true allows sending crash reports to -->\n-        <!-- the ClickHouse core developers team via Sentry https://sentry.io -->\n-        <!-- Doing so at least in pre-production environments is highly appreciated -->\n-        <enabled>false</enabled>\n-        <!-- Change <anonymize> to true if you don't feel comfortable attaching the server hostname to the crash report -->\n-        <anonymize>false</anonymize>\n-        <!-- Default endpoint should be changed to different Sentry DSN only if you have -->\n-        <!-- some in-house engineers or hired consultants who're going to debug ClickHouse issues for you -->\n-        <endpoint>https://6f33034cfe684dd7a3ab9875e57b1c8d@o388870.ingest.sentry.io/5226277</endpoint>\n-    </send_crash_reports>\n-\n-    <!-- Uncomment to disable ClickHouse internal DNS caching. -->\n-    <!-- <disable_internal_dns_cache>1</disable_internal_dns_cache> -->\n-\n-    <!-- You can also configure rocksdb like this: -->\n-    <!--\n-    <rocksdb>\n-        <options>\n-            <max_background_jobs>8</max_background_jobs>\n-        </options>\n-        <column_family_options>\n-            <num_levels>2</num_levels>\n-        </column_family_options>\n-        <tables>\n-            <table>\n-                <name>TABLE</name>\n-                <options>\n-                    <max_background_jobs>8</max_background_jobs>\n-                </options>\n-                <column_family_options>\n-                    <num_levels>2</num_levels>\n-                </column_family_options>\n-            </table>\n-        </tables>\n-    </rocksdb>\n-    -->\n-    <storage_configuration>\n-        <disks>\n-            <s3>\n-                <type>s3</type>\n-                <endpoint>https://storage.yandexcloud.net/my-bucket/root-path/</endpoint>\n-                <access_key_id>REPLACE_ME</access_key_id>\n-                <secret_access_key>REPLACE_ME</secret_access_key>\n-                <region></region>\n-                <header>Authorization: Bearer SOME-TOKEN</header>\n-                <server_side_encryption_customer_key_base64>your_base64_encoded_customer_key\n-                </server_side_encryption_customer_key_base64>\n-                <server_side_encryption_kms_key_id>REPLACE_ME</server_side_encryption_kms_key_id>\n-                <server_side_encryption_kms_encryption_context>REPLACE_ME</server_side_encryption_kms_encryption_context>\n-                <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled>\n-                <proxy>\n-                    <uri>http://proxy1</uri>\n-                    <uri>http://proxy2</uri>\n-                </proxy>\n-                <connect_timeout_ms>10000</connect_timeout_ms>\n-                <request_timeout_ms>5000</request_timeout_ms>\n-                <retry_attempts>10</retry_attempts>\n-                <single_read_retries>4</single_read_retries>\n-                <min_bytes_for_seek>1000</min_bytes_for_seek>\n-                <metadata_path>/var/lib/clickhouse/disks/s3/</metadata_path>\n-                <skip_access_check>false</skip_access_check>\n-            </s3>\n-        </disks>\n-    </storage_configuration>\n-</clickhouse>\ndiff --git a/programs/diagnostics/testdata/configs/xml/users.d/default-password.xml b/programs/diagnostics/testdata/configs/xml/users.d/default-password.xml\ndeleted file mode 100644\nindex 242a6a4b02e4..000000000000\n--- a/programs/diagnostics/testdata/configs/xml/users.d/default-password.xml\n+++ /dev/null\n@@ -1,8 +0,0 @@\n-<clickhouse>\n-    <users>\n-        <default>\n-            <password remove=\"1\"/>\n-            <password_sha256_hex>REPLACE_ME</password_sha256_hex>\n-        </default>\n-    </users>\n-</clickhouse>\n\\ No newline at end of file\ndiff --git a/programs/diagnostics/testdata/configs/xml/users.xml b/programs/diagnostics/testdata/configs/xml/users.xml\ndeleted file mode 100644\nindex cd5f17e922ec..000000000000\n--- a/programs/diagnostics/testdata/configs/xml/users.xml\n+++ /dev/null\n@@ -1,57 +0,0 @@\n-<clickhouse>\n-    <!-- See also the files in users.d directory where the settings can be overridden. -->\n-    <!-- Profiles of settings. -->\n-    <include_from>../include/xml/user-include.xml</include_from>\n-\n-    <profiles>\n-        <!-- Default settings. -->\n-        <default>\n-            <!-- Maximum memory usage for processing single query, in bytes. -->\n-            <max_memory_usage>10000000000</max_memory_usage>\n-\n-            <load_balancing>random</load_balancing>\n-            <log_query_threads>1</log_query_threads>\n-        </default>\n-        <!-- Profile that allows only read queries. -->\n-        <readonly>\n-            <readonly>1</readonly>\n-        </readonly>\n-    </profiles>\n-    <!-- Users and ACL. -->\n-    <users>\n-        <test_user>\n-            <include incl=\"test_user\"></include>\n-        </test_user>\n-        <!-- If user name was not specified, 'default' user is used. -->\n-        <default>\n-            <password>REPLACE_ME</password>\n-            <networks>\n-                <ip>::/0</ip>\n-            </networks>\n-            <!-- Settings profile for user. -->\n-            <profile>default</profile>\n-            <!-- Quota for user. -->\n-            <quota>default</quota>\n-            <!-- User can create other users and grant rights to them. -->\n-            <!-- <access_management>1</access_management> -->\n-        </default>\n-    </users>\n-    <!-- Quotas. -->\n-    <quotas>\n-        <!-- Name of quota. -->\n-        <default>\n-            <!-- Limits for time interval. You could specify many intervals with different limits. -->\n-            <interval>\n-                <!-- Length of interval. -->\n-                <duration>3600</duration>\n-\n-                <!-- No limits. Just calculate resource usage for time interval. -->\n-                <queries>0</queries>\n-                <errors>0</errors>\n-                <result_rows>0</result_rows>\n-                <read_rows>0</read_rows>\n-                <execution_time>0</execution_time>\n-            </interval>\n-        </default>\n-    </quotas>\n-</clickhouse>\ndiff --git a/programs/diagnostics/testdata/configs/yaml/config.yaml b/programs/diagnostics/testdata/configs/yaml/config.yaml\ndeleted file mode 100644\nindex e577a99e675f..000000000000\n--- a/programs/diagnostics/testdata/configs/yaml/config.yaml\n+++ /dev/null\n@@ -1,927 +0,0 @@\n-# This is an example of a configuration file \"config.xml\" rewritten in YAML\n-# You can read this documentation for detailed information about YAML configuration:\n-# https://clickhouse.com/docs/en/operations/configuration-files/\n-\n-# NOTE: User and query level settings are set up in \"users.yaml\" file.\n-# If you have accidentally specified user-level settings here, server won't start.\n-# You can either move the settings to the right place inside \"users.xml\" file\n-# or add skip_check_for_incorrect_settings: 1 here.\n-include_from: \"../include/yaml/server-include.yaml\"\n-logger:\n-  # Possible levels [1]:\n-  # - none (turns off logging)\n-  # - fatal\n-  # - critical\n-  # - error\n-  # - warning\n-  # - notice\n-  # - information\n-  # - debug\n-  # - trace\n-  # [1]: https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/Logger.h#L105-L114\n-  level: trace\n-  log: /var/log/clickhouse-server/clickhouse-server.log\n-  errorlog: /var/log/clickhouse-server/clickhouse-server.err.log\n-  # Rotation policy\n-  # See https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/FileChannel.h#L54-L85\n-  size: 1000M\n-  count: 10\n-  # console: 1\n-  # Default behavior is autodetection (log to console if not daemon mode and is tty)\n-\n-  # Per level overrides (legacy):\n-  # For example to suppress logging of the ConfigReloader you can use:\n-  # NOTE: levels.logger is reserved, see below.\n-  # levels:\n-  #     ConfigReloader: none\n-\n-  # Per level overrides:\n-  # For example to suppress logging of the RBAC for default user you can use:\n-  # (But please note that the logger name maybe changed from version to version, even after minor upgrade)\n-  # levels:\n-  #     - logger:\n-  #         name: 'ContextAccess (default)'\n-  #         level: none\n-  #     - logger:\n-  #         name: 'DatabaseOrdinary (test)'\n-  #         level: none\n-\n-# It is the name that will be shown in the clickhouse-client.\n-# By default, anything with \"production\" will be highlighted in red in query prompt.\n-# display_name: production\n-\n-# Port for HTTP API. See also 'https_port' for secure connections.\n-# This interface is also used by ODBC and JDBC drivers (DataGrip, Dbeaver, ...)\n-# and by most of web interfaces (embedded UI, Grafana, Redash, ...).\n-http_port: 8123\n-\n-# Port for interaction by native protocol with:\n-# - clickhouse-client and other native ClickHouse tools (clickhouse-benchmark);\n-# - clickhouse-server with other clickhouse-servers for distributed query processing;\n-# - ClickHouse drivers and applications supporting native protocol\n-# (this protocol is also informally called as \"the TCP protocol\");\n-# See also 'tcp_port_secure' for secure connections.\n-tcp_port: 9000\n-\n-# Compatibility with MySQL protocol.\n-# ClickHouse will pretend to be MySQL for applications connecting to this port.\n-mysql_port: 9004\n-\n-# Compatibility with PostgreSQL protocol.\n-# ClickHouse will pretend to be PostgreSQL for applications connecting to this port.\n-postgresql_port: 9005\n-\n-# HTTP API with TLS (HTTPS).\n-# You have to configure certificate to enable this interface.\n-# See the openSSL section below.\n-# https_port: 8443\n-\n-# Native interface with TLS.\n-# You have to configure certificate to enable this interface.\n-# See the openSSL section below.\n-# tcp_port_secure: 9440\n-\n-# Native interface wrapped with PROXYv1 protocol\n-# PROXYv1 header sent for every connection.\n-# ClickHouse will extract information about proxy-forwarded client address from the header.\n-# tcp_with_proxy_port: 9011\n-\n-# Port for communication between replicas. Used for data exchange.\n-# It provides low-level data access between servers.\n-# This port should not be accessible from untrusted networks.\n-# See also 'interserver_http_credentials'.\n-# Data transferred over connections to this port should not go through untrusted networks.\n-# See also 'interserver_https_port'.\n-interserver_http_port: 9009\n-\n-# Port for communication between replicas with TLS.\n-# You have to configure certificate to enable this interface.\n-# See the openSSL section below.\n-# See also 'interserver_http_credentials'.\n-# interserver_https_port: 9010\n-\n-# Hostname that is used by other replicas to request this server.\n-# If not specified, than it is determined analogous to 'hostname -f' command.\n-# This setting could be used to switch replication to another network interface\n-# (the server may be connected to multiple networks via multiple addresses)\n-# interserver_http_host: example.yandex.ru\n-\n-# You can specify credentials for authenthication between replicas.\n-# This is required when interserver_https_port is accessible from untrusted networks,\n-# and also recommended to avoid SSRF attacks from possibly compromised services in your network.\n-# interserver_http_credentials:\n-#     user: interserver\n-#     password: ''\n-\n-# Listen specified address.\n-# Use :: (wildcard IPv6 address), if you want to accept connections both with IPv4 and IPv6 from everywhere.\n-# Notes:\n-# If you open connections from wildcard address, make sure that at least one of the following measures applied:\n-# - server is protected by firewall and not accessible from untrusted networks;\n-# - all users are restricted to subset of network addresses (see users.xml);\n-# - all users have strong passwords, only secure (TLS) interfaces are accessible, or connections are only made via TLS interfaces.\n-# - users without password have readonly access.\n-# See also: https://www.shodan.io/search?query=clickhouse\n-# listen_host: '::'\n-\n-# Same for hosts without support for IPv6:\n-# listen_host: 0.0.0.0\n-\n-# Default values - try listen localhost on IPv4 and IPv6.\n-# listen_host: '::1'\n-# listen_host: 127.0.0.1\n-\n-# Don't exit if IPv6 or IPv4 networks are unavailable while trying to listen.\n-# listen_try: 0\n-\n-# Allow multiple servers to listen on the same address:port. This is not recommended.\n-# listen_reuse_port: 0\n-\n-# listen_backlog: 64\n-max_connections: 4096\n-\n-# For 'Connection: keep-alive' in HTTP 1.1\n-keep_alive_timeout: 3\n-\n-# gRPC protocol (see src/Server/grpc_protos/clickhouse_grpc.proto for the API)\n-# grpc_port: 9100\n-grpc:\n-  enable_ssl: false\n-\n-  # The following two files are used only if enable_ssl=1\n-  ssl_cert_file: /path/to/ssl_cert_file\n-  ssl_key_file: /path/to/ssl_key_file\n-\n-  # Whether server will request client for a certificate\n-  ssl_require_client_auth: false\n-\n-  # The following file is used only if ssl_require_client_auth=1\n-  ssl_ca_cert_file: /path/to/ssl_ca_cert_file\n-\n-  # Default compression algorithm (applied if client doesn't specify another algorithm).\n-  # Supported algorithms: none, deflate, gzip, stream_gzip\n-  compression: deflate\n-\n-  # Default compression level (applied if client doesn't specify another level).\n-  # Supported levels: none, low, medium, high\n-  compression_level: medium\n-\n-  # Send/receive message size limits in bytes. -1 means unlimited\n-  max_send_message_size: -1\n-  max_receive_message_size: -1\n-\n-  # Enable if you want very detailed logs\n-  verbose_logs: false\n-\n-# Used with https_port and tcp_port_secure. Full ssl options list: https://github.com/ClickHouse-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/SSLManager.h#L71\n-openSSL:\n-  server:\n-    # Used for https server AND secure tcp port\n-    # openssl req -subj \"/CN=localhost\" -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/clickhouse-server/server.key -out /etc/clickhouse-server/server.crt\n-    certificateFile: /etc/clickhouse-server/server.crt\n-    privateKeyFile: /etc/clickhouse-server/server.key\n-\n-    # dhparams are optional. You can delete the dhParamsFile: element.\n-    # To generate dhparams, use the following command:\n-    # openssl dhparam -out /etc/clickhouse-server/dhparam.pem 4096\n-    # Only file format with BEGIN DH PARAMETERS is supported.\n-    dhParamsFile: /etc/clickhouse-server/dhparam.pem\n-    verificationMode: none\n-    loadDefaultCAFile: true\n-    cacheSessions: true\n-    disableProtocols: 'sslv2,sslv3'\n-    preferServerCiphers: true\n-  client:\n-    # Used for connecting to https dictionary source and secured Zookeeper communication\n-    loadDefaultCAFile: true\n-    cacheSessions: true\n-    disableProtocols: 'sslv2,sslv3'\n-    preferServerCiphers: true\n-\n-    # Use for self-signed: verificationMode: none\n-    invalidCertificateHandler:\n-      # Use for self-signed: name: AcceptCertificateHandler\n-      name: RejectCertificateHandler\n-\n-# Default root page on http[s] server. For example load UI from https://tabix.io/ when opening http://localhost:8123\n-# http_server_default_response: |-\n-#     <html ng-app=\"SMI2\"><head><base href=\"http://ui.tabix.io/\"></head><body><div ui-view=\"\" class=\"content-ui\"></div><script src=\"http://loader.tabix.io/master.js\"></script></body></html>\n-\n-# Maximum number of concurrent queries.\n-max_concurrent_queries: 100\n-\n-# Maximum memory usage (resident set size) for server process.\n-# Zero value or unset means default. Default is \"max_server_memory_usage_to_ram_ratio\" of available physical RAM.\n-# If the value is larger than \"max_server_memory_usage_to_ram_ratio\" of available physical RAM, it will be cut down.\n-\n-# The constraint is checked on query execution time.\n-# If a query tries to allocate memory and the current memory usage plus allocation is greater\n-# than specified threshold, exception will be thrown.\n-\n-# It is not practical to set this constraint to small values like just a few gigabytes,\n-# because memory allocator will keep this amount of memory in caches and the server will deny service of queries.\n-max_server_memory_usage: 0\n-\n-# Maximum number of threads in the Global thread pool.\n-# This will default to a maximum of 10000 threads if not specified.\n-# This setting will be useful in scenarios where there are a large number\n-# of distributed queries that are running concurrently but are idling most\n-# of the time, in which case a higher number of threads might be required.\n-max_thread_pool_size: 10000\n-\n-# On memory constrained environments you may have to set this to value larger than 1.\n-max_server_memory_usage_to_ram_ratio: 0.9\n-\n-# Simple server-wide memory profiler. Collect a stack trace at every peak allocation step (in bytes).\n-# Data will be stored in system.trace_log table with query_id = empty string.\n-# Zero means disabled.\n-total_memory_profiler_step: 4194304\n-\n-# Collect random allocations and deallocations and write them into system.trace_log with 'MemorySample' trace_type.\n-# The probability is for every alloc/free regardless to the size of the allocation.\n-# Note that sampling happens only when the amount of untracked memory exceeds the untracked memory limit,\n-# which is 4 MiB by default but can be lowered if 'total_memory_profiler_step' is lowered.\n-# You may want to set 'total_memory_profiler_step' to 1 for extra fine grained sampling.\n-total_memory_tracker_sample_probability: 0\n-\n-# Set limit on number of open files (default: maximum). This setting makes sense on Mac OS X because getrlimit() fails to retrieve\n-# correct maximum value.\n-# max_open_files: 262144\n-\n-# Size of cache of uncompressed blocks of data, used in tables of MergeTree family.\n-# In bytes. Cache is single for server. Memory is allocated only on demand.\n-# Cache is used when 'use_uncompressed_cache' user setting turned on (off by default).\n-# Uncompressed cache is advantageous only for very short queries and in rare cases.\n-\n-# Note: uncompressed cache can be pointless for lz4, because memory bandwidth\n-# is slower than multi-core decompression on some server configurations.\n-# Enabling it can sometimes paradoxically make queries slower.\n-uncompressed_cache_size: 8589934592\n-\n-# Approximate size of mark cache, used in tables of MergeTree family.\n-# In bytes. Cache is single for server. Memory is allocated only on demand.\n-# You should not lower this value.\n-mark_cache_size: 5368709120\n-\n-# If you enable the `min_bytes_to_use_mmap_io` setting,\n-# the data in MergeTree tables can be read with mmap to avoid copying from kernel to userspace.\n-# It makes sense only for large files and helps only if data reside in page cache.\n-# To avoid frequent open/mmap/munmap/close calls (which are very expensive due to consequent page faults)\n-# and to reuse mappings from several threads and queries,\n-# the cache of mapped files is maintained. Its size is the number of mapped regions (usually equal to the number of mapped files).\n-# The amount of data in mapped files can be monitored\n-# in system.metrics, system.metric_log by the MMappedFiles, MMappedFileBytes metrics\n-# and in system.asynchronous_metrics, system.asynchronous_metrics_log by the MMapCacheCells metric,\n-# and also in system.events, system.processes, system.query_log, system.query_thread_log, system.query_views_log by the\n-# CreatedReadBufferMMap, CreatedReadBufferMMapFailed, MMappedFileCacheHits, MMappedFileCacheMisses events.\n-# Note that the amount of data in mapped files does not consume memory directly and is not accounted\n-# in query or server memory usage - because this memory can be discarded similar to OS page cache.\n-# The cache is dropped (the files are closed) automatically on removal of old parts in MergeTree,\n-# also it can be dropped manually by the SYSTEM DROP MMAP CACHE query.\n-mmap_cache_size: 1000\n-\n-# Cache size in bytes for compiled expressions.\n-compiled_expression_cache_size: 134217728\n-\n-# Cache size in elements for compiled expressions.\n-compiled_expression_cache_elements_size: 10000\n-\n-# Path to data directory, with trailing slash.\n-path: /var/lib/clickhouse/\n-\n-# Path to temporary data for processing hard queries.\n-tmp_path: /var/lib/clickhouse/tmp/\n-\n-# Policy from the <storage_configuration> for the temporary files.\n-# If not set <tmp_path> is used, otherwise <tmp_path> is ignored.\n-\n-# Notes:\n-# - move_factor              is ignored\n-# - keep_free_space_bytes    is ignored\n-# - max_data_part_size_bytes is ignored\n-# - you must have exactly one volume in that policy\n-# tmp_policy: tmp\n-\n-# Directory with user provided files that are accessible by 'file' table function.\n-user_files_path: /var/lib/clickhouse/user_files/\n-\n-# LDAP server definitions.\n-ldap_servers: ''\n-\n-# List LDAP servers with their connection parameters here to later 1) use them as authenticators for dedicated local users,\n-# who have 'ldap' authentication mechanism specified instead of 'password', or to 2) use them as remote user directories.\n-# Parameters:\n-# host - LDAP server hostname or IP, this parameter is mandatory and cannot be empty.\n-# port - LDAP server port, default is 636 if enable_tls is set to true, 389 otherwise.\n-# bind_dn - template used to construct the DN to bind to.\n-# The resulting DN will be constructed by replacing all '{user_name}' substrings of the template with the actual\n-# user name during each authentication attempt.\n-# user_dn_detection - section with LDAP search parameters for detecting the actual user DN of the bound user.\n-# This is mainly used in search filters for further role mapping when the server is Active Directory. The\n-# resulting user DN will be used when replacing '{user_dn}' substrings wherever they are allowed. By default,\n-# user DN is set equal to bind DN, but once search is performed, it will be updated with to the actual detected\n-# user DN value.\n-# base_dn - template used to construct the base DN for the LDAP search.\n-# The resulting DN will be constructed by replacing all '{user_name}' and '{bind_dn}' substrings\n-# of the template with the actual user name and bind DN during the LDAP search.\n-# scope - scope of the LDAP search.\n-# Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).\n-# search_filter - template used to construct the search filter for the LDAP search.\n-# The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', and '{base_dn}'\n-# substrings of the template with the actual user name, bind DN, and base DN during the LDAP search.\n-# Note, that the special characters must be escaped properly in XML.\n-# verification_cooldown - a period of time, in seconds, after a successful bind attempt, during which a user will be assumed\n-# to be successfully authenticated for all consecutive requests without contacting the LDAP server.\n-# Specify 0 (the default) to disable caching and force contacting the LDAP server for each authentication request.\n-# enable_tls - flag to trigger use of secure connection to the LDAP server.\n-# Specify 'no' for plain text (ldap://) protocol (not recommended).\n-# Specify 'yes' for LDAP over SSL/TLS (ldaps://) protocol (recommended, the default).\n-# Specify 'starttls' for legacy StartTLS protocol (plain text (ldap://) protocol, upgraded to TLS).\n-# tls_minimum_protocol_version - the minimum protocol version of SSL/TLS.\n-# Accepted values are: 'ssl2', 'ssl3', 'tls1.0', 'tls1.1', 'tls1.2' (the default).\n-# tls_require_cert - SSL/TLS peer certificate verification behavior.\n-# Accepted values are: 'never', 'allow', 'try', 'demand' (the default).\n-# tls_cert_file - path to certificate file.\n-# tls_key_file - path to certificate key file.\n-# tls_ca_cert_file - path to CA certificate file.\n-# tls_ca_cert_dir - path to the directory containing CA certificates.\n-# tls_cipher_suite - allowed cipher suite (in OpenSSL notation).\n-# Example:\n-# my_ldap_server:\n-#     host: localhost\n-#     port: 636\n-#     bind_dn: 'uid={user_name},ou=users,dc=example,dc=com'\n-#     verification_cooldown: 300\n-#     enable_tls: yes\n-#     tls_minimum_protocol_version: tls1.2\n-#     tls_require_cert: demand\n-#     tls_cert_file: /path/to/tls_cert_file\n-#     tls_key_file: /path/to/tls_key_file\n-#     tls_ca_cert_file: /path/to/tls_ca_cert_file\n-#     tls_ca_cert_dir: /path/to/tls_ca_cert_dir\n-#     tls_cipher_suite: ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:AES256-GCM-SHA384\n-\n-# Example (typical Active Directory with configured user DN detection for further role mapping):\n-# my_ad_server:\n-#     host: localhost\n-#     port: 389\n-#     bind_dn: 'EXAMPLE\\{user_name}'\n-#     user_dn_detection:\n-#         base_dn: CN=Users,DC=example,DC=com\n-#         search_filter: '(&amp;(objectClass=user)(sAMAccountName={user_name}))'\n-#     enable_tls: no\n-\n-# To enable Kerberos authentication support for HTTP requests (GSS-SPNEGO), for those users who are explicitly configured\n-# to authenticate via Kerberos, define a single 'kerberos' section here.\n-# Parameters:\n-# principal - canonical service principal name, that will be acquired and used when accepting security contexts.\n-# This parameter is optional, if omitted, the default principal will be used.\n-# This parameter cannot be specified together with 'realm' parameter.\n-# realm - a realm, that will be used to restrict authentication to only those requests whose initiator's realm matches it.\n-# This parameter is optional, if omitted, no additional filtering by realm will be applied.\n-# This parameter cannot be specified together with 'principal' parameter.\n-# Example:\n-# kerberos: ''\n-\n-# Example:\n-# kerberos:\n-#     principal: HTTP/clickhouse.example.com@EXAMPLE.COM\n-\n-# Example:\n-# kerberos:\n-#     realm: EXAMPLE.COM\n-\n-# Sources to read users, roles, access rights, profiles of settings, quotas.\n-user_directories:\n-  users_xml:\n-    # Path to configuration file with predefined users.\n-    path: users.yaml\n-  local_directory:\n-    # Path to folder where users created by SQL commands are stored.\n-    path: /var/lib/clickhouse/access/\n-\n-#   # To add an LDAP server as a remote user directory of users that are not defined locally, define a single 'ldap' section\n-#   # with the following parameters:\n-#   # server - one of LDAP server names defined in 'ldap_servers' config section above.\n-#   # This parameter is mandatory and cannot be empty.\n-#   # roles - section with a list of locally defined roles that will be assigned to each user retrieved from the LDAP server.\n-#   # If no roles are specified here or assigned during role mapping (below), user will not be able to perform any\n-#   # actions after authentication.\n-#   # role_mapping - section with LDAP search parameters and mapping rules.\n-#   # When a user authenticates, while still bound to LDAP, an LDAP search is performed using search_filter and the\n-#   # name of the logged in user. For each entry found during that search, the value of the specified attribute is\n-#   # extracted. For each attribute value that has the specified prefix, the prefix is removed, and the rest of the\n-#   # value becomes the name of a local role defined in ClickHouse, which is expected to be created beforehand by\n-#   # CREATE ROLE command.\n-#   # There can be multiple 'role_mapping' sections defined inside the same 'ldap' section. All of them will be\n-#   # applied.\n-#   # base_dn - template used to construct the base DN for the LDAP search.\n-#   # The resulting DN will be constructed by replacing all '{user_name}', '{bind_dn}', and '{user_dn}'\n-#   # substrings of the template with the actual user name, bind DN, and user DN during each LDAP search.\n-#   # scope - scope of the LDAP search.\n-#   # Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).\n-#   # search_filter - template used to construct the search filter for the LDAP search.\n-#   # The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', '{user_dn}', and\n-#   # '{base_dn}' substrings of the template with the actual user name, bind DN, user DN, and base DN during\n-#   # each LDAP search.\n-#   # Note, that the special characters must be escaped properly in XML.\n-#   # attribute - attribute name whose values will be returned by the LDAP search. 'cn', by default.\n-#   # prefix - prefix, that will be expected to be in front of each string in the original list of strings returned by\n-#   # the LDAP search. Prefix will be removed from the original strings and resulting strings will be treated\n-#   # as local role names. Empty, by default.\n-#   # Example:\n-#   # ldap:\n-#   #     server: my_ldap_server\n-#   #     roles:\n-#   #         my_local_role1: ''\n-#   #         my_local_role2: ''\n-#   #     role_mapping:\n-#   #         base_dn: 'ou=groups,dc=example,dc=com'\n-#   #         scope: subtree\n-#   #         search_filter: '(&amp;(objectClass=groupOfNames)(member={bind_dn}))'\n-#   #         attribute: cn\n-#   #         prefix: clickhouse_\n-#   # Example (typical Active Directory with role mapping that relies on the detected user DN):\n-#   # ldap:\n-#   #     server: my_ad_server\n-#   #     role_mapping:\n-#   #         base_dn: 'CN=Users,DC=example,DC=com'\n-#   #         attribute: CN\n-#   #         scope: subtree\n-#   #         search_filter: '(&amp;(objectClass=group)(member={user_dn}))'\n-#   #         prefix: clickhouse_\n-\n-# Default profile of settings.\n-default_profile: default\n-\n-# Comma-separated list of prefixes for user-defined settings.\n-# custom_settings_prefixes: ''\n-# System profile of settings. This settings are used by internal processes (Distributed DDL worker and so on).\n-# system_profile: default\n-\n-# Buffer profile of settings.\n-# This settings are used by Buffer storage to flush data to the underlying table.\n-# Default: used from system_profile directive.\n-# buffer_profile: default\n-\n-# Default database.\n-default_database: default\n-\n-# Server time zone could be set here.\n-\n-# Time zone is used when converting between String and DateTime types,\n-# when printing DateTime in text formats and parsing DateTime from text,\n-# it is used in date and time related functions, if specific time zone was not passed as an argument.\n-\n-# Time zone is specified as identifier from IANA time zone database, like UTC or Africa/Abidjan.\n-# If not specified, system time zone at server startup is used.\n-\n-# Please note, that server could display time zone alias instead of specified name.\n-# Example: W-SU is an alias for Europe/Moscow and Zulu is an alias for UTC.\n-# timezone: Europe/Moscow\n-\n-# You can specify umask here (see \"man umask\"). Server will apply it on startup.\n-# Number is always parsed as octal. Default umask is 027 (other users cannot read logs, data files, etc; group can only read).\n-# umask: 022\n-\n-# Perform mlockall after startup to lower first queries latency\n-# and to prevent clickhouse executable from being paged out under high IO load.\n-# Enabling this option is recommended but will lead to increased startup time for up to a few seconds.\n-mlock_executable: true\n-\n-# Reallocate memory for machine code (\"text\") using huge pages. Highly experimental.\n-remap_executable: false\n-\n-# Uncomment below in order to use JDBC table engine and function.\n-# To install and run JDBC bridge in background:\n-# * [Debian/Ubuntu]\n-# export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge\n-# export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\\(.*\\)<.*|\\1|')\n-# wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge_$PKG_VER-1_all.deb\n-# apt install --no-install-recommends -f ./clickhouse-jdbc-bridge_$PKG_VER-1_all.deb\n-# clickhouse-jdbc-bridge &\n-# * [CentOS/RHEL]\n-# export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge\n-# export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\\(.*\\)<.*|\\1|')\n-# wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm\n-# yum localinstall -y clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm\n-# clickhouse-jdbc-bridge &\n-# Please refer to https://github.com/ClickHouse/clickhouse-jdbc-bridge#usage for more information.\n-\n-# jdbc_bridge:\n-#     host: 127.0.0.1\n-#     port: 9019\n-\n-# Configuration of clusters that could be used in Distributed tables.\n-# https://clickhouse.com/docs/en/operations/table_engines/distributed/\n-remote_servers:\n-  # Test only shard config for testing distributed storage\n-  test_shard_localhost:\n-    # Inter-server per-cluster secret for Distributed queries\n-    # default: no secret (no authentication will be performed)\n-\n-    # If set, then Distributed queries will be validated on shards, so at least:\n-    # - such cluster should exist on the shard,\n-    # - such cluster should have the same secret.\n-\n-    # And also (and which is more important), the initial_user will\n-    # be used as current user for the query.\n-\n-    # Right now the protocol is pretty simple and it only takes into account:\n-    # - cluster name\n-    # - query\n-\n-    # Also it will be nice if the following will be implemented:\n-    # - source hostname (see interserver_http_host), but then it will depends from DNS,\n-    # it can use IP address instead, but then the you need to get correct on the initiator node.\n-    # - target hostname / ip address (same notes as for source hostname)\n-    # - time-based security tokens\n-    secret: 'REPLACE_ME'\n-    shard:\n-      # Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas).\n-      # internal_replication: false\n-      # Optional. Shard weight when writing data. Default: 1.\n-      # weight: 1\n-      replica:\n-        host: localhost\n-        port: 9000\n-        # Optional. Priority of the replica for load_balancing. Default: 1 (less value has more priority).\n-        # priority: 1\n-\n-# The list of hosts allowed to use in URL-related storage engines and table functions.\n-# If this section is not present in configuration, all hosts are allowed.\n-# remote_url_allow_hosts:\n-\n-# Host should be specified exactly as in URL. The name is checked before DNS resolution.\n-# Example: \"yandex.ru\", \"yandex.ru.\" and \"www.yandex.ru\" are different hosts.\n-# If port is explicitly specified in URL, the host:port is checked as a whole.\n-# If host specified here without port, any port with this host allowed.\n-# \"yandex.ru\" -> \"yandex.ru:443\", \"yandex.ru:80\" etc. is allowed, but \"yandex.ru:80\" -> only \"yandex.ru:80\" is allowed.\n-# If the host is specified as IP address, it is checked as specified in URL. Example: \"[2a02:6b8:a::a]\".\n-# If there are redirects and support for redirects is enabled, every redirect (the Location field) is checked.\n-\n-# Regular expression can be specified. RE2 engine is used for regexps.\n-# Regexps are not aligned: don't forget to add ^ and $. Also don't forget to escape dot (.) metacharacter\n-# (forgetting to do so is a common source of error).\n-\n-# If element has 'incl' attribute, then for it's value will be used corresponding substitution from another file.\n-# By default, path to file with substitutions is /etc/metrika.xml. It could be changed in config in 'include_from' element.\n-# Values for substitutions are specified in /clickhouse/name_of_substitution elements in that file.\n-\n-# ZooKeeper is used to store metadata about replicas, when using Replicated tables.\n-# Optional. If you don't use replicated tables, you could omit that.\n-# See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/\n-\n-# zookeeper:\n-#     - node:\n-#         host: example1\n-#         port: 2181\n-#     - node:\n-#         host: example2\n-#         port: 2181\n-#     - node:\n-#         host: example3\n-#         port: 2181\n-\n-# Substitutions for parameters of replicated tables.\n-# Optional. If you don't use replicated tables, you could omit that.\n-# See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/#creating-replicated-tables\n-# macros:\n-#     shard: 01\n-#     replica: example01-01-1\n-\n-# Reloading interval for embedded dictionaries, in seconds. Default: 3600.\n-builtin_dictionaries_reload_interval: 3600\n-\n-# Maximum session timeout, in seconds. Default: 3600.\n-max_session_timeout: 3600\n-\n-# Default session timeout, in seconds. Default: 60.\n-default_session_timeout: 60\n-\n-# Sending data to Graphite for monitoring. Several sections can be defined.\n-# interval - send every X second\n-# root_path - prefix for keys\n-# hostname_in_path - append hostname to root_path (default = true)\n-# metrics - send data from table system.metrics\n-# events - send data from table system.events\n-# asynchronous_metrics - send data from table system.asynchronous_metrics\n-\n-# graphite:\n-#     host: localhost\n-#     port: 42000\n-#     timeout: 0.1\n-#     interval: 60\n-#     root_path: one_min\n-#     hostname_in_path: true\n-\n-#     metrics: true\n-#     events: true\n-#     events_cumulative: false\n-#     asynchronous_metrics: true\n-\n-# graphite:\n-#     host: localhost\n-#     port: 42000\n-#     timeout: 0.1\n-#     interval: 1\n-#     root_path: one_sec\n-\n-#     metrics: true\n-#     events: true\n-#     events_cumulative: false\n-#     asynchronous_metrics: false\n-\n-# Serve endpoint for Prometheus monitoring.\n-# endpoint - mertics path (relative to root, statring with \"/\")\n-# port - port to setup server. If not defined or 0 than http_port used\n-# metrics - send data from table system.metrics\n-# events - send data from table system.events\n-# asynchronous_metrics - send data from table system.asynchronous_metrics\n-\n-# prometheus:\n-#     endpoint: /metrics\n-#     port: 9363\n-\n-#     metrics: true\n-#     events: true\n-#     asynchronous_metrics: true\n-\n-# Query log. Used only for queries with setting log_queries = 1.\n-query_log:\n-  # What table to insert data. If table is not exist, it will be created.\n-  # When query log structure is changed after system update,\n-  # then old table will be renamed and new table will be created automatically.\n-  database: system\n-  table: query_log\n-\n-  # PARTITION BY expr: https://clickhouse.com/docs/en/table_engines/mergetree-family/custom_partitioning_key/\n-  # Example:\n-  # event_date\n-  # toMonday(event_date)\n-  # toYYYYMM(event_date)\n-  # toStartOfHour(event_time)\n-  partition_by: toYYYYMM(event_date)\n-\n-  # Table TTL specification: https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#mergetree-table-ttl\n-  # Example:\n-  # event_date + INTERVAL 1 WEEK\n-  # event_date + INTERVAL 7 DAY DELETE\n-  # event_date + INTERVAL 2 WEEK TO DISK 'bbb'\n-\n-  # ttl: 'event_date + INTERVAL 30 DAY DELETE'\n-\n-  # Instead of partition_by, you can provide full engine expression (starting with ENGINE = ) with parameters,\n-  # Example: engine: 'ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024'\n-\n-  # Interval of flushing data.\n-  flush_interval_milliseconds: 7500\n-\n-# Trace log. Stores stack traces collected by query profilers.\n-# See query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings.\n-trace_log:\n-  database: system\n-  table: trace_log\n-  partition_by: toYYYYMM(event_date)\n-  flush_interval_milliseconds: 7500\n-\n-# Query thread log. Has information about all threads participated in query execution.\n-# Used only for queries with setting log_query_threads = 1.\n-query_thread_log:\n-  database: system\n-  table: query_thread_log\n-  partition_by: toYYYYMM(event_date)\n-  flush_interval_milliseconds: 7500\n-\n-# Query views log. Has information about all dependent views associated with a query.\n-# Used only for queries with setting log_query_views = 1.\n-query_views_log:\n-  database: system\n-  table: query_views_log\n-  partition_by: toYYYYMM(event_date)\n-  flush_interval_milliseconds: 7500\n-\n-# Uncomment if use part log.\n-# Part log contains information about all actions with parts in MergeTree tables (creation, deletion, merges, downloads).\n-part_log:\n-  database: system\n-  table: part_log\n-  partition_by: toYYYYMM(event_date)\n-  flush_interval_milliseconds: 7500\n-\n-# Uncomment to write text log into table.\n-# Text log contains all information from usual server log but stores it in structured and efficient way.\n-# The level of the messages that goes to the table can be limited (<level>), if not specified all messages will go to the table.\n-# text_log:\n-#     database: system\n-#     table: text_log\n-#     flush_interval_milliseconds: 7500\n-#     level: ''\n-\n-# Metric log contains rows with current values of ProfileEvents, CurrentMetrics collected with \"collect_interval_milliseconds\" interval.\n-metric_log:\n-  database: system\n-  table: metric_log\n-  flush_interval_milliseconds: 7500\n-  collect_interval_milliseconds: 1000\n-\n-# Asynchronous metric log contains values of metrics from\n-# system.asynchronous_metrics.\n-asynchronous_metric_log:\n-  database: system\n-  table: asynchronous_metric_log\n-\n-  # Asynchronous metrics are updated once a minute, so there is\n-  # no need to flush more often.\n-  flush_interval_milliseconds: 60000\n-\n-# OpenTelemetry log contains OpenTelemetry trace spans.\n-opentelemetry_span_log:\n-\n-  # The default table creation code is insufficient, this <engine> spec\n-  # is a workaround. There is no 'event_time' for this log, but two times,\n-  # start and finish. It is sorted by finish time, to avoid inserting\n-  # data too far away in the past (probably we can sometimes insert a span\n-  # that is seconds earlier than the last span in the table, due to a race\n-  # between several spans inserted in parallel). This gives the spans a\n-  # global order that we can use to e.g. retry insertion into some external\n-  # system.\n-  engine: |-\n-    engine MergeTree\n-         partition by toYYYYMM(finish_date)\n-         order by (finish_date, finish_time_us, trace_id)\n-  database: system\n-  table: opentelemetry_span_log\n-  flush_interval_milliseconds: 7500\n-\n-# Crash log. Stores stack traces for fatal errors.\n-# This table is normally empty.\n-crash_log:\n-  database: system\n-  table: crash_log\n-  partition_by: ''\n-  flush_interval_milliseconds: 1000\n-\n-# Parameters for embedded dictionaries, used in Yandex.Metrica.\n-# See https://clickhouse.com/docs/en/dicts/internal_dicts/\n-\n-# Path to file with region hierarchy.\n-# path_to_regions_hierarchy_file: /opt/geo/regions_hierarchy.txt\n-\n-# Path to directory with files containing names of regions\n-# path_to_regions_names_files: /opt/geo/\n-\n-\n-# top_level_domains_path: /var/lib/clickhouse/top_level_domains/\n-# Custom TLD lists.\n-# Format: name: /path/to/file\n-\n-# Changes will not be applied w/o server restart.\n-# Path to the list is under top_level_domains_path (see above).\n-top_level_domains_lists: ''\n-\n-# public_suffix_list: /path/to/public_suffix_list.dat\n-\n-# Configuration of external dictionaries. See:\n-# https://clickhouse.com/docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts\n-dictionaries_config: '*_dictionary.xml'\n-\n-# Uncomment if you want data to be compressed 30-100% better.\n-# Don't do that if you just started using ClickHouse.\n-\n-# compression:\n-#     # Set of variants. Checked in order. Last matching case wins. If nothing matches, lz4 will be used.\n-#     case:\n-#         Conditions. All must be satisfied. Some conditions may be omitted.\n-#         # min_part_size: 10000000000    # Min part size in bytes.\n-#         # min_part_size_ratio: 0.01     # Min size of part relative to whole table size.\n-#         # What compression method to use.\n-#         method: zstd\n-\n-# Allow to execute distributed DDL queries (CREATE, DROP, ALTER, RENAME) on cluster.\n-# Works only if ZooKeeper is enabled. Comment it if such functionality isn't required.\n-distributed_ddl:\n-  # Path in ZooKeeper to queue with DDL queries\n-  path: /clickhouse/task_queue/ddl\n-\n-  # Settings from this profile will be used to execute DDL queries\n-  # profile: default\n-\n-  # Controls how much ON CLUSTER queries can be run simultaneously.\n-  # pool_size: 1\n-\n-  # Cleanup settings (active tasks will not be removed)\n-\n-  # Controls task TTL (default 1 week)\n-  # task_max_lifetime: 604800\n-\n-  # Controls how often cleanup should be performed (in seconds)\n-  # cleanup_delay_period: 60\n-\n-  # Controls how many tasks could be in the queue\n-  # max_tasks_in_queue: 1000\n-\n-# Settings to fine tune MergeTree tables. See documentation in source code, in MergeTreeSettings.h\n-# merge_tree:\n-#     max_suspicious_broken_parts: 5\n-\n-# Protection from accidental DROP.\n-# If size of a MergeTree table is greater than max_table_size_to_drop (in bytes) than table could not be dropped with any DROP query.\n-# If you want do delete one table and don't want to change clickhouse-server config, you could create special file <clickhouse-path>/flags/force_drop_table and make DROP once.\n-# By default max_table_size_to_drop is 50GB; max_table_size_to_drop=0 allows to DROP any tables.\n-# The same for max_partition_size_to_drop.\n-# Uncomment to disable protection.\n-\n-# max_table_size_to_drop: 0\n-# max_partition_size_to_drop: 0\n-\n-# Example of parameters for GraphiteMergeTree table engine\n-graphite_rollup_example:\n-  pattern:\n-    regexp: click_cost\n-    function: any\n-    retention:\n-      - age: 0\n-        precision: 3600\n-      - age: 86400\n-        precision: 60\n-  default:\n-    function: max\n-    retention:\n-      - age: 0\n-        precision: 60\n-      - age: 3600\n-        precision: 300\n-      - age: 86400\n-        precision: 3600\n-\n-# Directory in <clickhouse-path> containing schema files for various input formats.\n-# The directory will be created if it doesn't exist.\n-format_schema_path: /var/lib/clickhouse/format_schemas/\n-\n-# Default query masking rules, matching lines would be replaced with something else in the logs\n-# (both text logs and system.query_log).\n-# name - name for the rule (optional)\n-# regexp - RE2 compatible regular expression (mandatory)\n-# replace - substitution string for sensitive data (optional, by default - six asterisks)\n-query_masking_rules:\n-  rule:\n-    name: hide encrypt/decrypt arguments\n-    regexp: '((?:aes_)?(?:encrypt|decrypt)(?:_mysql)?)\\s*\\(\\s*(?:''(?:\\\\''|.)+''|.*?)\\s*\\)'\n-    # or more secure, but also more invasive:\n-    # (aes_\\w+)\\s*\\(.*\\)\n-    replace: \\1(???)\n-\n-# Uncomment to use custom http handlers.\n-# rules are checked from top to bottom, first match runs the handler\n-# url - to match request URL, you can use 'regex:' prefix to use regex match(optional)\n-# methods - to match request method, you can use commas to separate multiple method matches(optional)\n-# headers - to match request headers, match each child element(child element name is header name), you can use 'regex:' prefix to use regex match(optional)\n-# handler is request handler\n-# type - supported types: static, dynamic_query_handler, predefined_query_handler\n-# query - use with predefined_query_handler type, executes query when the handler is called\n-# query_param_name - use with dynamic_query_handler type, extracts and executes the value corresponding to the <query_param_name> value in HTTP request params\n-# status - use with static type, response status code\n-# content_type - use with static type, response content-type\n-# response_content - use with static type, Response content sent to client, when using the prefix 'file://' or 'config://', find the content from the file or configuration send to client.\n-\n-# http_handlers:\n-#     - rule:\n-#         url: /\n-#         methods: POST,GET\n-#         headers:\n-#           pragma: no-cache\n-#         handler:\n-#           type: dynamic_query_handler\n-#           query_param_name: query\n-#     - rule:\n-#         url: /predefined_query\n-#         methods: POST,GET\n-#         handler:\n-#           type: predefined_query_handler\n-#           query: 'SELECT * FROM system.settings'\n-#     - rule:\n-#         handler:\n-#           type: static\n-#           status: 200\n-#           content_type: 'text/plain; charset=UTF-8'\n-#           response_content: config://http_server_default_response\n-\n-send_crash_reports:\n-  # Changing <enabled> to true allows sending crash reports to\n-  # the ClickHouse core developers team via Sentry https://sentry.io\n-  # Doing so at least in pre-production environments is highly appreciated\n-  enabled: false\n-  # Change <anonymize> to true if you don't feel comfortable attaching the server hostname to the crash report\n-  anonymize: false\n-  # Default endpoint should be changed to different Sentry DSN only if you have\n-  # some in-house engineers or hired consultants who're going to debug ClickHouse issues for you\n-  endpoint: 'https://6f33034cfe684dd7a3ab9875e57b1c8d@o388870.ingest.sentry.io/5226277'\n-  # Uncomment to disable ClickHouse internal DNS caching.\n-  # disable_internal_dns_cache: 1\n-\n-storage_configuration:\n-  disks:\n-    s3:\n-      secret_access_key: REPLACE_ME\n-      access_key_id: 'REPLACE_ME'\ndiff --git a/programs/diagnostics/testdata/configs/yaml/users.d/default-password.yaml b/programs/diagnostics/testdata/configs/yaml/users.d/default-password.yaml\ndeleted file mode 100644\nindex c27bb7cb0714..000000000000\n--- a/programs/diagnostics/testdata/configs/yaml/users.d/default-password.yaml\n+++ /dev/null\n@@ -1,6 +0,0 @@\n-# Users and ACL.\n-users:\n-  # If user name was not specified, 'default' user is used.\n-  default:\n-\n-    password_sha256_hex: \"REPLACE_ME\"\ndiff --git a/programs/diagnostics/testdata/configs/yaml/users.yaml b/programs/diagnostics/testdata/configs/yaml/users.yaml\ndeleted file mode 100644\nindex 82f2d67f2a4c..000000000000\n--- a/programs/diagnostics/testdata/configs/yaml/users.yaml\n+++ /dev/null\n@@ -1,47 +0,0 @@\n-include_from: \"../include/yaml/user-include.yaml\"\n-# Profiles of settings.\n-profiles:\n-  # Default settings.\n-  default:\n-    # Maximum memory usage for processing single query, in bytes.\n-    max_memory_usage: 10000000000\n-    load_balancing: random\n-\n-    # Profile that allows only read queries.\n-  readonly:\n-    readonly: 1\n-\n-# Users and ACL.\n-users:\n-  # If user name was not specified, 'default' user is used.\n-  default:\n-\n-    password: 'REPLACE_ME'\n-\n-    networks:\n-      ip: '::/0'\n-\n-    # Settings profile for user.\n-    profile: default\n-\n-    # Quota for user.\n-    quota: default\n-\n-    # User can create other users and grant rights to them.\n-    # access_management: 1\n-\n-# Quotas.\n-quotas:\n-  # Name of quota.\n-  default:\n-    # Limits for time interval. You could specify many intervals with different limits.\n-    interval:\n-      # Length of interval.\n-      duration: 3600\n-\n-      # No limits. Just calculate resource usage for time interval.\n-      queries: 0\n-      errors: 0\n-      result_rows: 0\n-      read_rows: 0\n-      execution_time: 0\ndiff --git a/programs/diagnostics/testdata/configs/yandex_xml/config.xml b/programs/diagnostics/testdata/configs/yandex_xml/config.xml\ndeleted file mode 100644\nindex 40d1fa34b1a9..000000000000\n--- a/programs/diagnostics/testdata/configs/yandex_xml/config.xml\n+++ /dev/null\n@@ -1,1167 +0,0 @@\n-<!--\n-  NOTE: User and query level settings are set up in \"users.xml\" file.\n-  If you have accidentally specified user-level settings here, server won't start.\n-  You can either move the settings to the right place inside \"users.xml\" file\n-   or add <skip_check_for_incorrect_settings>1</skip_check_for_incorrect_settings> here.\n--->\n-<yandex>\n-    <include_from>../include/xml/server-include.xml</include_from>\n-    <logger>\n-        <!-- Possible levels [1]:\n-\n-          - none (turns off logging)\n-          - fatal\n-          - critical\n-          - error\n-          - warning\n-          - notice\n-          - information\n-          - debug\n-          - trace\n-          - test (not for production usage)\n-\n-            [1]: https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/Logger.h#L105-L114\n-        -->\n-        <level>trace</level>\n-        <log>/var/log/clickhouse-server/clickhouse-server.log</log>\n-        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>\n-        <!-- Rotation policy\n-             See https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/FileChannel.h#L54-L85\n-          -->\n-        <size>1000M</size>\n-        <count>10</count>\n-        <!-- <console>1</console> --> <!-- Default behavior is autodetection (log to console if not daemon mode and is tty) -->\n-\n-        <!-- Per level overrides (legacy):\n-\n-        For example to suppress logging of the ConfigReloader you can use:\n-        NOTE: levels.logger is reserved, see below.\n-        -->\n-        <!--\n-        <levels>\n-          <ConfigReloader>none</ConfigReloader>\n-        </levels>\n-        -->\n-\n-        <!-- Per level overrides:\n-\n-        For example to suppress logging of the RBAC for default user you can use:\n-        (But please note that the logger name maybe changed from version to version, even after minor upgrade)\n-        -->\n-        <!--\n-        <levels>\n-          <logger>\n-            <name>ContextAccess (default)</name>\n-            <level>none</level>\n-          </logger>\n-          <logger>\n-            <name>DatabaseOrdinary (test)</name>\n-            <level>none</level>\n-          </logger>\n-        </levels>\n-        -->\n-    </logger>\n-    <!-- Add headers to response in options request. OPTIONS method is used in CORS preflight requests. -->\n-    <!-- It is off by default. Next headers are obligate for CORS.-->\n-    <!-- http_options_response>\n-        <header>\n-            <name>Access-Control-Allow-Origin</name>\n-            <value>*</value>\n-        </header>\n-        <header>\n-            <name>Access-Control-Allow-Headers</name>\n-            <value>origin, x-requested-with</value>\n-        </header>\n-        <header>\n-            <name>Access-Control-Allow-Methods</name>\n-            <value>POST, GET, OPTIONS</value>\n-        </header>\n-        <header>\n-            <name>Access-Control-Max-Age</name>\n-            <value>86400</value>\n-        </header>\n-    </http_options_response -->\n-\n-    <!-- It is the name that will be shown in the clickhouse-client.\n-         By default, anything with \"production\" will be highlighted in red in query prompt.\n-    -->\n-    <!--display_name>production</display_name-->\n-\n-    <!-- Port for HTTP API. See also 'https_port' for secure connections.\n-         This interface is also used by ODBC and JDBC drivers (DataGrip, Dbeaver, ...)\n-         and by most of web interfaces (embedded UI, Grafana, Redash, ...).\n-      -->\n-    <http_port>8123</http_port>\n-\n-    <!-- Port for interaction by native protocol with:\n-         - clickhouse-client and other native ClickHouse tools (clickhouse-benchmark);\n-         - clickhouse-server with other clickhouse-servers for distributed query processing;\n-         - ClickHouse drivers and applications supporting native protocol\n-         (this protocol is also informally called as \"the TCP protocol\");\n-         See also 'tcp_port_secure' for secure connections.\n-    -->\n-    <tcp_port>9000</tcp_port>\n-\n-    <!-- Compatibility with MySQL protocol.\n-         ClickHouse will pretend to be MySQL for applications connecting to this port.\n-    -->\n-    <mysql_port>9004</mysql_port>\n-\n-    <!-- Compatibility with PostgreSQL protocol.\n-         ClickHouse will pretend to be PostgreSQL for applications connecting to this port.\n-    -->\n-    <postgresql_port>9005</postgresql_port>\n-\n-    <!-- HTTP API with TLS (HTTPS).\n-         You have to configure certificate to enable this interface.\n-         See the openSSL section below.\n-    -->\n-    <!-- <https_port>8443</https_port> -->\n-\n-    <!-- Native interface with TLS.\n-         You have to configure certificate to enable this interface.\n-         See the openSSL section below.\n-    -->\n-    <!-- <tcp_port_secure>9440</tcp_port_secure> -->\n-\n-    <!-- Native interface wrapped with PROXYv1 protocol\n-         PROXYv1 header sent for every connection.\n-         ClickHouse will extract information about proxy-forwarded client address from the header.\n-    -->\n-    <!-- <tcp_with_proxy_port>9011</tcp_with_proxy_port> -->\n-\n-    <!-- Port for communication between replicas. Used for data exchange.\n-         It provides low-level data access between servers.\n-         This port should not be accessible from untrusted networks.\n-         See also 'interserver_http_credentials'.\n-         Data transferred over connections to this port should not go through untrusted networks.\n-         See also 'interserver_https_port'.\n-      -->\n-    <interserver_http_port>9009</interserver_http_port>\n-\n-    <!-- Port for communication between replicas with TLS.\n-         You have to configure certificate to enable this interface.\n-         See the openSSL section below.\n-         See also 'interserver_http_credentials'.\n-      -->\n-    <!-- <interserver_https_port>9010</interserver_https_port> -->\n-\n-    <!-- Hostname that is used by other replicas to request this server.\n-         If not specified, than it is determined analogous to 'hostname -f' command.\n-         This setting could be used to switch replication to another network interface\n-         (the server may be connected to multiple networks via multiple addresses)\n-      -->\n-    <!--\n-    <interserver_http_host>example.yandex.ru</interserver_http_host>\n-    -->\n-\n-    <!-- You can specify credentials for authenthication between replicas.\n-         This is required when interserver_https_port is accessible from untrusted networks,\n-         and also recommended to avoid SSRF attacks from possibly compromised services in your network.\n-      -->\n-    <!--<interserver_http_credentials>\n-        <user>interserver</user>\n-        <password></password>\n-    </interserver_http_credentials>-->\n-\n-    <!-- Listen specified address.\n-         Use :: (wildcard IPv6 address), if you want to accept connections both with IPv4 and IPv6 from everywhere.\n-         Notes:\n-         If you open connections from wildcard address, make sure that at least one of the following measures applied:\n-         - server is protected by firewall and not accessible from untrusted networks;\n-         - all users are restricted to subset of network addresses (see users.xml);\n-         - all users have strong passwords, only secure (TLS) interfaces are accessible, or connections are only made via TLS interfaces.\n-         - users without password have readonly access.\n-         See also: https://www.shodan.io/search?query=clickhouse\n-      -->\n-    <!-- <listen_host>::</listen_host> -->\n-\n-    <!-- Same for hosts without support for IPv6: -->\n-    <!-- <listen_host>0.0.0.0</listen_host> -->\n-\n-    <!-- Default values - try listen localhost on IPv4 and IPv6. -->\n-    <!--\n-    <listen_host>::1</listen_host>\n-    <listen_host>127.0.0.1</listen_host>\n-    -->\n-\n-    <!-- Don't exit if IPv6 or IPv4 networks are unavailable while trying to listen. -->\n-    <!-- <listen_try>0</listen_try> -->\n-\n-    <!-- Allow multiple servers to listen on the same address:port. This is not recommended.\n-      -->\n-    <!-- <listen_reuse_port>0</listen_reuse_port> -->\n-\n-    <!-- <listen_backlog>4096</listen_backlog> -->\n-\n-    <max_connections>4096</max_connections>\n-\n-    <!-- For 'Connection: keep-alive' in HTTP 1.1 -->\n-    <keep_alive_timeout>3</keep_alive_timeout>\n-\n-    <!-- gRPC protocol (see src/Server/grpc_protos/clickhouse_grpc.proto for the API) -->\n-    <!-- <grpc_port>9100</grpc_port> -->\n-    <grpc>\n-        <enable_ssl>false</enable_ssl>\n-\n-        <!-- The following two files are used only if enable_ssl=1 -->\n-        <ssl_cert_file>/path/to/ssl_cert_file</ssl_cert_file>\n-        <ssl_key_file>/path/to/ssl_key_file</ssl_key_file>\n-\n-        <!-- Whether server will request client for a certificate -->\n-        <ssl_require_client_auth>false</ssl_require_client_auth>\n-\n-        <!-- The following file is used only if ssl_require_client_auth=1 -->\n-        <ssl_ca_cert_file>/path/to/ssl_ca_cert_file</ssl_ca_cert_file>\n-\n-        <!-- Default compression algorithm (applied if client doesn't specify another algorithm, see result_compression in QueryInfo).\n-             Supported algorithms: none, deflate, gzip, stream_gzip -->\n-        <compression>deflate</compression>\n-\n-        <!-- Default compression level (applied if client doesn't specify another level, see result_compression in QueryInfo).\n-             Supported levels: none, low, medium, high -->\n-        <compression_level>medium</compression_level>\n-\n-        <!-- Send/receive message size limits in bytes. -1 means unlimited -->\n-        <max_send_message_size>-1</max_send_message_size>\n-        <max_receive_message_size>-1</max_receive_message_size>\n-\n-        <!-- Enable if you want very detailed logs -->\n-        <verbose_logs>false</verbose_logs>\n-    </grpc>\n-\n-    <!-- Used with https_port and tcp_port_secure. Full ssl options list: https://github.com/ClickHouse-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/SSLManager.h#L71 -->\n-    <openSSL>\n-        <server> <!-- Used for https server AND secure tcp port -->\n-            <!-- openssl req -subj \"/CN=localhost\" -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/clickhouse-server/server.key -out /etc/clickhouse-server/server.crt -->\n-            <certificateFile>/etc/clickhouse-server/server.crt</certificateFile>\n-            <privateKeyFile>/etc/clickhouse-server/server.key</privateKeyFile>\n-            <!-- dhparams are optional. You can delete the <dhParamsFile> element.\n-                 To generate dhparams, use the following command:\n-                  openssl dhparam -out /etc/clickhouse-server/dhparam.pem 4096\n-                 Only file format with BEGIN DH PARAMETERS is supported.\n-              -->\n-            <dhParamsFile>/etc/clickhouse-server/dhparam.pem</dhParamsFile>\n-            <verificationMode>none</verificationMode>\n-            <loadDefaultCAFile>true</loadDefaultCAFile>\n-            <cacheSessions>true</cacheSessions>\n-            <disableProtocols>sslv2,sslv3</disableProtocols>\n-            <preferServerCiphers>true</preferServerCiphers>\n-        </server>\n-\n-        <client> <!-- Used for connecting to https dictionary source and secured Zookeeper communication -->\n-            <loadDefaultCAFile>true</loadDefaultCAFile>\n-            <cacheSessions>true</cacheSessions>\n-            <disableProtocols>sslv2,sslv3</disableProtocols>\n-            <preferServerCiphers>true</preferServerCiphers>\n-            <!-- Use for self-signed: <verificationMode>none</verificationMode> -->\n-            <invalidCertificateHandler>\n-                <!-- Use for self-signed: <name>AcceptCertificateHandler</name> -->\n-                <name>RejectCertificateHandler</name>\n-            </invalidCertificateHandler>\n-        </client>\n-    </openSSL>\n-\n-    <!-- Default root page on http[s] server. For example load UI from https://tabix.io/ when opening http://localhost:8123 -->\n-    <!--\n-    <http_server_default_response><![CDATA[<html ng-app=\"SMI2\"><head><base href=\"http://ui.tabix.io/\"></head><body><div ui-view=\"\" class=\"content-ui\"></div><script src=\"http://loader.tabix.io/master.js\"></script></body></html>]]></http_server_default_response>\n-    -->\n-\n-    <!-- Maximum number of concurrent queries. -->\n-    <max_concurrent_queries>100</max_concurrent_queries>\n-\n-    <!-- Maximum memory usage (resident set size) for server process.\n-         Zero value or unset means default. Default is \"max_server_memory_usage_to_ram_ratio\" of available physical RAM.\n-         If the value is larger than \"max_server_memory_usage_to_ram_ratio\" of available physical RAM, it will be cut down.\n-\n-         The constraint is checked on query execution time.\n-         If a query tries to allocate memory and the current memory usage plus allocation is greater\n-          than specified threshold, exception will be thrown.\n-\n-         It is not practical to set this constraint to small values like just a few gigabytes,\n-          because memory allocator will keep this amount of memory in caches and the server will deny service of queries.\n-      -->\n-    <max_server_memory_usage>0</max_server_memory_usage>\n-\n-    <!-- Maximum number of threads in the Global thread pool.\n-    This will default to a maximum of 10000 threads if not specified.\n-    This setting will be useful in scenarios where there are a large number\n-    of distributed queries that are running concurrently but are idling most\n-    of the time, in which case a higher number of threads might be required.\n-    -->\n-\n-    <max_thread_pool_size>10000</max_thread_pool_size>\n-\n-    <!-- On memory constrained environments you may have to set this to value larger than 1.\n-      -->\n-    <max_server_memory_usage_to_ram_ratio>0.9</max_server_memory_usage_to_ram_ratio>\n-\n-    <!-- Simple server-wide memory profiler. Collect a stack trace at every peak allocation step (in bytes).\n-         Data will be stored in system.trace_log table with query_id = empty string.\n-         Zero means disabled.\n-      -->\n-    <total_memory_profiler_step>4194304</total_memory_profiler_step>\n-\n-    <!-- Collect random allocations and deallocations and write them into system.trace_log with 'MemorySample' trace_type.\n-         The probability is for every alloc/free regardless to the size of the allocation.\n-         Note that sampling happens only when the amount of untracked memory exceeds the untracked memory limit,\n-          which is 4 MiB by default but can be lowered if 'total_memory_profiler_step' is lowered.\n-         You may want to set 'total_memory_profiler_step' to 1 for extra fine grained sampling.\n-      -->\n-    <total_memory_tracker_sample_probability>0</total_memory_tracker_sample_probability>\n-\n-    <!-- Set limit on number of open files (default: maximum). This setting makes sense on Mac OS X because getrlimit() fails to retrieve\n-         correct maximum value. -->\n-    <!-- <max_open_files>262144</max_open_files> -->\n-\n-    <!-- Size of cache of uncompressed blocks of data, used in tables of MergeTree family.\n-         In bytes. Cache is single for server. Memory is allocated only on demand.\n-         Cache is used when 'use_uncompressed_cache' user setting turned on (off by default).\n-         Uncompressed cache is advantageous only for very short queries and in rare cases.\n-\n-         Note: uncompressed cache can be pointless for lz4, because memory bandwidth\n-         is slower than multi-core decompression on some server configurations.\n-         Enabling it can sometimes paradoxically make queries slower.\n-      -->\n-    <uncompressed_cache_size>8589934592</uncompressed_cache_size>\n-\n-    <!-- Approximate size of mark cache, used in tables of MergeTree family.\n-         In bytes. Cache is single for server. Memory is allocated only on demand.\n-         You should not lower this value.\n-      -->\n-    <mark_cache_size>5368709120</mark_cache_size>\n-\n-\n-    <!-- If you enable the `min_bytes_to_use_mmap_io` setting,\n-         the data in MergeTree tables can be read with mmap to avoid copying from kernel to userspace.\n-         It makes sense only for large files and helps only if data reside in page cache.\n-         To avoid frequent open/mmap/munmap/close calls (which are very expensive due to consequent page faults)\n-         and to reuse mappings from several threads and queries,\n-         the cache of mapped files is maintained. Its size is the number of mapped regions (usually equal to the number of mapped files).\n-         The amount of data in mapped files can be monitored\n-         in system.metrics, system.metric_log by the MMappedFiles, MMappedFileBytes metrics\n-         and in system.asynchronous_metrics, system.asynchronous_metrics_log by the MMapCacheCells metric,\n-         and also in system.events, system.processes, system.query_log, system.query_thread_log, system.query_views_log by the\n-         CreatedReadBufferMMap, CreatedReadBufferMMapFailed, MMappedFileCacheHits, MMappedFileCacheMisses events.\n-         Note that the amount of data in mapped files does not consume memory directly and is not accounted\n-         in query or server memory usage - because this memory can be discarded similar to OS page cache.\n-         The cache is dropped (the files are closed) automatically on removal of old parts in MergeTree,\n-         also it can be dropped manually by the SYSTEM DROP MMAP CACHE query.\n-      -->\n-    <mmap_cache_size>1000</mmap_cache_size>\n-\n-    <!-- Cache size in bytes for compiled expressions.-->\n-    <compiled_expression_cache_size>134217728</compiled_expression_cache_size>\n-\n-    <!-- Cache size in elements for compiled expressions.-->\n-    <compiled_expression_cache_elements_size>10000</compiled_expression_cache_elements_size>\n-\n-    <!-- Path to data directory, with trailing slash. -->\n-    <path>/var/lib/clickhouse/</path>\n-\n-    <!-- Path to temporary data for processing hard queries. -->\n-    <tmp_path>/var/lib/clickhouse/tmp/</tmp_path>\n-\n-    <!-- Policy from the <storage_configuration> for the temporary files.\n-         If not set <tmp_path> is used, otherwise <tmp_path> is ignored.\n-\n-         Notes:\n-         - move_factor              is ignored\n-         - keep_free_space_bytes    is ignored\n-         - max_data_part_size_bytes is ignored\n-         - you must have exactly one volume in that policy\n-    -->\n-    <!-- <tmp_policy>tmp</tmp_policy> -->\n-\n-    <!-- Directory with user provided files that are accessible by 'file' table function. -->\n-    <user_files_path>/var/lib/clickhouse/user_files/</user_files_path>\n-\n-    <!-- LDAP server definitions. -->\n-    <ldap_servers>\n-        <!-- List LDAP servers with their connection parameters here to later 1) use them as authenticators for dedicated local users,\n-              who have 'ldap' authentication mechanism specified instead of 'password', or to 2) use them as remote user directories.\n-             Parameters:\n-                host - LDAP server hostname or IP, this parameter is mandatory and cannot be empty.\n-                port - LDAP server port, default is 636 if enable_tls is set to true, 389 otherwise.\n-                bind_dn - template used to construct the DN to bind to.\n-                        The resulting DN will be constructed by replacing all '{user_name}' substrings of the template with the actual\n-                         user name during each authentication attempt.\n-                user_dn_detection - section with LDAP search parameters for detecting the actual user DN of the bound user.\n-                        This is mainly used in search filters for further role mapping when the server is Active Directory. The\n-                         resulting user DN will be used when replacing '{user_dn}' substrings wherever they are allowed. By default,\n-                         user DN is set equal to bind DN, but once search is performed, it will be updated with to the actual detected\n-                         user DN value.\n-                    base_dn - template used to construct the base DN for the LDAP search.\n-                            The resulting DN will be constructed by replacing all '{user_name}' and '{bind_dn}' substrings\n-                             of the template with the actual user name and bind DN during the LDAP search.\n-                    scope - scope of the LDAP search.\n-                            Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).\n-                    search_filter - template used to construct the search filter for the LDAP search.\n-                            The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', and '{base_dn}'\n-                             substrings of the template with the actual user name, bind DN, and base DN during the LDAP search.\n-                            Note, that the special characters must be escaped properly in XML.\n-                verification_cooldown - a period of time, in seconds, after a successful bind attempt, during which a user will be assumed\n-                         to be successfully authenticated for all consecutive requests without contacting the LDAP server.\n-                        Specify 0 (the default) to disable caching and force contacting the LDAP server for each authentication request.\n-                enable_tls - flag to trigger use of secure connection to the LDAP server.\n-                        Specify 'no' for plain text (ldap://) protocol (not recommended).\n-                        Specify 'yes' for LDAP over SSL/TLS (ldaps://) protocol (recommended, the default).\n-                        Specify 'starttls' for legacy StartTLS protocol (plain text (ldap://) protocol, upgraded to TLS).\n-                tls_minimum_protocol_version - the minimum protocol version of SSL/TLS.\n-                        Accepted values are: 'ssl2', 'ssl3', 'tls1.0', 'tls1.1', 'tls1.2' (the default).\n-                tls_require_cert - SSL/TLS peer certificate verification behavior.\n-                        Accepted values are: 'never', 'allow', 'try', 'demand' (the default).\n-                tls_cert_file - path to certificate file.\n-                tls_key_file - path to certificate key file.\n-                tls_ca_cert_file - path to CA certificate file.\n-                tls_ca_cert_dir - path to the directory containing CA certificates.\n-                tls_cipher_suite - allowed cipher suite (in OpenSSL notation).\n-             Example:\n-                <my_ldap_server>\n-                    <host>localhost</host>\n-                    <port>636</port>\n-                    <bind_dn>uid={user_name},ou=users,dc=example,dc=com</bind_dn>\n-                    <verification_cooldown>300</verification_cooldown>\n-                    <enable_tls>yes</enable_tls>\n-                    <tls_minimum_protocol_version>tls1.2</tls_minimum_protocol_version>\n-                    <tls_require_cert>demand</tls_require_cert>\n-                    <tls_cert_file>/path/to/tls_cert_file</tls_cert_file>\n-                    <tls_key_file>/path/to/tls_key_file</tls_key_file>\n-                    <tls_ca_cert_file>/path/to/tls_ca_cert_file</tls_ca_cert_file>\n-                    <tls_ca_cert_dir>/path/to/tls_ca_cert_dir</tls_ca_cert_dir>\n-                    <tls_cipher_suite>ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:AES256-GCM-SHA384</tls_cipher_suite>\n-                </my_ldap_server>\n-             Example (typical Active Directory with configured user DN detection for further role mapping):\n-                <my_ad_server>\n-                    <host>localhost</host>\n-                    <port>389</port>\n-                    <bind_dn>EXAMPLE\\{user_name}</bind_dn>\n-                    <user_dn_detection>\n-                        <base_dn>CN=Users,DC=example,DC=com</base_dn>\n-                        <search_filter>(&amp;(objectClass=user)(sAMAccountName={user_name}))</search_filter>\n-                    </user_dn_detection>\n-                    <enable_tls>no</enable_tls>\n-                </my_ad_server>\n-        -->\n-    </ldap_servers>\n-\n-    <!-- To enable Kerberos authentication support for HTTP requests (GSS-SPNEGO), for those users who are explicitly configured\n-          to authenticate via Kerberos, define a single 'kerberos' section here.\n-         Parameters:\n-            principal - canonical service principal name, that will be acquired and used when accepting security contexts.\n-                    This parameter is optional, if omitted, the default principal will be used.\n-                    This parameter cannot be specified together with 'realm' parameter.\n-            realm - a realm, that will be used to restrict authentication to only those requests whose initiator's realm matches it.\n-                    This parameter is optional, if omitted, no additional filtering by realm will be applied.\n-                    This parameter cannot be specified together with 'principal' parameter.\n-         Example:\n-            <kerberos />\n-         Example:\n-            <kerberos>\n-                <principal>HTTP/clickhouse.example.com@EXAMPLE.COM</principal>\n-            </kerberos>\n-         Example:\n-            <kerberos>\n-                <realm>EXAMPLE.COM</realm>\n-            </kerberos>\n-    -->\n-\n-    <!-- Sources to read users, roles, access rights, profiles of settings, quotas. -->\n-    <user_directories>\n-        <users_xml>\n-            <!-- Path to configuration file with predefined users. -->\n-            <path>users.xml</path>\n-        </users_xml>\n-        <local_directory>\n-            <!-- Path to folder where users created by SQL commands are stored. -->\n-            <path>/var/lib/clickhouse/access/</path>\n-        </local_directory>\n-\n-        <!-- To add an LDAP server as a remote user directory of users that are not defined locally, define a single 'ldap' section\n-              with the following parameters:\n-                server - one of LDAP server names defined in 'ldap_servers' config section above.\n-                        This parameter is mandatory and cannot be empty.\n-                roles - section with a list of locally defined roles that will be assigned to each user retrieved from the LDAP server.\n-                        If no roles are specified here or assigned during role mapping (below), user will not be able to perform any\n-                         actions after authentication.\n-                role_mapping - section with LDAP search parameters and mapping rules.\n-                        When a user authenticates, while still bound to LDAP, an LDAP search is performed using search_filter and the\n-                         name of the logged in user. For each entry found during that search, the value of the specified attribute is\n-                         extracted. For each attribute value that has the specified prefix, the prefix is removed, and the rest of the\n-                         value becomes the name of a local role defined in ClickHouse, which is expected to be created beforehand by\n-                         CREATE ROLE command.\n-                        There can be multiple 'role_mapping' sections defined inside the same 'ldap' section. All of them will be\n-                         applied.\n-                    base_dn - template used to construct the base DN for the LDAP search.\n-                            The resulting DN will be constructed by replacing all '{user_name}', '{bind_dn}', and '{user_dn}'\n-                             substrings of the template with the actual user name, bind DN, and user DN during each LDAP search.\n-                    scope - scope of the LDAP search.\n-                            Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).\n-                    search_filter - template used to construct the search filter for the LDAP search.\n-                            The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', '{user_dn}', and\n-                             '{base_dn}' substrings of the template with the actual user name, bind DN, user DN, and base DN during\n-                             each LDAP search.\n-                            Note, that the special characters must be escaped properly in XML.\n-                    attribute - attribute name whose values will be returned by the LDAP search. 'cn', by default.\n-                    prefix - prefix, that will be expected to be in front of each string in the original list of strings returned by\n-                             the LDAP search. Prefix will be removed from the original strings and resulting strings will be treated\n-                             as local role names. Empty, by default.\n-             Example:\n-                <ldap>\n-                    <server>my_ldap_server</server>\n-                    <roles>\n-                        <my_local_role1 />\n-                        <my_local_role2 />\n-                    </roles>\n-                    <role_mapping>\n-                        <base_dn>ou=groups,dc=example,dc=com</base_dn>\n-                        <scope>subtree</scope>\n-                        <search_filter>(&amp;(objectClass=groupOfNames)(member={bind_dn}))</search_filter>\n-                        <attribute>cn</attribute>\n-                        <prefix>clickhouse_</prefix>\n-                    </role_mapping>\n-                </ldap>\n-             Example (typical Active Directory with role mapping that relies on the detected user DN):\n-                <ldap>\n-                    <server>my_ad_server</server>\n-                    <role_mapping>\n-                        <base_dn>CN=Users,DC=example,DC=com</base_dn>\n-                        <attribute>CN</attribute>\n-                        <scope>subtree</scope>\n-                        <search_filter>(&amp;(objectClass=group)(member={user_dn}))</search_filter>\n-                        <prefix>clickhouse_</prefix>\n-                    </role_mapping>\n-                </ldap>\n-        -->\n-    </user_directories>\n-\n-    <!-- Default profile of settings. -->\n-    <default_profile>default</default_profile>\n-\n-    <!-- Comma-separated list of prefixes for user-defined settings. -->\n-    <custom_settings_prefixes></custom_settings_prefixes>\n-\n-    <!-- System profile of settings. This settings are used by internal processes (Distributed DDL worker and so on). -->\n-    <!-- <system_profile>default</system_profile> -->\n-\n-    <!-- Buffer profile of settings.\n-         This settings are used by Buffer storage to flush data to the underlying table.\n-         Default: used from system_profile directive.\n-    -->\n-    <!-- <buffer_profile>default</buffer_profile> -->\n-\n-    <!-- Default database. -->\n-    <default_database>default</default_database>\n-\n-    <!-- Server time zone could be set here.\n-\n-         Time zone is used when converting between String and DateTime types,\n-          when printing DateTime in text formats and parsing DateTime from text,\n-          it is used in date and time related functions, if specific time zone was not passed as an argument.\n-\n-         Time zone is specified as identifier from IANA time zone database, like UTC or Africa/Abidjan.\n-         If not specified, system time zone at server startup is used.\n-\n-         Please note, that server could display time zone alias instead of specified name.\n-         Example: W-SU is an alias for Europe/Moscow and Zulu is an alias for UTC.\n-    -->\n-    <!-- <timezone>Europe/Moscow</timezone> -->\n-\n-    <!-- You can specify umask here (see \"man umask\"). Server will apply it on startup.\n-         Number is always parsed as octal. Default umask is 027 (other users cannot read logs, data files, etc; group can only read).\n-    -->\n-    <!-- <umask>022</umask> -->\n-\n-    <!-- Perform mlockall after startup to lower first queries latency\n-          and to prevent clickhouse executable from being paged out under high IO load.\n-         Enabling this option is recommended but will lead to increased startup time for up to a few seconds.\n-    -->\n-    <mlock_executable>true</mlock_executable>\n-\n-    <!-- Reallocate memory for machine code (\"text\") using huge pages. Highly experimental. -->\n-    <remap_executable>false</remap_executable>\n-\n-    <![CDATA[\n-         Uncomment below in order to use JDBC table engine and function.\n-\n-         To install and run JDBC bridge in background:\n-         * [Debian/Ubuntu]\n-           export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge\n-           export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\\(.*\\)<.*|\\1|')\n-           wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge_$PKG_VER-1_all.deb\n-           apt install --no-install-recommends -f ./clickhouse-jdbc-bridge_$PKG_VER-1_all.deb\n-           clickhouse-jdbc-bridge &\n-\n-         * [CentOS/RHEL]\n-           export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge\n-           export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\\(.*\\)<.*|\\1|')\n-           wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm\n-           yum localinstall -y clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm\n-           clickhouse-jdbc-bridge &\n-\n-         Please refer to https://github.com/ClickHouse/clickhouse-jdbc-bridge#usage for more information.\n-    ]]>\n-    <!--\n-    <jdbc_bridge>\n-        <host>127.0.0.1</host>\n-        <port>9019</port>\n-    </jdbc_bridge>\n-    -->\n-\n-    <!-- Configuration of clusters that could be used in Distributed tables.\n-         https://clickhouse.com/docs/en/operations/table_engines/distributed/\n-      -->\n-    <remote_servers>\n-        <!-- Test only shard config for testing distributed storage -->\n-        <test_shard_localhost>\n-            <!-- Inter-server per-cluster secret for Distributed queries\n-                 default: no secret (no authentication will be performed)\n-\n-                 If set, then Distributed queries will be validated on shards, so at least:\n-                 - such cluster should exist on the shard,\n-                 - such cluster should have the same secret.\n-\n-                 And also (and which is more important), the initial_user will\n-                 be used as current user for the query.\n-\n-                 Right now the protocol is pretty simple and it only takes into account:\n-                 - cluster name\n-                 - query\n-\n-                 Also it will be nice if the following will be implemented:\n-                 - source hostname (see interserver_http_host), but then it will depends from DNS,\n-                   it can use IP address instead, but then the you need to get correct on the initiator node.\n-                 - target hostname / ip address (same notes as for source hostname)\n-                 - time-based security tokens\n-            -->\n-            <!-- <secret></secret> -->\n-\n-            <shard>\n-                <!-- Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas). -->\n-                <!-- <internal_replication>false</internal_replication> -->\n-                <!-- Optional. Shard weight when writing data. Default: 1. -->\n-                <!-- <weight>1</weight> -->\n-                <replica>\n-                    <host>localhost</host>\n-                    <port>9000</port>\n-                    <!-- Optional. Priority of the replica for load_balancing. Default: 1 (less value has more priority). -->\n-                    <!-- <priority>1</priority> -->\n-                </replica>\n-            </shard>\n-        </test_shard_localhost>\n-    </remote_servers>\n-\n-    <!-- The list of hosts allowed to use in URL-related storage engines and table functions.\n-        If this section is not present in configuration, all hosts are allowed.\n-    -->\n-    <!--<remote_url_allow_hosts>-->\n-    <!-- Host should be specified exactly as in URL. The name is checked before DNS resolution.\n-        Example: \"yandex.ru\", \"yandex.ru.\" and \"www.yandex.ru\" are different hosts.\n-                If port is explicitly specified in URL, the host:port is checked as a whole.\n-                If host specified here without port, any port with this host allowed.\n-                \"yandex.ru\" -> \"yandex.ru:443\", \"yandex.ru:80\" etc. is allowed, but \"yandex.ru:80\" -> only \"yandex.ru:80\" is allowed.\n-        If the host is specified as IP address, it is checked as specified in URL. Example: \"[2a02:6b8:a::a]\".\n-        If there are redirects and support for redirects is enabled, every redirect (the Location field) is checked.\n-        Host should be specified using the host xml tag:\n-                <host>yandex.ru</host>\n-    -->\n-\n-    <!-- Regular expression can be specified. RE2 engine is used for regexps.\n-        Regexps are not aligned: don't forget to add ^ and $. Also don't forget to escape dot (.) metacharacter\n-        (forgetting to do so is a common source of error).\n-    -->\n-    <!--</remote_url_allow_hosts>-->\n-\n-    <!-- If element has 'incl' attribute, then for it's value will be used corresponding substitution from another file.\n-         By default, path to file with substitutions is /etc/metrika.xml. It could be changed in config in 'include_from' element.\n-         Values for substitutions are specified in /clickhouse/name_of_substitution elements in that file.\n-      -->\n-\n-    <!-- ZooKeeper is used to store metadata about replicas, when using Replicated tables.\n-         Optional. If you don't use replicated tables, you could omit that.\n-\n-         See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/\n-      -->\n-\n-    <!--\n-    <zookeeper>\n-        <node>\n-            <host>example1</host>\n-            <port>2181</port>\n-        </node>\n-        <node>\n-            <host>example2</host>\n-            <port>2181</port>\n-        </node>\n-        <node>\n-            <host>example3</host>\n-            <port>2181</port>\n-        </node>\n-    </zookeeper>\n-    -->\n-\n-    <!-- Substitutions for parameters of replicated tables.\n-          Optional. If you don't use replicated tables, you could omit that.\n-\n-         See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/#creating-replicated-tables\n-      -->\n-    <!--\n-    <macros>\n-        <shard>01</shard>\n-        <replica>example01-01-1</replica>\n-    </macros>\n-    -->\n-\n-\n-    <!-- Reloading interval for embedded dictionaries, in seconds. Default: 3600. -->\n-    <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>\n-\n-\n-    <!-- Maximum session timeout, in seconds. Default: 3600. -->\n-    <max_session_timeout>3600</max_session_timeout>\n-\n-    <!-- Default session timeout, in seconds. Default: 60. -->\n-    <default_session_timeout>60</default_session_timeout>\n-\n-    <!-- Sending data to Graphite for monitoring. Several sections can be defined. -->\n-    <!--\n-        interval - send every X second\n-        root_path - prefix for keys\n-        hostname_in_path - append hostname to root_path (default = true)\n-        metrics - send data from table system.metrics\n-        events - send data from table system.events\n-        asynchronous_metrics - send data from table system.asynchronous_metrics\n-    -->\n-    <!--\n-    <graphite>\n-        <host>localhost</host>\n-        <port>42000</port>\n-        <timeout>0.1</timeout>\n-        <interval>60</interval>\n-        <root_path>one_min</root_path>\n-        <hostname_in_path>true</hostname_in_path>\n-\n-        <metrics>true</metrics>\n-        <events>true</events>\n-        <events_cumulative>false</events_cumulative>\n-        <asynchronous_metrics>true</asynchronous_metrics>\n-    </graphite>\n-    <graphite>\n-        <host>localhost</host>\n-        <port>42000</port>\n-        <timeout>0.1</timeout>\n-        <interval>1</interval>\n-        <root_path>one_sec</root_path>\n-\n-        <metrics>true</metrics>\n-        <events>true</events>\n-        <events_cumulative>false</events_cumulative>\n-        <asynchronous_metrics>false</asynchronous_metrics>\n-    </graphite>\n-    -->\n-\n-    <!-- Query log. Used only for queries with setting log_queries = 1. -->\n-    <query_log>\n-        <!-- What table to insert data. If table is not exist, it will be created.\n-             When query log structure is changed after system update,\n-              then old table will be renamed and new table will be created automatically.\n-        -->\n-        <database>system</database>\n-        <table>query_log</table>\n-        <!--\n-            PARTITION BY expr: https://clickhouse.com/docs/en/table_engines/mergetree-family/custom_partitioning_key/\n-            Example:\n-                event_date\n-                toMonday(event_date)\n-                toYYYYMM(event_date)\n-                toStartOfHour(event_time)\n-        -->\n-        <partition_by>toYYYYMM(event_date)</partition_by>\n-        <!--\n-            Table TTL specification: https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#mergetree-table-ttl\n-            Example:\n-                event_date + INTERVAL 1 WEEK\n-                event_date + INTERVAL 7 DAY DELETE\n-                event_date + INTERVAL 2 WEEK TO DISK 'bbb'\n-\n-        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>\n-        -->\n-\n-        <!-- Instead of partition_by, you can provide full engine expression (starting with ENGINE = ) with parameters,\n-             Example: <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024</engine>\n-          -->\n-\n-        <!-- Interval of flushing data. -->\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-    </query_log>\n-\n-    <!-- Trace log. Stores stack traces collected by query profilers.\n-         See query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings. -->\n-    <trace_log>\n-        <database>system</database>\n-        <table>trace_log</table>\n-\n-        <partition_by>toYYYYMM(event_date)</partition_by>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-    </trace_log>\n-\n-    <!-- Query thread log. Has information about all threads participated in query execution.\n-         Used only for queries with setting log_query_threads = 1. -->\n-    <query_thread_log>\n-        <database>system</database>\n-        <table>query_thread_log</table>\n-        <partition_by>toYYYYMM(event_date)</partition_by>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-    </query_thread_log>\n-\n-    <!-- Query views log. Has information about all dependent views associated with a query.\n-         Used only for queries with setting log_query_views = 1. -->\n-    <query_views_log>\n-        <database>system</database>\n-        <table>query_views_log</table>\n-        <partition_by>toYYYYMM(event_date)</partition_by>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-    </query_views_log>\n-\n-    <!-- Uncomment if use part log.\n-         Part log contains information about all actions with parts in MergeTree tables (creation, deletion, merges, downloads).-->\n-    <part_log>\n-        <database>system</database>\n-        <table>part_log</table>\n-        <partition_by>toYYYYMM(event_date)</partition_by>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-    </part_log>\n-\n-    <!-- Uncomment to write text log into table.\n-         Text log contains all information from usual server log but stores it in structured and efficient way.\n-         The level of the messages that goes to the table can be limited (<level>), if not specified all messages will go to the table.\n-    <text_log>\n-        <database>system</database>\n-        <table>text_log</table>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-        <level></level>\n-    </text_log>\n-    -->\n-\n-    <!-- Metric log contains rows with current values of ProfileEvents, CurrentMetrics collected with \"collect_interval_milliseconds\" interval. -->\n-    <metric_log>\n-        <database>system</database>\n-        <table>metric_log</table>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-        <collect_interval_milliseconds>1000</collect_interval_milliseconds>\n-    </metric_log>\n-\n-    <!--\n-        Asynchronous metric log contains values of metrics from\n-        system.asynchronous_metrics.\n-    -->\n-    <asynchronous_metric_log>\n-        <database>system</database>\n-        <table>asynchronous_metric_log</table>\n-        <!--\n-            Asynchronous metrics are updated once a minute, so there is\n-            no need to flush more often.\n-        -->\n-        <flush_interval_milliseconds>7000</flush_interval_milliseconds>\n-    </asynchronous_metric_log>\n-\n-    <!--\n-        OpenTelemetry log contains OpenTelemetry trace spans.\n-    -->\n-    <opentelemetry_span_log>\n-        <!--\n-            The default table creation code is insufficient, this <engine> spec\n-            is a workaround. There is no 'event_time' for this log, but two times,\n-            start and finish. It is sorted by finish time, to avoid inserting\n-            data too far away in the past (probably we can sometimes insert a span\n-            that is seconds earlier than the last span in the table, due to a race\n-            between several spans inserted in parallel). This gives the spans a\n-            global order that we can use to e.g. retry insertion into some external\n-            system.\n-        -->\n-        <engine>\n-            engine MergeTree\n-            partition by toYYYYMM(finish_date)\n-            order by (finish_date, finish_time_us, trace_id)\n-        </engine>\n-        <database>system</database>\n-        <table>opentelemetry_span_log</table>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-    </opentelemetry_span_log>\n-\n-\n-    <!-- Crash log. Stores stack traces for fatal errors.\n-         This table is normally empty. -->\n-    <crash_log>\n-        <database>system</database>\n-        <table>crash_log</table>\n-\n-        <partition_by />\n-        <flush_interval_milliseconds>1000</flush_interval_milliseconds>\n-    </crash_log>\n-\n-    <!-- Session log. Stores user log in (successful or not) and log out events. -->\n-    <session_log>\n-        <database>system</database>\n-        <table>session_log</table>\n-\n-        <partition_by>toYYYYMM(event_date)</partition_by>\n-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n-    </session_log>\n-\n-    <!-- Parameters for embedded dictionaries, used in Yandex.Metrica.\n-         See https://clickhouse.com/docs/en/dicts/internal_dicts/\n-    -->\n-\n-    <!-- Path to file with region hierarchy. -->\n-    <!-- <path_to_regions_hierarchy_file>/opt/geo/regions_hierarchy.txt</path_to_regions_hierarchy_file> -->\n-\n-    <!-- Path to directory with files containing names of regions -->\n-    <!-- <path_to_regions_names_files>/opt/geo/</path_to_regions_names_files> -->\n-\n-\n-    <!-- <top_level_domains_path>/var/lib/clickhouse/top_level_domains/</top_level_domains_path> -->\n-    <!-- Custom TLD lists.\n-         Format: <name>/path/to/file</name>\n-\n-         Changes will not be applied w/o server restart.\n-         Path to the list is under top_level_domains_path (see above).\n-    -->\n-    <top_level_domains_lists>\n-        <!--\n-        <public_suffix_list>/path/to/public_suffix_list.dat</public_suffix_list>\n-        -->\n-    </top_level_domains_lists>\n-\n-    <!-- Configuration of external dictionaries. See:\n-         https://clickhouse.com/docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts\n-    -->\n-    <dictionaries_config>*_dictionary.xml</dictionaries_config>\n-\n-    <!-- Configuration of user defined executable functions -->\n-    <user_defined_executable_functions_config>*_function.xml</user_defined_executable_functions_config>\n-\n-    <!-- Uncomment if you want data to be compressed 30-100% better.\n-         Don't do that if you just started using ClickHouse.\n-      -->\n-    <!--\n-    <compression>\n-        <!- - Set of variants. Checked in order. Last matching case wins. If nothing matches, lz4 will be used. - ->\n-        <case>\n-\n-            <!- - Conditions. All must be satisfied. Some conditions may be omitted. - ->\n-            <min_part_size>10000000000</min_part_size>        <!- - Min part size in bytes. - ->\n-            <min_part_size_ratio>0.01</min_part_size_ratio>   <!- - Min size of part relative to whole table size. - ->\n-\n-            <!- - What compression method to use. - ->\n-            <method>zstd</method>\n-        </case>\n-    </compression>\n-    -->\n-\n-    <!-- Configuration of encryption. The server executes a command to\n-         obtain an encryption key at startup if such a command is\n-         defined, or encryption codecs will be disabled otherwise. The\n-         command is executed through /bin/sh and is expected to write\n-         a Base64-encoded key to the stdout. -->\n-    <encryption_codecs>\n-        <!-- aes_128_gcm_siv -->\n-        <!-- Example of getting hex key from env -->\n-        <!-- the code should use this key and throw an exception if its length is not 16 bytes -->\n-        <!--key_hex from_env=\"...\"></key_hex -->\n-\n-        <!-- Example of multiple hex keys. They can be imported from env or be written down in config-->\n-        <!-- the code should use these keys and throw an exception if their length is not 16 bytes -->\n-        <!-- key_hex id=\"0\">...</key_hex -->\n-        <!-- key_hex id=\"1\" from_env=\"..\"></key_hex -->\n-        <!-- key_hex id=\"2\">...</key_hex -->\n-        <!-- current_key_id>2</current_key_id -->\n-\n-        <!-- Example of getting hex key from config -->\n-        <!-- the code should use this key and throw an exception if its length is not 16 bytes -->\n-        <!-- key>...</key -->\n-\n-        <!-- example of adding nonce -->\n-        <!-- nonce>...</nonce -->\n-\n-        <!-- /aes_128_gcm_siv -->\n-    </encryption_codecs>\n-\n-    <!-- Allow to execute distributed DDL queries (CREATE, DROP, ALTER, RENAME) on cluster.\n-         Works only if ZooKeeper is enabled. Comment it if such functionality isn't required. -->\n-    <distributed_ddl>\n-        <!-- Path in ZooKeeper to queue with DDL queries -->\n-        <path>/clickhouse/task_queue/ddl</path>\n-\n-        <!-- Settings from this profile will be used to execute DDL queries -->\n-        <!-- <profile>default</profile> -->\n-\n-        <!-- Controls how much ON CLUSTER queries can be run simultaneously. -->\n-        <!-- <pool_size>1</pool_size> -->\n-\n-        <!--\n-             Cleanup settings (active tasks will not be removed)\n-        -->\n-\n-        <!-- Controls task TTL (default 1 week) -->\n-        <!-- <task_max_lifetime>604800</task_max_lifetime> -->\n-\n-        <!-- Controls how often cleanup should be performed (in seconds) -->\n-        <!-- <cleanup_delay_period>60</cleanup_delay_period> -->\n-\n-        <!-- Controls how many tasks could be in the queue -->\n-        <!-- <max_tasks_in_queue>1000</max_tasks_in_queue> -->\n-    </distributed_ddl>\n-\n-    <!-- Settings to fine tune MergeTree tables. See documentation in source code, in MergeTreeSettings.h -->\n-    <!--\n-    <merge_tree>\n-        <max_suspicious_broken_parts>5</max_suspicious_broken_parts>\n-    </merge_tree>\n-    -->\n-\n-    <!-- Protection from accidental DROP.\n-         If size of a MergeTree table is greater than max_table_size_to_drop (in bytes) than table could not be dropped with any DROP query.\n-         If you want do delete one table and don't want to change clickhouse-server config, you could create special file <clickhouse-path>/flags/force_drop_table and make DROP once.\n-         By default max_table_size_to_drop is 50GB; max_table_size_to_drop=0 allows to DROP any tables.\n-         The same for max_partition_size_to_drop.\n-         Uncomment to disable protection.\n-    -->\n-    <!-- <max_table_size_to_drop>0</max_table_size_to_drop> -->\n-    <!-- <max_partition_size_to_drop>0</max_partition_size_to_drop> -->\n-\n-    <!-- Example of parameters for GraphiteMergeTree table engine -->\n-    <graphite_rollup_example>\n-        <pattern>\n-            <regexp>click_cost</regexp>\n-            <function>any</function>\n-            <retention>\n-                <age>0</age>\n-                <precision>3600</precision>\n-            </retention>\n-            <retention>\n-                <age>86400</age>\n-                <precision>60</precision>\n-            </retention>\n-        </pattern>\n-        <default>\n-            <function>max</function>\n-            <retention>\n-                <age>0</age>\n-                <precision>60</precision>\n-            </retention>\n-            <retention>\n-                <age>3600</age>\n-                <precision>300</precision>\n-            </retention>\n-            <retention>\n-                <age>86400</age>\n-                <precision>3600</precision>\n-            </retention>\n-        </default>\n-    </graphite_rollup_example>\n-\n-    <!-- Directory in <clickhouse-path> containing schema files for various input formats.\n-         The directory will be created if it doesn't exist.\n-      -->\n-    <format_schema_path>/var/lib/clickhouse/format_schemas/</format_schema_path>\n-\n-    <!-- Default query masking rules, matching lines would be replaced with something else in the logs\n-        (both text logs and system.query_log).\n-        name - name for the rule (optional)\n-        regexp - RE2 compatible regular expression (mandatory)\n-        replace - substitution string for sensitive data (optional, by default - six asterisks)\n-    -->\n-    <query_masking_rules>\n-        <rule>\n-            <name>hide encrypt/decrypt arguments</name>\n-            <regexp>((?:aes_)?(?:encrypt|decrypt)(?:_mysql)?)\\s*\\(\\s*(?:'(?:\\\\'|.)+'|.*?)\\s*\\)</regexp>\n-            <!-- or more secure, but also more invasive:\n-                (aes_\\w+)\\s*\\(.*\\)\n-            -->\n-            <replace>\\1(???)</replace>\n-        </rule>\n-    </query_masking_rules>\n-\n-    <!-- Uncomment to use custom http handlers.\n-        rules are checked from top to bottom, first match runs the handler\n-            url - to match request URL, you can use 'regex:' prefix to use regex match(optional)\n-            methods - to match request method, you can use commas to separate multiple method matches(optional)\n-            headers - to match request headers, match each child element(child element name is header name), you can use 'regex:' prefix to use regex match(optional)\n-        handler is request handler\n-            type - supported types: static, dynamic_query_handler, predefined_query_handler\n-            query - use with predefined_query_handler type, executes query when the handler is called\n-            query_param_name - use with dynamic_query_handler type, extracts and executes the value corresponding to the <query_param_name> value in HTTP request params\n-            status - use with static type, response status code\n-            content_type - use with static type, response content-type\n-            response_content - use with static type, Response content sent to client, when using the prefix 'file://' or 'config://', find the content from the file or configuration send to client.\n-\n-    <http_handlers>\n-        <rule>\n-            <url>/</url>\n-            <methods>POST,GET</methods>\n-            <headers><pragma>no-cache</pragma></headers>\n-            <handler>\n-                <type>dynamic_query_handler</type>\n-                <query_param_name>query</query_param_name>\n-            </handler>\n-        </rule>\n-\n-        <rule>\n-            <url>/predefined_query</url>\n-            <methods>POST,GET</methods>\n-            <handler>\n-                <type>predefined_query_handler</type>\n-                <query>SELECT * FROM system.settings</query>\n-            </handler>\n-        </rule>\n-\n-        <rule>\n-            <handler>\n-                <type>static</type>\n-                <status>200</status>\n-                <content_type>text/plain; charset=UTF-8</content_type>\n-                <response_content>config://http_server_default_response</response_content>\n-            </handler>\n-        </rule>\n-    </http_handlers>\n-    -->\n-\n-    <send_crash_reports>\n-        <!-- Changing <enabled> to true allows sending crash reports to -->\n-        <!-- the ClickHouse core developers team via Sentry https://sentry.io -->\n-        <!-- Doing so at least in pre-production environments is highly appreciated -->\n-        <enabled>false</enabled>\n-        <!-- Change <anonymize> to true if you don't feel comfortable attaching the server hostname to the crash report -->\n-        <anonymize>false</anonymize>\n-        <!-- Default endpoint should be changed to different Sentry DSN only if you have -->\n-        <!-- some in-house engineers or hired consultants who're going to debug ClickHouse issues for you -->\n-        <endpoint>https://6f33034cfe684dd7a3ab9875e57b1c8d@o388870.ingest.sentry.io/5226277</endpoint>\n-    </send_crash_reports>\n-\n-    <!-- Uncomment to disable ClickHouse internal DNS caching. -->\n-    <!-- <disable_internal_dns_cache>1</disable_internal_dns_cache> -->\n-\n-    <!-- You can also configure rocksdb like this: -->\n-    <!--\n-    <rocksdb>\n-        <options>\n-            <max_background_jobs>8</max_background_jobs>\n-        </options>\n-        <column_family_options>\n-            <num_levels>2</num_levels>\n-        </column_family_options>\n-        <tables>\n-            <table>\n-                <name>TABLE</name>\n-                <options>\n-                    <max_background_jobs>8</max_background_jobs>\n-                </options>\n-                <column_family_options>\n-                    <num_levels>2</num_levels>\n-                </column_family_options>\n-            </table>\n-        </tables>\n-    </rocksdb>\n-    -->\n-</yandex>\ndiff --git a/programs/diagnostics/testdata/docker/admin.xml b/programs/diagnostics/testdata/docker/admin.xml\ndeleted file mode 100644\nindex 76aa670dcfe3..000000000000\n--- a/programs/diagnostics/testdata/docker/admin.xml\n+++ /dev/null\n@@ -1,15 +0,0 @@\n-<clickhouse>\n-    <!-- Profiles of settings. -->\n-    <profiles>\n-        <!-- Default settings. -->\n-        <default>\n-            <!-- Allows us to create replicated databases. -->\n-            <allow_experimental_database_replicated>1</allow_experimental_database_replicated>\n-        </default>\n-    </profiles>\n-    <users>\n-        <default>\n-            <access_management>1</access_management>\n-        </default>\n-    </users>\n-</clickhouse>\n\\ No newline at end of file\ndiff --git a/programs/diagnostics/testdata/docker/custom.xml b/programs/diagnostics/testdata/docker/custom.xml\ndeleted file mode 100644\nindex bc1051178ca8..000000000000\n--- a/programs/diagnostics/testdata/docker/custom.xml\n+++ /dev/null\n@@ -1,8 +0,0 @@\n-<clickhouse>\n-    <listen_host>::</listen_host>\n-    <listen_host>0.0.0.0</listen_host>\n-    <listen_try>1</listen_try>\n-    <logger>\n-        <console>1</console>\n-    </logger>\n-</clickhouse>\ndiff --git a/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.err.log b/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.err.log\ndeleted file mode 100644\nindex 1a1768fe87ef..000000000000\n--- a/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.err.log\n+++ /dev/null\n@@ -1,10 +0,0 @@\n-2021.12.13 10:12:26.940169 [ 38398 ] {} <Warning> Access(local directory): File /var/lib/clickhouse/access/users.list doesn't exist\n-2021.12.13 10:12:26.940204 [ 38398 ] {} <Warning> Access(local directory): Recovering lists in directory /var/lib/clickhouse/access/\n-2021.12.13 10:12:40.649453 [ 38445 ] {} <Error> Access(user directories): from: 127.0.0.1, user: default: Authentication failed: Code: 193. DB::Exception: Invalid credentials. (WRONG_PASSWORD), Stack trace (when copying this message, always include the lines below):\n-\n-0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x9b722d4 in /usr/bin/clickhouse\n-1. DB::IAccessStorage::throwInvalidCredentials() @ 0x119d9b27 in /usr/bin/clickhouse\n-2. DB::IAccessStorage::loginImpl(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&) const @ 0x119d98d7 in /usr/bin/clickhouse\n-3. DB::IAccessStorage::login(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&, bool) const @ 0x119d9084 in /usr/bin/clickhouse\n-4. DB::MultipleAccessStorage::loginImpl(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&) const @ 0x119ff93c in /usr/bin/clickhouse\n-5. DB::IAccessStorage::login(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&, bool) const @ 0x119d9084 in /usr/bin/clickhouse\ndiff --git a/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.log b/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.log\ndeleted file mode 100644\nindex f6abe7764ba7..000000000000\n--- a/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.log\n+++ /dev/null\n@@ -1,10 +0,0 @@\n-2022.02.02 14:49:32.458680 [ 200404 ] {} <Debug> DiskLocal: Reserving 2.47 MiB on disk `default`, having unreserved 1.56 TiB.\n-2022.02.02 14:49:32.459086 [ 200359 ] {de87df8b-2250-439c-9e87-df8b2250339c::202202_147058_147550_344} <Debug> MergeTask::PrepareStage: Merging 2 parts: from 202202_147058_147549_343 to 202202_147550_147550_0 into Wide\n-2022.02.02 14:49:32.459201 [ 200359 ] {de87df8b-2250-439c-9e87-df8b2250339c::202202_147058_147550_344} <Debug> MergeTask::PrepareStage: Selected MergeAlgorithm: Horizontal\n-2022.02.02 14:49:32.459262 [ 200359 ] {de87df8b-2250-439c-9e87-df8b2250339c::202202_147058_147550_344} <Debug> MergeTreeSequentialSource: Reading 159 marks from part 202202_147058_147549_343, total 1289014 rows starting from the beginning of the part\n-2022.02.02 14:49:32.459614 [ 200359 ] {de87df8b-2250-439c-9e87-df8b2250339c::202202_147058_147550_344} <Debug> MergeTreeSequentialSource: Reading 2 marks from part 202202_147550_147550_0, total 2618 rows starting from the beginning of the part\n-2022.02.02 14:49:32.507755 [ 200359 ] {de87df8b-2250-439c-9e87-df8b2250339c::202202_147058_147550_344} <Debug> MergeTask::MergeProjectionsStage: Merge sorted 1291632 rows, containing 5 columns (5 merged, 0 gathered) in 0.048711404 sec., 26516008.448452853 rows/sec., 639.52 MiB/sec.\n-2022.02.02 14:49:32.508332 [ 200359 ] {de87df8b-2250-439c-9e87-df8b2250339c::202202_147058_147550_344} <Trace> system.asynchronous_metric_log (de87df8b-2250-439c-9e87-df8b2250339c): Renaming temporary part tmp_merge_202202_147058_147550_344 to 202202_147058_147550_344.\n-2022.02.02 14:49:32.508406 [ 200359 ] {de87df8b-2250-439c-9e87-df8b2250339c::202202_147058_147550_344} <Trace> system.asynchronous_metric_log (de87df8b-2250-439c-9e87-df8b2250339c) (MergerMutator): Merged 2 parts: from 202202_147058_147549_343 to 202202_147550_147550_0\n-2022.02.02 14:49:32.508440 [ 200359 ] {} <Debug> MemoryTracker: Peak memory usage Mutate/Merge: 16.31 MiB.\n-2022.02.02 14:49:33.000148 [ 200388 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 774.16 MiB, peak 2.51 GiB, will set to 772.30 MiB (RSS), difference: -1.86 MiB\ndiff --git a/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.log.gz b/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.log.gz\ndeleted file mode 100644\nindex 136bf5913aaf..000000000000\n--- a/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.log.gz\n+++ /dev/null\n@@ -1,1 +0,0 @@\n-dummy hz file for tests\n",
  "problem_statement": "Remove `clickhouse-diagnostics` from the package\nIt is written in Go, and frequently gets attention of security scanners due to a high number of library dependencies. While this motivation is not reasonable, this tool is rarely used, and we can remove it to avoid the burden.\n",
  "hints_text": "",
  "created_at": "2024-03-21T01:53:31Z"
}