{
  "repo": "ydb-platform/ydb",
  "pull_number": 21377,
  "instance_id": "ydb-platform__ydb-21377",
  "issue_numbers": [
    "19842"
  ],
  "base_commit": "aa0f9a210c2bc92aacaeb236634a526cdef19f54",
  "patch": "diff --git a/ydb/core/tx/coordinator/mediator_queue.cpp b/ydb/core/tx/coordinator/mediator_queue.cpp\nindex 8cb7557d80aa..02fdcc64643e 100644\n--- a/ydb/core/tx/coordinator/mediator_queue.cpp\n+++ b/ydb/core/tx/coordinator/mediator_queue.cpp\n@@ -20,6 +20,16 @@ static constexpr size_t ConfirmedStepsToFlush = 2;\n // the number of rows as large transactions are problematic to commit.\n static constexpr size_t ConfirmedParticipantsToFlush = 10'000;\n \n+// Coordinator must reconnect with mediator as quickly as possible if connection is lost.\n+// Retry policy prevents the reconnect loop from becoming too aggressive.\n+static NTabletPipe::TClientRetryPolicy MediatorSyncRetryPolicy{\n+    .RetryLimitCount = std::numeric_limits<ui32>::max(),\n+    .MinRetryTime = TDuration::MilliSeconds(1),\n+    .MaxRetryTime = TDuration::MilliSeconds(10),\n+    .BackoffMultiplier = 2,\n+    .DoFirstRetryInstantly = true,\n+};\n+\n void TMediatorStep::SerializeTo(TEvTxCoordinator::TEvCoordinatorStep *msg) const {\n     for (const TTx &tx : Transactions) {\n         NKikimrTx::TCoordinatorTransaction *x = msg->Record.AddTransactions();\n@@ -50,6 +60,7 @@ class TTxCoordinatorMediatorQueue : public TActorBootstrapped<TTxCoordinatorMedi\n     size_t ConfirmedParticipants = 0;\n     size_t ConfirmedSteps = 0;\n \n+\n     void Die(const TActorContext &ctx) override {\n         if (PipeClient) {\n             NTabletPipe::CloseClient(ctx, PipeClient);\n@@ -66,7 +77,7 @@ class TTxCoordinatorMediatorQueue : public TActorBootstrapped<TTxCoordinatorMedi\n             PipeClient = TActorId();\n         }\n \n-        PipeClient = ctx.RegisterWithSameMailbox(NTabletPipe::CreateClient(ctx.SelfID, Mediator));\n+        PipeClient = ctx.RegisterWithSameMailbox(NTabletPipe::CreateClient(ctx.SelfID, Mediator, MediatorSyncRetryPolicy));\n \n         LOG_DEBUG_S(ctx, NKikimrServices::TX_COORDINATOR_MEDIATOR_QUEUE, \"Actor# \" << ctx.SelfID.ToString()\n             << \" tablet# \" << Coordinator << \" SEND EvCoordinatorSync to# \" << Mediator << \" Mediator\");\ndiff --git a/ydb/core/tx/schemeshard/schemeshard__clean_pathes.cpp b/ydb/core/tx/schemeshard/schemeshard__clean_pathes.cpp\nindex 83313007f0d3..8c938bb9b2ca 100644\n--- a/ydb/core/tx/schemeshard/schemeshard__clean_pathes.cpp\n+++ b/ydb/core/tx/schemeshard/schemeshard__clean_pathes.cpp\n@@ -224,6 +224,13 @@ struct TSchemeShard::TTxCleanDroppedSubDomains : public TTransactionBase<TScheme\n                     << \", at schemeshard: \"<< Self->TabletID());\n                 Self->PersistRemoveSubDomain(db, pathId);\n                 ++RemovedCount;\n+\n+                // This is for tests, so that tests could wait for actual lifetime end of a subdomain.\n+                // It's kinda ok to reply from execute, and actually required for tests with reboots\n+                // (to not lose event on a tablet reboot).\n+                {\n+                    ctx.Send(Self->SelfId(), new TEvPrivate::TEvTestNotifySubdomainCleanup(pathId));\n+                }\n             } else {\n                 // Probably never happens, but better safe than sorry.\n                 ++SkippedCount;\n@@ -237,12 +244,13 @@ struct TSchemeShard::TTxCleanDroppedSubDomains : public TTransactionBase<TScheme\n         Y_ABORT_UNLESS(Self->CleanDroppedSubDomainsInFly);\n \n         if (RemovedCount || SkippedCount) {\n-            LOG_NOTICE_S(ctx, NKikimrServices::FLAT_TX_SCHEMESHARD,\n-                         \"TTxCleanDroppedSubDomains Complete\"\n-                           << \", done PersistRemoveSubDomain for \" << RemovedCount << \" paths\"\n-                           << \", skipped \" << SkippedCount\n-                           << \", left \" << Self->CleanDroppedSubDomainsCandidates.size() << \" candidates\"\n-                           << \", at schemeshard: \"<< Self->TabletID());\n+            LOG_NOTICE_S(ctx, NKikimrServices::FLAT_TX_SCHEMESHARD, \"TTxCleanDroppedSubDomains Complete\"\n+                << \", done PersistRemoveSubDomain for \" << RemovedCount << \" subdomains\"\n+                << \", skipped \" << SkippedCount\n+                << \", left \" << Self->CleanDroppedSubDomainsCandidates.size() << \" candidates\"\n+                << \", at schemeshard: \"<< Self->TabletID()\n+            );\n+\n         }\n \n         if (!Self->CleanDroppedSubDomainsCandidates.empty()) {\ndiff --git a/ydb/core/tx/schemeshard/schemeshard__init.cpp b/ydb/core/tx/schemeshard/schemeshard__init.cpp\nindex c0956dcf2695..fdedbe7a55e3 100644\n--- a/ydb/core/tx/schemeshard/schemeshard__init.cpp\n+++ b/ydb/core/tx/schemeshard/schemeshard__init.cpp\n@@ -674,6 +674,25 @@ struct TSchemeShard::TTxInit : public TTransactionBase<TSchemeShard> {\n         return true;\n     }\n \n+    bool LoadSystemShardsToDelete(NIceDb::TNiceDb& db, TShardsToDeleteRows& shardsToDelete) const {\n+        {\n+            auto rowSet = db.Table<Schema::SystemShardsToDelete>().Range().Select();\n+            if (!rowSet.IsReady()) {\n+                return false;\n+            }\n+            while (!rowSet.EndOfSet()) {\n+                const auto shardIdx = Self->MakeLocalId(rowSet.GetValue<Schema::SystemShardsToDelete::ShardIdx>());\n+                shardsToDelete.emplace_back(shardIdx);\n+\n+                if (!rowSet.Next()) {\n+                    return false;\n+                }\n+            }\n+        }\n+\n+        return true;\n+    }\n+\n     typedef std::tuple<TOperationId, TShardIdx, TTxState::ETxState> TTxShardRec;\n     typedef TVector<TTxShardRec> TTxShardsRows;\n \n@@ -3844,6 +3863,23 @@ struct TSchemeShard::TTxInit : public TTransactionBase<TSchemeShard> {\n             }\n         }\n \n+        // Read system shards to delete\n+        {\n+            TShardsToDeleteRows shardsToDelete;\n+            if (!LoadSystemShardsToDelete(db, shardsToDelete)) {\n+                return false;\n+            }\n+\n+            LOG_NOTICE_S(ctx, NKikimrServices::FLAT_TX_SCHEMESHARD,\n+                         \"TTxInit for SystemShardToDelete\"\n+                             << \", read records: \" << shardsToDelete.size()\n+                             << \", at schemeshard: \" << Self->TabletID());\n+\n+            for (auto& rec: shardsToDelete) {\n+                OnComplete.DeleteSystemShard(std::get<0>(rec));\n+            }\n+        }\n+\n         // Read backup settings\n         {\n             TBackupSettingsRows backupSettings;\n@@ -5265,14 +5301,14 @@ struct TSchemeShard::TTxInit : public TTransactionBase<TSchemeShard> {\n                             if (op.HasFullBackupTrimmedName()) {\n                                 TString fullBackupName = op.GetFullBackupTrimmedName() + \"_full\";\n                                 TString fullBackupPath = backupCollectionPathStr + \"/\" + fullBackupName;\n-                                \n+\n                                 // Set state for each table in the full backup\n                                 for (const auto& tablePath : op.GetTablePathList()) {\n                                     TPath originalTablePath = TPath::Resolve(tablePath, Self);\n                                     if (originalTablePath.IsResolved()) {\n                                         TString tableName = originalTablePath.LeafName();\n                                         TString fullBackupTablePath = fullBackupPath + \"/\" + tableName;\n-                                        \n+\n                                         TPath fullBackupTableResolvedPath = TPath::Resolve(fullBackupTablePath, Self);\n                                         if (fullBackupTableResolvedPath.IsResolved() && Self->PathsById.contains(fullBackupTableResolvedPath.Base()->PathId)) {\n                                             auto backupTablePathElement = Self->PathsById.at(fullBackupTableResolvedPath.Base()->PathId);\n@@ -5286,14 +5322,14 @@ struct TSchemeShard::TTxInit : public TTransactionBase<TSchemeShard> {\n                             for (const auto& trimmedIncrName : op.GetIncrementalBackupTrimmedNames()) {\n                                 TString incrBackupName = trimmedIncrName + \"_incremental\";\n                                 TString incrBackupPath = backupCollectionPathStr + \"/\" + incrBackupName;\n-                                \n+\n                                 // Set state for each table in the incremental backup\n                                 for (const auto& tablePath : op.GetTablePathList()) {\n                                     TPath originalTablePath = TPath::Resolve(tablePath, Self);\n                                     if (originalTablePath.IsResolved()) {\n                                         TString tableName = originalTablePath.LeafName();\n                                         TString incrBackupTablePath = incrBackupPath + \"/\" + tableName;\n-                                        \n+\n                                         TPath incrBackupTableResolvedPath = TPath::Resolve(incrBackupTablePath, Self);\n                                         if (incrBackupTableResolvedPath.IsResolved() && Self->PathsById.contains(incrBackupTableResolvedPath.Base()->PathId)) {\n                                             auto backupTablePathElement = Self->PathsById.at(incrBackupTableResolvedPath.Base()->PathId);\ndiff --git a/ydb/core/tx/schemeshard/schemeshard__operation_drop_extsubdomain.cpp b/ydb/core/tx/schemeshard/schemeshard__operation_drop_extsubdomain.cpp\nindex 953da76a73f6..54a9a89616e3 100644\n--- a/ydb/core/tx/schemeshard/schemeshard__operation_drop_extsubdomain.cpp\n+++ b/ydb/core/tx/schemeshard/schemeshard__operation_drop_extsubdomain.cpp\n@@ -10,13 +10,46 @@ namespace {\n using namespace NKikimr;\n using namespace NSchemeShard;\n \n-class TDeletePrivateShards: public TDeleteParts {\n+class TDeleteSubdomainSystemShards: public TSubOperationState {\n+protected:\n+    const TOperationId OperationId;\n+\n+    TString DebugHint() const override {\n+        return TStringBuilder() << \"TDeleteSubdomainSystemShards\" << \" opId# \" << OperationId << \" \";\n+    }\n+\n public:\n-    explicit TDeletePrivateShards(const TOperationId& id)\n-        : TDeleteParts(id, TTxState::Done)\n+    explicit TDeleteSubdomainSystemShards(const TOperationId& id)\n+        : OperationId(id)\n     {\n         IgnoreMessages(DebugHint(), AllIncomingEvents());\n     }\n+\n+    bool ProgressState(TOperationContext& context) override {\n+        LOG_INFO_S(context.Ctx, NKikimrServices::FLAT_TX_SCHEMESHARD, \"[\" << context.SS->SelfTabletId() << \"] \" << DebugHint() << \"ProgressState\");\n+\n+        const auto* txState = context.SS->FindTx(OperationId);\n+        Y_ABORT_UNLESS(txState);\n+        Y_ABORT_UNLESS(txState->TxType == TTxState::TxForceDropExtSubDomain);\n+\n+        auto subdomain = context.SS->SubDomains.at(txState->TargetPathId);\n+        Y_ABORT_UNLESS(subdomain);\n+\n+        // Initiate asynchronous deletion of system shards\n+        if (subdomain->GetSharedHive()) {\n+            for (const auto& shard : txState->Shards) {\n+                context.OnComplete.DeleteShard(shard.Idx);\n+            }\n+        } else {\n+            for (const auto& shard : txState->Shards) {\n+                context.OnComplete.DeleteSystemShard(shard.Idx);\n+            }\n+        }\n+\n+        NIceDb::TNiceDb db(context.GetDB());\n+        context.SS->ChangeTxState(db, OperationId, TTxState::Done);\n+        return true;\n+    }\n };\n \n class TDeleteExternalShards: public TSubOperationState {\n@@ -237,7 +270,7 @@ class TDropExtSubdomain: public TSubOperation {\n         case TTxState::DeleteExternalShards:\n             return MakeHolder<TDeleteExternalShards>(OperationId);\n         case TTxState::DeletePrivateShards:\n-            return MakeHolder<TDeletePrivateShards>(OperationId);\n+            return MakeHolder<TDeleteSubdomainSystemShards>(OperationId);\n         case TTxState::Done:\n             return MakeHolder<TDone>(OperationId);\n         default:\ndiff --git a/ydb/core/tx/schemeshard/schemeshard__operation_side_effects.cpp b/ydb/core/tx/schemeshard/schemeshard__operation_side_effects.cpp\nindex b1a71ea62b86..1cd3282af875 100644\n--- a/ydb/core/tx/schemeshard/schemeshard__operation_side_effects.cpp\n+++ b/ydb/core/tx/schemeshard/schemeshard__operation_side_effects.cpp\n@@ -140,6 +140,13 @@ void TSideEffects::DeleteShard(TShardIdx idx) {\n     ToDeleteShards.insert(idx);\n }\n \n+void TSideEffects::DeleteSystemShard(TShardIdx idx) {\n+    if (!idx) {\n+        return; //KIKIMR-8507\n+    }\n+    ToDeleteSystemShards.insert(idx);\n+}\n+\n void TSideEffects::ToProgress(TIndexBuildId id) {\n     IndexToProgress.push_back(id);\n }\n@@ -187,6 +194,7 @@ void TSideEffects::ApplyOnExecute(TSchemeShard* ss, NTabletFlatExecutor::TTransa\n     DoPersistDependencies(ss,txc, ctx);\n \n     DoPersistDeleteShards(ss, txc, ctx);\n+    DoPersistDeleteSystemShards(ss, txc, ctx);\n \n     SetupRoutingLongOps(ss, ctx);\n }\n@@ -226,6 +234,7 @@ void TSideEffects::ApplyOnComplete(TSchemeShard* ss, const TActorContext& ctx) {\n     DoRegisterRelations(ss, ctx);\n \n     DoTriggerDeleteShards(ss, ctx);\n+    DoTriggerDeleteSystemShards(ss, ctx);\n \n     ResumeLongOps(ss, ctx);\n }\n@@ -755,6 +764,10 @@ void TSideEffects::DoTriggerDeleteShards(TSchemeShard *ss, const TActorContext &\n     ss->DoShardsDeletion(ToDeleteShards, ctx);\n }\n \n+void TSideEffects::DoTriggerDeleteSystemShards(TSchemeShard *ss, const TActorContext &ctx) {\n+    ss->DoDeleteSystemShards(ToDeleteSystemShards, ctx);\n+}\n+\n void TSideEffects::DoReleasePathState(TSchemeShard *ss, const TActorContext &) {\n     for (auto& rec: ReleasePathStateRecs) {\n         TOperationId opId = InvalidOperationId;\n@@ -781,6 +794,11 @@ void TSideEffects::DoPersistDeleteShards(TSchemeShard *ss, NTabletFlatExecutor::\n     ss->PersistShardsToDelete(db, ToDeleteShards);\n }\n \n+void TSideEffects::DoPersistDeleteSystemShards(TSchemeShard *ss, NTabletFlatExecutor::TTransactionContext &txc, const TActorContext &) {\n+    NIceDb::TNiceDb db(txc.DB);\n+    ss->PersistSystemShardsToDelete(db, ToDeleteSystemShards);\n+}\n+\n void TSideEffects::DoUpdateTempDirsToMakeState(TSchemeShard* ss, const TActorContext &ctx) {\n     for (auto& [ownerActorId, tempDirs]: TempDirsToMakeState) {\n \ndiff --git a/ydb/core/tx/schemeshard/schemeshard__operation_side_effects.h b/ydb/core/tx/schemeshard/schemeshard__operation_side_effects.h\nindex 2ccb01cc82d3..3796e29ab7dd 100644\n--- a/ydb/core/tx/schemeshard/schemeshard__operation_side_effects.h\n+++ b/ydb/core/tx/schemeshard/schemeshard__operation_side_effects.h\n@@ -56,6 +56,7 @@ class TSideEffects: public TSimpleRefCount<TSideEffects> {\n     THashSet<TOperationId> DoneOperations;\n     THashSet<TTxId> DoneTransactions;\n     THashSet<TShardIdx> ToDeleteShards;\n+    THashSet<TShardIdx> ToDeleteSystemShards;  // temporary: special case for deleting tenant's system shards\n     TDeque<TDependence> Dependencies;\n     TDeque<TPathStateRec> ReleasePathStateRecs;\n     THashSet<TPathId> TenantsToUpdate;\n@@ -124,6 +125,7 @@ class TSideEffects: public TSimpleRefCount<TSideEffects> {\n     void PublishAndWaitPublication(TOperationId opId, TPathId pathId);\n \n     void DeleteShard(TShardIdx idx);\n+    void DeleteSystemShard(TShardIdx idx);\n \n     void ToProgress(TIndexBuildId id);\n \n@@ -153,6 +155,7 @@ class TSideEffects: public TSimpleRefCount<TSideEffects> {\n \n     void DoRegisterRelations(TSchemeShard* ss, const TActorContext& ctx);\n     void DoTriggerDeleteShards(TSchemeShard* ss, const TActorContext &ctx);\n+    void DoTriggerDeleteSystemShards(TSchemeShard *ss, const TActorContext &ctx);\n \n     void DoReleasePathState(TSchemeShard* ss, const TActorContext &ctx);\n     void DoDoneParts(TSchemeShard* ss, const TActorContext& ctx);\n@@ -163,6 +166,7 @@ class TSideEffects: public TSimpleRefCount<TSideEffects> {\n     void DoActivateOps(TSchemeShard* ss, const TActorContext& ctx);\n \n     void DoPersistDeleteShards(TSchemeShard* ss, NTabletFlatExecutor::TTransactionContext &txc, const TActorContext &ctx);\n+    void DoPersistDeleteSystemShards(TSchemeShard* ss, NTabletFlatExecutor::TTransactionContext &txc, const TActorContext &ctx);\n \n     void DoUpdateTempDirsToMakeState(TSchemeShard* ss, const TActorContext &ctx);\n     void DoUpdateTempDirsToRemoveState(TSchemeShard* ss, const TActorContext &ctx);\ndiff --git a/ydb/core/tx/schemeshard/schemeshard_impl.cpp b/ydb/core/tx/schemeshard/schemeshard_impl.cpp\nindex bed967c3d382..102dbd98c5f3 100644\n--- a/ydb/core/tx/schemeshard/schemeshard_impl.cpp\n+++ b/ydb/core/tx/schemeshard/schemeshard_impl.cpp\n@@ -3570,9 +3570,17 @@ void TSchemeShard::PersistShardsToDelete(NIceDb::TNiceDb& db, const THashSet<TSh\n     }\n }\n \n+void TSchemeShard::PersistSystemShardsToDelete(NIceDb::TNiceDb& db, const THashSet<TShardIdx>& shardsIdxs) {\n+    for (auto& shardIdx : shardsIdxs) {\n+        Y_ABORT_UNLESS(IsLocalId(shardIdx));\n+        db.Table<Schema::SystemShardsToDelete>().Key(shardIdx.GetLocalId()).Update();\n+    }\n+}\n+\n void TSchemeShard::PersistShardDeleted(NIceDb::TNiceDb& db, TShardIdx shardIdx, const TChannelsBindings& bindedChannels) {\n     if (shardIdx.GetOwnerId() == TabletID()) {\n         db.Table<Schema::ShardsToDelete>().Key(shardIdx.GetLocalId()).Delete();\n+        db.Table<Schema::SystemShardsToDelete>().Key(shardIdx.GetLocalId()).Delete();\n         db.Table<Schema::Shards>().Key(shardIdx.GetLocalId()).Delete();\n         for (ui32 channelId = 0; channelId < bindedChannels.size(); ++channelId) {\n             db.Table<Schema::ChannelsBinding>().Key(shardIdx.GetLocalId(), channelId).Delete();\n@@ -3591,6 +3599,7 @@ void TSchemeShard::PersistShardDeleted(NIceDb::TNiceDb& db, TShardIdx shardIdx,\n void TSchemeShard::PersistUnknownShardDeleted(NIceDb::TNiceDb& db, TShardIdx shardIdx) {\n     if (shardIdx.GetOwnerId() == TabletID()) {\n         db.Table<Schema::ShardsToDelete>().Key(shardIdx.GetLocalId()).Delete();\n+        db.Table<Schema::SystemShardsToDelete>().Key(shardIdx.GetLocalId()).Delete();\n     }\n \n     db.Table<Schema::MigratedShardsToDelete>().Key(shardIdx.GetOwnerId(), shardIdx.GetLocalId()).Delete();\n@@ -4496,6 +4505,12 @@ void TSchemeShard::DoShardsDeletion(const THashSet<TShardIdx>& shardIdxs, const\n     }\n }\n \n+void TSchemeShard::DoDeleteSystemShards(const THashSet<TShardIdx>& shards, const TActorContext& ctx) {\n+    if (!shards.empty()) {\n+        ShardDeleter.SendDeleteRequests(GetGlobalHive(), shards, ShardInfos, ctx);\n+    }\n+}\n+\n NKikimrSchemeOp::TPathVersion TSchemeShard::GetPathVersion(const TPath& path) const {\n     NKikimrSchemeOp::TPathVersion result;\n \n@@ -5240,6 +5255,9 @@ void TSchemeShard::StateWork(STFUNC_SIG) {\n         HFuncTraced(TEvPrivate::TEvCleanDroppedSubDomains, Handle);\n         HFuncTraced(TEvPrivate::TEvSubscribeToShardDeletion, Handle);\n \n+        // Test-only notification\n+        IgnoreFunc(TEvPrivate::TEvTestNotifySubdomainCleanup);\n+\n         HFuncTraced(TEvPrivate::TEvPersistTableStats, Handle);\n         HFuncTraced(TEvPrivate::TEvPersistTopicStats, Handle);\n \n@@ -6003,14 +6021,29 @@ void TSchemeShard::Handle(TEvTabletPipe::TEvServerDisconnected::TPtr &, const TA\n \n void TSchemeShard::Handle(TEvSchemeShard::TEvSyncTenantSchemeShard::TPtr& ev, const TActorContext& ctx) {\n     const auto& record = ev->Get()->Record;\n-    LOG_DEBUG_S(ctx, NKikimrServices::FLAT_TX_SCHEMESHARD,\n-               \"Handle TEvSyncTenantSchemeShard\"\n-                   << \", at schemeshard: \" << TabletID()\n-                   << \", msg: \" << record.DebugString());\n+    LOG_DEBUG_S(ctx, NKikimrServices::FLAT_TX_SCHEMESHARD, \"Handle TEvSyncTenantSchemeShard, at schemeshard: \" << TabletID()\n+        << \", msg: \" << record.ShortDebugString()\n+    );\n     Y_VERIFY_S(IsDomainSchemeShard, \"unexpected message: schemeshard: \" << TabletID() << \" mgs: \" << record.DebugString());\n \n+    const TPathId pathId(record.GetDomainSchemeShard(), record.GetDomainPathId());\n+\n+    if (!SubDomains.contains(pathId)) {\n+        LOG_WARN_S(ctx, NKikimrServices::FLAT_TX_SCHEMESHARD, \"Handle TEvSyncTenantSchemeShard, at schemeshard: \" << TabletID()\n+            << \", ignore spurious message from dropped subdomain's schemeshard (partial cleanup)\" << pathId\n+        );\n+        return;\n+    }\n+\n+    if (!PathsById.contains(pathId)) {\n+        LOG_WARN_S(ctx, NKikimrServices::FLAT_TX_SCHEMESHARD, \"Handle TEvSyncTenantSchemeShard, at schemeshard: \" << TabletID()\n+            << \", ignore spurious message from dropped subdomain's schemeshard (full cleanup)\" << pathId\n+        );\n+        return;\n+    }\n+\n     if (SubDomainsLinks.Sync(ev, ctx)) {\n-        Execute(CreateTxSyncTenant(TPathId(record.GetDomainSchemeShard(), record.GetDomainPathId())), ctx);\n+        Execute(CreateTxSyncTenant(pathId), ctx);\n     }\n }\n \ndiff --git a/ydb/core/tx/schemeshard/schemeshard_impl.h b/ydb/core/tx/schemeshard/schemeshard_impl.h\nindex 2e37a5b4b7c8..3c456feb5bb5 100644\n--- a/ydb/core/tx/schemeshard/schemeshard_impl.h\n+++ b/ydb/core/tx/schemeshard/schemeshard_impl.h\n@@ -680,6 +680,7 @@ class TSchemeShard\n     void DropPaths(const THashSet<TPathId>& paths, TStepId step, TTxId txId, NIceDb::TNiceDb& db, const TActorContext& ctx);\n \n     void DoShardsDeletion(const THashSet<TShardIdx>& shardIdx, const TActorContext& ctx);\n+    void DoDeleteSystemShards(const THashSet<TShardIdx>& shards, const TActorContext& ctx);\n \n     void SetPartitioning(TPathId pathId, const std::vector<TShardIdx>& partitioning);\n     void SetPartitioning(TPathId pathId, TOlapStoreInfo::TPtr storeInfo);\n@@ -778,6 +779,7 @@ class TSchemeShard\n     void PersistParentDomain(NIceDb::TNiceDb& db, TPathId parentDomain) const;\n     void PersistParentDomainEffectiveACL(NIceDb::TNiceDb& db, const TString& owner, const TString& effectiveACL, ui64 effectiveACLVersion) const;\n     void PersistShardsToDelete(NIceDb::TNiceDb& db, const THashSet<TShardIdx>& shardsIdxs);\n+    void PersistSystemShardsToDelete(NIceDb::TNiceDb& db, const THashSet<TShardIdx>& shardsIdxs);\n     void PersistShardDeleted(NIceDb::TNiceDb& db, TShardIdx shardIdx, const TChannelsBindings& bindedChannels);\n     void PersistUnknownShardDeleted(NIceDb::TNiceDb& db, TShardIdx shardIdx);\n     void PersistTxShardStatus(NIceDb::TNiceDb& db, TOperationId opId, TShardIdx shardIdx, const TTxState::TShardStatus& status);\ndiff --git a/ydb/core/tx/schemeshard/schemeshard_private.h b/ydb/core/tx/schemeshard/schemeshard_private.h\nindex ae0cf7dc657b..ccb887bbe72c 100644\n--- a/ydb/core/tx/schemeshard/schemeshard_private.h\n+++ b/ydb/core/tx/schemeshard/schemeshard_private.h\n@@ -51,6 +51,7 @@ namespace TEvPrivate {\n         EvVerifyPassword,\n         EvLoginFinalize,\n         EvContinuousBackupCleanerResult,\n+        EvTestNotifySubdomainCleanup,\n         EvEnd\n     };\n \n@@ -205,6 +206,14 @@ namespace TEvPrivate {\n         { }\n     };\n \n+    struct TEvTestNotifySubdomainCleanup : public TEventLocal<TEvTestNotifySubdomainCleanup, EvTestNotifySubdomainCleanup> {\n+        TPathId SubdomainPathId;\n+\n+        explicit TEvTestNotifySubdomainCleanup(const TPathId& subdomainPathId)\n+            : SubdomainPathId(subdomainPathId)\n+        { }\n+    };\n+\n     struct TEvCompletePublication: public TEventLocal<TEvCompletePublication, EvCompletePublication> {\n         const TOperationId OpId;\n         const TPathId PathId;\n@@ -284,7 +293,7 @@ namespace TEvPrivate {\n \n     struct TEvProgressIncrementalRestore : public TEventLocal<TEvProgressIncrementalRestore, EvProgressIncrementalRestore> {\n         ui64 OperationId;\n-        \n+\n         explicit TEvProgressIncrementalRestore(ui64 operationId)\n             : OperationId(operationId)\n         {}\ndiff --git a/ydb/core/tx/schemeshard/schemeshard_schema.h b/ydb/core/tx/schemeshard/schemeshard_schema.h\nindex 73683d51cac9..5fde56887606 100644\n--- a/ydb/core/tx/schemeshard/schemeshard_schema.h\n+++ b/ydb/core/tx/schemeshard/schemeshard_schema.h\n@@ -2145,7 +2145,7 @@ struct Schema : NIceDb::Schema {\n         struct OperationId : Column<1, NScheme::NTypeIds::Uint64> {};\n         struct State : Column<2, NScheme::NTypeIds::Uint32> {};\n         struct CurrentIncrementalIdx : Column<3, NScheme::NTypeIds::Uint32> {};\n-        \n+\n         using TKey = TableKey<OperationId>;\n         using TColumns = TableColumns<OperationId, State, CurrentIncrementalIdx>;\n     };\n@@ -2156,11 +2156,18 @@ struct Schema : NIceDb::Schema {\n         struct ShardIdx : Column<2, NScheme::NTypeIds::Uint64> {};\n         struct Status : Column<3, NScheme::NTypeIds::Uint32> {};\n         struct LastKey : Column<4, NScheme::NTypeIds::String> {};\n-        \n+\n         using TKey = TableKey<OperationId, ShardIdx>;\n         using TColumns = TableColumns<OperationId, ShardIdx, Status, LastKey>;\n     };\n \n+    struct SystemShardsToDelete : Table<124> {\n+        struct ShardIdx : Column<1, NScheme::NTypeIds::Uint64> { using Type = TLocalShardIdx; };\n+\n+        using TKey = TableKey<ShardIdx>;\n+        using TColumns = TableColumns<ShardIdx>;\n+    };\n+\n     using TTables = SchemaTables<\n         Paths,\n         TxInFlight,\n@@ -2282,7 +2289,8 @@ struct Schema : NIceDb::Schema {\n         IncrementalRestoreOperations,\n         KMeansTreeClusters,\n         IncrementalRestoreState,\n-        IncrementalRestoreShardProgress\n+        IncrementalRestoreShardProgress,\n+        SystemShardsToDelete\n     >;\n \n     static constexpr ui64 SysParam_NextPathId = 1;\ndiff --git a/ydb/core/tx/schemeshard/schemeshard_shard_deleter.cpp b/ydb/core/tx/schemeshard/schemeshard_shard_deleter.cpp\nindex fc63ecb8bf78..96036de55680 100644\n--- a/ydb/core/tx/schemeshard/schemeshard_shard_deleter.cpp\n+++ b/ydb/core/tx/schemeshard/schemeshard_shard_deleter.cpp\n@@ -17,6 +17,8 @@ void TShardDeleter::SendDeleteRequests(TTabletId hiveTabletId,\n         NKikimr::NSchemeShard::TShardInfo>& shardsInfos,\n         const NActors::TActorContext &ctx\n     ) {\n+    LOG_DEBUG_S(ctx, NKikimrServices::FLAT_TX_SCHEMESHARD, \"SendDeleteRequests, shardsToDelete \" << shardsToDelete.size()<< \", to hive \" << hiveTabletId << \", at schemeshard \" << MyTabletID);\n+\n     if (shardsToDelete.empty())\n         return;\n \ndiff --git a/ydb/core/tx/schemeshard/ut_extsubdomain/ut_extsubdomain.cpp b/ydb/core/tx/schemeshard/ut_extsubdomain/ut_extsubdomain.cpp\nindex 320ec89dc7e5..c5c181115279 100644\n--- a/ydb/core/tx/schemeshard/ut_extsubdomain/ut_extsubdomain.cpp\n+++ b/ydb/core/tx/schemeshard/ut_extsubdomain/ut_extsubdomain.cpp\n@@ -1,9 +1,12 @@\n #include <ydb/core/tx/schemeshard/ut_helpers/helpers.h>\n \n+#include <ydb/public/lib/value/value.h>\n+\n using namespace NKikimr;\n using namespace NSchemeShard;\n using namespace NSchemeShardUT_Private;\n \n+\n Y_UNIT_TEST_SUITE(TSchemeShardExtSubDomainTest) {\n     Y_UNIT_TEST(Fake) {\n     }\n@@ -1478,6 +1481,145 @@ Y_UNIT_TEST_SUITE(TSchemeShardExtSubDomainTest) {\n         UNIT_ASSERT(!CheckLocalRowExists(runtime, TTestTxConfig::SchemeShard, \"Paths\", \"Id\", 2));\n     }\n \n+    Y_UNIT_TEST_FLAG(DropWithDeadTenantHive, AlterDatabaseCreateHiveFirst) {\n+        TTestBasicRuntime runtime;\n+        TTestEnv env(runtime, TTestEnvOptions().EnableAlterDatabaseCreateHiveFirst(AlterDatabaseCreateHiveFirst));\n+        ui64 txId = 100;\n+\n+        // EnableAlterDatabaseCreateHiveFirst = false puts extsubdomain's system tablets into the root hive control.\n+        // EnableAlterDatabaseCreateHiveFirst = true puts extsubdomain's tenant hive into the root hive control\n+        // and other system tablets into the tenant hive control.\n+\n+        TestCreateExtSubDomain(runtime, ++txId,  \"/MyRoot\",\n+            R\"(Name: \"USER_0\")\"\n+        );\n+\n+        TestAlterExtSubDomain(runtime, ++txId,  \"/MyRoot\", R\"(\n+            Name: \"USER_0\"\n+            ExternalSchemeShard: true\n+            PlanResolution: 50\n+            Coordinators: 1\n+            Mediators: 1\n+            TimeCastBucketsPerMediator: 2\n+            StoragePools {\n+                Name: \"pool-1\"\n+                Kind: \"hdd\"\n+            }\n+\n+            ExternalHive: true\n+        )\");\n+        env.TestWaitNotification(runtime, {txId, txId - 1});\n+\n+        ui64 tenantHiveId = 0;\n+        TPathId subdomainPathId;\n+        {\n+            auto describe = DescribePath(runtime, \"/MyRoot/USER_0\");\n+            TestDescribeResult(describe, {\n+                NLs::PathExist,\n+                NLs::IsExternalSubDomain(\"USER_0\"),\n+                NLs::ExtractDomainHive(&tenantHiveId),\n+            });\n+            TSubDomainKey subdomainKey(describe.GetPathDescription().GetDomainDescription().GetDomainKey());\n+            subdomainPathId = TPathId(subdomainKey.GetSchemeShard(), subdomainKey.GetPathId());\n+        }\n+\n+        // check that there is a new path in the root schemeshard\n+        UNIT_ASSERT(CheckLocalRowExists(runtime, TTestTxConfig::SchemeShard, \"Paths\", \"Id\", subdomainPathId.LocalPathId));\n+        UNIT_ASSERT(CheckLocalRowExists(runtime, TTestTxConfig::SchemeShard, \"SubDomains\", \"PathId\", subdomainPathId.LocalPathId));\n+\n+        // check what extsubdomain's system tablets controls root hive\n+        const auto expectedTabletsInRootHive = [&]() {\n+            if (AlterDatabaseCreateHiveFirst) {\n+                return std::vector<ETabletType::EType>({\n+                    ETabletType::Hive,\n+                });\n+            } else {\n+                return std::vector<ETabletType::EType>{{\n+                    ETabletType::Hive,\n+                    ETabletType::SchemeShard,\n+                    ETabletType::Coordinator,\n+                    ETabletType::Mediator\n+                }};\n+            }\n+        }();\n+        {\n+            const auto& expectedTypes = expectedTabletsInRootHive;\n+            const auto tablets = HiveGetSubdomainTablets(runtime, TTestTxConfig::Hive, subdomainPathId);\n+            UNIT_ASSERT_VALUES_EQUAL_C(tablets.size(), expectedTypes.size(), \"-- unexpected tablet count in root hive for the tenant\");\n+            for (const auto& tablet : tablets) {\n+                Cerr << \"root hive, tablets for subdomain \" << subdomainPathId << \", tablet type \" << tablet.GetTabletType() << Endl;\n+                auto found = std::find(expectedTypes.begin(), expectedTypes.end(), tablet.GetTabletType());\n+                UNIT_ASSERT_C(found != expectedTypes.end(), \"-- root hive holds tablet of unexpected type \" << tablet.GetTabletType());\n+            }\n+        }\n+\n+        // check what extsubdomain's system tablets controls tenant hive\n+        const auto expectedTabletsInTenantHive = [&]() {\n+            if (AlterDatabaseCreateHiveFirst) {\n+                return std::vector<ETabletType::EType>{{\n+                    ETabletType::SchemeShard,\n+                    ETabletType::Coordinator,\n+                    ETabletType::Mediator\n+                }};\n+            } else {\n+                return std::vector<ETabletType::EType>{};\n+            }\n+        }();\n+        {\n+            const auto& expectedTypes = expectedTabletsInTenantHive;\n+            const auto tablets = HiveGetSubdomainTablets(runtime, tenantHiveId, subdomainPathId);\n+            UNIT_ASSERT_VALUES_EQUAL_C(tablets.size(), expectedTypes.size(), \"-- unexpected tablet count in tenant hive\");\n+            for (const auto& tablet : tablets) {\n+                Cerr << \"tenant hive, tablets for subdomain \" << subdomainPathId << \", tablet type \" << tablet.GetTabletType() << Endl;\n+                auto found = std::find(expectedTypes.begin(), expectedTypes.end(), tablet.GetTabletType());\n+                UNIT_ASSERT_C(found != expectedTypes.end(), \"-- root hive holds tablet of unexpected type \" << tablet.GetTabletType());\n+            }\n+        }\n+\n+        // extsubdomain drop should be independent of tenant hive's state.\n+        // It must correctly remove database whether tenant nodes and tablets are alive or not.\n+        //\n+        // Make tenant hive inaccessible by stopping its tablet.\n+        // In real life that could be, for example, due to absence of tenant nodes.\n+        //\n+        // Tenant hive is controlled by the root hive (running at node 0).\n+        HiveStopTablet(runtime, TTestTxConfig::Hive, tenantHiveId, 0);\n+\n+        // drop extsubdomain\n+        TestForceDropExtSubDomain(runtime, ++txId, \"/MyRoot\", \"USER_0\");\n+        env.TestWaitNotification(runtime, txId);\n+\n+        TestDescribeResult(DescribePath(runtime, \"/MyRoot/USER_0\"), {NLs::PathNotExist});\n+\n+        TestDescribeResult(DescribePath(runtime, \"/MyRoot\"), {\n+            NLs::PathExist,\n+            NLs::PathsInsideDomain(0),\n+            NLs::ShardsInsideDomain(0)\n+        });\n+\n+        // check that extsubdomain's system tablets are deleted from the root hive\n+        // and not-working state of the tenant hive is unable to hinder that\n+        {\n+            const auto tablets = HiveGetSubdomainTablets(runtime, TTestTxConfig::Hive, subdomainPathId);\n+            UNIT_ASSERT_C(tablets.size() == 0, TStringBuilder()\n+                << \"-- existing subdomain's system tablets in the root hive: expected 0, got \" << tablets.size()\n+            );\n+        }\n+\n+        // check that extsubdomain's path is really erased from the root schemeshard\n+\n+        {\n+            const auto result = ReadLocalTableRecords(runtime, TTestTxConfig::SchemeShard, \"SystemShardsToDelete\", \"ShardIdx\");\n+            const auto records = NKikimr::NClient::TValue::Create(result)[0][\"List\"];\n+            //DEBUG:  Cerr << \"TEST: SystemShardsToDelete: \" << records.GetValueText<NKikimr::NClient::TFormatJSON>() << Endl;\n+            //DEBUG:  Cerr << \"TEST: \" << records.DumpToString() << Endl;\n+            UNIT_ASSERT_VALUES_EQUAL(records.Size(), 0);\n+        }\n+\n+        UNIT_ASSERT(!CheckLocalRowExists(runtime, TTestTxConfig::SchemeShard, \"SubDomains\", \"PathId\", subdomainPathId.LocalPathId));\n+        UNIT_ASSERT(!CheckLocalRowExists(runtime, TTestTxConfig::SchemeShard, \"Paths\", \"Id\", subdomainPathId.LocalPathId));\n+    }\n+\n     Y_UNIT_TEST_FLAG(CreateThenDropChangesParent, AlterDatabaseCreateHiveFirst) {\n         TTestBasicRuntime runtime;\n         TTestEnv env(runtime,\ndiff --git a/ydb/core/tx/schemeshard/ut_extsubdomain_reboots/ut_extsubdomain_reboots.cpp b/ydb/core/tx/schemeshard/ut_extsubdomain_reboots/ut_extsubdomain_reboots.cpp\nindex 97570db7133b..866c485d2e1a 100644\n--- a/ydb/core/tx/schemeshard/ut_extsubdomain_reboots/ut_extsubdomain_reboots.cpp\n+++ b/ydb/core/tx/schemeshard/ut_extsubdomain_reboots/ut_extsubdomain_reboots.cpp\n@@ -1,9 +1,8 @@\n-#include <ydb/core/protos/flat_scheme_op.pb.h>\n-#include <ydb/core/testlib/actors/wait_events.h>\n-#include <ydb/core/tx/datashard/datashard.h>\n #include <ydb/core/tx/schemeshard/ut_helpers/helpers.h>\n+#include <ydb/core/testlib/actors/wait_events.h>\n+\n+#include <ydb/public/lib/value/value.h>\n \n-#include <google/protobuf/text_format.h>\n \n using namespace NKikimr;\n using namespace NSchemeShard;\n@@ -13,7 +12,7 @@ Y_UNIT_TEST_SUITE(TSchemeShardTestExtSubdomainReboots) {\n     Y_UNIT_TEST(Fake) {\n     }\n \n-    Y_UNIT_TEST_FLAG(CreateExternalSubdomain, AlterDatabaseCreateHiveFirst) {\n+    Y_UNIT_TEST_FLAG(CreateExtSubdomainWithHive, AlterDatabaseCreateHiveFirst) {\n         TTestWithReboots t;\n         t.GetTestEnvOptions()\n             .EnableAlterDatabaseCreateHiveFirst(AlterDatabaseCreateHiveFirst)\n@@ -101,7 +100,142 @@ Y_UNIT_TEST_SUITE(TSchemeShardTestExtSubdomainReboots) {\n         });\n     }\n \n-    Y_UNIT_TEST_FLAG(CreateExternalSubdomainWithoutHive, AlterDatabaseCreateHiveFirst) {\n+    Y_UNIT_TEST_FLAGS(DropExtSubdomain, AlterDatabaseCreateHiveFirst, ExternalHive) {\n+        TTestWithReboots t;\n+        t.GetTestEnvOptions()\n+            .EnableAlterDatabaseCreateHiveFirst(AlterDatabaseCreateHiveFirst)\n+            .EnableRealSystemViewPaths(false);\n+\n+        t.Run([&](TTestActorRuntime& runtime, bool& activeZone) {\n+\n+            TestCreateExtSubDomain(runtime, ++t.TxId,  \"/MyRoot\",\n+                R\"(Name: \"USER_0\")\"\n+            );\n+            t.TestEnv->TestWaitNotification(runtime, t.TxId);\n+\n+            TPathId subdomainPathId;\n+            {\n+                TInactiveZone inactive(activeZone);\n+\n+                auto describe = DescribePath(runtime, \"/MyRoot/USER_0\");\n+                TestDescribeResult(describe, {\n+                    NLs::PathExist,\n+                    NLs::IsExternalSubDomain(\"USER_0\"),\n+                    NLs::DomainCoordinators({}),\n+                    NLs::DomainMediators({}),\n+                    NLs::DomainSchemeshard(0),\n+                    NLs::DomainHive(0)\n+                });\n+                TSubDomainKey subdomainKey = TSubDomainKey(describe.GetPathDescription().GetDomainDescription().GetDomainKey());\n+                subdomainPathId = TPathId(subdomainKey.GetSchemeShard(), subdomainKey.GetPathId());\n+                UNIT_ASSERT_VALUES_EQUAL(subdomainPathId.LocalPathId, 3);\n+\n+                TestDescribeResult(DescribePath(runtime, \"/MyRoot\"), {\n+                    NLs::ChildrenCount(2)\n+                });\n+\n+                UNIT_ASSERT(CheckLocalRowExists(runtime, TTestTxConfig::SchemeShard, \"Paths\", \"Id\", subdomainPathId.LocalPathId));\n+                UNIT_ASSERT(CheckLocalRowExists(runtime, TTestTxConfig::SchemeShard, \"SubDomains\", \"PathId\", subdomainPathId.LocalPathId));\n+\n+                // Register observer for future extsubdomain cleanup notification\n+                t.TestEnv->AddExtSubdomainCleanupObserver(runtime, subdomainPathId);\n+            }\n+\n+            TestAlterExtSubDomain(runtime, ++t.TxId,  \"/MyRoot\",\n+                Sprintf(R\"(\n+                        Name: \"USER_0\"\n+\n+                        StoragePools {\n+                            Name: \"tenant-1:hdd\"\n+                            Kind: \"hdd\"\n+                        }\n+                        PlanResolution: 50\n+                        Coordinators: 1\n+                        Mediators: 1\n+                        TimeCastBucketsPerMediator: 2\n+\n+                        ExternalHive: %s\n+                        ExternalSchemeShard: true\n+                    )\",\n+                    ToString(ExternalHive).c_str()\n+                )\n+            );\n+            t.TestEnv->TestWaitNotification(runtime, t.TxId);\n+\n+            ui64 tenantHiveId = 0;\n+            {\n+                TInactiveZone inactive(activeZone);\n+\n+                auto describe = DescribePath(runtime, \"/MyRoot/USER_0\");\n+                TestDescribeResult(describe, {\n+                    NLs::PathExist,\n+                    NLs::IsExternalSubDomain(\"USER_0\"),\n+                    NLs::ExtractDomainHive(&tenantHiveId),\n+                });\n+\n+                if (ExternalHive) {\n+                    // extsubdomain drop should be independent of tenant hive's state.\n+                    // It must correctly remove database whether tenant nodes and tablets are alive or not.\n+                    //\n+                    // Make tenant hive inaccessible by stopping its tablet.\n+                    // In real life that could be, for example, due to absence of tenant nodes.\n+                    //\n+                    // Tenant hive is controlled by the root hive (running at node 0).\n+                    HiveStopTablet(runtime, TTestTxConfig::Hive, tenantHiveId, 0);\n+                }\n+            }\n+\n+            // drop extsubdomain\n+            TestForceDropExtSubDomain(runtime, ++t.TxId, \"/MyRoot\", \"USER_0\");\n+            t.TestEnv->TestWaitNotification(runtime, t.TxId);\n+\n+            {\n+                TInactiveZone inactive(activeZone);\n+\n+                TestDescribeResult(DescribePath(runtime, \"/MyRoot/USER_0\"), {\n+                    NLs::PathNotExist\n+                });\n+\n+                TestDescribeResult(DescribePath(runtime, \"/MyRoot\"), {\n+                    NLs::PathExist,\n+                    NLs::ShardsInsideDomain(0)\n+                });\n+\n+                // wait for it to be really cleaned up\n+                t.TestEnv->WaitForExtSubdomainCleanup(runtime, subdomainPathId);\n+\n+                // check that extsubdomain's system tablets are deleted from the root hive\n+                // and not-working state of the tenant hive was unable to hinder that\n+                {\n+                    const auto tablets = HiveGetSubdomainTablets(runtime, TTestTxConfig::Hive, subdomainPathId);\n+                    UNIT_ASSERT_C(tablets.size() == 0, TStringBuilder()\n+                        << \"-- existing subdomain's system tablets in the root hive: expected 0, got \" << tablets.size()\n+                    );\n+                }\n+\n+                // check that extsubdomain's path is really erased from the root schemeshard\n+\n+                TestDescribeResult(DescribePath(runtime, \"/MyRoot\"), {\n+                    NLs::PathExist,\n+                    NLs::PathsInsideDomain(1)  // infamous /MyRoot/DirA, created in TTestWithReboots::Prepare()\n+                });\n+\n+                {\n+                    const auto result = ReadLocalTableRecords(runtime, TTestTxConfig::SchemeShard, \"SystemShardsToDelete\", \"ShardIdx\");\n+                    const auto records = NKikimr::NClient::TValue::Create(result)[0][\"List\"];\n+                    //DEBUG: Cerr << \"TEST: SystemShardsToDelete: \" << records.GetValueText<NKikimr::NClient::TFormatJSON>() << Endl;\n+                    //DEBUG: Cerr << \"TEST: \" << records.DumpToString() << Endl;\n+                    UNIT_ASSERT_VALUES_EQUAL(records.Size(), 0);\n+                }\n+\n+                UNIT_ASSERT(!CheckLocalRowExists(runtime, TTestTxConfig::SchemeShard, \"SubDomains\", \"PathId\", subdomainPathId.LocalPathId));\n+                UNIT_ASSERT(!CheckLocalRowExists(runtime, TTestTxConfig::SchemeShard, \"Paths\", \"Id\", subdomainPathId.LocalPathId));\n+            }\n+\n+        });\n+    }\n+\n+    Y_UNIT_TEST_FLAG(CreateExtSubdomainNoHive, AlterDatabaseCreateHiveFirst) {\n         TTestWithReboots t;\n         t.GetTestEnvOptions()\n             .EnableAlterDatabaseCreateHiveFirst(AlterDatabaseCreateHiveFirst)\n@@ -207,7 +341,7 @@ Y_UNIT_TEST_SUITE(TSchemeShardTestExtSubdomainReboots) {\n         });\n     }\n \n-    Y_UNIT_TEST_FLAG(AlterForceDrop, AlterDatabaseCreateHiveFirst) {\n+    Y_UNIT_TEST_FLAGS(AlterForceDrop, AlterDatabaseCreateHiveFirst, ExternalHive) {\n         TTestWithReboots t;\n         t.GetTestEnvOptions()\n             .EnableAlterDatabaseCreateHiveFirst(AlterDatabaseCreateHiveFirst)\n@@ -223,18 +357,22 @@ Y_UNIT_TEST_SUITE(TSchemeShardTestExtSubdomainReboots) {\n             }\n \n             AsyncAlterExtSubDomain(runtime, ++t.TxId,  \"/MyRoot\",\n-                R\"(\n-                    StoragePools {\n-                        Name: \"tenant-1:hdd\"\n-                        Kind: \"hdd\"\n-                    }\n-                    PlanResolution: 50\n-                    Coordinators: 3\n-                    Mediators: 2\n-                    TimeCastBucketsPerMediator: 2\n-                    ExternalSchemeShard: true\n-                    Name: \"USER_0\"\n-                )\"\n+                Sprintf(R\"(\n+                        StoragePools {\n+                            Name: \"tenant-1:hdd\"\n+                            Kind: \"hdd\"\n+                        }\n+                        PlanResolution: 50\n+                        Coordinators: 3\n+                        Mediators: 2\n+                        TimeCastBucketsPerMediator: 2\n+                        ExternalSchemeShard: true\n+                        Name: \"USER_0\"\n+\n+                        ExternalHive: %s\n+                    )\",\n+                    ToString(ExternalHive).c_str()\n+                )\n             );\n             t.TestEnv->ReliablePropose(runtime, ForceDropExtSubDomainRequest(++t.TxId, \"/MyRoot\", \"USER_0\"),\n                                        {NKikimrScheme::StatusAccepted});\n@@ -256,17 +394,18 @@ Y_UNIT_TEST_SUITE(TSchemeShardTestExtSubdomainReboots) {\n     }\n \n \n-    Y_UNIT_TEST_FLAG(SchemeLimits, AlterDatabaseCreateHiveFirst) {\n+    Y_UNIT_TEST_FLAGS(SchemeLimits, AlterDatabaseCreateHiveFirst, ExternalHive) {\n         TTestWithReboots t;\n         t.GetTestEnvOptions()\n             .EnableAlterDatabaseCreateHiveFirst(AlterDatabaseCreateHiveFirst)\n-            .EnableRealSystemViewPaths(false);\n+            .EnableRealSystemViewPaths(false)\n+        ;\n \n         t.Run([&](TTestActorRuntime& runtime, bool& activeZone) {\n             TSchemeLimits limits;\n             limits.MaxDepth = 2;\n-            limits.MaxShards = 3;\n             limits.MaxPaths = 2;\n+            limits.MaxShards = 3 + (ExternalHive ? 1 : 0);\n \n             {\n                 TInactiveZone inactive(activeZone);\n@@ -280,47 +419,40 @@ Y_UNIT_TEST_SUITE(TSchemeShardTestExtSubdomainReboots) {\n             }\n \n             TestAlterExtSubDomain(runtime, ++t.TxId,  \"/MyRoot\",\n-                R\"(\n-                    StoragePools {\n-                        Name: \"tenant-1:hdd\"\n-                        Kind: \"hdd\"\n-                    }\n-                    PlanResolution: 50\n-                    Coordinators: 1\n-                    Mediators: 1\n-                    TimeCastBucketsPerMediator: 2\n-                    ExternalSchemeShard: true\n-                    Name: \"USER_0\"\n-                )\"\n+                Sprintf(R\"(\n+                        StoragePools {\n+                            Name: \"tenant-1:hdd\"\n+                            Kind: \"hdd\"\n+                        }\n+                        PlanResolution: 50\n+                        Coordinators: 1\n+                        Mediators: 1\n+                        TimeCastBucketsPerMediator: 2\n+                        ExternalSchemeShard: true\n+                        Name: \"USER_0\"\n+\n+                        ExternalHive: %s\n+                    )\",\n+                    ToString(ExternalHive).c_str()\n+                )\n             );\n             t.TestEnv->TestWaitNotification(runtime, t.TxId);\n \n             {\n                 TInactiveZone inactive(activeZone);\n+\n+                ui64 subdomainSchemeshard;\n                 TestDescribeResult(DescribePath(runtime, \"/MyRoot/USER_0\"),\n                                    {NLs::PathExist,\n                                     NLs::IsExternalSubDomain(\"USER_0\"),\n+                                    NLs::ExtractTenantSchemeshard(&subdomainSchemeshard),\n+                                    NLs::ShardsInsideDomain(limits.MaxShards),\n                                     NLs::DomainLimitsIs(limits.MaxPaths, limits.MaxShards)});\n \n                 TestDescribeResult(DescribePath(runtime, \"/MyRoot\"),\n                                    {NLs::ChildrenCount(2),\n                                     NLs::DomainLimitsIs(limits.MaxPaths, limits.MaxShards)});\n \n-                ui64 subdomainSchemeshard = TTestTxConfig::FakeHiveTablets;\n-\n-                TestDescribeResult(DescribePath(runtime, subdomainSchemeshard, \"/MyRoot/USER_0\"),\n-                                   {NLs::PathExist,\n-                                    NLs::IsSubDomain(\"MyRoot/USER_0\"),\n-                                    NLs::DomainKey(3, TTestTxConfig::SchemeShard),\n-                                    // internal knowledge of shard declaration sequence is used here\n-                                    NLs::DomainSchemeshard(subdomainSchemeshard),\n-                                    NLs::DomainCoordinators({TTestTxConfig::FakeHiveTablets+1}),\n-                                    NLs::DomainMediators({TTestTxConfig::FakeHiveTablets+2}),\n-                                    NLs::DomainLimitsIs(limits.MaxPaths, limits.MaxShards),\n-                                    NLs::ShardsInsideDomain(3),\n-                                    NLs::PathsInsideDomain(0)\n-                                   });\n-\n                 TestCreateTable(runtime, subdomainSchemeshard, ++t.TxId, \"/MyRoot/USER_0\", R\"(\n                             Name: \"Table\"\n                             Columns { Name: \"Id\" Type: \"Uint32\" }\ndiff --git a/ydb/core/tx/schemeshard/ut_helpers/helpers.cpp b/ydb/core/tx/schemeshard/ut_helpers/helpers.cpp\nindex 5696753c4a86..a6b09223a99e 100644\n--- a/ydb/core/tx/schemeshard/ut_helpers/helpers.cpp\n+++ b/ydb/core/tx/schemeshard/ut_helpers/helpers.cpp\n@@ -44,6 +44,44 @@ namespace NSchemeShardUT_Private {\n         runtime.GrabEdgeEventRethrow<NConsole::TEvConsole::TEvConfigNotificationResponse>(handle);\n     }\n \n+    ////////// Hive\n+\n+    // Stop tablet.\n+    // Also see ydb/core/mind/hive/hive_ut.cpp, SendStopTablet\n+    void HiveStopTablet(TTestActorRuntime &runtime, ui64 hiveTablet, ui64 tabletId, ui32 nodeIndex) {\n+        TActorId senderB = runtime.AllocateEdgeActor(nodeIndex);\n+        runtime.SendToPipe(hiveTablet, senderB, new TEvHive::TEvStopTablet(tabletId), 0, GetPipeConfigWithRetries());\n+        TAutoPtr<IEventHandle> handle;\n+        auto event = runtime.GrabEdgeEventRethrow<TEvHive::TEvStopTabletResult>(handle);\n+        UNIT_ASSERT(event);\n+        const auto& stopResult = event->Record;\n+        UNIT_ASSERT_EQUAL_C(stopResult.GetTabletID(), tabletId, stopResult.GetTabletID() << \" != \" << tabletId);\n+        UNIT_ASSERT_EQUAL_C(stopResult.GetStatus(), NKikimrProto::OK, (ui32)stopResult.GetStatus() << \" != \" << (ui32)NKikimrProto::OK);\n+    }\n+\n+    // Retrieve tablets that belong to the given subdomain\n+    std::vector<NKikimrHive::TTabletInfo> HiveGetSubdomainTablets(TTestActorRuntime &runtime, const ui64 hiveTablet, const TPathId& subdomainPathId) {\n+        TActorId senderA = runtime.AllocateEdgeActor();\n+        runtime.SendToPipe(hiveTablet, senderA, new TEvHive::TEvRequestHiveInfo(), 0, GetPipeConfigWithRetries());\n+        TAutoPtr<IEventHandle> handle;\n+        auto event = runtime.GrabEdgeEventRethrow<TEvHive::TEvResponseHiveInfo>(handle);\n+        UNIT_ASSERT(event);\n+        const auto& response = event->Record;\n+\n+        // Cerr << \"TEST: HiveGetSubdomainTablets: \" << response.ShortDebugString() << Endl;\n+\n+        std::vector<NKikimrHive::TTabletInfo> result;\n+        std::copy_if(response.GetTablets().begin(), response.GetTablets().end(),\n+            std::back_inserter(result),\n+            [&subdomainPathId] (auto& tablet) {\n+                return (tablet.GetObjectDomain().GetSchemeShard() == subdomainPathId.OwnerId\n+                    && tablet.GetObjectDomain().GetPathId() == subdomainPathId.LocalPathId\n+                );\n+            }\n+        );\n+        return result;\n+    }\n+\n     template <typename TEvResponse, typename TEvRequest, typename TStatus>\n     static ui32 ReliableProposeImpl(\n         NActors::TTestActorRuntime& runtime, const TActorId& proposer,\n@@ -1421,6 +1459,30 @@ namespace NSchemeShardUT_Private {\n         return result.GetValue().GetStruct(0).GetOptional().HasOptional();\n     }\n \n+    // Read records from a local table by a single component key\n+    NKikimrMiniKQL::TResult ReadLocalTableRecords(TTestActorRuntime& runtime, ui64 tabletId, const TString& tableName, const TString& keyColumn) {\n+        const auto query = Sprintf(\n+            R\"(\n+                (\n+                    (let range '('('%s (Null) (Void))))\n+                    (let fields '('%s))\n+                    (return (AsList\n+                        (SetResult 'Result (SelectRange '%s range fields '()))\n+                    ))\n+                )\n+            )\",\n+            keyColumn.c_str(),\n+            keyColumn.c_str(),\n+            tableName.c_str()\n+        );\n+        auto result = LocalMiniKQL(runtime, tabletId, query);\n+        // Result: Value { Struct { Optional { Struct { List {\n+        //     Struct { Optional { Uint64: 2 } } }\n+        //     ...\n+        // } } } } }\n+        return result;\n+    };\n+\n     ui64 GetDatashardState(TTestActorRuntime& runtime, ui64 tabletId) {\n         NKikimrMiniKQL::TResult result;\n         TString err;\ndiff --git a/ydb/core/tx/schemeshard/ut_helpers/helpers.h b/ydb/core/tx/schemeshard/ut_helpers/helpers.h\nindex b0e65a9dc46f..f5a7fbe6255c 100644\n--- a/ydb/core/tx/schemeshard/ut_helpers/helpers.h\n+++ b/ydb/core/tx/schemeshard/ut_helpers/helpers.h\n@@ -83,6 +83,11 @@ namespace NSchemeShardUT_Private {\n     NKikimrMiniKQL::TResult LocalMiniKQL(TTestActorRuntime& runtime, ui64 tabletId, const TString& query);\n \n     bool CheckLocalRowExists(TTestActorRuntime& runtime, ui64 tabletId, const TString& tableName, const TString& keyColumn, ui64 keyValue);\n+    NKikimrMiniKQL::TResult ReadLocalTableRecords(TTestActorRuntime& runtime, ui64 tabletId, const TString& tableName, const TString& keyColumn);\n+\n+    ////////// hive\n+    void HiveStopTablet(TTestActorRuntime &runtime, ui64 hiveTablet, ui64 tabletId, ui32 nodeIndex);\n+    std::vector<NKikimrHive::TTabletInfo> HiveGetSubdomainTablets(TTestActorRuntime &runtime, const ui64 hiveTablet, const TPathId& subdomainPathId);\n \n     ////////// describe options\n     struct TDescribeOptionsBuilder : public NKikimrSchemeOp::TDescribeOptions {\n",
  "test_patch": "diff --git a/ydb/core/testlib/tablet_helpers.cpp b/ydb/core/testlib/tablet_helpers.cpp\nindex 4f4e0a2319b0..a517e2fedc77 100644\n--- a/ydb/core/testlib/tablet_helpers.cpp\n+++ b/ydb/core/testlib/tablet_helpers.cpp\n@@ -1151,6 +1151,7 @@ namespace NKikimr {\n                 HFunc(TEvHive::TEvAdoptTablet, Handle);\n                 HFunc(TEvHive::TEvDeleteTablet, Handle);\n                 HFunc(TEvHive::TEvDeleteOwnerTablets, Handle);\n+                HFunc(TEvHive::TEvStopTablet, Handle);\n                 HFunc(TEvHive::TEvRequestHiveInfo, Handle);\n                 HFunc(TEvHive::TEvInitiateTabletExternalBoot, Handle);\n                 HFunc(TEvHive::TEvUpdateTabletsObject, Handle);\n@@ -1179,7 +1180,7 @@ namespace NKikimr {\n \n         void Handle(TEvHive::TEvCreateTablet::TPtr& ev, const TActorContext& ctx) {\n             LOG_INFO_S(ctx, NKikimrServices::HIVE, \"[\" << TabletID() << \"] TEvCreateTablet, msg: \" << ev->Get()->Record.ShortDebugString());\n-            Cout << \"FAKEHIVE \" << TabletID() << \" TEvCreateTablet \" << ev->Get()->Record.ShortDebugString() << Endl;\n+            Cerr << \"FAKEHIVE \" << TabletID() << \" TEvCreateTablet \" << ev->Get()->Record.ShortDebugString() << Endl;\n             NKikimrProto::EReplyStatus status = NKikimrProto::OK;\n             const std::pair<ui64, ui64> key(ev->Get()->Record.GetOwner(), ev->Get()->Record.GetOwnerIdx());\n             const auto type = ev->Get()->Record.GetTabletType();\n@@ -1256,6 +1257,9 @@ namespace NKikimr {\n                 auto& boundChannels = ev->Get()->Record.GetBindedChannels();\n                 it->second.BoundChannels.assign(boundChannels.begin(), boundChannels.end());\n                 it->second.ChannelsProfile = ev->Get()->Record.GetChannelsProfile();\n+\n+                it->second.State = ETabletState::ReadyToWork;\n+                it->second.ObjectDomain = TSubDomainKey(ev->Get()->Record.GetObjectDomain());\n             }\n \n             ctx.Send(ev->Sender, new TEvHive::TEvCreateTabletReply(status, key.first,\n@@ -1355,7 +1359,7 @@ namespace NKikimr {\n         void Handle(TEvHive::TEvDeleteTablet::TPtr &ev, const TActorContext &ctx) {\n             LOG_INFO_S(ctx, NKikimrServices::HIVE, \"[\" << TabletID() << \"] TEvDeleteTablet, msg: \" << ev->Get()->Record.ShortDebugString());\n             NKikimrHive::TEvDeleteTablet& rec = ev->Get()->Record;\n-            Cout << \"FAKEHIVE \" << TabletID() << \" TEvDeleteTablet \" << rec.ShortDebugString() << Endl;\n+            Cerr << \"FAKEHIVE \" << TabletID() << \" TEvDeleteTablet \" << rec.ShortDebugString() << Endl;\n             TVector<ui64> deletedIdx;\n             for (size_t i = 0; i < rec.ShardLocalIdxSize(); ++i) {\n                 auto id = std::make_pair<ui64, ui64>(rec.GetShardOwnerId(), rec.GetShardLocalIdx(i));\n@@ -1368,7 +1372,7 @@ namespace NKikimr {\n         void Handle(TEvHive::TEvDeleteOwnerTablets::TPtr &ev, const TActorContext &ctx) {\n             LOG_INFO_S(ctx, NKikimrServices::HIVE, \"[\" << TabletID() << \"] TEvDeleteOwnerTablets, msg: \" << ev->Get()->Record);\n             NKikimrHive::TEvDeleteOwnerTablets& rec = ev->Get()->Record;\n-            Cout << \"FAKEHIVE \" << TabletID() << \" TEvDeleteOwnerTablets \" << rec.ShortDebugString() << Endl;\n+            Cerr << \"FAKEHIVE \" << TabletID() << \" TEvDeleteOwnerTablets \" << rec.ShortDebugString() << Endl;\n             auto ownerId = rec.GetOwner();\n             TVector<ui64> toDelete;\n \n@@ -1400,6 +1404,34 @@ namespace NKikimr {\n             ctx.Send(ev->Sender, new TEvHive::TEvDeleteOwnerTabletsReply(NKikimrProto::OK, TabletID(), ownerId, rec.GetTxId()));\n         }\n \n+        void StopTablet(const ui64& tabletId, const TActorContext &ctx) {\n+            auto ownerIt = State->TabletIdToOwner.find(tabletId);\n+            if (ownerIt == State->TabletIdToOwner.end()) {\n+                return;\n+            }\n+            auto it = State->Tablets.find(ownerIt->second);\n+            if (it == State->Tablets.end()) {\n+                return;\n+            }\n+\n+            TFakeHiveTabletInfo& tabletInfo = it->second;\n+\n+            // Very similar to DeleteTablet but don't actually removes tablet\n+            // Kill the tablet and don't restart it\n+            TActorId bootstrapperActorId = tabletInfo.BootstrapperActorId;\n+            ctx.Send(bootstrapperActorId, new TEvBootstrapper::TEvStandBy());\n+\n+            tabletInfo.State = ETabletState::Stopped;\n+        }\n+\n+        void Handle(TEvHive::TEvStopTablet::TPtr &ev, const TActorContext &ctx) {\n+            LOG_INFO_S(ctx, NKikimrServices::HIVE, \"[\" << TabletID() << \"] TEvStopTablet, msg: \" << ev->Get()->Record.ShortDebugString());\n+            NKikimrHive::TEvStopTablet& rec = ev->Get()->Record;\n+            Cerr << \"FAKEHIVE \" << TabletID() << \" TEvStopTablet \" << rec.ShortDebugString() << Endl;\n+            StopTablet(rec.GetTabletID(), ctx);\n+            ctx.Send(ev->Sender, new TEvHive::TEvStopTabletResult(NKikimrProto::OK, rec.GetTabletID()));\n+        }\n+\n         void Handle(TEvHive::TEvRequestHiveInfo::TPtr &ev, const TActorContext &ctx) {\n             LOG_INFO_S(ctx, NKikimrServices::HIVE, \"[\" << TabletID() << \"] TEvRequestHiveInfo, msg: \" << ev->Get()->Record.ShortDebugString());\n             const auto& record = ev->Get()->Record;\n@@ -1451,7 +1483,7 @@ namespace NKikimr {\n \n         void Handle(TEvHive::TEvUpdateDomain::TPtr &ev, const TActorContext &ctx) {\n             LOG_INFO_S(ctx, NKikimrServices::HIVE, \"[\" << TabletID() << \"] TEvUpdateDomain, msg: \" << ev->Get()->Record.ShortDebugString());\n-            \n+\n             const TSubDomainKey subdomainKey(ev->Get()->Record.GetDomainKey());\n             NHive::TDomainInfo& domainInfo = State->Domains[subdomainKey];\n             if (ev->Get()->Record.HasServerlessComputeResourcesMode()) {\n@@ -1459,7 +1491,7 @@ namespace NKikimr {\n             } else {\n                 domainInfo.ServerlessComputeResourcesMode.Clear();\n             }\n-            \n+\n             auto response = std::make_unique<TEvHive::TEvUpdateDomainReply>();\n             response->Record.SetTxId(ev->Get()->Record.GetTxId());\n             response->Record.SetOrigin(TabletID());\n@@ -1511,7 +1543,8 @@ namespace NKikimr {\n             tabletInfo.SetTabletID(tabletId);\n             if (info) {\n                 tabletInfo.SetTabletType(info->Type);\n-                tabletInfo.SetState(200); // THive::ReadyToWork\n+                tabletInfo.SetState(ui32(info->State)); // THive::ETabletState::*\n+                tabletInfo.MutableObjectDomain()->CopyFrom(info->ObjectDomain);\n \n                 // TODO: fill other fields when needed\n             }\ndiff --git a/ydb/core/testlib/tablet_helpers.h b/ydb/core/testlib/tablet_helpers.h\nindex 9ba0e58a5693..29ef83c5e195 100644\n--- a/ydb/core/testlib/tablet_helpers.h\n+++ b/ydb/core/testlib/tablet_helpers.h\n@@ -126,7 +126,7 @@ namespace NKikimr {\n                 : DomainKey(domainKey)\n             {}\n         };\n-        \n+\n         struct TEvRequestDomainInfoReply: public TEventLocal<TEvRequestDomainInfoReply, EvRequestDomainInfoReply> {\n             NHive::TDomainInfo DomainInfo;\n \n@@ -137,10 +137,20 @@ namespace NKikimr {\n \n     };\n \n+\n+    // partial mirror of NHive::ETabletState states from ydb/core/mind/hive/hive.h\n+    enum class ETabletState : ui64 {\n+        Unknown = 0,        // THive::ETabletState::Unknown\n+        Stopped = 100,      // THive::ETabletState::Stopped\n+        ReadyToWork = 200,  // THive::ETabletState::ReadyToWork\n+    };\n+\n     struct TFakeHiveTabletInfo {\n         const TTabletTypes::EType Type;\n         const ui64 TabletId;\n         TActorId BootstrapperActorId;\n+        ETabletState State = ETabletState::Unknown;\n+        TSubDomainKey ObjectDomain;  // what subdomain tablet belongs to\n \n         TChannelsBindings BoundChannels;\n         ui32 ChannelsProfile;\ndiff --git a/ydb/core/tx/schemeshard/ut_helpers/test_env.cpp b/ydb/core/tx/schemeshard/ut_helpers/test_env.cpp\nindex 79494db786e9..103d51790628 100644\n--- a/ydb/core/tx/schemeshard/ut_helpers/test_env.cpp\n+++ b/ydb/core/tx/schemeshard/ut_helpers/test_env.cpp\n@@ -956,6 +956,23 @@ void NSchemeShardUT_Private::TTestEnv::WaitForSysViewsRosterUpdate(NActors::TTes\n     }\n }\n \n+void NSchemeShardUT_Private::TTestEnv::AddExtSubdomainCleanupObserver(NActors::TTestActorRuntime& runtime, const TPathId& subdomainPathId) {\n+    Cerr << \"TESTENV: subdomain cleanup, start waiting, subdomain \" << subdomainPathId << Endl;\n+    ExtSubdomainCleanupComplete.erase(subdomainPathId);\n+    ExtSubdomainCleanupObserver = runtime.AddObserver<TEvPrivate::TEvTestNotifySubdomainCleanup>([this](const auto& ev) {\n+        ExtSubdomainCleanupComplete.insert(ev->Get()->SubdomainPathId);\n+    });\n+}\n+\n+void NSchemeShardUT_Private::TTestEnv::WaitForExtSubdomainCleanup(NActors::TTestActorRuntime& runtime, const TPathId& subdomainPathId) {\n+    runtime.WaitFor(\"ExtSubdomainCleanup\", [this, &subdomainPathId] {\n+        return ExtSubdomainCleanupComplete.contains(subdomainPathId);\n+    });\n+    ExtSubdomainCleanupComplete.erase(subdomainPathId);\n+    ExtSubdomainCleanupObserver.Remove();\n+    Cerr << \"TESTENV: subdomain cleanup, wait complete, subdomain \" << subdomainPathId << Endl;\n+}\n+\n void NSchemeShardUT_Private::TTestEnv::SimulateSleep(NActors::TTestActorRuntime &runtime, TDuration duration) {\n     auto sender = runtime.AllocateEdgeActor();\n     runtime.Schedule(new IEventHandle(sender, sender, new TEvents::TEvWakeup()), duration);\n@@ -1089,6 +1106,14 @@ NSchemeShardUT_Private::TTestWithReboots::TTestWithReboots(bool killOnCommit, NS\n     TabletIds.push_back(datashard+7);\n     TabletIds.push_back(datashard+8);\n \n+    // Events used exclusively by test frameworks.\n+    // No need for reboots on these as they aren't used in real operation mode.\n+    NoRebootEventTypes.insert(TEvPrivate::EvTestNotifySubdomainCleanup);\n+    NoRebootEventTypes.insert(TEvFakeHive::EvSubscribeToTabletDeletion);\n+    NoRebootEventTypes.insert(TEvFakeHive::EvNotifyTabletDeleted);\n+    NoRebootEventTypes.insert(TEvFakeHive::EvRequestDomainInfo);\n+    NoRebootEventTypes.insert(TEvFakeHive::EvRequestDomainInfoReply);\n+\n     NoRebootEventTypes.insert(TEvSchemeShard::EvModifySchemeTransaction);\n     NoRebootEventTypes.insert(TEvSchemeShard::EvDescribeScheme);\n     NoRebootEventTypes.insert(TEvSchemeShard::EvNotifyTxCompletion);\n@@ -1138,34 +1163,48 @@ struct NSchemeShardUT_Private::TTestWithReboots::TFinalizer {\n };\n \n void NSchemeShardUT_Private::TTestWithReboots::RunWithTabletReboots(std::function<void (TTestActorRuntime &, bool &)> testScenario) {\n-    RunTestWithReboots(TabletIds, [&]() {\n-        return [this](TTestActorRuntimeBase& runtime, TAutoPtr<IEventHandle>& event) {\n-            return PassUserRequests(runtime, event);\n-        };\n-    },\n-    [&](const TString& dispatchName, std::function<void(TTestActorRuntime&)> setup, bool& activeZone) {\n-        TFinalizer finalizer(*this);\n-        Prepare(dispatchName, setup, activeZone);\n-\n-        activeZone = true;\n-        testScenario(*Runtime, activeZone);\n-    }, Max<ui32>(), Max<ui64>(), 0, 0, KillOnCommit);\n+    RunTestWithReboots(\n+        TabletIds,\n+        // filterFactory\n+        [&]() {\n+            return [this](TTestActorRuntimeBase& runtime, TAutoPtr<IEventHandle>& event) {\n+                return PassUserRequests(runtime, event);\n+            };\n+        },\n+        // testFunc\n+        [&](const TString& dispatchName, std::function<void(TTestActorRuntime&)> setup, bool& activeZone) {\n+            TFinalizer finalizer(*this);\n+            Prepare(dispatchName, setup, activeZone);\n+\n+            activeZone = true;\n+            testScenario(*Runtime, activeZone);\n+        },\n+        Max<ui32>(),\n+        Max<ui64>(),\n+        0,\n+        0,\n+        KillOnCommit\n+    );\n }\n \n void NSchemeShardUT_Private::TTestWithReboots::RunWithPipeResets(std::function<void (TTestActorRuntime &, bool &)> testScenario) {\n-    RunTestWithPipeResets(TabletIds,\n-                          [&]() {\n-        return [this](TTestActorRuntimeBase& runtime, TAutoPtr<IEventHandle>& event) {\n-            return PassUserRequests(runtime, event);\n-        };\n-    },\n-    [&](const TString& dispatchName, std::function<void(TTestActorRuntime&)> setup, bool& activeZone) {\n-        TFinalizer finalizer(*this);\n-        Prepare(dispatchName, setup, activeZone);\n-\n-        activeZone = true;\n-        testScenario(*Runtime, activeZone);\n-    });\n+    RunTestWithPipeResets(\n+        TabletIds,\n+        // filterFactory\n+        [&]() {\n+            return [this](TTestActorRuntimeBase& runtime, TAutoPtr<IEventHandle>& event) {\n+                return PassUserRequests(runtime, event);\n+            };\n+        },\n+        // testFunc\n+        [&](const TString& dispatchName, std::function<void(TTestActorRuntime&)> setup, bool& activeZone) {\n+            TFinalizer finalizer(*this);\n+            Prepare(dispatchName, setup, activeZone);\n+\n+            activeZone = true;\n+            testScenario(*Runtime, activeZone);\n+        }\n+    );\n }\n \n void NSchemeShardUT_Private::TTestWithReboots::RunWithDelays(std::function<void (TTestActorRuntime &, bool &)> testScenario) {\ndiff --git a/ydb/core/tx/schemeshard/ut_helpers/test_env.h b/ydb/core/tx/schemeshard/ut_helpers/test_env.h\nindex 7081b6f5105a..36bdf3c4255f 100644\n--- a/ydb/core/tx/schemeshard/ut_helpers/test_env.h\n+++ b/ydb/core/tx/schemeshard/ut_helpers/test_env.h\n@@ -103,6 +103,9 @@ namespace NSchemeShardUT_Private {\n         TTestActorRuntime::TEventObserverHolder SysViewsRosterUpdateObserver;\n         bool SysViewsRosterUpdateFinished;\n \n+        TTestActorRuntime::TEventObserverHolder ExtSubdomainCleanupObserver;\n+        THashSet<TPathId> ExtSubdomainCleanupComplete;\n+\n     public:\n         static bool ENABLE_SCHEMESHARD_LOG;\n \n@@ -139,6 +142,9 @@ namespace NSchemeShardUT_Private {\n         void TestWaitShardDeletion(TTestActorRuntime& runtime, ui64 schemeShard, TSet<ui64> localIds);\n         void TestWaitShardDeletion(TTestActorRuntime& runtime, ui64 schemeShard, TSet<TShardIdx> shardIds);\n \n+        void AddExtSubdomainCleanupObserver(NActors::TTestActorRuntime& runtime, const TPathId& subdomainPathId);\n+        void WaitForExtSubdomainCleanup(NActors::TTestActorRuntime& runtime, const TPathId& subdomainPathId);\n+\n         void AddSysViewsRosterUpdateObserver(TTestActorRuntime& runtime);\n         void WaitForSysViewsRosterUpdate(TTestActorRuntime& runtime);\n \ndiff --git a/ydb/tests/functional/scheme_tests/canondata/tablet_scheme_tests.TestTabletSchemes.test_tablet_schemes_flat_schemeshard_/flat_schemeshard.schema b/ydb/tests/functional/scheme_tests/canondata/tablet_scheme_tests.TestTabletSchemes.test_tablet_schemes_flat_schemeshard_/flat_schemeshard.schema\nindex 9b732c627c86..20f30a119a4d 100644\n--- a/ydb/tests/functional/scheme_tests/canondata/tablet_scheme_tests.TestTabletSchemes.test_tablet_schemes_flat_schemeshard_/flat_schemeshard.schema\n+++ b/ydb/tests/functional/scheme_tests/canondata/tablet_scheme_tests.TestTabletSchemes.test_tablet_schemes_flat_schemeshard_/flat_schemeshard.schema\n@@ -8883,5 +8883,43 @@\n                 ]\n             }\n         }\n+    },\n+    {\n+        \"TableId\": 124,\n+        \"TableName\": \"SystemShardsToDelete\",\n+        \"TableKey\": [\n+            1\n+        ],\n+        \"ColumnsAdded\": [\n+            {\n+                \"ColumnId\": 1,\n+                \"ColumnName\": \"ShardIdx\",\n+                \"ColumnType\": \"Uint64\"\n+            }\n+        ],\n+        \"ColumnsDropped\": [],\n+        \"ColumnFamilies\": {\n+            \"0\": {\n+                \"Columns\": [\n+                    1\n+                ],\n+                \"RoomID\": 0,\n+                \"Codec\": 0,\n+                \"InMemory\": false,\n+                \"Cache\": 0,\n+                \"Small\": 4294967295,\n+                \"Large\": 4294967295\n+            }\n+        },\n+        \"Rooms\": {\n+            \"0\": {\n+                \"Main\": 1,\n+                \"Outer\": 1,\n+                \"Blobs\": 1,\n+                \"ExternalBlobs\": [\n+                    1\n+                ]\n+            }\n+        }\n     }\n ]\n\\ No newline at end of file\n",
  "problem_statement": "database drop may leave system tablets in root hive\nYDBOPS-12120\n\nInvestigate the problem, and find if it can be related to enabling feature flag `enable_alter_database_create_hive_first` in 25-1.\n\nUPD: No it is not. It seams that the issue is in race condition between deletion of tenant's system tablets (including tenant hive), which require tenant hive to be alive, and the actual disappearance of the tenant hive itself (due to tenant nodes shutdown for example).\n",
  "hints_text": "",
  "created_at": "2025-07-18T17:03:59Z"
}