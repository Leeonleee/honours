{
  "repo": "ydb-platform/ydb",
  "pull_number": 2975,
  "instance_id": "ydb-platform__ydb-2975",
  "issue_numbers": [
    "2974"
  ],
  "base_commit": "a8e2f918232da5594ed80c406dddd3cb78c69eb1",
  "patch": "diff --git a/ydb/core/mind/hive/drain.cpp b/ydb/core/mind/hive/drain.cpp\nindex 9b1de009e649..56f1b7b0d79b 100644\n--- a/ydb/core/mind/hive/drain.cpp\n+++ b/ydb/core/mind/hive/drain.cpp\n@@ -140,7 +140,7 @@ class THiveDrain : public NActors::TActorBootstrapped<THiveDrain>, public ISubAc\n         pipeConfig.RetryPolicy = {.RetryLimitCount = 13};\n         DomainHivePipeClient = Register(NTabletPipe::CreateClient(SelfId(), DomainHiveId, pipeConfig));\n         THolder<TEvHive::TEvDrainNode> event = MakeHolder<TEvHive::TEvDrainNode>(NodeId);\n-        event->Record.SetKeepDown(Settings.KeepDown);\n+        event->Record.SetDownPolicy(Settings.DownPolicy);\n         event->Record.SetPersist(Settings.Persist);\n         event->Record.SetDrainInFlight(Settings.DrainInFlight);\n         NTabletPipe::SendData(SelfId(), DomainHivePipeClient, event.Release());\ndiff --git a/ydb/core/mind/hive/hive.h b/ydb/core/mind/hive/hive.h\nindex 6a911d39805d..1f8fb4e2ee84 100644\n--- a/ydb/core/mind/hive/hive.h\n+++ b/ydb/core/mind/hive/hive.h\n@@ -268,7 +268,7 @@ struct THiveSharedSettings {\n \n struct TDrainSettings {\n     bool Persist = true;\n-    bool KeepDown = false;\n+    NKikimrHive::EDrainDownPolicy DownPolicy = NKikimrHive::EDrainDownPolicy::DRAIN_POLICY_KEEP_DOWN_UNTIL_RESTART;\n     ui32 DrainInFlight = 0;\n };\n \ndiff --git a/ydb/core/mind/hive/hive_impl.cpp b/ydb/core/mind/hive/hive_impl.cpp\nindex ff08db47acd0..5e3dffea4cb3 100644\n--- a/ydb/core/mind/hive/hive_impl.cpp\n+++ b/ydb/core/mind/hive/hive_impl.cpp\n@@ -2126,10 +2126,20 @@ void THive::Handle(TEvHive::TEvCutTabletHistory::TPtr& ev) {\n }\n \n void THive::Handle(TEvHive::TEvDrainNode::TPtr& ev) {\n+    NKikimrHive::EDrainDownPolicy policy;\n+    if (!ev->Get()->Record.HasDownPolicy() && ev->Get()->Record.HasKeepDown()) {\n+        if (ev->Get()->Record.GetKeepDown()) {\n+            policy = NKikimrHive::EDrainDownPolicy::DRAIN_POLICY_KEEP_DOWN;\n+        } else {\n+            policy = NKikimrHive::EDrainDownPolicy::DRAIN_POLICY_NO_DOWN;\n+        }\n+    } else {\n+        policy = ev->Get()->Record.GetDownPolicy();\n+    }\n     Execute(CreateSwitchDrainOn(ev->Get()->Record.GetNodeID(),\n     {\n         .Persist = ev->Get()->Record.GetPersist(),\n-        .KeepDown = ev->Get()->Record.GetKeepDown(),\n+        .DownPolicy = policy,\n         .DrainInFlight = ev->Get()->Record.GetDrainInFlight(),\n     }, ev->Sender));\n }\ndiff --git a/ydb/core/mind/hive/hive_schema.h b/ydb/core/mind/hive/hive_schema.h\nindex f2239a99282f..763deee1b887 100644\n--- a/ydb/core/mind/hive/hive_schema.h\n+++ b/ydb/core/mind/hive/hive_schema.h\n@@ -193,9 +193,10 @@ struct Schema : NIceDb::Schema {\n         struct DrainInitiators : Column<8, NScheme::NTypeIds::String> { using Type = TVector<TActorId>; };\n         struct Location : Column<9, NScheme::NTypeIds::String> { using Type = NActorsInterconnect::TNodeLocation; };\n         struct Name : Column<10, NScheme::NTypeIds::String> {};\n+        struct BecomeUpOnRestart : Column<11, NScheme::NTypeIds::Bool> {};\n \n         using TKey = TableKey<ID>;\n-        using TColumns = TableColumns<ID, Local, Down, Freeze, ServicedDomains, Statistics, Drain, DrainInitiators, Location, Name>;\n+        using TColumns = TableColumns<ID, Local, Down, Freeze, ServicedDomains, Statistics, Drain, DrainInitiators, Location, Name, BecomeUpOnRestart>;\n     };\n \n     struct TabletCategory : Table<6> {\ndiff --git a/ydb/core/mind/hive/hive_ut.cpp b/ydb/core/mind/hive/hive_ut.cpp\nindex 3bd80fae7d76..84aed81472ee 100644\n--- a/ydb/core/mind/hive/hive_ut.cpp\n+++ b/ydb/core/mind/hive/hive_ut.cpp\n@@ -972,6 +972,91 @@ Y_UNIT_TEST_SUITE(THiveTest) {\n         UNIT_ASSERT_VALUES_EQUAL(tabletStates[NKikimrWhiteboard::TTabletStateInfo::Dead], drainMovements);\n     }\n \n+    Y_UNIT_TEST(TestDownAfterDrain) {\n+        // 1. Drain node\n+        // 2. Create some more tablets\n+        // 3. Ensure none of them started on the node\n+        // 4. Restart the node\n+        // 5. Create more tablets\n+        // 6. Ensure that now there are tablets on the node\n+\n+        const int NUM_NODES = 3;\n+        const int NUM_TABLETS = 10;\n+        TTestBasicRuntime runtime(NUM_NODES, false);\n+        Setup(runtime, true);\n+        const ui64 hiveTablet = MakeDefaultHiveID();\n+        const ui64 testerTablet = MakeTabletID(false, 1);\n+        CreateTestBootstrapper(runtime, CreateTestTabletInfo(hiveTablet, TTabletTypes::Hive), &CreateDefaultHive);\n+        {\n+            TDispatchOptions options;\n+            options.FinalEvents.emplace_back(TEvLocal::EvStatus, NUM_NODES);\n+            runtime.DispatchEvents(options);\n+        }\n+        TTabletTypes::EType tabletType = TTabletTypes::Dummy;\n+        std::unordered_set<TTabletId> tablets;\n+        TActorId senderA = runtime.AllocateEdgeActor(0);\n+        auto createTablets = [&] {\n+            for (int i = 0; i < NUM_TABLETS; ++i) {\n+                THolder<TEvHive::TEvCreateTablet> ev(new TEvHive::TEvCreateTablet(testerTablet, 100500 + tablets.size() + i, tabletType, BINDED_CHANNELS));\n+                runtime.SendToPipe(hiveTablet, senderA, ev.Release(), 0, GetPipeConfigWithRetries());\n+            }\n+            for (int i = 0; i < NUM_TABLETS; ++i) {\n+                TAutoPtr<IEventHandle> handle;\n+                auto createTabletReply = runtime.GrabEdgeEventRethrow<TEvHive::TEvCreateTabletReply>(handle);\n+                ui64 tabletId = createTabletReply->Record.GetTabletID();\n+                tablets.insert(tabletId);\n+            }\n+            NTabletPipe::TClientConfig pipeConfig;\n+            pipeConfig.RetryPolicy = NTabletPipe::TClientRetryPolicy::WithRetries();\n+            for (TTabletId tabletId : tablets) {\n+                Ctest << \"wait for tablet \" << tabletId << Endl;\n+                MakeSureTabletIsUp(runtime, tabletId, 0, &pipeConfig);\n+            }\n+        };\n+\n+        createTablets();\n+\n+        ui32 nodeId = runtime.GetNodeId(0);\n+        {\n+            runtime.SendToPipe(hiveTablet, senderA, new TEvHive::TEvDrainNode(nodeId));\n+            TAutoPtr<IEventHandle> handle;\n+            auto drainResponse = runtime.GrabEdgeEventRethrow<TEvHive::TEvDrainNodeResult>(handle, TDuration::Seconds(30));\n+            UNIT_ASSERT_VALUES_EQUAL(drainResponse->Record.GetStatus(), NKikimrProto::EReplyStatus::OK);\n+        }\n+\n+        auto isNodeEmpty = [&](ui32 nodeId) -> bool {\n+            bool empty = true;\n+            TAutoPtr<IEventHandle> handle;\n+            TActorId whiteboard = NNodeWhiteboard::MakeNodeWhiteboardServiceId(nodeId);\n+            runtime.Send(new IEventHandle(whiteboard, senderA, new NNodeWhiteboard::TEvWhiteboard::TEvTabletStateRequest()));\n+            NNodeWhiteboard::TEvWhiteboard::TEvTabletStateResponse* wbResponse = runtime.GrabEdgeEventRethrow<NNodeWhiteboard::TEvWhiteboard::TEvTabletStateResponse>(handle);\n+            for (const NKikimrWhiteboard::TTabletStateInfo& tabletInfo : wbResponse->Record.GetTabletStateInfo()) {\n+                if (tablets.contains(tabletInfo.GetTabletId()) && tabletInfo.GetState() != NKikimrWhiteboard::TTabletStateInfo::Dead) {\n+                    Ctest << \"Tablet \" << tabletInfo.GetTabletId() << \".\" << tabletInfo.GetFollowerId()\n+                        << \" is not dead yet (\" << NKikimrWhiteboard::TTabletStateInfo::ETabletState_Name(tabletInfo.GetState()) << \")\" << Endl;\n+                    empty = false;\n+                }\n+            }\n+            return empty;\n+        };\n+\n+        createTablets();\n+\n+        UNIT_ASSERT(isNodeEmpty(nodeId));\n+\n+        SendKillLocal(runtime, 0);\n+        CreateLocal(runtime, 0);\n+        {\n+            TDispatchOptions options;\n+            options.FinalEvents.emplace_back(TEvLocal::EvStatus, 2);\n+            runtime.DispatchEvents(options);\n+        }\n+\n+        createTablets();\n+\n+        UNIT_ASSERT(!isNodeEmpty(nodeId));\n+    }\n+\n     Y_UNIT_TEST(TestCreateSubHiveCreateTablet) {\n         TTestBasicRuntime runtime(1, false);\n         Setup(runtime, true);\ndiff --git a/ydb/core/mind/hive/monitoring.cpp b/ydb/core/mind/hive/monitoring.cpp\nindex e9e5f91b9c1f..7f4ff6b3a1ea 100644\n--- a/ydb/core/mind/hive/monitoring.cpp\n+++ b/ydb/core/mind/hive/monitoring.cpp\n@@ -2691,9 +2691,9 @@ class TTxMonEvent_DrainNode : public TTransactionBase<THive> {\n \n     void Complete(const TActorContext& ctx) override {\n         if (Wait) {\n-            Self->Execute(Self->CreateSwitchDrainOn(NodeId, {.Persist = true, .KeepDown = true}, WaitActorId));\n+            Self->Execute(Self->CreateSwitchDrainOn(NodeId, {}, WaitActorId));\n         } else {\n-            Self->Execute(Self->CreateSwitchDrainOn(NodeId, {.Persist = true, .KeepDown = true}, {}));\n+            Self->Execute(Self->CreateSwitchDrainOn(NodeId, {}, {}));\n             ctx.Send(Source, new NMon::TEvRemoteJsonInfoRes(\"{\\\"status\\\":\\\"SCHEDULED\\\"}\"));\n         }\n     }\ndiff --git a/ydb/core/mind/hive/node_info.h b/ydb/core/mind/hive/node_info.h\nindex e5c3149872fa..34fa1a28661d 100644\n--- a/ydb/core/mind/hive/node_info.h\n+++ b/ydb/core/mind/hive/node_info.h\n@@ -64,6 +64,7 @@ struct TNodeInfo {\n     bool Down;\n     bool Freeze;\n     bool Drain;\n+    bool BecomeUpOnRestart = false;\n     TVector<TActorId> DrainInitiators;\n     TDrainSettings DrainSettings;\n     std::unordered_map<TTabletInfo::EVolatileState, std::unordered_set<TTabletInfo*>> Tablets;\ndiff --git a/ydb/core/mind/hive/tx__load_everything.cpp b/ydb/core/mind/hive/tx__load_everything.cpp\nindex 5083f3bd0ce1..3d5289845494 100644\n--- a/ydb/core/mind/hive/tx__load_everything.cpp\n+++ b/ydb/core/mind/hive/tx__load_everything.cpp\n@@ -315,6 +315,7 @@ class TTxLoadEverything : public TTransactionBase<THive> {\n                 node.ServicedDomains = nodeRowset.GetValueOrDefault<Schema::Node::ServicedDomains>();\n                 node.Statistics = nodeRowset.GetValueOrDefault<Schema::Node::Statistics>();\n                 node.Name = nodeRowset.GetValueOrDefault<Schema::Node::Name>();\n+                node.BecomeUpOnRestart = nodeRowset.GetValueOrDefault<Schema::Node::BecomeUpOnRestart>(false);\n                 if (nodeRowset.HaveValue<Schema::Node::Location>()) {\n                     auto location = nodeRowset.GetValue<Schema::Node::Location>();\n                     if (location.HasDataCenter()) {\ndiff --git a/ydb/core/mind/hive/tx__register_node.cpp b/ydb/core/mind/hive/tx__register_node.cpp\nindex 1928c96a1b03..e03431ef4789 100644\n--- a/ydb/core/mind/hive/tx__register_node.cpp\n+++ b/ydb/core/mind/hive/tx__register_node.cpp\n@@ -56,6 +56,11 @@ class TTxRegisterNode : public TTransactionBase<THive> {\n                 node.SetFreeze(false);\n                 db.Table<Schema::Node>().Key(nodeId).Update<Schema::Node::Down, Schema::Node::Freeze>(false, false);\n             }\n+            if (node.BecomeUpOnRestart) {\n+                node.SetDown(false);\n+                node.BecomeUpOnRestart = false;\n+                db.Table<Schema::Node>().Key(nodeId).Update<Schema::Node::Down, Schema::Node::BecomeUpOnRestart>(false, false);\n+            }\n             node.Local = Local;\n             node.ServicedDomains.swap(servicedDomains);\n             node.LastSeenServicedDomains = node.ServicedDomains;\ndiff --git a/ydb/core/mind/hive/tx__status.cpp b/ydb/core/mind/hive/tx__status.cpp\nindex 2cacf39804a4..d3fce32ec96f 100644\n--- a/ydb/core/mind/hive/tx__status.cpp\n+++ b/ydb/core/mind/hive/tx__status.cpp\n@@ -49,7 +49,7 @@ class TTxStatus : public TTransactionBase<THive> {\n             Self->ProcessWaitQueue(); // new node connected\n             if (node.Drain && Self->BalancerNodes.count(nodeId) == 0) {\n                 BLOG_D(\"THive::TTxStatus(\" << nodeId << \")::Complete - continuing node drain\");\n-                Self->StartHiveDrain(nodeId, {.Persist = true, .KeepDown = node.Down});\n+                Self->StartHiveDrain(nodeId, {.Persist = true, .DownPolicy = NKikimrHive::EDrainDownPolicy::DRAIN_POLICY_NO_DOWN});\n             }\n             Self->ObjectDistributions.AddNode(node);\n         } else {\ndiff --git a/ydb/core/mind/hive/tx__switch_drain.cpp b/ydb/core/mind/hive/tx__switch_drain.cpp\nindex 1a09d87993a4..1e87a70b1e49 100644\n--- a/ydb/core/mind/hive/tx__switch_drain.cpp\n+++ b/ydb/core/mind/hive/tx__switch_drain.cpp\n@@ -19,7 +19,7 @@ class TTxSwitchDrainOn : public TTransactionBase<THive> {\n \n     bool Execute(TTransactionContext& txc, const TActorContext&) override {\n         BLOG_D(\"THive::TTxSwitchDrainOn::Execute Node: \" << NodeId\n-                << \" Persist: \" << Settings.Persist << \" KeepDown: \" << Settings.KeepDown);\n+                << \" Persist: \" << Settings.Persist << \" DownPolicy: \" << static_cast<int>(Settings.DownPolicy));\n         NIceDb::TNiceDb db(txc.DB);\n         TNodeInfo* node = Self->FindNode(NodeId);\n         if (node != nullptr) {\n@@ -27,18 +27,18 @@ class TTxSwitchDrainOn : public TTransactionBase<THive> {\n                 Status = NKikimrProto::ALREADY; // another balancer is active on the node\n             } else {\n                 Status = NKikimrProto::OK;\n-                if (!node->Drain && node->Down) {\n-                    Settings.KeepDown = true;\n-                }\n                 node->Drain = true;\n                 node->DrainInitiators.emplace_back(Initiator);\n                 if (Settings.Persist) {\n                     db.Table<Schema::Node>().Key(NodeId).Update<Schema::Node::Drain, Schema::Node::DrainInitiators>(node->Drain, node->DrainInitiators);\n                 }\n-                if (Settings.KeepDown) {\n+                if (Settings.DownPolicy != NKikimrHive::EDrainDownPolicy::DRAIN_POLICY_NO_DOWN) {\n+                    if (!node->Down && Settings.DownPolicy == NKikimrHive::EDrainDownPolicy::DRAIN_POLICY_KEEP_DOWN_UNTIL_RESTART) {\n+                        node->BecomeUpOnRestart = true;\n+                    }\n                     node->SetDown(true);\n                     if (Settings.Persist) {\n-                        db.Table<Schema::Node>().Key(NodeId).Update<Schema::Node::Down>(true);\n+                        db.Table<Schema::Node>().Key(NodeId).Update<Schema::Node::Down, Schema::Node::BecomeUpOnRestart>(true, node->BecomeUpOnRestart);\n                     }\n                 }\n                 Self->StartHiveDrain(NodeId, std::move(Settings));\n@@ -84,7 +84,7 @@ class TTxSwitchDrainOff : public TTransactionBase<THive> {\n             node->Drain = false;\n             node->DrainInitiators.clear();\n             db.Table<Schema::Node>().Key(NodeId).Update<Schema::Node::Drain, Schema::Node::DrainInitiators>(node->Drain, node->DrainInitiators);\n-            if (!Settings.KeepDown) {\n+            if (Settings.DownPolicy == NKikimrHive::EDrainDownPolicy::DRAIN_POLICY_NO_DOWN) {\n                 // node->SetDown(false); // it has already been dropped by Drain actor\n                 if (Settings.Persist) {\n                     db.Table<Schema::Node>().Key(NodeId).Update<Schema::Node::Down>(false);\ndiff --git a/ydb/core/protos/hive.proto b/ydb/core/protos/hive.proto\nindex 64f4f250060a..bcfed0a868d9 100644\n--- a/ydb/core/protos/hive.proto\n+++ b/ydb/core/protos/hive.proto\n@@ -47,6 +47,12 @@ enum EMigrationState {\n     MIGRATION_COMPLETE = 3;\n }\n \n+enum EDrainDownPolicy {\n+    DRAIN_POLICY_NO_DOWN = 0;\n+    DRAIN_POLICY_KEEP_DOWN_UNTIL_RESTART = 1;\n+    DRAIN_POLICY_KEEP_DOWN = 2;\n+}\n+\n message TChannelInfo {\n     message THistorySlot {\n         optional uint32 FromGeneration = 1;\n@@ -355,8 +361,9 @@ message TTabletOwner {\n message TEvDrainNode {\n     optional uint32 NodeID = 1;\n     optional bool Persist = 3 [default = true];\n-    optional bool KeepDown = 4 [default = false];\n+    optional bool KeepDown = 4 [default = false]; // deprecated in favor of DownPolicy\n     optional uint32 DrainInFlight = 5;\n+    optional EDrainDownPolicy DownPolicy = 6 [default = DRAIN_POLICY_KEEP_DOWN_UNTIL_RESTART];\n }\n \n message TEvDrainNodeResult {\n",
  "test_patch": "diff --git a/ydb/tests/functional/scheme_tests/canondata/tablet_scheme_tests.TestTabletSchemes.test_tablet_schemes_flat_hive_/flat_hive.schema b/ydb/tests/functional/scheme_tests/canondata/tablet_scheme_tests.TestTabletSchemes.test_tablet_schemes_flat_hive_/flat_hive.schema\nindex 6eeaca17c779..86fb8c6e5ab7 100644\n--- a/ydb/tests/functional/scheme_tests/canondata/tablet_scheme_tests.TestTabletSchemes.test_tablet_schemes_flat_hive_/flat_hive.schema\n+++ b/ydb/tests/functional/scheme_tests/canondata/tablet_scheme_tests.TestTabletSchemes.test_tablet_schemes_flat_hive_/flat_hive.schema\n@@ -652,6 +652,11 @@\n                 \"ColumnId\": 10,\n                 \"ColumnName\": \"Name\",\n                 \"ColumnType\": \"String\"\n+            },\n+            {\n+                \"ColumnId\": 11,\n+                \"ColumnName\": \"BecomeUpOnRestart\",\n+                \"ColumnType\": \"Bool\"\n             }\n         ],\n         \"ColumnsDropped\": [],\n@@ -667,7 +672,8 @@\n                     6,\n                     8,\n                     9,\n-                    10\n+                    10,\n+                    11\n                 ],\n                 \"RoomID\": 0,\n                 \"Codec\": 0,\n",
  "problem_statement": "by default drain must set node to down and keep this flag until restart\n\n",
  "hints_text": "",
  "created_at": "2024-03-19T22:19:21Z"
}