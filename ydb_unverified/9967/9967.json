{
  "repo": "ydb-platform/ydb",
  "pull_number": 9967,
  "instance_id": "ydb-platform__ydb-9967",
  "issue_numbers": [
    "6486"
  ],
  "base_commit": "576812d9c73b2e0850b013afb57d158c76c76dcb",
  "patch": "diff --git a/ydb/core/kqp/ut/common/kqp_ut_common.cpp b/ydb/core/kqp/ut/common/kqp_ut_common.cpp\nindex 9997745b5da9..2f7e5668acce 100644\n--- a/ydb/core/kqp/ut/common/kqp_ut_common.cpp\n+++ b/ydb/core/kqp/ut/common/kqp_ut_common.cpp\n@@ -1289,7 +1289,6 @@ THolder<NSchemeCache::TSchemeCacheNavigate> Navigate(TTestActorRuntime& runtime,\n {\n     auto &runtime = *server->GetRuntime();\n     TAutoPtr<IEventHandle> handle;\n-    TVector<ui64> shards;\n \n     auto request = MakeHolder<TEvTxUserProxy::TEvNavigate>();\n     request->Record.MutableDescribePath()->SetPath(path);\ndiff --git a/ydb/core/scheme/scheme_tablecell.cpp b/ydb/core/scheme/scheme_tablecell.cpp\nindex c2b541bb7984..77c0e3319196 100644\n--- a/ydb/core/scheme/scheme_tablecell.cpp\n+++ b/ydb/core/scheme/scheme_tablecell.cpp\n@@ -349,13 +349,7 @@ size_t TOwnedCellVecBatch::Append(TConstArrayRef<TCell> cells) {\n         return 0;\n     }\n \n-    size_t size = sizeof(TCell) * cellsSize;\n-    for (auto& cell : cells) {\n-        if (!cell.IsNull() && !cell.IsInline()) {\n-            const size_t cellSize = cell.Size();\n-            size += AlignUp(cellSize);\n-        }\n-    }\n+    size_t size = EstimateSize(cells);\n \n     char * allocatedBuffer = reinterpret_cast<char *>(Pool->Allocate(size));\n \ndiff --git a/ydb/core/scheme/scheme_tablecell.h b/ydb/core/scheme/scheme_tablecell.h\nindex d2abd9f5d548..752be860abc6 100644\n--- a/ydb/core/scheme/scheme_tablecell.h\n+++ b/ydb/core/scheme/scheme_tablecell.h\n@@ -169,6 +169,20 @@ struct TCell {\n static_assert(sizeof(TCell) == 12, \"TCell must be 12 bytes\");\n using TCellsRef = TConstArrayRef<const TCell>;\n \n+inline size_t EstimateSize(TCellsRef cells) {\n+    size_t cellsSize = cells.size();\n+\n+    size_t size = sizeof(TCell) * cellsSize;\n+    for (auto& cell : cells) {\n+        if (!cell.IsNull() && !cell.IsInline()) {\n+            const size_t cellSize = cell.Size();\n+            size += AlignUp(cellSize);\n+        }\n+    }\n+    \n+    return size;\n+}\n+\n inline int CompareCellsAsByteString(const TCell& a, const TCell& b, bool isDescending) {\n     const char* pa = (const char*)a.Data();\n     const char* pb = (const char*)b.Data();\ndiff --git a/ydb/core/tablet_flat/flat_executor_tx_env.h b/ydb/core/tablet_flat/flat_executor_tx_env.h\nindex 48d5a1c41ccf..31e26deb3d8d 100644\n--- a/ydb/core/tablet_flat/flat_executor_tx_env.h\n+++ b/ydb/core/tablet_flat/flat_executor_tx_env.h\n@@ -29,7 +29,13 @@ namespace NTabletFlatExecutor {\n         {\n             auto *partStore = CheckedCast<const NTable::TPartStore*>(part);\n \n-            return { true, Lookup(partStore->Locate(lob, ref), ref) };\n+            const TSharedData* page = Lookup(partStore->Locate(lob, ref), ref);\n+\n+            if (!page && ReadMissingReferences) {\n+                MissingReferencesSize_ += Max<ui64>(1, part->GetPageSize(lob, ref));\n+            }\n+\n+            return { !ReadMissingReferences, page };\n         }\n \n         const TSharedData* TryGetPage(const TPart* part, TPageId page, TGroupId groupId) override\n@@ -39,6 +45,20 @@ namespace NTabletFlatExecutor {\n             return Lookup(partStore->PageCollections.at(groupId.Index).Get(), page);\n         }\n \n+        void EnableReadMissingReferences() noexcept {\n+            ReadMissingReferences = true;\n+        }\n+\n+        void DisableReadMissingReferences() noexcept {\n+            ReadMissingReferences = false;\n+            MissingReferencesSize_ = 0;\n+        }\n+\n+        ui64 MissingReferencesSize() const noexcept\n+        { \n+            return MissingReferencesSize_;\n+        }\n+\n     private:\n         const TSharedData* Lookup(TPrivatePageCache::TInfo *info, TPageId pageId) noexcept\n         {\n@@ -47,6 +67,11 @@ namespace NTabletFlatExecutor {\n \n     public:\n         TPrivatePageCache& Cache;\n+    \n+    private:\n+        bool ReadMissingReferences = false;\n+\n+        ui64 MissingReferencesSize_ = 0;\n     };\n \n     struct TPageCollectionTxEnv : public TPageCollectionReadEnv, public IExecuting {\n@@ -187,6 +212,20 @@ namespace NTabletFlatExecutor {\n             LoanConfirmation.insert(std::make_pair(bundle, TLoanConfirmation{borrow}));\n         }\n \n+        void EnableReadMissingReferences() noexcept override\n+        {\n+            TPageCollectionReadEnv::EnableReadMissingReferences();\n+        }\n+\n+        void DisableReadMissingReferences() noexcept override\n+        {\n+            TPageCollectionReadEnv::DisableReadMissingReferences();\n+        }\n+\n+        ui64 MissingReferencesSize() const noexcept override\n+        {\n+            return TPageCollectionReadEnv::MissingReferencesSize();\n+        }\n     protected:\n         NTable::TDatabase& DB;\n \ndiff --git a/ydb/core/tablet_flat/flat_part_iter.h b/ydb/core/tablet_flat/flat_part_iter.h\nindex 84c2843a4c14..42171084fe2b 100644\n--- a/ydb/core/tablet_flat/flat_part_iter.h\n+++ b/ydb/core/tablet_flat/flat_part_iter.h\n@@ -1319,6 +1319,7 @@ namespace NTable {\n \n                 if (ref >> (sizeof(ui32) * 8))\n                     Y_ABORT(\"Upper bits of ELargeObj ref now isn't used\");\n+\n                 if (auto blob = Env->Locate(Part, ref, op)) {\n                     const auto got = NPage::TLabelWrapper().Read(**blob);\n \n@@ -1332,13 +1333,12 @@ namespace NTable {\n                 } else {\n                     Y_ABORT_UNLESS(ref < (*Part->Blobs)->size(), \"out of blobs catalog\");\n \n+                    op = TCellOp(blob.Need ? ECellOp::Null : ECellOp(op), ELargeObj::GlobId);\n+\n                     /* Have to preserve reference to memory with TGlobId until\n                         of next iterator alteration method invocation. This is\n                         why here direct array of TGlobId is used.\n                     */\n-\n-                    op = TCellOp(blob.Need ? ECellOp::Null : ECellOp(op), ELargeObj::GlobId);\n-\n                     row.Set(pin.To, op, TCell::Make((**Part->Blobs)[ref]));\n                 }\n             } else {\ndiff --git a/ydb/core/tablet_flat/flat_part_store.h b/ydb/core/tablet_flat/flat_part_store.h\nindex daa26955e5b8..34facda3de8a 100644\n--- a/ydb/core/tablet_flat/flat_part_store.h\n+++ b/ydb/core/tablet_flat/flat_part_store.h\n@@ -79,6 +79,13 @@ class TPartStore : public TPart, public IBundle {\n         return PageCollections[groupId.Index]->GetPageSize(pageId);\n     }\n \n+    ui64 GetPageSize(ELargeObj lob, ui64 ref) const override\n+    {\n+        auto* cache = Locate(lob, ref);\n+\n+        return cache->PageCollection->Page(ref).Size;\n+    }\n+\n     NPage::EPage GetPageType(NPage::TPageId pageId, NPage::TGroupId groupId) const override\n     {\n         Y_ABORT_UNLESS(groupId.Index < PageCollections.size());\ndiff --git a/ydb/core/tablet_flat/flat_table_part.h b/ydb/core/tablet_flat/flat_table_part.h\nindex 83e5f15e4f2a..917390be358b 100644\n--- a/ydb/core/tablet_flat/flat_table_part.h\n+++ b/ydb/core/tablet_flat/flat_table_part.h\n@@ -156,6 +156,7 @@ namespace NTable {\n         virtual ui64 DataSize() const = 0;\n         virtual ui64 BackingSize() const = 0;\n         virtual ui64 GetPageSize(NPage::TPageId pageId, NPage::TGroupId groupId) const = 0;\n+        virtual ui64 GetPageSize(ELargeObj lob, ui64 ref) const = 0;\n         virtual NPage::EPage GetPageType(NPage::TPageId pageId, NPage::TGroupId groupId) const = 0;\n         virtual ui8 GetGroupChannel(NPage::TGroupId groupId) const = 0;\n         virtual ui8 GetPageChannel(ELargeObj lob, ui64 ref) const = 0;\ndiff --git a/ydb/core/tablet_flat/tablet_flat_executor.h b/ydb/core/tablet_flat/tablet_flat_executor.h\nindex 761d7ebe8a02..439d181362d5 100644\n--- a/ydb/core/tablet_flat/tablet_flat_executor.h\n+++ b/ydb/core/tablet_flat/tablet_flat_executor.h\n@@ -103,6 +103,9 @@ struct IExecuting {\n     virtual void LoanTable(ui32 tableId, const TString &partsInfo) = 0; // attach table parts to table (called on part destination)\n     virtual void CleanupLoan(const TLogoBlobID &bundleId, ui64 from) = 0; // mark loan completion (called on part source)\n     virtual void ConfirmLoan(const TLogoBlobID &bundleId, const TLogoBlobID &borrowId) = 0; // confirm loan update delivery (called on part destination)\n+    virtual void EnableReadMissingReferences() noexcept = 0;\n+    virtual void DisableReadMissingReferences() noexcept = 0;\n+    virtual ui64 MissingReferencesSize() const noexcept = 0;\n };\n \n class TTxMemoryProviderBase : TNonCopyable {\ndiff --git a/ydb/core/tx/datashard/datashard__read_iterator.cpp b/ydb/core/tx/datashard/datashard__read_iterator.cpp\nindex 6be261d99bee..b056c481688f 100644\n--- a/ydb/core/tx/datashard/datashard__read_iterator.cpp\n+++ b/ydb/core/tx/datashard/datashard__read_iterator.cpp\n@@ -220,15 +220,15 @@ std::pair<std::unique_ptr<IBlockBuilder>, TString> CreateBlockBuilder(\n }\n \n std::vector<TRawTypeValue> ToRawTypeValue(\n-    const TSerializedCellVec& keyCells,\n+    TArrayRef<const TCell> keyCells,\n     const TShortTableInfo& tableInfo,\n     bool addNulls)\n {\n     std::vector<TRawTypeValue> result;\n-    result.reserve(keyCells.GetCells().size());\n+    result.reserve(keyCells.size());\n \n-    for (ui32 i = 0; i < keyCells.GetCells().size(); ++i) {\n-        result.push_back(TRawTypeValue(keyCells.GetCells()[i].AsRef(), tableInfo.KeyColumnTypes[i]));\n+    for (ui32 i = 0; i < keyCells.size(); ++i) {\n+        result.push_back(TRawTypeValue(keyCells[i].AsRef(), tableInfo.KeyColumnTypes[i]));\n     }\n \n     // note that currently without nulls it is [prefix, +inf, +inf],\n@@ -298,9 +298,9 @@ class TReader {\n     bool VolatileWaitForCommit = false;\n \n     enum class EReadStatus {\n-        Done = 0,\n+        Done,\n         NeedData,\n-        StoppedByLimit,\n+        NeedContinue,\n     };\n \n public:\n@@ -325,7 +325,6 @@ class TReader {\n \n     EReadStatus ReadRange(\n         TTransactionContext& txc,\n-        const TActorContext& ctx,\n         const TSerializedTableRange& range)\n     {\n         bool fromInclusive;\n@@ -355,8 +354,8 @@ class TReader {\n             toInclusive = range.ToInclusive;\n         }\n \n-        const auto keyFrom = ToRawTypeValue(keyFromCells, TableInfo, fromInclusive);\n-        const auto keyTo = ToRawTypeValue(keyToCells, TableInfo, !toInclusive);\n+        const auto keyFrom = ToRawTypeValue(keyFromCells.GetCells(), TableInfo, fromInclusive);\n+        const auto keyTo = ToRawTypeValue(keyToCells.GetCells(), TableInfo, !toInclusive);\n \n         // TODO: split range into parts like in read_columns\n \n@@ -367,7 +366,7 @@ class TReader {\n         iterRange.MaxInclusive = toInclusive;\n         const bool reverse = State.Reverse;\n \n-        if (TArrayRef<const TCell> cells = keyFromCells.GetCells()) {\n+        if (TArrayRef<const TCell> cells = (reverse ? keyToCells.GetCells() : keyFromCells.GetCells())) {\n             if (!fromInclusive || cells.size() >= TableInfo.KeyColumnTypes.size()) {\n                 Self->GetKeyAccessSampler()->AddSample(TableId, cells);\n             } else {\n@@ -378,31 +377,24 @@ class TReader {\n         }\n \n         EReadStatus result;\n+\n+        txc.Env.EnableReadMissingReferences();\n+\n         if (!reverse) {\n             auto iter = txc.DB.IterateRange(TableInfo.LocalTid, iterRange, State.Columns, State.ReadVersion, GetReadTxMap(), GetReadTxObserver());\n-            result = IterateRange(iter.Get(), ctx);\n+            result = IterateRange(iter.Get(), iterRange, txc);\n         } else {\n             auto iter = txc.DB.IterateRangeReverse(TableInfo.LocalTid, iterRange, State.Columns, State.ReadVersion, GetReadTxMap(), GetReadTxObserver());\n-            result = IterateRange(iter.Get(), ctx);\n+            result = IterateRange(iter.Get(), iterRange, txc);\n         }\n \n-        if (result == EReadStatus::NeedData && !(RowsProcessed && CanResume())) {\n-            if (LastProcessedKey) {\n-                keyFromCells = TSerializedCellVec(LastProcessedKey);\n-                const auto keyFrom = ToRawTypeValue(keyFromCells, TableInfo, false);\n-                Precharge(txc.DB, keyFrom, iterRange.MaxKey, reverse);\n-            } else {\n-                Precharge(txc.DB, iterRange.MinKey, iterRange.MaxKey, reverse);\n-            }\n-            return EReadStatus::NeedData;\n-        }\n+        txc.Env.DisableReadMissingReferences();\n \n         return result;\n     }\n \n     EReadStatus ReadKey(\n         TTransactionContext& txc,\n-        const TActorContext& ctx,\n         const TSerializedCellVec& keyCells,\n         size_t keyIndex)\n     {\n@@ -413,7 +405,7 @@ class TReader {\n             range.To = keyCells;\n             range.ToInclusive = true;\n             range.FromInclusive = true;\n-            return ReadRange(txc, ctx, range);\n+            return ReadRange(txc, range);\n         }\n \n         if (ColumnTypes.empty()) {\n@@ -424,13 +416,16 @@ class TReader {\n             }\n         }\n \n-        const auto key = ToRawTypeValue(keyCells, TableInfo, true);\n+        const auto key = ToRawTypeValue(keyCells.GetCells(), TableInfo, true);\n \n         NTable::TRowState rowState;\n         rowState.Init(State.Columns.size());\n         NTable::TSelectStats stats;\n         auto ready = txc.DB.Select(TableInfo.LocalTid, key, State.Columns, rowState, stats, 0, State.ReadVersion, GetReadTxMap(), GetReadTxObserver());\n         if (ready == NTable::EReady::Page) {\n+            if (RowsProcessed && CanResume()) {\n+                return EReadStatus::NeedContinue;\n+            }\n             return EReadStatus::NeedData;\n         }\n \n@@ -462,11 +457,11 @@ class TReader {\n     {\n         if (keyCells.GetCells().size() != TableInfo.KeyColumnCount) {\n             // key prefix, treat it as range [prefix, null, null] - [prefix, +inf, +inf]\n-            auto minKey = ToRawTypeValue(keyCells, TableInfo, true);\n-            auto maxKey = ToRawTypeValue(keyCells, TableInfo, false);\n+            auto minKey = ToRawTypeValue(keyCells.GetCells(), TableInfo, true);\n+            auto maxKey = ToRawTypeValue(keyCells.GetCells(), TableInfo, false);\n             return Precharge(txc.DB, minKey, maxKey, State.Reverse);\n         } else {\n-            auto key = ToRawTypeValue(keyCells, TableInfo, true);\n+            auto key = ToRawTypeValue(keyCells.GetCells(), TableInfo, true);\n             return Precharge(txc.DB, key, key, State.Reverse);\n         }\n     }\n@@ -497,7 +492,7 @@ class TReader {\n \n     // TODO: merge ReadRanges and ReadKeys to single template Read?\n \n-    bool ReadRanges(TTransactionContext& txc, const TActorContext& ctx) {\n+    bool ReadRanges(TTransactionContext& txc) {\n         // note that FirstUnprocessedQuery is unsigned and if we do reverse iteration,\n         // then it will also become less than size() when finished\n         while (FirstUnprocessedQuery < State.Request->Ranges.size()) {\n@@ -511,19 +506,16 @@ class TReader {\n                 return true;\n \n             const auto& range = State.Request->Ranges[FirstUnprocessedQuery];\n-            auto status = ReadRange(txc, ctx, range);\n+            auto status = ReadRange(txc, range);\n             switch (status) {\n             case EReadStatus::Done:\n                 break;\n-            case EReadStatus::StoppedByLimit:\n-                return true;\n             case EReadStatus::NeedData:\n-                if (RowsProcessed && CanResume())\n-                    return true;\n-\n                 // Note: ReadRange has already precharged current range and\n                 //       we don't precharge multiple ranges as opposed to keys\n                 return false;\n+            case EReadStatus::NeedContinue:\n+                return true;\n             }\n \n             if (!State.Reverse)\n@@ -536,7 +528,7 @@ class TReader {\n         return true;\n     }\n \n-    bool ReadKeys(TTransactionContext& txc, const TActorContext& ctx) {\n+    bool ReadKeys(TTransactionContext& txc) {\n         // note that FirstUnprocessedQuery is unsigned and if we do reverse iteration,\n         // then it will also become less than size() when finished\n         while (FirstUnprocessedQuery < State.Request->Keys.size()) {\n@@ -550,18 +542,15 @@ class TReader {\n                 return true;\n \n             const auto& key = State.Request->Keys[FirstUnprocessedQuery];\n-            auto status = ReadKey(txc, ctx, key, FirstUnprocessedQuery);\n+            auto status = ReadKey(txc, key, FirstUnprocessedQuery);\n             switch (status) {\n             case EReadStatus::Done:\n                 break;\n-            case EReadStatus::StoppedByLimit:\n-                return true;\n             case EReadStatus::NeedData:\n-                if (RowsProcessed && CanResume())\n-                    return true;\n-\n                 PrechargeKeysAfter(txc, FirstUnprocessedQuery);\n                 return false;\n+            case EReadStatus::NeedContinue:\n+                return true;\n             }\n \n             if (!State.Reverse)\n@@ -575,16 +564,16 @@ class TReader {\n     }\n \n     // return semantics the same as in the Execute()\n-    bool Read(TTransactionContext& txc, const TActorContext& ctx) {\n+    bool Read(TTransactionContext& txc) {\n         // TODO: consider trying to precharge multiple records at once in case\n         // when first precharge fails?\n \n         if (!State.Request->Keys.empty()) {\n-            return ReadKeys(txc, ctx);\n+            return ReadKeys(txc);\n         }\n \n         // since no keys, then we must have ranges (has been checked initially)\n-        return ReadRanges(txc, ctx);\n+        return ReadRanges(txc);\n     }\n \n     bool HasUnreadQueries() const {\n@@ -784,22 +773,22 @@ class TReader {\n         return true;\n     }\n \n-    bool OutOfQuota() const {\n-        return RowsRead >= State.Quota.Rows ||\n-            BlockBuilder.Bytes() >= State.Quota.Bytes||\n-            BytesInResult >= State.Quota.Bytes;\n+    bool OutOfQuota(ui64 prechargedCount = 0, ui64 bytesPrecharged = 0) const {\n+        return RowsRead + prechargedCount >= State.Quota.Rows ||\n+            BlockBuilder.Bytes() + bytesPrecharged >= State.Quota.Bytes ||\n+            BytesInResult + bytesPrecharged >= State.Quota.Bytes;\n     }\n \n-    bool HasMaxRowsInResult() const {\n-        return RowsRead >= State.MaxRowsInResult;\n+    bool HasMaxRowsInResult(ui64 prechargedCount = 0) const {\n+        return RowsRead + prechargedCount >= State.MaxRowsInResult;\n     }\n \n-    bool ReachedTotalRowsLimit() const {\n+    bool ReachedTotalRowsLimit(ui64 prechargedCount = 0) const {\n         if (State.TotalRowsLimit == Max<ui64>()) {\n             return false;\n         }\n \n-        return State.TotalRows + RowsRead >= State.TotalRowsLimit;\n+        return State.TotalRows + RowsRead + prechargedCount >= State.TotalRowsLimit;\n     }\n \n     ui64 GetTotalRowsLeft() const {\n@@ -815,12 +804,12 @@ class TReader {\n         return State.TotalRowsLimit - State.TotalRows - RowsRead;\n     }\n \n-    bool ShouldStop() {\n+    bool ShouldStop(ui64 prechargedCount = 0, ui64 bytesPrecharged = 0) {\n         if (!CanResume()) {\n             return false;\n         }\n \n-        return OutOfQuota() || HasMaxRowsInResult() || ShouldStopByElapsedTime();\n+        return OutOfQuota(prechargedCount, bytesPrecharged) || HasMaxRowsInResult(prechargedCount) || ShouldStopByElapsedTime();\n     }\n \n     ui64 GetRowsLeft() {\n@@ -847,8 +836,8 @@ class TReader {\n \n     bool Precharge(\n         NTable::TDatabase& db,\n-        NTable::TRawVals keyFrom,\n-        NTable::TRawVals keyTo,\n+        NTable::TRawVals minKey,\n+        NTable::TRawVals maxKey,\n         bool reverse)\n     {\n         ui64 rowsLeft = GetRowsLeft();\n@@ -856,8 +845,8 @@ class TReader {\n \n         auto direction = reverse ? NTable::EDirection::Reverse : NTable::EDirection::Forward;\n         return db.Precharge(TableInfo.LocalTid,\n-                            keyFrom,\n-                            keyTo,\n+                            minKey,\n+                            maxKey,\n                             State.Columns,\n                             0,\n                             rowsLeft,\n@@ -867,26 +856,50 @@ class TReader {\n     }\n \n     template <typename TIterator>\n-    EReadStatus IterateRange(TIterator* iter, const TActorContext& ctx) {\n-        Y_UNUSED(ctx);\n-\n+    EReadStatus IterateRange(TIterator* iter, NTable::TKeyRange& iterRange, TTransactionContext& txc) {\n         auto keyAccessSampler = Self->GetKeyAccessSampler();\n \n         bool advanced = false;\n+\n+        bool precharging = false;\n+        ui64 prechargedCount = 0;\n+        ui64 prechargedRowsSize = 0; // Without referenced blobs (external, outer)\n+\n         while (iter->Next(NTable::ENext::Data) == NTable::EReady::Data) {\n+            TDbTupleRef rowKey = iter->GetKey();\n+            TDbTupleRef rowValues = iter->GetValues();\n+\n+            if (!precharging && txc.Env.MissingReferencesSize()) {\n+                // Note: the current key must be returned to reader, but the\n+                // previous key is lost, and we cannot safely resume. We can\n+                // only restart query from the beginning, and don't want to\n+                // keep track of any stats.\n+                precharging = true;\n+            }\n+\n+            if (precharging) {\n+                // Note: RowsProcessed, RowsSinceLastCheck and LastProcessed key are not updated,\n+                // so we will restart the transaction from the exact same key we started iterating from.\n+                prechargedCount++;\n+                prechargedRowsSize += EstimateSize(rowValues.Cells());\n+\n+                if (ReachedTotalRowsLimit(prechargedCount) || ShouldStop(prechargedCount, prechargedRowsSize + txc.Env.MissingReferencesSize())) {\n+                    break;\n+                }\n+\n+                continue;\n+            }\n+\n             advanced = true;\n+\n             DeletedRowSkips += iter->Stats.DeletedRowSkips;\n             InvisibleRowSkips += iter->Stats.InvisibleRowSkips;\n \n-            TDbTupleRef rowKey = iter->GetKey();\n-\n             keyAccessSampler->AddSample(TableId, rowKey.Cells());\n             const ui64 processedRecords = 1 + ResetRowSkips(iter->Stats);\n             RowsSinceLastCheck += processedRecords;\n             RowsProcessed += processedRecords;\n \n-            TDbTupleRef rowValues = iter->GetValues();\n-\n             // note that if user requests key columns then they will be in\n             // rowValues and we don't have to add rowKey columns\n             BlockBuilder.AddRow(TDbTupleRef(), rowValues);\n@@ -900,7 +913,7 @@ class TReader {\n             if (ShouldStop()) {\n                 LastProcessedKey = TSerializedCellVec::Serialize(rowKey.Cells());\n                 LastProcessedKeyErased = false;\n-                return EReadStatus::StoppedByLimit;\n+                return EReadStatus::NeedContinue;\n             }\n         }\n \n@@ -913,6 +926,27 @@ class TReader {\n         // the same transaction, instead of starting a new one, in which case\n         // we will not update stats and will not update RowsProcessed.\n         auto lastKey = iter->GetKey().Cells();\n+\n+        auto prechargeFromLastKey = [&]() {\n+            if (lastKey) {\n+                const auto key = ToRawTypeValue(lastKey, TableInfo, false);\n+                if (!State.Reverse) {\n+                    Precharge(txc.DB, key, iterRange.MaxKey, State.Reverse);\n+                } else {\n+                    Precharge(txc.DB, iterRange.MinKey, key, State.Reverse);\n+                }\n+            } else {\n+                Precharge(txc.DB, iterRange.MinKey, iterRange.MaxKey, State.Reverse);\n+            }\n+        };\n+\n+        if (precharging) {\n+            if (iter->Last() == NTable::EReady::Page) {\n+                prechargeFromLastKey();\n+            }\n+            return EReadStatus::NeedData;\n+        }\n+\n         if (lastKey && (advanced || iter->Stats.DeletedRowSkips >= 4) && iter->Last() == NTable::EReady::Page) {\n             LastProcessedKey = TSerializedCellVec::Serialize(lastKey);\n             LastProcessedKeyErased = iter->GetKeyState() == NTable::ERowOp::Erase;\n@@ -930,9 +964,14 @@ class TReader {\n             RowsProcessed += processedRecords;\n         }\n \n-        // TODO: consider restart when Page and too few data read\n-        // (how much is too few, less than user's limit?)\n         if (iter->Last() == NTable::EReady::Page) {\n+            // TODO: consider restart when Page and too few data read\n+            // (how much is too few, less than user's limit?)\n+            if (RowsProcessed && CanResume()) {\n+                return EReadStatus::NeedContinue;\n+            }\n+\n+            prechargeFromLastKey();\n             return EReadStatus::NeedData;\n         }\n \n@@ -1773,7 +1812,7 @@ class TDataShard::TReadOperation : public TOperation, public IReadOperation {\n             AppData()->MonotonicTimeProvider->Now(),\n             Self));\n \n-        return Reader->Read(txc, ctx);\n+        return Reader->Read(txc);\n     }\n \n     void PrepareValidationInfo(const TActorContext&, const TReadIteratorState& state) {\n@@ -2430,7 +2469,8 @@ class TDataShard::TTxReadContinue : public NTabletFlatExecutor::TTransactionBase\n             Self));\n \n         LWTRACK(ReadExecute, state.Orbit);\n-        if (Reader->Read(txc, ctx)) {\n+\n+        if (Reader->Read(txc)) {\n             // Retry later when dependencies are resolved\n             if (!Reader->GetVolatileReadDependencies().empty()) {\n                 state.ReadContinuePending = true;\ndiff --git a/ydb/core/tx/datashard/datashard_ut_read_iterator_ext_blobs.cpp b/ydb/core/tx/datashard/datashard_ut_read_iterator_ext_blobs.cpp\nnew file mode 100644\nindex 000000000000..8b845b0f4b6a\n--- /dev/null\n+++ b/ydb/core/tx/datashard/datashard_ut_read_iterator_ext_blobs.cpp\n@@ -0,0 +1,705 @@\n+#include <ydb/core/tx/datashard/ut_common/datashard_ut_common.h>\n+#include \"datashard_ut_common_kqp.h\"\n+\n+#include <ydb/core/testlib/tablet_helpers.h>\n+#include <ydb/core/formats/arrow/arrow_helpers.h>\n+#include <ydb/core/formats/arrow/converter.h>\n+#include <ydb/core/kqp/ut/common/kqp_ut_common.h>\n+#include <ydb/core/tablet_flat/shared_cache_events.h>\n+#include <ydb/core/tx/tx_proxy/proxy.h>\n+#include <ydb/core/tx/tx_proxy/read_table.h>\n+#include <ydb/core/tx/long_tx_service/public/lock_handle.h>\n+\n+#include <ydb/core/tx/data_events/events.h>\n+#include <ydb/core/tx/data_events/payload_helper.h>\n+\n+#include <ydb/public/sdk/cpp/client/ydb_result/result.h>\n+\n+namespace NKikimr {\n+\n+using namespace NKikimr::NDataShard;\n+using namespace NKikimr::NDataShard::NKqpHelpers;\n+using namespace NSchemeShard;\n+using namespace Tests;\n+\n+Y_UNIT_TEST_SUITE(ReadIteratorExternalBlobs) {\n+\n+    struct TReadIteratorCounter {\n+        int Reads = 0;\n+        int Continues = 0;\n+        int EvGets = 0;\n+        int BlobsRequested = 0;\n+\n+        bool operator==(const TReadIteratorCounter&) const = default;\n+\n+        friend inline IOutputStream& operator<<(IOutputStream& out, const TReadIteratorCounter& c) {\n+            out << \"{ \" << c.Reads << \", \" << c.Continues << \", \" << c.EvGets << \", \" << c.BlobsRequested << \" }\";\n+            return out;\n+        }\n+    };\n+\n+    std::unique_ptr<TReadIteratorCounter> SetupReadIteratorObserver(TTestActorRuntime& runtime) {\n+        std::unique_ptr<TReadIteratorCounter> iteratorCounter = std::make_unique<TReadIteratorCounter>();\n+\n+        auto captureEvents = [&](TAutoPtr<IEventHandle> &event) -> auto {\n+            switch (event->GetTypeRewrite()) {\n+                case TEvDataShard::EvRead: {\n+                    iteratorCounter->Reads++;\n+                    break;\n+                }\n+                case TEvDataShard::EvReadContinue: {\n+                    iteratorCounter->Continues++;\n+                    break;\n+                }\n+                case TEvBlobStorage::EvGet: {\n+                    auto* msg = event->Get<TEvBlobStorage::TEvGet>();\n+                    iteratorCounter->EvGets++;\n+                    iteratorCounter->BlobsRequested += msg->QuerySize;\n+                    break;\n+                }\n+            }\n+            return TTestActorRuntime::EEventAction::PROCESS;\n+        };\n+\n+        auto prevObserverFunc = runtime.SetObserverFunc(captureEvents);\n+\n+        return iteratorCounter;\n+    }\n+\n+    struct TNode {\n+        TPortManager Pm;\n+        TServerSettings ServerSettings;\n+        TServer::TPtr Server;\n+        ui64 Shard;\n+        TTableId TableId;\n+        TActorId Sender;\n+        TTestActorRuntime* Runtime;\n+\n+        TNode(bool useExternalBlobs, int externalBlobColumns = 1) : ServerSettings(Pm.GetPort(2134)) {\n+            ServerSettings.SetDomainName(\"Root\")\n+                .SetUseRealThreads(false)\n+                .AddStoragePool(\"ssd\")\n+                .AddStoragePool(\"hdd\")\n+                .AddStoragePool(\"ext\")\n+                .SetEnableUuidAsPrimaryKey(true);\n+\n+            Server = new TServer(ServerSettings);\n+            \n+            Runtime = Server->GetRuntime();\n+\n+            Sender = Runtime->AllocateEdgeActor();\n+        \n+            InitRoot(Server, Sender);\n+            \n+            TShardedTableOptions::TFamily fam;\n+            \n+            if (useExternalBlobs) {\n+                fam = {.Name = \"default\", .LogPoolKind = \"ssd\", .SysLogPoolKind = \"ssd\", .DataPoolKind = \"ssd\", \n+                        .ExternalPoolKind = \"ext\", .DataThreshold = 100u, .ExternalThreshold = 512_KB};\n+            } else {\n+                fam = {.Name = \"default\", .LogPoolKind = \"ssd\", .SysLogPoolKind = \"ssd\", .DataPoolKind = \"ssd\", .DataThreshold = 100u};\n+            }\n+            \n+            TVector<TShardedTableOptions::TColumn> columns = {\n+                    {\"blob_id\", \"Uuid\", true, false}, \n+                    {\"chunk_num\", \"Int32\", true, false}\n+            };\n+\n+            for (int i = 0; i < externalBlobColumns; i++) {\n+                columns.push_back({\"data\" + ToString(i), \"String\", false, false});\n+            }\n+\n+            auto opts = TShardedTableOptions()\n+                .Columns(columns)\n+                .Families({fam});\n+\n+            CreateShardedTable(Server, Sender, \"/Root\", \"table-1\", opts);\n+\n+            Shard = GetTableShards(Server, Sender, \"/Root/table-1\").at(0);\n+            TableId = ResolveTableId(Server, Sender, \"/Root/table-1\");\n+        }\n+    };\n+\n+    void ValidateReadResult(TTestActorRuntime& runtime,\n+            NThreading::TFuture<Ydb::Table::ExecuteDataQueryResponse> readFuture,\n+            int rowsCount,\n+            int firstBlobChunkNum = 0,\n+            int extBlobColumnCount = 1)\n+    {\n+        Ydb::Table::ExecuteDataQueryResponse res = AwaitResponse(runtime, std::move(readFuture));\n+        auto& operation = res.Getoperation();\n+        UNIT_ASSERT_VALUES_EQUAL(operation.status(), Ydb::StatusIds::SUCCESS);\n+        Ydb::Table::ExecuteQueryResult result;\n+        operation.result().UnpackTo(&result);\n+        UNIT_ASSERT_EQUAL(result.result_sets().size(), 1);\n+        auto& resultSet = result.result_sets()[0];\n+        UNIT_ASSERT_EQUAL(resultSet.rows_size(), rowsCount);\n+\n+        for (int i = 0; i < resultSet.rows_size(); i++) {\n+            auto& row = resultSet.get_idx_rows(i);\n+\n+            UNIT_ASSERT_EQUAL(row.items_size(), 2 + extBlobColumnCount);\n+\n+            auto& chunkNumValue = row.get_idx_items(1);\n+\n+            UNIT_ASSERT(chunkNumValue.has_int32_value());\n+            UNIT_ASSERT_EQUAL(chunkNumValue.Getint32_value(), firstBlobChunkNum + i);\n+\n+            for (int j = 0; j < extBlobColumnCount; j++) {\n+                auto& dataValue = row.get_idx_items(2 + j);\n+                UNIT_ASSERT(dataValue.has_bytes_value());\n+                UNIT_ASSERT_EQUAL(dataValue.bytes_value().size(), 1_MB);\n+            }\n+        }\n+    }\n+    \n+    void ValidateReadResult(TTestActorRuntime& runtime,\n+            NThreading::TFuture<Ydb::Table::ExecuteDataQueryResponse> readFuture,\n+            const std::vector<i32>& expectedResult)\n+    {\n+        Ydb::Table::ExecuteDataQueryResponse res = AwaitResponse(runtime, std::move(readFuture));\n+        auto& operation = res.Getoperation();\n+        UNIT_ASSERT_VALUES_EQUAL(operation.status(), Ydb::StatusIds::SUCCESS);\n+        Ydb::Table::ExecuteQueryResult result;\n+        operation.result().UnpackTo(&result);\n+        UNIT_ASSERT_EQUAL(result.result_sets().size(), 1);\n+        auto& resultSet = result.result_sets()[0];\n+        UNIT_ASSERT_EQUAL(size_t(resultSet.rows_size()), expectedResult.size());\n+\n+        for (int i = 0; i < resultSet.rows_size(); i++) {\n+            auto& row = resultSet.get_idx_rows(i);\n+\n+            UNIT_ASSERT_EQUAL(row.items_size(), 3);\n+\n+            auto& chunkNumValue = row.get_idx_items(1);\n+\n+            UNIT_ASSERT(chunkNumValue.has_int32_value());\n+            UNIT_ASSERT_EQUAL(chunkNumValue.Getint32_value(), expectedResult[i]);\n+\n+            auto& dataValue = row.get_idx_items(2);\n+            UNIT_ASSERT(dataValue.has_bytes_value());\n+            UNIT_ASSERT_EQUAL(dataValue.bytes_value().size(), 1_MB);\n+        }\n+    }\n+\n+    Y_UNIT_TEST(ExtBlobs) {\n+        TNode node(true);\n+\n+        auto server = node.Server;\n+        auto& runtime = *node.Runtime;\n+        auto& sender = node.Sender;\n+        auto shard1 = node.Shard;\n+        auto tableId1 = node.TableId;\n+\n+        TString largeValue(1_MB, 'L');\n+\n+        for (int i = 0; i < 10; i++) {\n+            TString chunkNum = ToString(i);\n+            TString query = R\"___(\n+                UPSERT INTO `/Root/table-1` (blob_id, chunk_num, data0) VALUES\n+                    (Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\"), )___\" + chunkNum + \", \\\"\" + largeValue + \"\\\");\";\n+            \n+            ExecSQL(server, sender, query);    \n+        }\n+\n+        {\n+            Cerr << \"... waiting for stats after upsert\" << Endl;\n+            auto stats = WaitTableStats(runtime, shard1);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetDatashardId(), shard1);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetTableStats().GetRowCount(), 10);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetTableStats().GetPartCount(), 0);\n+        }\n+\n+        CompactTable(runtime, shard1, tableId1, false);\n+\n+        {\n+            Cerr << \"... waiting for stats after compaction\" << Endl;\n+            auto stats = WaitTableStats(runtime, shard1, 1);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetDatashardId(), shard1);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetTableStats().GetRowCount(), 10);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetTableStats().GetPartCount(), 1);\n+        }\n+\n+        auto iteratorCounter = SetupReadIteratorObserver(runtime);\n+\n+        auto readFuture = KqpSimpleSend(runtime, R\"(SELECT blob_id, chunk_num, data0\n+                FROM `/Root/table-1`\n+                WHERE\n+                    blob_id = Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\") AND\n+                    chunk_num >= 0\n+                ORDER BY blob_id, chunk_num ASC\n+                LIMIT 100;)\");\n+\n+        ValidateReadResult(runtime, std::move(readFuture), 10);\n+\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->Reads, 1);\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->Continues, 2);\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->EvGets, 2);\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->BlobsRequested, 10);\n+    }\n+\n+    Y_UNIT_TEST(ExtBlobsWithDeletesInTheBeginning) {\n+        TNode node(true);\n+\n+        auto server = node.Server;\n+        auto& runtime = *node.Runtime;\n+        auto& sender = node.Sender;\n+        auto shard1 = node.Shard;\n+        auto tableId1 = node.TableId;\n+\n+        TString largeValue(1_MB, 'L');\n+\n+        for (int i = 0; i < 10; i++) {\n+            TString chunkNum = ToString(i);\n+            TString query = R\"___(\n+                UPSERT INTO `/Root/table-1` (blob_id, chunk_num, data0) VALUES\n+                    (Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\"), )___\" + chunkNum + \", \\\"\" + largeValue + \"\\\");\";\n+            \n+            ExecSQL(server, sender, query);    \n+        }\n+\n+        CompactTable(runtime, shard1, tableId1, false);\n+\n+        for (int i = 0; i < 7; i++) {\n+            TString chunkNum = ToString(i);\n+            TString query = R\"___(\n+                DELETE FROM `/Root/table-1` WHERE blob_id=Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\") and chunk_num=)___\"\n+                + chunkNum + \";\";\n+            \n+            ExecSQL(server, sender, query);    \n+        }\n+\n+        auto iteratorCounter = SetupReadIteratorObserver(runtime);\n+\n+        auto readFuture = KqpSimpleSend(runtime, R\"(SELECT blob_id, chunk_num, data0\n+                FROM `/Root/table-1`\n+                WHERE\n+                    blob_id = Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\") AND\n+                    chunk_num >= 0\n+                ORDER BY blob_id, chunk_num ASC\n+                LIMIT 100;)\");\n+\n+        ValidateReadResult(runtime, std::move(readFuture), 3, 7);\n+\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->Reads, 1);\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->Continues, 0);\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->EvGets, 1);\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->BlobsRequested, 3);\n+    }\n+\n+    Y_UNIT_TEST(ExtBlobsWithDeletesInTheEnd) {\n+        TNode node(true);\n+\n+        auto server = node.Server;\n+        auto& runtime = *node.Runtime;\n+        auto& sender = node.Sender;\n+        auto shard1 = node.Shard;\n+        auto tableId1 = node.TableId;\n+\n+        TString largeValue(1_MB, 'L');\n+\n+        for (int i = 0; i < 10; i++) {\n+            TString chunkNum = ToString(i);\n+            TString query = R\"___(\n+                UPSERT INTO `/Root/table-1` (blob_id, chunk_num, data0) VALUES\n+                    (Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\"), )___\" + chunkNum + \", \\\"\" + largeValue + \"\\\");\";\n+            \n+            ExecSQL(server, sender, query);    \n+        }\n+        \n+        CompactTable(runtime, shard1, tableId1, false);\n+\n+        for (int i = 3; i < 10; i++) {\n+            TString chunkNum = ToString(i);\n+            TString query = R\"___(\n+                DELETE FROM `/Root/table-1` WHERE blob_id=Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\") and chunk_num=)___\"\n+                + chunkNum + \";\";\n+            \n+            ExecSQL(server, sender, query);    \n+        }\n+\n+        auto iteratorCounter = SetupReadIteratorObserver(runtime);\n+\n+        auto readFuture = KqpSimpleSend(runtime, R\"(SELECT blob_id, chunk_num, data0\n+                FROM `/Root/table-1`\n+                WHERE\n+                    blob_id = Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\") AND\n+                    chunk_num >= 0\n+                ORDER BY blob_id, chunk_num ASC\n+                LIMIT 100;)\");\n+\n+        ValidateReadResult(runtime, std::move(readFuture), 3);\n+\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->Reads, 1);\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->Continues, 0);\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->EvGets, 1);\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->BlobsRequested, 3);\n+    }\n+\n+    Y_UNIT_TEST(ExtBlobsWithDeletesInTheMiddle) {\n+        TNode node(true);\n+\n+        auto server = node.Server;\n+        auto& runtime = *node.Runtime;\n+        auto& sender = node.Sender;\n+        auto shard1 = node.Shard;\n+        auto tableId1 = node.TableId;\n+\n+        TString largeValue(1_MB, 'L');\n+\n+        for (int i = 0; i < 10; i++) {\n+            TString chunkNum = ToString(i);\n+            TString query = R\"___(\n+                UPSERT INTO `/Root/table-1` (blob_id, chunk_num, data0) VALUES\n+                    (Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\"), )___\" + chunkNum + \", \\\"\" + largeValue + \"\\\");\";\n+            \n+            ExecSQL(server, sender, query);    \n+        }\n+\n+        CompactTable(runtime, shard1, tableId1, false);\n+\n+        {\n+            TString query = R\"___(\n+                DELETE FROM `/Root/table-1` WHERE blob_id=Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\") and chunk_num=0;)___\";\n+            \n+            ExecSQL(server, sender, query);    \n+        }\n+\n+        for (int i = 2; i < 5; i++) {\n+            TString chunkNum = ToString(i);\n+            TString query = R\"___(\n+                DELETE FROM `/Root/table-1` WHERE blob_id=Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\") and chunk_num=)___\"\n+                + chunkNum + \";\";\n+            \n+            ExecSQL(server, sender, query);    \n+        }\n+\n+        auto iteratorCounter = SetupReadIteratorObserver(runtime);\n+\n+        auto readFuture = KqpSimpleSend(runtime, R\"(SELECT blob_id, chunk_num, data0\n+                FROM `/Root/table-1`\n+                WHERE\n+                    blob_id = Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\") AND\n+                    chunk_num >= 0\n+                ORDER BY blob_id, chunk_num ASC\n+                LIMIT 100;)\");\n+\n+        ValidateReadResult(runtime, std::move(readFuture), {1, 5, 6, 7, 8, 9});\n+\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->Reads, 1);\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->Continues, 1);\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->EvGets, 2);\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->BlobsRequested, 6);\n+    }\n+\n+    void DoExtBlobsWithFirstRowPreloaded(bool withReboot) {\n+        TNode node(true);\n+\n+        auto server = node.Server;\n+        auto& runtime = *node.Runtime;\n+        auto& sender = node.Sender;\n+        auto shard1 = node.Shard;\n+        auto tableId1 = node.TableId;\n+\n+        TString largeValue(1_MB, 'L');\n+\n+        for (int i = 0; i < 10; i++) {\n+            TString chunkNum = ToString(i);\n+            TString query = R\"___(\n+                UPSERT INTO `/Root/table-1` (blob_id, chunk_num, data0) VALUES\n+                    (Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\"), )___\" + chunkNum + \", \\\"\" + largeValue + \"\\\");\";\n+            ExecSQL(server, sender, query);\n+        }\n+\n+        CompactTable(runtime, shard1, tableId1, false);\n+\n+        runtime.SimulateSleep(TDuration::Seconds(1));\n+\n+        auto preloadFuture = KqpSimpleSend(runtime, R\"(\n+                SELECT blob_id, chunk_num, data0\n+                FROM `/Root/table-1`\n+                WHERE\n+                    blob_id = Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\") AND\n+                    chunk_num = 0;\n+            )\");\n+\n+        ValidateReadResult(runtime, std::move(preloadFuture), 1);\n+\n+        size_t passedRows = 0;\n+        bool finished = false;\n+        std::vector<TEvDataShard::TEvReadResult::TPtr> blockedResults;\n+        std::optional<std::pair<TActorId, ui64>> dropReadId;\n+        auto blockResults = runtime.AddObserver<TEvDataShard::TEvReadResult>(\n+            [&](TEvDataShard::TEvReadResult::TPtr& ev) {\n+                auto* msg = ev->Get();\n+                if (dropReadId) {\n+                    if (*dropReadId == std::make_pair(ev->GetRecipientRewrite(), msg->Record.GetReadId())) {\n+                        ev.Reset();\n+                    }\n+                    return;\n+                }\n+                if (passedRows > 0) {\n+                    blockedResults.push_back(std::move(ev));\n+                    return;\n+                }\n+                passedRows += msg->GetRowsCount();\n+                if (msg->Record.GetFinished()) {\n+                    finished = true;\n+                }\n+            });\n+\n+        auto readFuture = KqpSimpleSend(runtime, R\"(\n+                SELECT blob_id, chunk_num, data0\n+                FROM `/Root/table-1`\n+                WHERE\n+                    blob_id = Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\") AND\n+                    chunk_num >= 0\n+                ORDER BY blob_id, chunk_num ASC\n+                LIMIT 5;\n+            )\");\n+\n+        runtime.WaitFor(\"blocked results\", [&]{ return blockedResults.size() > 0 || finished; });\n+\n+        if (!finished) {\n+            UNIT_ASSERT_VALUES_EQUAL(passedRows, 1u);\n+\n+            if (withReboot) {\n+                dropReadId.emplace(\n+                    blockedResults[0]->GetRecipientRewrite(),\n+                    blockedResults[0]->Get()->Record.GetReadId());\n+\n+                RebootTablet(runtime, shard1, sender);\n+            } else {\n+                blockResults.Remove();\n+                for (auto& ev : blockedResults) {\n+                    runtime.Send(ev.Release(), 0, true);\n+                }\n+                blockedResults.clear();\n+            }\n+        }\n+\n+        ValidateReadResult(runtime, std::move(readFuture), 5);\n+    }\n+\n+    Y_UNIT_TEST(ExtBlobsWithFirstRowPreloaded) {\n+        DoExtBlobsWithFirstRowPreloaded(false);\n+    }\n+\n+    Y_UNIT_TEST(ExtBlobsWithFirstRowPreloadedWithReboot) {\n+        DoExtBlobsWithFirstRowPreloaded(true);\n+    }\n+\n+    Y_UNIT_TEST(ExtBlobsMultipleColumns) {\n+        TNode node(true, 2);\n+\n+        auto server = node.Server;\n+        auto& runtime = *node.Runtime;\n+        auto& sender = node.Sender;\n+        auto shard1 = node.Shard;\n+        auto tableId1 = node.TableId;\n+\n+        TString largeValue(1_MB, 'L');\n+\n+        for (int i = 0; i < 10; i++) {\n+            TString chunkNum = ToString(i);\n+            TString query = R\"___(\n+                UPSERT INTO `/Root/table-1` (blob_id, chunk_num, data0, data1) VALUES\n+                    (Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\"), )___\"\n+                     + chunkNum + \", \\\"\" + largeValue + \"\\\", \\\"\" + largeValue + \"\\\");\";\n+            \n+            ExecSQL(server, sender, query);    \n+        }\n+\n+        {\n+            Cerr << \"... waiting for stats after upsert\" << Endl;\n+            auto stats = WaitTableStats(runtime, shard1);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetDatashardId(), shard1);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetTableStats().GetRowCount(), 10);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetTableStats().GetPartCount(), 1);\n+        }\n+\n+        CompactTable(runtime, shard1, tableId1, false);\n+\n+        {\n+            Cerr << \"... waiting for stats after compaction\" << Endl;\n+            auto stats = WaitTableStats(runtime, shard1, 1);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetDatashardId(), shard1);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetTableStats().GetRowCount(), 10);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetTableStats().GetPartCount(), 1);\n+        }\n+\n+        auto iteratorCounter = SetupReadIteratorObserver(runtime);\n+\n+        auto readFuture = KqpSimpleSend(runtime, R\"(SELECT blob_id, chunk_num, data0, data1\n+                FROM `/Root/table-1`\n+                WHERE\n+                    blob_id = Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\") AND\n+                    chunk_num >= 0\n+                ORDER BY blob_id, chunk_num ASC\n+                LIMIT 100;)\");\n+\n+        ValidateReadResult(runtime, std::move(readFuture), 10, 0, 2);\n+\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->Reads, 1);\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->Continues, 3);\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->EvGets, 4);\n+        UNIT_ASSERT_VALUES_EQUAL(iteratorCounter->BlobsRequested, 20);\n+    }\n+\n+    Y_UNIT_TEST(ExtBlobsWithCompactingMiddleRows) {\n+        std::unordered_map<int, TReadIteratorCounter> expectedResults;\n+        expectedResults[1] = {1, 4, 4, 18};\n+        expectedResults[2] = {1, 4, 4, 16};\n+        expectedResults[3] = {1, 4, 4, 14};\n+        expectedResults[4] = {1, 4, 4, 12};\n+        expectedResults[5] = {1, 4, 2, 10};\n+\n+        // We write 20 rows, some of them are compacted, then we write some more rows \"before\" and \"after\" and read all of them\n+        // The quantity of rows before, in the middle and after is different for each test. For example the first one is\n+        // 1 row before, 18 rows in the middle and 1 row after.\n+        for (int test = 1; test < 6; test++) {\n+            int compactedPart = 20 - (test * 2);\n+\n+            TNode node(true);\n+\n+            auto server = node.Server;\n+            auto& runtime = *node.Runtime;\n+            auto& sender = node.Sender;\n+            auto shard1 = node.Shard;\n+            auto tableId1 = node.TableId;\n+\n+            TString largeValue(1_MB, 'L');\n+\n+            for (int i = 0; i < compactedPart; i++) {\n+                TString chunkNum = ToString(test + i);\n+\n+                TString query = R\"___(\n+                    UPSERT INTO `/Root/table-1` (blob_id, chunk_num, data0) VALUES\n+                        (Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\"), )___\" + chunkNum + \", \\\"\" + largeValue + \"\\\");\";\n+                \n+                ExecSQL(server, sender, query);    \n+            }\n+\n+            {\n+                Cerr << \"... waiting for stats after upsert\" << Endl;\n+                auto stats = WaitTableStats(runtime, shard1);\n+                UNIT_ASSERT_VALUES_EQUAL(stats.GetDatashardId(), shard1);\n+                UNIT_ASSERT_VALUES_EQUAL(stats.GetTableStats().GetRowCount(), compactedPart);\n+            }\n+\n+            CompactTable(runtime, shard1, tableId1, false);\n+\n+            {\n+                Cerr << \"... waiting for stats after compaction\" << Endl;\n+                auto stats = WaitTableStats(runtime, shard1, 1);\n+                UNIT_ASSERT_VALUES_EQUAL(stats.GetDatashardId(), shard1);\n+                UNIT_ASSERT_VALUES_EQUAL(stats.GetTableStats().GetRowCount(), compactedPart);\n+                UNIT_ASSERT_VALUES_EQUAL(stats.GetTableStats().GetPartCount(), 1);\n+            }\n+\n+            for (int i = 0; i < test; i++) {\n+                TString chunkNum = ToString(i);\n+\n+                TString query = R\"___(\n+                    UPSERT INTO `/Root/table-1` (blob_id, chunk_num, data0) VALUES\n+                        (Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\"), )___\" + chunkNum + \", \\\"\" + largeValue + \"\\\");\";\n+                \n+                ExecSQL(server, sender, query);    \n+            }\n+\n+            for (int i = compactedPart + test; i < 20; i++) {\n+                TString chunkNum = ToString(i);\n+\n+                TString query = R\"___(\n+                    UPSERT INTO `/Root/table-1` (blob_id, chunk_num, data0) VALUES\n+                        (Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\"), )___\" + chunkNum + \", \\\"\" + largeValue + \"\\\");\";\n+                \n+                ExecSQL(server, sender, query);    \n+            }\n+\n+            auto iteratorCounter = SetupReadIteratorObserver(runtime);\n+\n+            auto readFuture = KqpSimpleSend(runtime, R\"(SELECT blob_id, chunk_num, data0\n+                    FROM `/Root/table-1`\n+                    WHERE\n+                        blob_id = Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\") AND\n+                        chunk_num >= 0\n+                    ORDER BY blob_id, chunk_num ASC\n+                    LIMIT 100;)\");\n+\n+            ValidateReadResult(runtime, std::move(readFuture), 20);\n+\n+            auto& expectedResult = expectedResults[test];\n+\n+            UNIT_ASSERT_VALUES_EQUAL_C(*iteratorCounter, expectedResult, \"test \" << test);\n+        }\n+    }\n+\n+    Y_UNIT_TEST(ExtBlobsEmptyTable) {\n+        TNode node(true);\n+\n+        auto& runtime = *node.Runtime;\n+\n+        auto readFuture = KqpSimpleSend(runtime, R\"(SELECT blob_id, chunk_num, data0\n+                FROM `/Root/table-1`\n+                WHERE\n+                    blob_id = Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\") AND\n+                    chunk_num >= 0\n+                ORDER BY blob_id, chunk_num ASC\n+                LIMIT 100;)\");\n+\n+        ValidateReadResult(runtime, std::move(readFuture), 0);\n+    }\n+\n+    Y_UNIT_TEST(NotExtBlobs) {\n+        TNode node(false);\n+\n+        auto server = node.Server;\n+        auto& runtime = *node.Runtime;\n+        auto& sender = node.Sender;\n+        auto shard1 = node.Shard;\n+        auto tableId1 = node.TableId;\n+\n+        TString largeValue(1_MB, 'L');\n+\n+        for (int i = 0; i < 10; i++) {\n+            TString chunkNum = ToString(i);\n+            TString query = R\"___(\n+                UPSERT INTO `/Root/table-1` (blob_id, chunk_num, data0) VALUES\n+                    (Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\"), )___\" + chunkNum + \", \\\"\" + largeValue + \"\\\");\";\n+            \n+            ExecSQL(server, sender, query);    \n+        }\n+\n+        {\n+            Cerr << \"... waiting for stats after upsert\" << Endl;\n+            auto stats = WaitTableStats(runtime, shard1);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetDatashardId(), shard1);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetTableStats().GetRowCount(), 10);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetTableStats().GetPartCount(), 0);\n+        }\n+\n+        CompactTable(runtime, shard1, tableId1, false);\n+\n+        {\n+            Cerr << \"... waiting for stats after compaction\" << Endl;\n+            auto stats = WaitTableStats(runtime, shard1, 1);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetDatashardId(), shard1);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetTableStats().GetRowCount(), 10);\n+            UNIT_ASSERT_VALUES_EQUAL(stats.GetTableStats().GetPartCount(), 1);\n+        }\n+\n+        auto readFuture = KqpSimpleSend(runtime, R\"(SELECT blob_id, chunk_num, data0\n+                FROM `/Root/table-1`\n+                WHERE\n+                    blob_id = Uuid(\"65df1ec1-a97d-47b2-ae56-3c023da6ee8c\") AND\n+                    chunk_num >= 0\n+                ORDER BY blob_id, chunk_num ASC\n+                LIMIT 100;)\");\n+        \n+        ValidateReadResult(runtime, std::move(readFuture), 10);\n+    }\n+\n+}\n+\n+} // namespace NKikimr\ndiff --git a/ydb/core/tx/datashard/ut_read_iterator/ya.make b/ydb/core/tx/datashard/ut_read_iterator/ya.make\nindex 04c4c9f3b718..e252560145a6 100644\n--- a/ydb/core/tx/datashard/ut_read_iterator/ya.make\n+++ b/ydb/core/tx/datashard/ut_read_iterator/ya.make\n@@ -31,6 +31,7 @@ YQL_LAST_ABI_VERSION()\n \n SRCS(\n     datashard_ut_read_iterator.cpp\n+    datashard_ut_read_iterator_ext_blobs.cpp\n )\n \n REQUIREMENTS(ram:32)\n",
  "test_patch": "diff --git a/ydb/core/tablet_flat/test/libs/table/test_part.h b/ydb/core/tablet_flat/test/libs/table/test_part.h\nindex 3b1eab2f7d2d..ab856b912cfd 100644\n--- a/ydb/core/tablet_flat/test/libs/table/test_part.h\n+++ b/ydb/core/tablet_flat/test/libs/table/test_part.h\n@@ -52,6 +52,13 @@ namespace NTest {\n             return Store->GetPageSize(groupId.Index, pageId);\n         }\n \n+        ui64 GetPageSize(ELargeObj lob, ui64 ref) const override\n+        {\n+            Y_UNUSED(lob);\n+            Y_UNUSED(ref);\n+            return 0;\n+        }\n+\n         NPage::EPage GetPageType(NPage::TPageId pageId, NPage::TGroupId groupId) const override\n         {\n             return Store->GetPageType(groupId.Index, pageId);\n",
  "problem_statement": "[[s3 perf]]: make precharge work with ext-blobs\nExternal blobs are not being precharged if read with read iterator. \nWithout precharging each external blob is requested separately. \nMeasured speed up is 2 times.\n",
  "hints_text": "https://github.com/ydb-platform/ydb/pull/5758",
  "created_at": "2024-10-02T10:41:00Z"
}