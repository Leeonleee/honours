You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
duckdb_param_type returns 0
### What happens?

Apologies as this may not have to do with duckdb C api and may be a result of the autogenerated Nim wrapper, but I wanted to mention it here. duckdb_prepare, and duckdb_nparams, bind and execute all seem to work as expected.  I'm using Nim 1.6.8

I have a query like `SELECT sum(i) FROM integers WHERE i > ?` 

I expect duckdb_param_type(pStmt, 1) to return DUCKDB_TYPE_INTEGER.  Instead I get 0 or DUCKDB_TYPE_INVALID. 

``` nim
proc getValue*(db: var DbConn, query: SqlQuery, args: varargs[string,
    `$`]): string {.tags: [ReadDbEffect].} =
  var
    con: duckdb_connection
    res: duckdb_result
    pStmt: duckdb_prepared_statement
  if (duckdb_connect(db, addr(con)) == DuckDBError):
    raise newException(DbError, "error occured when connecting to database")

  if (duckdb_prepare(con, cstring(query), addr(
      pStmt)) == DuckDBError):
    var msg = duckdb_prepare_error(pStmt)
    duckdb_disconnect(addr(con))
    raise newException(DbError, "error occured when preparing statement: " & $msg)

  echo duckdb_param_type(pStmt, 1.idx_t)
...

```
Any thoughts or suggestions are appreciated.

### To Reproduce

In theory if you create nim file with this code, and make sure the dll and header file are found it should compile or be run with `nim r file.nim`

The first bit is generated by nimterop and does lots of things I don't fully grok.  

```nim
{.push hint[ConvFromXtoItselfNotNeeded]: off.}
import macros

macro defineEnum(typ: untyped): untyped =
  result = newNimNode(nnkStmtList)

  # Enum mapped to distinct cint
  result.add quote do:
    type `typ`* = distinct cint

  for i in ["+", "-", "*", "div", "mod", "shl", "shr", "or", "and", "xor", "<",
      "<=", "==", ">", ">="]:
    let
      ni = newIdentNode(i)
      typout = if i[0] in "<=>": newIdentNode("bool") else: typ # comparisons return bool
    if i[0] == '>': # cannot borrow `>` and `>=` from templates
      let
        nopp = if i.len == 2: newIdentNode("<=") else: newIdentNode("<")
      result.add quote do:
        proc `ni`*(x: `typ`, y: cint): `typout` = `nopp`(y, x)
        proc `ni`*(x: cint, y: `typ`): `typout` = `nopp`(y, x)
        proc `ni`*(x, y: `typ`): `typout` = `nopp`(y, x)
    else:
      result.add quote do:
        proc `ni`*(x: `typ`, y: cint): `typout` {.borrow.}
        proc `ni`*(x: cint, y: `typ`): `typout` {.borrow.}
        proc `ni`*(x, y: `typ`): `typout` {.borrow.}
    result.add quote do:
      proc `ni`*(x: `typ`, y: int): `typout` = `ni`(x, y.cint)
      proc `ni`*(x: int, y: `typ`): `typout` = `ni`(x.cint, y)

  let
    divop = newIdentNode("/")   # `/`()
    dlrop = newIdentNode("$")   # `$`()
    notop = newIdentNode("not") # `not`()
  result.add quote do:
    proc `divop`*(x, y: `typ`): `typ` = `typ`((x.float / y.float).cint)
    proc `divop`*(x: `typ`, y: cint): `typ` = `divop`(x, `typ`(y))
    proc `divop`*(x: cint, y: `typ`): `typ` = `divop`(`typ`(x), y)
    proc `divop`*(x: `typ`, y: int): `typ` = `divop`(x, y.cint)
    proc `divop`*(x: int, y: `typ`): `typ` = `divop`(x.cint, y)

    proc `dlrop`*(x: `typ`): string {.borrow.}
    proc `notop`*(x: `typ`): `typ` {.borrow.}


{.pragma: impduckdbHdr,
  header: "duckdb.h".}
{.pragma: impduckdbDyn, dynlib: "duckdb.dll".}
{.experimental: "codeReordering".}
defineEnum(DUCKDB_TYPE)
defineEnum(duckdb_state)
defineEnum(duckdb_pending_state)
const
  DUCKDB_API_0_3_1* = 1
  DUCKDB_API_0_3_2* = 2
  DUCKDB_TYPE_INVALID* = (0).DUCKDB_TYPE
  DUCKDB_TYPE_BOOLEAN* = (DUCKDB_TYPE_INVALID + 1).DUCKDB_TYPE
  DUCKDB_TYPE_TINYINT* = (DUCKDB_TYPE_BOOLEAN + 1).DUCKDB_TYPE
  DUCKDB_TYPE_SMALLINT* = (DUCKDB_TYPE_TINYINT + 1).DUCKDB_TYPE
  DUCKDB_TYPE_INTEGER* = (DUCKDB_TYPE_SMALLINT + 1).DUCKDB_TYPE
  DUCKDB_TYPE_BIGINT* = (DUCKDB_TYPE_INTEGER + 1).DUCKDB_TYPE
  DUCKDB_TYPE_UTINYINT* = (DUCKDB_TYPE_BIGINT + 1).DUCKDB_TYPE
  DUCKDB_TYPE_USMALLINT* = (DUCKDB_TYPE_UTINYINT + 1).DUCKDB_TYPE
  DUCKDB_TYPE_UINTEGER* = (DUCKDB_TYPE_USMALLINT + 1).DUCKDB_TYPE
  DUCKDB_TYPE_UBIGINT* = (DUCKDB_TYPE_UINTEGER + 1).DUCKDB_TYPE
  DUCKDB_TYPE_FLOAT* = (DUCKDB_TYPE_UBIGINT + 1).DUCKDB_TYPE
  DUCKDB_TYPE_DOUBLE* = (DUCKDB_TYPE_FLOAT + 1).DUCKDB_TYPE
  DUCKDB_TYPE_TIMESTAMP* = (DUCKDB_TYPE_DOUBLE + 1).DUCKDB_TYPE
  DUCKDB_TYPE_DATE* = (DUCKDB_TYPE_TIMESTAMP + 1).DUCKDB_TYPE
  DUCKDB_TYPE_TIME* = (DUCKDB_TYPE_DATE + 1).DUCKDB_TYPE
  DUCKDB_TYPE_INTERVAL* = (DUCKDB_TYPE_TIME + 1).DUCKDB_TYPE
  DUCKDB_TYPE_HUGEINT* = (DUCKDB_TYPE_INTERVAL + 1).DUCKDB_TYPE
  DUCKDB_TYPE_VARCHAR* = (DUCKDB_TYPE_HUGEINT + 1).DUCKDB_TYPE
  DUCKDB_TYPE_BLOB* = (DUCKDB_TYPE_VARCHAR + 1).DUCKDB_TYPE
  DUCKDB_TYPE_DECIMAL* = (DUCKDB_TYPE_BLOB + 1).DUCKDB_TYPE
  DUCKDB_TYPE_TIMESTAMP_S* = (DUCKDB_TYPE_DECIMAL + 1).DUCKDB_TYPE
  DUCKDB_TYPE_TIMESTAMP_MS* = (DUCKDB_TYPE_TIMESTAMP_S + 1).DUCKDB_TYPE
  DUCKDB_TYPE_TIMESTAMP_NS* = (DUCKDB_TYPE_TIMESTAMP_MS + 1).DUCKDB_TYPE
  DUCKDB_TYPE_ENUM* = (DUCKDB_TYPE_TIMESTAMP_NS + 1).DUCKDB_TYPE
  DUCKDB_TYPE_LIST* = (DUCKDB_TYPE_ENUM + 1).DUCKDB_TYPE
  DUCKDB_TYPE_STRUCT* = (DUCKDB_TYPE_LIST + 1).DUCKDB_TYPE
  DUCKDB_TYPE_MAP* = (DUCKDB_TYPE_STRUCT + 1).DUCKDB_TYPE
  DUCKDB_TYPE_UUID* = (DUCKDB_TYPE_MAP + 1).DUCKDB_TYPE
  DUCKDB_TYPE_JSON* = (DUCKDB_TYPE_UUID + 1).DUCKDB_TYPE
  DuckDBSuccess* = (0).duckdb_state
  DuckDBError* = (1).duckdb_state
  DUCKDB_PENDING_RESULT_READY* = (0).duckdb_pending_state
  DUCKDB_PENDING_RESULT_NOT_READY* = (1).duckdb_pending_state
  DUCKDB_PENDING_ERROR* = (2).duckdb_pending_state
type
  idx_t* {.importc, impduckdbHdr.} = uint64
  duckdb_type* {.importc, impduckdbHdr.} = DUCKDB_TYPE
  duckdb_date* {.bycopy, importc, impduckdbHdr.} = object
    days*: int32

  duckdb_date_struct* {.bycopy, importc, impduckdbHdr.} = object
    year*: int32
    month*: int8
    day*: int8

  duckdb_time* {.bycopy, importc, impduckdbHdr.} = object
    micros*: int64

  duckdb_time_struct* {.bycopy, importc, impduckdbHdr.} = object
    hour*: int8
    min*: int8
    sec*: int8
    micros*: int32

  duckdb_timestamp* {.bycopy, importc, impduckdbHdr.} = object
    micros*: int64

  duckdb_timestamp_struct* {.bycopy, importc, impduckdbHdr.} = object
    date*: duckdb_date_struct
    time*: duckdb_time_struct

  duckdb_interval* {.bycopy, importc, impduckdbHdr.} = object
    months*: int32
    days*: int32
    micros*: int64

  duckdb_hugeint* {.bycopy, importc, impduckdbHdr.} = object
    lower*: uint64
    upper*: int64

  duckdb_decimal* {.bycopy, importc, impduckdbHdr.} = object
    width*: uint8
    scale*: uint8
    value*: duckdb_hugeint

  duckdb_blob* {.bycopy, importc, impduckdbHdr.} = object
    data*: pointer
    size*: idx_t

  duckdb_column* {.bycopy, importc, impduckdbHdr.} = object
    deprecated_data*: pointer
    deprecated_nullmask*: ptr bool
    deprecated_type*: duckdb_type
    deprecated_name*: cstring
    internal_data*: pointer

  duckdb_result* {.bycopy, importc, impduckdbHdr.} = object
    deprecated_column_count*: idx_t
    deprecated_row_count*: idx_t
    deprecated_rows_changed*: idx_t
    deprecated_columns*: ptr duckdb_column
    deprecated_error_message*: cstring
    internal_data*: pointer

  duckdb_database* {.importc, impduckdbHdr.} = pointer
  duckdb_connection* {.importc, impduckdbHdr.} = pointer
  duckdb_prepared_statement* {.importc, impduckdbHdr.} = pointer
  duckdb_pending_result* {.importc, impduckdbHdr.} = pointer
  duckdb_appender* {.importc, impduckdbHdr.} = pointer
  duckdb_arrow* {.importc, impduckdbHdr.} = pointer
  duckdb_config* {.importc, impduckdbHdr.} = pointer
  duckdb_arrow_schema* {.importc, impduckdbHdr.} = pointer
  duckdb_arrow_array* {.importc, impduckdbHdr.} = pointer
  duckdb_logical_type* {.importc, impduckdbHdr.} = pointer
  duckdb_data_chunk* {.importc, impduckdbHdr.} = pointer
  duckdb_vector* {.importc, impduckdbHdr.} = pointer
  duckdb_value* {.importc, impduckdbHdr.} = pointer
  duckdb_table_function* {.importc, impduckdbHdr.} = pointer
  duckdb_bind_info* {.importc, impduckdbHdr.} = pointer
  duckdb_init_info* {.importc, impduckdbHdr.} = pointer
  duckdb_function_info* {.importc, impduckdbHdr.} = pointer
  duckdb_table_function_bind_t* {.importc, impduckdbHdr.} = proc (
      info: duckdb_bind_info) {.cdecl.}
  duckdb_table_function_init_t* {.importc, impduckdbHdr.} = proc (
      info: duckdb_init_info) {.cdecl.}
  duckdb_table_function_t* {.importc, impduckdbHdr.} = proc (
      info: duckdb_function_info; output: duckdb_data_chunk) {.cdecl.}
  duckdb_delete_callback_t* {.importc, impduckdbHdr.} = proc (data: pointer) {.
      cdecl.}
  duckdb_replacement_scan_info* {.importc, impduckdbHdr.} = pointer
  duckdb_replacement_callback_t* {.importc, impduckdbHdr.} = proc (
      info: duckdb_replacement_scan_info; table_name: cstring; data: pointer) {.
      cdecl.}
  duckdb_task_state* {.importc, impduckdbHdr.} = pointer
proc duckdb_open*(path: cstring; out_database: ptr duckdb_database): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_open_ext*(path: cstring; out_database: ptr duckdb_database;
                      config: duckdb_config;
                          out_error: ptr cstring): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_close*(database: ptr duckdb_database) {.importc, cdecl, impduckdbDyn.}
proc duckdb_connect*(database: duckdb_database;
                     out_connection: ptr duckdb_connection): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_disconnect*(connection: ptr duckdb_connection) {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_create_config*(out_config: ptr duckdb_config): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_config_count*(): uint {.importc, cdecl, impduckdbDyn.}
proc duckdb_get_config_flag*(index: uint; out_name: ptr cstring;
                             out_description: ptr cstring): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_set_config*(config: duckdb_config; name: cstring;
    option: cstring): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_destroy_config*(config: ptr duckdb_config) {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_query*(connection: duckdb_connection; query: cstring;
                   out_result: ptr duckdb_result): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_destroy_result*(result: ptr duckdb_result) {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_column_name*(result: ptr duckdb_result; col: idx_t): cstring {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_column_type*(result: ptr duckdb_result; col: idx_t): duckdb_type {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_column_logical_type*(result: ptr duckdb_result;
    col: idx_t): duckdb_logical_type {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_column_count*(result: ptr duckdb_result): idx_t {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_row_count*(result: ptr duckdb_result): idx_t {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_rows_changed*(result: ptr duckdb_result): idx_t {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_column_data*(result: ptr duckdb_result; col: idx_t): pointer {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_nullmask_data*(result: ptr duckdb_result; col: idx_t): ptr bool {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_result_error*(result: ptr duckdb_result): cstring {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_result_get_chunk*(result: duckdb_result;
    chunk_index: idx_t): duckdb_data_chunk {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_result_chunk_count*(result: duckdb_result): idx_t {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_value_boolean*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): bool {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_int8*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): int8 {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_int16*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): int16 {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_int32*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): int32 {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_int64*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): int64 {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_hugeint*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): duckdb_hugeint {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_decimal*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): duckdb_decimal {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_uint8*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): uint8 {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_uint16*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): uint16 {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_uint32*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): uint32 {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_uint64*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): uint64 {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_float*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): cfloat {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_double*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): cdouble {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_date*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): duckdb_date {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_time*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): duckdb_time {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_timestamp*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): duckdb_timestamp {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_interval*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): duckdb_interval {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_varchar*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): cstring {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_varchar_internal*(result: ptr duckdb_result; col: idx_t;
                                    row: idx_t): cstring {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_value_blob*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): duckdb_blob {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_value_is_null*(result: ptr duckdb_result; col: idx_t;
    row: idx_t): bool {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_malloc*(size: uint): pointer {.importc, cdecl, impduckdbDyn.}
proc duckdb_free*(`ptr`: pointer) {.importc, cdecl, impduckdbDyn.}
proc duckdb_vector_size*(): idx_t {.importc, cdecl, impduckdbDyn.}
proc duckdb_from_date*(date: duckdb_date): duckdb_date_struct {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_to_date*(date: duckdb_date_struct): duckdb_date {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_from_time*(time: duckdb_time): duckdb_time_struct {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_to_time*(time: duckdb_time_struct): duckdb_time {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_from_timestamp*(ts: duckdb_timestamp): duckdb_timestamp_struct {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_to_timestamp*(ts: duckdb_timestamp_struct): duckdb_timestamp {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_hugeint_to_double*(val: duckdb_hugeint): cdouble {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_double_to_hugeint*(val: cdouble): duckdb_hugeint {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_decimal_to_double*(val: duckdb_decimal): cdouble {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_prepare*(connection: duckdb_connection; query: cstring;
                     out_prepared_statement: ptr duckdb_prepared_statement): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_destroy_prepare*(prepared_statement: ptr duckdb_prepared_statement) {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_prepare_error*(prepared_statement: duckdb_prepared_statement): cstring {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_nparams*(prepared_statement: duckdb_prepared_statement): idx_t {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_param_type*(prepared_statement: duckdb_prepared_statement;
                        param_idx: idx_t): duckdb_type {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_clear_bindings*(prepared_statement: duckdb_prepared_statement): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_bind_boolean*(prepared_statement: duckdb_prepared_statement;
                          param_idx: idx_t; val: bool): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_bind_int8*(prepared_statement: duckdb_prepared_statement;
                       param_idx: idx_t; val: int8): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_bind_int16*(prepared_statement: duckdb_prepared_statement;
                        param_idx: idx_t; val: int16): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_bind_int32*(prepared_statement: duckdb_prepared_statement;
                        param_idx: idx_t; val: int32): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_bind_int64*(prepared_statement: duckdb_prepared_statement;
                        param_idx: idx_t; val: int64): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_bind_hugeint*(prepared_statement: duckdb_prepared_statement;
                          param_idx: idx_t;
                              val: duckdb_hugeint): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_bind_uint8*(prepared_statement: duckdb_prepared_statement;
                        param_idx: idx_t; val: uint8): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_bind_uint16*(prepared_statement: duckdb_prepared_statement;
                         param_idx: idx_t; val: uint16): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_bind_uint32*(prepared_statement: duckdb_prepared_statement;
                         param_idx: idx_t; val: uint32): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_bind_uint64*(prepared_statement: duckdb_prepared_statement;
                         param_idx: idx_t; val: uint64): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_bind_float*(prepared_statement: duckdb_prepared_statement;
                        param_idx: idx_t; val: cfloat): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_bind_double*(prepared_statement: duckdb_prepared_statement;
                         param_idx: idx_t; val: cdouble): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_bind_date*(prepared_statement: duckdb_prepared_statement;
                       param_idx: idx_t; val: duckdb_date): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_bind_time*(prepared_statement: duckdb_prepared_statement;
                       param_idx: idx_t; val: duckdb_time): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_bind_timestamp*(prepared_statement: duckdb_prepared_statement;
                            param_idx: idx_t;
                                val: duckdb_timestamp): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_bind_interval*(prepared_statement: duckdb_prepared_statement;
                           param_idx: idx_t;
                               val: duckdb_interval): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_bind_varchar*(prepared_statement: duckdb_prepared_statement;
                          param_idx: idx_t; val: cstring): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_bind_varchar_length*(prepared_statement: duckdb_prepared_statement;
                                 param_idx: idx_t; val: cstring;
                                     length: idx_t): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_bind_blob*(prepared_statement: duckdb_prepared_statement;
                       param_idx: idx_t; data: pointer;
                           length: idx_t): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_bind_null*(prepared_statement: duckdb_prepared_statement;
                       param_idx: idx_t): duckdb_state {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_execute_prepared*(prepared_statement: duckdb_prepared_statement;
                              out_result: ptr duckdb_result): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_execute_prepared_arrow*(prepared_statement: duckdb_prepared_statement;
                                    out_result: ptr duckdb_arrow): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_pending_prepared*(prepared_statement: duckdb_prepared_statement;
                              out_result: ptr duckdb_pending_result): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_destroy_pending*(pending_result: ptr duckdb_pending_result) {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_pending_error*(pending_result: duckdb_pending_result): cstring {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_pending_execute_task*(pending_result: duckdb_pending_result): duckdb_pending_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_execute_pending*(pending_result: duckdb_pending_result;
                             out_result: ptr duckdb_result): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_destroy_value*(value: ptr duckdb_value) {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_create_varchar*(text: cstring): duckdb_value {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_create_varchar_length*(text: cstring;
    length: idx_t): duckdb_value {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_create_int64*(val: int64): duckdb_value {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_get_varchar*(value: duckdb_value): cstring {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_get_int64*(value: duckdb_value): int64 {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_create_logical_type*(`type`: duckdb_type): duckdb_logical_type {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_create_list_type*(`type`: duckdb_logical_type): duckdb_logical_type {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_create_map_type*(key_type: duckdb_logical_type;
                             value_type: duckdb_logical_type): duckdb_logical_type {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_create_decimal_type*(width: uint8;
    scale: uint8): duckdb_logical_type {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_get_type_id*(`type`: duckdb_logical_type): duckdb_type {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_decimal_width*(`type`: duckdb_logical_type): uint8 {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_decimal_scale*(`type`: duckdb_logical_type): uint8 {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_decimal_internal_type*(`type`: duckdb_logical_type): duckdb_type {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_enum_internal_type*(`type`: duckdb_logical_type): duckdb_type {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_enum_dictionary_size*(`type`: duckdb_logical_type): uint32 {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_enum_dictionary_value*(`type`: duckdb_logical_type;
    index: idx_t): cstring {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_list_type_child_type*(`type`: duckdb_logical_type): duckdb_logical_type {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_map_type_key_type*(`type`: duckdb_logical_type): duckdb_logical_type {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_map_type_value_type*(`type`: duckdb_logical_type): duckdb_logical_type {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_struct_type_child_count*(`type`: duckdb_logical_type): idx_t {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_struct_type_child_name*(`type`: duckdb_logical_type;
    index: idx_t): cstring {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_struct_type_child_type*(`type`: duckdb_logical_type;
    index: idx_t): duckdb_logical_type {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_destroy_logical_type*(`type`: ptr duckdb_logical_type) {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_create_data_chunk*(types: ptr duckdb_logical_type;
                               column_count: idx_t): duckdb_data_chunk {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_destroy_data_chunk*(chunk: ptr duckdb_data_chunk) {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_data_chunk_reset*(chunk: duckdb_data_chunk) {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_data_chunk_get_column_count*(chunk: duckdb_data_chunk): idx_t {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_data_chunk_get_vector*(chunk: duckdb_data_chunk;
    col_idx: idx_t): duckdb_vector {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_data_chunk_get_size*(chunk: duckdb_data_chunk): idx_t {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_data_chunk_set_size*(chunk: duckdb_data_chunk; size: idx_t) {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_vector_get_column_type*(vector: duckdb_vector): duckdb_logical_type {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_vector_get_data*(vector: duckdb_vector): pointer {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_vector_get_validity*(vector: duckdb_vector): ptr uint64 {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_vector_ensure_validity_writable*(vector: duckdb_vector) {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_vector_assign_string_element*(vector: duckdb_vector; index: idx_t;
    str: cstring) {.importc, cdecl, impduckdbDyn.}
proc duckdb_vector_assign_string_element_len*(vector: duckdb_vector;
    index: idx_t; str: cstring; str_len: idx_t) {.importc, cdecl, impduckdbDyn.}
proc duckdb_list_vector_get_child*(vector: duckdb_vector): duckdb_vector {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_list_vector_get_size*(vector: duckdb_vector): idx_t {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_struct_vector_get_child*(vector: duckdb_vector;
    index: idx_t): duckdb_vector {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_validity_row_is_valid*(validity: ptr uint64; row: idx_t): bool {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_validity_set_row_validity*(validity: ptr uint64; row: idx_t;
                                       valid: bool) {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_validity_set_row_invalid*(validity: ptr uint64; row: idx_t) {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_validity_set_row_valid*(validity: ptr uint64; row: idx_t) {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_create_table_function*(): duckdb_table_function {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_destroy_table_function*(table_function: ptr duckdb_table_function) {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_table_function_set_name*(table_function: duckdb_table_function;
                                     name: cstring) {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_table_function_add_parameter*(table_function: duckdb_table_function;
    `type`: duckdb_logical_type) {.importc, cdecl, impduckdbDyn.}
proc duckdb_table_function_set_extra_info*(
    table_function: duckdb_table_function; extra_info: pointer;
    destroy: duckdb_delete_callback_t) {.importc, cdecl, impduckdbDyn.}
proc duckdb_table_function_set_bind*(table_function: duckdb_table_function;
                                     `bind`: duckdb_table_function_bind_t) {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_table_function_set_init*(table_function: duckdb_table_function;
                                     init: duckdb_table_function_init_t) {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_table_function_set_local_init*(
    table_function: duckdb_table_function;
        init: duckdb_table_function_init_t) {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_table_function_set_function*(table_function: duckdb_table_function;
    function: duckdb_table_function_t) {.importc, cdecl, impduckdbDyn.}
proc duckdb_table_function_supports_projection_pushdown*(
    table_function: duckdb_table_function; pushdown: bool) {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_register_table_function*(con: duckdb_connection;
                                     function: duckdb_table_function): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_bind_get_extra_info*(info: duckdb_bind_info): pointer {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_bind_add_result_column*(info: duckdb_bind_info; name: cstring;
                                    `type`: duckdb_logical_type) {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_bind_get_parameter_count*(info: duckdb_bind_info): idx_t {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_bind_get_parameter*(info: duckdb_bind_info;
    index: idx_t): duckdb_value {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_bind_set_bind_data*(info: duckdb_bind_info; bind_data: pointer;
                                destroy: duckdb_delete_callback_t) {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_bind_set_cardinality*(info: duckdb_bind_info; cardinality: idx_t;
                                  is_exact: bool) {.importc, cdecl, impduckdbDyn.}
proc duckdb_bind_set_error*(info: duckdb_bind_info; error: cstring) {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_init_get_extra_info*(info: duckdb_init_info): pointer {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_init_get_bind_data*(info: duckdb_init_info): pointer {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_init_set_init_data*(info: duckdb_init_info; init_data: pointer;
                                destroy: duckdb_delete_callback_t) {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_init_get_column_count*(info: duckdb_init_info): idx_t {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_init_get_column_index*(info: duckdb_init_info;
    column_index: idx_t): idx_t {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_init_set_max_threads*(info: duckdb_init_info; max_threads: idx_t) {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_init_set_error*(info: duckdb_init_info; error: cstring) {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_function_get_extra_info*(info: duckdb_function_info): pointer {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_function_get_bind_data*(info: duckdb_function_info): pointer {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_function_get_init_data*(info: duckdb_function_info): pointer {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_function_get_local_init_data*(
  info: duckdb_function_info): pointer {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_function_set_error*(info: duckdb_function_info; error: cstring) {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_add_replacement_scan*(db: duckdb_database;
                                  replacement: duckdb_replacement_callback_t;
                                  extra_data: pointer;
                                  delete_callback: duckdb_delete_callback_t) {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_replacement_scan_set_function_name*(
    info: duckdb_replacement_scan_info; function_name: cstring) {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_replacement_scan_add_parameter*(info: duckdb_replacement_scan_info;
    parameter: duckdb_value) {.importc, cdecl, impduckdbDyn.}
proc duckdb_appender_create*(connection: duckdb_connection; schema: cstring;
                             table: cstring;
                                 out_appender: ptr duckdb_appender): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_appender_error*(appender: duckdb_appender): cstring {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_appender_flush*(appender: duckdb_appender): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_appender_close*(appender: duckdb_appender): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_appender_destroy*(appender: ptr duckdb_appender): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_appender_begin_row*(appender: duckdb_appender): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_appender_end_row*(appender: duckdb_appender): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_bool*(appender: duckdb_appender;
    value: bool): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_int8*(appender: duckdb_appender;
    value: int8): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_int16*(appender: duckdb_appender;
    value: int16): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_int32*(appender: duckdb_appender;
    value: int32): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_int64*(appender: duckdb_appender;
    value: int64): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_hugeint*(appender: duckdb_appender;
    value: duckdb_hugeint): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_uint8*(appender: duckdb_appender;
    value: uint8): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_uint16*(appender: duckdb_appender;
    value: uint16): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_uint32*(appender: duckdb_appender;
    value: uint32): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_uint64*(appender: duckdb_appender;
    value: uint64): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_float*(appender: duckdb_appender;
    value: cfloat): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_double*(appender: duckdb_appender;
    value: cdouble): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_date*(appender: duckdb_appender;
    value: duckdb_date): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_time*(appender: duckdb_appender;
    value: duckdb_time): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_timestamp*(appender: duckdb_appender;
    value: duckdb_timestamp): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_interval*(appender: duckdb_appender;
    value: duckdb_interval): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_varchar*(appender: duckdb_appender;
    val: cstring): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_varchar_length*(appender: duckdb_appender; val: cstring;
                                   length: idx_t): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_append_blob*(appender: duckdb_appender; data: pointer;
    length: idx_t): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_append_null*(appender: duckdb_appender): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_append_data_chunk*(appender: duckdb_appender;
                               chunk: duckdb_data_chunk): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_query_arrow*(connection: duckdb_connection; query: cstring;
                         out_result: ptr duckdb_arrow): duckdb_state {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_query_arrow_schema*(result: duckdb_arrow;
                                out_schema: ptr duckdb_arrow_schema): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_query_arrow_array*(result: duckdb_arrow;
                               out_array: ptr duckdb_arrow_array): duckdb_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_arrow_column_count*(result: duckdb_arrow): idx_t {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_arrow_row_count*(result: duckdb_arrow): idx_t {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_arrow_rows_changed*(result: duckdb_arrow): idx_t {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_query_arrow_error*(result: duckdb_arrow): cstring {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_destroy_arrow*(result: ptr duckdb_arrow) {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_execute_tasks*(database: duckdb_database; max_tasks: idx_t) {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_create_task_state*(database: duckdb_database): duckdb_task_state {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_execute_tasks_state*(state: duckdb_task_state) {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_execute_n_tasks_state*(state: duckdb_task_state;
    max_tasks: idx_t): idx_t {.
    importc, cdecl, impduckdbDyn.}
proc duckdb_finish_execution*(state: duckdb_task_state) {.importc, cdecl,
    impduckdbDyn.}
proc duckdb_task_state_is_finished*(state: duckdb_task_state): bool {.importc,
    cdecl, impduckdbDyn.}
proc duckdb_destroy_task_state*(state: duckdb_task_state) {.importc, cdecl,
    impduckdbDyn.}
{.pop.}

import std/db_common
import std/json
import os

type
  DbConn* = duckdb_database


proc open*(connection, user, password, database: string): DbConn {.
  tags: [DbEffect].} =
  var db: DbConn
  if (duckdb_open(connection, db.addr) == DuckDBError):
    var msg = osErrorMsg(osLastError())
    raise newException(DbError, "couldn't open database. " & msg)
  result = db

proc getValue*(db: var DbConn, query: SqlQuery, args: varargs[string,
    `$`]): string {.tags: [ReadDbEffect].} =
  var
    con: duckdb_connection
    res: duckdb_result
    pStmt: duckdb_prepared_statement
  if (duckdb_connect(db, con.addr) == DuckDBError):
    raise newException(DbError, "error occured when connecting to database")

  if (duckdb_prepare(con, cstring(query), addr(
      pStmt)) == DuckDBError):
    var msg = duckdb_prepare_error(pStmt)
    duckdb_disconnect(addr(con))
    raise newException(DbError, "error occured when preparing statement: " & $msg)

  echo duckdb_param_type(pStmt, 1.idx_t)

  if args.len != 0:
    if duckdb_nparams(pStmt) != args.len.idx_t:
      duckdb_destroy_prepare(addr(pStmt))
      duckdb_disconnect(addr(con))
      raise newException(DbError, "wrong number of binding parameters")

    for i, arg in args.pairs:
      if (duckdb_bind_varchar(pStmt, (i + 1).idx_t, cstring(arg))) == DuckDBError:
        duckdb_destroy_prepare(addr(pStmt))
        duckdb_disconnect(addr(con))
        raise newException(DbError, "error occured when attempting to bind value " &
            arg & " at position " & $i)
      # echo duckdb_param_type(pStmt, (i+1).idx_t)
      # if duckdb_param_type(pStmt, (i+1).idx_t) == DUCKDB_TYPE_INVALID:
      #   duckdb_destroy_prepare(addr(pStmt))
      #   duckdb_disconnect(addr(con))
      #   raise newException(DbError, "parameter type error value:" &
      #       arg & ", at position: " & $i)

  if duckdb_execute_prepared(pStmt, addr(res)) == DuckDBError:
    var msg = duckdb_result_error(addr(res))
    duckdb_destroy_prepare(addr(pStmt))
    duckdb_disconnect(addr(con))
    raise newException(DbError, "error occured when executing prepared statement: " & $msg)

  let val = duckdb_value_varchar(addr(res), 0.csize_t, 0.csize_t)
  if val.isNil:
    result = ""
  else:
    result = $val
  duckdb_free(val)
  duckdb_destroy_result(addr(res))
  duckdb_destroy_prepare(addr(pStmt))
  duckdb_disconnect(addr(con))

proc main() =
  var db = open("test.db", "", "", "")
  try:
    var x = db.getValue(sql"SELECT sum(i) FROM integers WHERE i > ?", "3")
    echo x
  except:
    echo getCurrentExceptionMsg()
  finally:
    duckdb_close(addr(db))

main()


```

### OS:

windows 10

### DuckDB Version:

0.5.1

### DuckDB Client:

Nim / C api

### Full Name:

Steve McAllister

### Affiliation:

None

### Have you tried this on the latest `master` branch?

- [X] I agree

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] I agree
Reading a CSV file with an empty header is problematic
### Discussed in https://github.com/duckdb/duckdb/discussions/4932

<div type='discussions-op-text'>

<sup>Originally posted by **szarnyasg** October  9, 2022</sup>
### What happens?

When loading a CSV file with an empty header, the first row with actual content is either skipped (with `HEADER true`) or it is read but its first attribute is not loaded correctly (with `HEADER false`).

Obviously, such a CSV file is malformed so it's unclear how it should be loaded, so one can argue this is not a bug. That said, IMHO loading half the first row violates the principle of least surprise -- either throwing an error message or loading the full row would be better alternatives.

### To Reproduce

Create `my.csv` with an empty first line:
```csv

a 1
b 2
c 3
```
Run the following commands in DuckDB:
```sql
CREATE OR REPLACE TABLE stats (file varchar, numEntities bigint);
COPY stats FROM 'my.csv' (DELIMITER ' ');
SELECT * FROM stats;
```
```console
┌──────┬─────────────┐
│ file │ numEntities │
├──────┼─────────────┤
│      │ 1           │
│ b    │ 2           │
│ c    │ 3           │
└──────┴─────────────┘
```
Note that the `file` attribute is empty for the first row.

With the `HEADER` setting, the line (`a 1`) is skipped in the CSV file:
```sql
CREATE OR REPLACE TABLE stats (file varchar, numEntities bigint);
COPY stats FROM 'my.csv' (DELIMITER ' ', HEADER);
SELECT * FROM stats;
```
```console
┌──────┬─────────────┐
│ file │ numEntities │
├──────┼─────────────┤
│ b    │ 2           │
│ c    │ 3           │
└──────┴─────────────┘
```


Originally reported by @szarnyasg , I had not noticed the missing data before pressing "convert". The missing stuff is problematic indeed.

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
[end of README.md]
[start of src/execution/operator/persistent/buffered_csv_reader.cpp]
1: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/file_system.hpp"
5: #include "duckdb/common/string_util.hpp"
6: #include "duckdb/common/to_string.hpp"
7: #include "duckdb/common/types/cast_helpers.hpp"
8: #include "duckdb/common/vector_operations/unary_executor.hpp"
9: #include "duckdb/common/vector_operations/vector_operations.hpp"
10: #include "duckdb/function/scalar/strftime.hpp"
11: #include "duckdb/main/database.hpp"
12: #include "duckdb/parser/column_definition.hpp"
13: #include "duckdb/storage/data_table.hpp"
14: #include "utf8proc_wrapper.hpp"
15: #include "utf8proc.hpp"
16: #include "duckdb/parser/keyword_helper.hpp"
17: 
18: #include <algorithm>
19: #include <cctype>
20: #include <cstring>
21: #include <fstream>
22: 
23: namespace duckdb {
24: 
25: static bool ParseBoolean(const Value &value, const string &loption);
26: 
27: static bool ParseBoolean(const vector<Value> &set, const string &loption) {
28: 	if (set.empty()) {
29: 		// no option specified: default to true
30: 		return true;
31: 	}
32: 	if (set.size() > 1) {
33: 		throw BinderException("\"%s\" expects a single argument as a boolean value (e.g. TRUE or 1)", loption);
34: 	}
35: 	return ParseBoolean(set[0], loption);
36: }
37: 
38: static bool ParseBoolean(const Value &value, const string &loption) {
39: 
40: 	if (value.type().id() == LogicalTypeId::LIST) {
41: 		auto &children = ListValue::GetChildren(value);
42: 		return ParseBoolean(children, loption);
43: 	}
44: 	if (value.type() == LogicalType::FLOAT || value.type() == LogicalType::DOUBLE ||
45: 	    value.type().id() == LogicalTypeId::DECIMAL) {
46: 		throw BinderException("\"%s\" expects a boolean value (e.g. TRUE or 1)", loption);
47: 	}
48: 	return BooleanValue::Get(value.DefaultCastAs(LogicalType::BOOLEAN));
49: }
50: 
51: static string ParseString(const Value &value, const string &loption) {
52: 	if (value.type().id() == LogicalTypeId::LIST) {
53: 		auto &children = ListValue::GetChildren(value);
54: 		if (children.size() != 1) {
55: 			throw BinderException("\"%s\" expects a single argument as a string value", loption);
56: 		}
57: 		return ParseString(children[0], loption);
58: 	}
59: 	if (value.type().id() != LogicalTypeId::VARCHAR) {
60: 		throw BinderException("\"%s\" expects a string argument!", loption);
61: 	}
62: 	return value.GetValue<string>();
63: }
64: 
65: static int64_t ParseInteger(const Value &value, const string &loption) {
66: 	if (value.type().id() == LogicalTypeId::LIST) {
67: 		auto &children = ListValue::GetChildren(value);
68: 		if (children.size() != 1) {
69: 			// no option specified or multiple options specified
70: 			throw BinderException("\"%s\" expects a single argument as an integer value", loption);
71: 		}
72: 		return ParseInteger(children[0], loption);
73: 	}
74: 	return value.GetValue<int64_t>();
75: }
76: 
77: static vector<bool> ParseColumnList(const vector<Value> &set, vector<string> &names, const string &loption) {
78: 	vector<bool> result;
79: 
80: 	if (set.empty()) {
81: 		throw BinderException("\"%s\" expects a column list or * as parameter", loption);
82: 	}
83: 	// list of options: parse the list
84: 	unordered_map<string, bool> option_map;
85: 	for (idx_t i = 0; i < set.size(); i++) {
86: 		option_map[set[i].ToString()] = false;
87: 	}
88: 	result.resize(names.size(), false);
89: 	for (idx_t i = 0; i < names.size(); i++) {
90: 		auto entry = option_map.find(names[i]);
91: 		if (entry != option_map.end()) {
92: 			result[i] = true;
93: 			entry->second = true;
94: 		}
95: 	}
96: 	for (auto &entry : option_map) {
97: 		if (!entry.second) {
98: 			throw BinderException("\"%s\" expected to find %s, but it was not found in the table", loption,
99: 			                      entry.first.c_str());
100: 		}
101: 	}
102: 	return result;
103: }
104: 
105: static vector<bool> ParseColumnList(const Value &value, vector<string> &names, const string &loption) {
106: 	vector<bool> result;
107: 
108: 	// Only accept a list of arguments
109: 	if (value.type().id() != LogicalTypeId::LIST) {
110: 		// Support a single argument if it's '*'
111: 		if (value.type().id() == LogicalTypeId::VARCHAR && value.GetValue<string>() == "*") {
112: 			result.resize(names.size(), true);
113: 			return result;
114: 		}
115: 		throw BinderException("\"%s\" expects a column list or * as parameter", loption);
116: 	}
117: 	auto &children = ListValue::GetChildren(value);
118: 	// accept '*' as single argument
119: 	if (children.size() == 1 && children[0].type().id() == LogicalTypeId::VARCHAR &&
120: 	    children[0].GetValue<string>() == "*") {
121: 		result.resize(names.size(), true);
122: 		return result;
123: 	}
124: 	return ParseColumnList(children, names, loption);
125: }
126: 
127: struct CSVFileHandle {
128: public:
129: 	explicit CSVFileHandle(unique_ptr<FileHandle> file_handle_p) : file_handle(move(file_handle_p)) {
130: 		can_seek = file_handle->CanSeek();
131: 		plain_file_source = file_handle->OnDiskFile() && can_seek;
132: 		file_size = file_handle->GetFileSize();
133: 	}
134: 
135: 	bool CanSeek() {
136: 		return can_seek;
137: 	}
138: 	void Seek(idx_t position) {
139: 		if (!can_seek) {
140: 			throw InternalException("Cannot seek in this file");
141: 		}
142: 		file_handle->Seek(position);
143: 	}
144: 	idx_t SeekPosition() {
145: 		if (!can_seek) {
146: 			throw InternalException("Cannot seek in this file");
147: 		}
148: 		return file_handle->SeekPosition();
149: 	}
150: 	void Reset() {
151: 		if (plain_file_source) {
152: 			file_handle->Reset();
153: 		} else {
154: 			if (!reset_enabled) {
155: 				throw InternalException("Reset called but reset is not enabled for this CSV Handle");
156: 			}
157: 			read_position = 0;
158: 		}
159: 	}
160: 	bool PlainFileSource() {
161: 		return plain_file_source;
162: 	}
163: 
164: 	bool OnDiskFile() {
165: 		return file_handle->OnDiskFile();
166: 	}
167: 
168: 	idx_t FileSize() {
169: 		return file_size;
170: 	}
171: 
172: 	idx_t Read(void *buffer, idx_t nr_bytes) {
173: 		if (!plain_file_source) {
174: 			// not a plain file source: we need to do some bookkeeping around the reset functionality
175: 			idx_t result_offset = 0;
176: 			if (read_position < buffer_size) {
177: 				// we need to read from our cached buffer
178: 				auto buffer_read_count = MinValue<idx_t>(nr_bytes, buffer_size - read_position);
179: 				memcpy(buffer, cached_buffer.get() + read_position, buffer_read_count);
180: 				result_offset += buffer_read_count;
181: 				read_position += buffer_read_count;
182: 				if (result_offset == nr_bytes) {
183: 					return nr_bytes;
184: 				}
185: 			} else if (!reset_enabled && cached_buffer) {
186: 				// reset is disabled but we still have cached data
187: 				// we can remove any cached data
188: 				cached_buffer.reset();
189: 				buffer_size = 0;
190: 				buffer_capacity = 0;
191: 				read_position = 0;
192: 			}
193: 			// we have data left to read from the file
194: 			// read directly into the buffer
195: 			auto bytes_read = file_handle->Read((char *)buffer + result_offset, nr_bytes - result_offset);
196: 			read_position += bytes_read;
197: 			if (reset_enabled) {
198: 				// if reset caching is enabled, we need to cache the bytes that we have read
199: 				if (buffer_size + bytes_read >= buffer_capacity) {
200: 					// no space; first enlarge the buffer
201: 					buffer_capacity = MaxValue<idx_t>(NextPowerOfTwo(buffer_size + bytes_read), buffer_capacity * 2);
202: 
203: 					auto new_buffer = unique_ptr<data_t[]>(new data_t[buffer_capacity]);
204: 					if (buffer_size > 0) {
205: 						memcpy(new_buffer.get(), cached_buffer.get(), buffer_size);
206: 					}
207: 					cached_buffer = move(new_buffer);
208: 				}
209: 				memcpy(cached_buffer.get() + buffer_size, (char *)buffer + result_offset, bytes_read);
210: 				buffer_size += bytes_read;
211: 			}
212: 
213: 			return result_offset + bytes_read;
214: 		} else {
215: 			return file_handle->Read(buffer, nr_bytes);
216: 		}
217: 	}
218: 
219: 	string ReadLine() {
220: 		string result;
221: 		char buffer[1];
222: 		while (true) {
223: 			idx_t tuples_read = Read(buffer, 1);
224: 			if (tuples_read == 0 || buffer[0] == '\n') {
225: 				return result;
226: 			}
227: 			if (buffer[0] != '\r') {
228: 				result += buffer[0];
229: 			}
230: 		}
231: 	}
232: 
233: 	void DisableReset() {
234: 		this->reset_enabled = false;
235: 	}
236: 
237: private:
238: 	unique_ptr<FileHandle> file_handle;
239: 	bool reset_enabled = true;
240: 	bool can_seek = false;
241: 	bool plain_file_source = false;
242: 	idx_t file_size = 0;
243: 	// reset support
244: 	unique_ptr<data_t[]> cached_buffer;
245: 	idx_t read_position = 0;
246: 	idx_t buffer_size = 0;
247: 	idx_t buffer_capacity = 0;
248: };
249: 
250: void BufferedCSVReaderOptions::SetDelimiter(const string &input) {
251: 	this->delimiter = StringUtil::Replace(input, "\\t", "\t");
252: 	this->has_delimiter = true;
253: 	if (input.empty()) {
254: 		this->delimiter = string("\0", 1);
255: 	}
256: }
257: 
258: void BufferedCSVReaderOptions::SetDateFormat(LogicalTypeId type, const string &format, bool read_format) {
259: 	string error;
260: 	if (read_format) {
261: 		auto &date_format = this->date_format[type];
262: 		error = StrTimeFormat::ParseFormatSpecifier(format, date_format);
263: 		date_format.format_specifier = format;
264: 	} else {
265: 		auto &date_format = this->write_date_format[type];
266: 		error = StrTimeFormat::ParseFormatSpecifier(format, date_format);
267: 	}
268: 	if (!error.empty()) {
269: 		throw InvalidInputException("Could not parse DATEFORMAT: %s", error.c_str());
270: 	}
271: 	has_format[type] = true;
272: }
273: 
274: void BufferedCSVReaderOptions::SetReadOption(const string &loption, const Value &value,
275:                                              vector<string> &expected_names) {
276: 	if (SetBaseOption(loption, value)) {
277: 		return;
278: 	}
279: 	if (loption == "auto_detect") {
280: 		auto_detect = ParseBoolean(value, loption);
281: 	} else if (loption == "sample_size") {
282: 		int64_t sample_size = ParseInteger(value, loption);
283: 		if (sample_size < 1 && sample_size != -1) {
284: 			throw BinderException("Unsupported parameter for SAMPLE_SIZE: cannot be smaller than 1");
285: 		}
286: 		if (sample_size == -1) {
287: 			sample_chunks = std::numeric_limits<uint64_t>::max();
288: 			sample_chunk_size = STANDARD_VECTOR_SIZE;
289: 		} else if (sample_size <= STANDARD_VECTOR_SIZE) {
290: 			sample_chunk_size = sample_size;
291: 			sample_chunks = 1;
292: 		} else {
293: 			sample_chunk_size = STANDARD_VECTOR_SIZE;
294: 			sample_chunks = sample_size / STANDARD_VECTOR_SIZE;
295: 		}
296: 	} else if (loption == "skip") {
297: 		skip_rows = ParseInteger(value, loption);
298: 	} else if (loption == "max_line_size" || loption == "maximum_line_size") {
299: 		maximum_line_size = ParseInteger(value, loption);
300: 	} else if (loption == "sample_chunk_size") {
301: 		sample_chunk_size = ParseInteger(value, loption);
302: 		if (sample_chunk_size > STANDARD_VECTOR_SIZE) {
303: 			throw BinderException(
304: 			    "Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be bigger than STANDARD_VECTOR_SIZE %d",
305: 			    STANDARD_VECTOR_SIZE);
306: 		} else if (sample_chunk_size < 1) {
307: 			throw BinderException("Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be smaller than 1");
308: 		}
309: 	} else if (loption == "sample_chunks") {
310: 		sample_chunks = ParseInteger(value, loption);
311: 		if (sample_chunks < 1) {
312: 			throw BinderException("Unsupported parameter for SAMPLE_CHUNKS: cannot be smaller than 1");
313: 		}
314: 	} else if (loption == "force_not_null") {
315: 		force_not_null = ParseColumnList(value, expected_names, loption);
316: 	} else if (loption == "date_format" || loption == "dateformat") {
317: 		string format = ParseString(value, loption);
318: 		SetDateFormat(LogicalTypeId::DATE, format, true);
319: 	} else if (loption == "timestamp_format" || loption == "timestampformat") {
320: 		string format = ParseString(value, loption);
321: 		SetDateFormat(LogicalTypeId::TIMESTAMP, format, true);
322: 	} else if (loption == "escape") {
323: 		escape = ParseString(value, loption);
324: 		has_escape = true;
325: 	} else if (loption == "ignore_errors") {
326: 		ignore_errors = ParseBoolean(value, loption);
327: 	} else if (loption == "union_by_name") {
328: 		union_by_name = ParseBoolean(value, loption);
329: 	} else {
330: 		throw BinderException("Unrecognized option for CSV reader \"%s\"", loption);
331: 	}
332: }
333: 
334: void BufferedCSVReaderOptions::SetWriteOption(const string &loption, const Value &value) {
335: 	if (SetBaseOption(loption, value)) {
336: 		return;
337: 	}
338: 
339: 	if (loption == "force_quote") {
340: 		force_quote = ParseColumnList(value, names, loption);
341: 	} else if (loption == "date_format" || loption == "dateformat") {
342: 		string format = ParseString(value, loption);
343: 		SetDateFormat(LogicalTypeId::DATE, format, false);
344: 	} else if (loption == "timestamp_format" || loption == "timestampformat") {
345: 		string format = ParseString(value, loption);
346: 		if (StringUtil::Lower(format) == "iso") {
347: 			format = "%Y-%m-%dT%H:%M:%S.%fZ";
348: 		}
349: 		SetDateFormat(LogicalTypeId::TIMESTAMP, format, false);
350: 	} else {
351: 		throw BinderException("Unrecognized option CSV writer \"%s\"", loption);
352: 	}
353: }
354: 
355: bool BufferedCSVReaderOptions::SetBaseOption(const string &loption, const Value &value) {
356: 	// Make sure this function was only called after the option was turned into lowercase
357: 	D_ASSERT(!std::any_of(loption.begin(), loption.end(), ::isupper));
358: 
359: 	if (StringUtil::StartsWith(loption, "delim") || StringUtil::StartsWith(loption, "sep")) {
360: 		SetDelimiter(ParseString(value, loption));
361: 	} else if (loption == "quote") {
362: 		quote = ParseString(value, loption);
363: 		has_quote = true;
364: 	} else if (loption == "escape") {
365: 		escape = ParseString(value, loption);
366: 		has_escape = true;
367: 	} else if (loption == "header") {
368: 		header = ParseBoolean(value, loption);
369: 		has_header = true;
370: 	} else if (loption == "null" || loption == "nullstr") {
371: 		null_str = ParseString(value, loption);
372: 	} else if (loption == "encoding") {
373: 		auto encoding = StringUtil::Lower(ParseString(value, loption));
374: 		if (encoding != "utf8" && encoding != "utf-8") {
375: 			throw BinderException("Copy is only supported for UTF-8 encoded files, ENCODING 'UTF-8'");
376: 		}
377: 	} else if (loption == "compression") {
378: 		compression = FileCompressionTypeFromString(ParseString(value, loption));
379: 	} else {
380: 		// unrecognized option in base CSV
381: 		return false;
382: 	}
383: 	return true;
384: }
385: 
386: std::string BufferedCSVReaderOptions::ToString() const {
387: 	return "DELIMITER='" + delimiter + (has_delimiter ? "'" : (auto_detect ? "' (auto detected)" : "' (default)")) +
388: 	       ", QUOTE='" + quote + (has_quote ? "'" : (auto_detect ? "' (auto detected)" : "' (default)")) +
389: 	       ", ESCAPE='" + escape + (has_escape ? "'" : (auto_detect ? "' (auto detected)" : "' (default)")) +
390: 	       ", HEADER=" + std::to_string(header) +
391: 	       (has_header ? "" : (auto_detect ? " (auto detected)" : "' (default)")) +
392: 	       ", SAMPLE_SIZE=" + std::to_string(sample_chunk_size * sample_chunks) +
393: 	       ", IGNORE_ERRORS=" + std::to_string(ignore_errors) + ", ALL_VARCHAR=" + std::to_string(all_varchar);
394: }
395: 
396: static string GetLineNumberStr(idx_t linenr, bool linenr_estimated) {
397: 	string estimated = (linenr_estimated ? string(" (estimated)") : string(""));
398: 	return to_string(linenr + 1) + estimated;
399: }
400: 
401: static bool StartsWithNumericDate(string &separator, const string &value) {
402: 	auto begin = value.c_str();
403: 	auto end = begin + value.size();
404: 
405: 	//	StrpTimeFormat::Parse will skip whitespace, so we can too
406: 	auto field1 = std::find_if_not(begin, end, StringUtil::CharacterIsSpace);
407: 	if (field1 == end) {
408: 		return false;
409: 	}
410: 
411: 	//	first numeric field must start immediately
412: 	if (!StringUtil::CharacterIsDigit(*field1)) {
413: 		return false;
414: 	}
415: 	auto literal1 = std::find_if_not(field1, end, StringUtil::CharacterIsDigit);
416: 	if (literal1 == end) {
417: 		return false;
418: 	}
419: 
420: 	//	second numeric field must exist
421: 	auto field2 = std::find_if(literal1, end, StringUtil::CharacterIsDigit);
422: 	if (field2 == end) {
423: 		return false;
424: 	}
425: 	auto literal2 = std::find_if_not(field2, end, StringUtil::CharacterIsDigit);
426: 	if (literal2 == end) {
427: 		return false;
428: 	}
429: 
430: 	//	third numeric field must exist
431: 	auto field3 = std::find_if(literal2, end, StringUtil::CharacterIsDigit);
432: 	if (field3 == end) {
433: 		return false;
434: 	}
435: 
436: 	//	second literal must match first
437: 	if (((field3 - literal2) != (field2 - literal1)) || strncmp(literal1, literal2, (field2 - literal1)) != 0) {
438: 		return false;
439: 	}
440: 
441: 	//	copy the literal as the separator, escaping percent signs
442: 	separator.clear();
443: 	while (literal1 < field2) {
444: 		const auto literal_char = *literal1++;
445: 		if (literal_char == '%') {
446: 			separator.push_back(literal_char);
447: 		}
448: 		separator.push_back(literal_char);
449: 	}
450: 
451: 	return true;
452: }
453: 
454: string GenerateDateFormat(const string &separator, const char *format_template) {
455: 	string format_specifier = format_template;
456: 	auto amount_of_dashes = std::count(format_specifier.begin(), format_specifier.end(), '-');
457: 	if (!amount_of_dashes) {
458: 		return format_specifier;
459: 	}
460: 	string result;
461: 	result.reserve(format_specifier.size() - amount_of_dashes + (amount_of_dashes * separator.size()));
462: 	for (auto &character : format_specifier) {
463: 		if (character == '-') {
464: 			result += separator;
465: 		} else {
466: 			result += character;
467: 		}
468: 	}
469: 	return result;
470: }
471: 
472: TextSearchShiftArray::TextSearchShiftArray() {
473: }
474: 
475: TextSearchShiftArray::TextSearchShiftArray(string search_term) : length(search_term.size()) {
476: 	if (length > 255) {
477: 		throw Exception("Size of delimiter/quote/escape in CSV reader is limited to 255 bytes");
478: 	}
479: 	// initialize the shifts array
480: 	shifts = unique_ptr<uint8_t[]>(new uint8_t[length * 255]);
481: 	memset(shifts.get(), 0, length * 255 * sizeof(uint8_t));
482: 	// iterate over each of the characters in the array
483: 	for (idx_t main_idx = 0; main_idx < length; main_idx++) {
484: 		uint8_t current_char = (uint8_t)search_term[main_idx];
485: 		// now move over all the remaining positions
486: 		for (idx_t i = main_idx; i < length; i++) {
487: 			bool is_match = true;
488: 			// check if the prefix matches at this position
489: 			// if it does, we move to this position after encountering the current character
490: 			for (idx_t j = 0; j < main_idx; j++) {
491: 				if (search_term[i - main_idx + j] != search_term[j]) {
492: 					is_match = false;
493: 				}
494: 			}
495: 			if (!is_match) {
496: 				continue;
497: 			}
498: 			shifts[i * 255 + current_char] = main_idx + 1;
499: 		}
500: 	}
501: }
502: 
503: BufferedCSVReader::BufferedCSVReader(FileSystem &fs_p, Allocator &allocator, FileOpener *opener_p,
504:                                      BufferedCSVReaderOptions options_p, const vector<LogicalType> &requested_types)
505:     : fs(fs_p), allocator(allocator), opener(opener_p), options(move(options_p)), buffer_size(0), position(0),
506:       start(0) {
507: 	file_handle = OpenCSV(options);
508: 	Initialize(requested_types);
509: }
510: 
511: BufferedCSVReader::BufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options_p,
512:                                      const vector<LogicalType> &requested_types)
513:     : BufferedCSVReader(FileSystem::GetFileSystem(context), Allocator::Get(context), FileSystem::GetFileOpener(context),
514:                         move(options_p), requested_types) {
515: }
516: 
517: BufferedCSVReader::~BufferedCSVReader() {
518: }
519: 
520: idx_t BufferedCSVReader::GetFileSize() {
521: 	return file_handle ? file_handle->FileSize() : 0;
522: }
523: 
524: void BufferedCSVReader::Initialize(const vector<LogicalType> &requested_types) {
525: 	PrepareComplexParser();
526: 	if (options.auto_detect) {
527: 		sql_types = SniffCSV(requested_types);
528: 		if (sql_types.empty()) {
529: 			throw Exception("Failed to detect column types from CSV: is the file a valid CSV file?");
530: 		}
531: 		if (cached_chunks.empty()) {
532: 			JumpToBeginning(options.skip_rows, options.header);
533: 		}
534: 	} else {
535: 		sql_types = requested_types;
536: 		ResetBuffer();
537: 		SkipRowsAndReadHeader(options.skip_rows, options.header);
538: 	}
539: 	InitParseChunk(sql_types.size());
540: 	InitInsertChunkIdx(sql_types.size());
541: 	// we only need reset support during the automatic CSV type detection
542: 	// since reset support might require caching (in the case of streams), we disable it for the remainder
543: 	file_handle->DisableReset();
544: }
545: 
546: void BufferedCSVReader::PrepareComplexParser() {
547: 	delimiter_search = TextSearchShiftArray(options.delimiter);
548: 	escape_search = TextSearchShiftArray(options.escape);
549: 	quote_search = TextSearchShiftArray(options.quote);
550: }
551: 
552: unique_ptr<CSVFileHandle> BufferedCSVReader::OpenCSV(const BufferedCSVReaderOptions &options) {
553: 	auto file_handle = fs.OpenFile(options.file_path.c_str(), FileFlags::FILE_FLAGS_READ, FileLockType::NO_LOCK,
554: 	                               options.compression, this->opener);
555: 	return make_unique<CSVFileHandle>(move(file_handle));
556: }
557: 
558: // Helper function to generate column names
559: static string GenerateColumnName(const idx_t total_cols, const idx_t col_number, const string &prefix = "column") {
560: 	int max_digits = NumericHelper::UnsignedLength(total_cols - 1);
561: 	int digits = NumericHelper::UnsignedLength(col_number);
562: 	string leading_zeros = string(max_digits - digits, '0');
563: 	string value = to_string(col_number);
564: 	return string(prefix + leading_zeros + value);
565: }
566: 
567: // Helper function for UTF-8 aware space trimming
568: static string TrimWhitespace(const string &col_name) {
569: 	utf8proc_int32_t codepoint;
570: 	auto str = reinterpret_cast<const utf8proc_uint8_t *>(col_name.c_str());
571: 	idx_t size = col_name.size();
572: 	// Find the first character that is not left trimmed
573: 	idx_t begin = 0;
574: 	while (begin < size) {
575: 		auto bytes = utf8proc_iterate(str + begin, size - begin, &codepoint);
576: 		D_ASSERT(bytes > 0);
577: 		if (utf8proc_category(codepoint) != UTF8PROC_CATEGORY_ZS) {
578: 			break;
579: 		}
580: 		begin += bytes;
581: 	}
582: 
583: 	// Find the last character that is not right trimmed
584: 	idx_t end;
585: 	end = begin;
586: 	for (auto next = begin; next < col_name.size();) {
587: 		auto bytes = utf8proc_iterate(str + next, size - next, &codepoint);
588: 		D_ASSERT(bytes > 0);
589: 		next += bytes;
590: 		if (utf8proc_category(codepoint) != UTF8PROC_CATEGORY_ZS) {
591: 			end = next;
592: 		}
593: 	}
594: 
595: 	// return the trimmed string
596: 	return col_name.substr(begin, end - begin);
597: }
598: 
599: static string NormalizeColumnName(const string &col_name) {
600: 	// normalize UTF8 characters to NFKD
601: 	auto nfkd = utf8proc_NFKD((const utf8proc_uint8_t *)col_name.c_str(), col_name.size());
602: 	const string col_name_nfkd = string((const char *)nfkd, strlen((const char *)nfkd));
603: 	free(nfkd);
604: 
605: 	// only keep ASCII characters 0-9 a-z A-Z and replace spaces with regular whitespace
606: 	string col_name_ascii = "";
607: 	for (idx_t i = 0; i < col_name_nfkd.size(); i++) {
608: 		if (col_name_nfkd[i] == '_' || (col_name_nfkd[i] >= '0' && col_name_nfkd[i] <= '9') ||
609: 		    (col_name_nfkd[i] >= 'A' && col_name_nfkd[i] <= 'Z') ||
610: 		    (col_name_nfkd[i] >= 'a' && col_name_nfkd[i] <= 'z')) {
611: 			col_name_ascii += col_name_nfkd[i];
612: 		} else if (StringUtil::CharacterIsSpace(col_name_nfkd[i])) {
613: 			col_name_ascii += " ";
614: 		}
615: 	}
616: 
617: 	// trim whitespace and replace remaining whitespace by _
618: 	string col_name_trimmed = TrimWhitespace(col_name_ascii);
619: 	string col_name_cleaned = "";
620: 	bool in_whitespace = false;
621: 	for (idx_t i = 0; i < col_name_trimmed.size(); i++) {
622: 		if (col_name_trimmed[i] == ' ') {
623: 			if (!in_whitespace) {
624: 				col_name_cleaned += "_";
625: 				in_whitespace = true;
626: 			}
627: 		} else {
628: 			col_name_cleaned += col_name_trimmed[i];
629: 			in_whitespace = false;
630: 		}
631: 	}
632: 
633: 	// don't leave string empty; if not empty, make lowercase
634: 	if (col_name_cleaned.empty()) {
635: 		col_name_cleaned = "_";
636: 	} else {
637: 		col_name_cleaned = StringUtil::Lower(col_name_cleaned);
638: 	}
639: 
640: 	// prepend _ if name starts with a digit or is a reserved keyword
641: 	if (KeywordHelper::IsKeyword(col_name_cleaned) || (col_name_cleaned[0] >= '0' && col_name_cleaned[0] <= '9')) {
642: 		col_name_cleaned = "_" + col_name_cleaned;
643: 	}
644: 	return col_name_cleaned;
645: }
646: 
647: void BufferedCSVReader::ResetBuffer() {
648: 	buffer.reset();
649: 	buffer_size = 0;
650: 	position = 0;
651: 	start = 0;
652: 	cached_buffers.clear();
653: }
654: 
655: void BufferedCSVReader::ResetStream() {
656: 	if (!file_handle->CanSeek()) {
657: 		// seeking to the beginning appears to not be supported in all compiler/os-scenarios,
658: 		// so we have to create a new stream source here for now
659: 		file_handle->Reset();
660: 	} else {
661: 		file_handle->Seek(0);
662: 	}
663: 	linenr = 0;
664: 	linenr_estimated = false;
665: 	bytes_per_line_avg = 0;
666: 	sample_chunk_idx = 0;
667: 	jumping_samples = false;
668: }
669: 
670: void BufferedCSVReader::InitParseChunk(idx_t num_cols) {
671: 	// adapt not null info
672: 	if (options.force_not_null.size() != num_cols) {
673: 		options.force_not_null.resize(num_cols, false);
674: 	}
675: 	if (num_cols == parse_chunk.ColumnCount()) {
676: 		parse_chunk.Reset();
677: 	} else {
678: 		parse_chunk.Destroy();
679: 
680: 		// initialize the parse_chunk with a set of VARCHAR types
681: 		vector<LogicalType> varchar_types(num_cols, LogicalType::VARCHAR);
682: 		parse_chunk.Initialize(allocator, varchar_types);
683: 	}
684: }
685: 
686: void BufferedCSVReader::InitInsertChunkIdx(idx_t num_cols) {
687: 	for (idx_t col = 0; col < num_cols; ++col) {
688: 		insert_cols_idx.push_back(col);
689: 	}
690: }
691: 
692: void BufferedCSVReader::JumpToBeginning(idx_t skip_rows = 0, bool skip_header = false) {
693: 	ResetBuffer();
694: 	ResetStream();
695: 	sample_chunk_idx = 0;
696: 	bytes_in_chunk = 0;
697: 	end_of_file_reached = false;
698: 	bom_checked = false;
699: 	SkipRowsAndReadHeader(skip_rows, skip_header);
700: }
701: 
702: void BufferedCSVReader::SkipRowsAndReadHeader(idx_t skip_rows, bool skip_header) {
703: 	for (idx_t i = 0; i < skip_rows; i++) {
704: 		// ignore skip rows
705: 		string read_line = file_handle->ReadLine();
706: 		linenr++;
707: 	}
708: 
709: 	if (skip_header) {
710: 		// ignore the first line as a header line
711: 		InitParseChunk(sql_types.size());
712: 		ParseCSV(ParserMode::PARSING_HEADER);
713: 	}
714: }
715: 
716: bool BufferedCSVReader::JumpToNextSample() {
717: 	// get bytes contained in the previously read chunk
718: 	idx_t remaining_bytes_in_buffer = buffer_size - start;
719: 	bytes_in_chunk -= remaining_bytes_in_buffer;
720: 	if (remaining_bytes_in_buffer == 0) {
721: 		return false;
722: 	}
723: 
724: 	// assess if it makes sense to jump, based on size of the first chunk relative to size of the entire file
725: 	if (sample_chunk_idx == 0) {
726: 		idx_t bytes_first_chunk = bytes_in_chunk;
727: 		double chunks_fit = (file_handle->FileSize() / (double)bytes_first_chunk);
728: 		jumping_samples = chunks_fit >= options.sample_chunks;
729: 
730: 		// jump back to the beginning
731: 		JumpToBeginning(options.skip_rows, options.header);
732: 		sample_chunk_idx++;
733: 		return true;
734: 	}
735: 
736: 	if (end_of_file_reached || sample_chunk_idx >= options.sample_chunks) {
737: 		return false;
738: 	}
739: 
740: 	// if we deal with any other sources than plaintext files, jumping_samples can be tricky. In that case
741: 	// we just read x continuous chunks from the stream TODO: make jumps possible for zipfiles.
742: 	if (!file_handle->PlainFileSource() || !jumping_samples) {
743: 		sample_chunk_idx++;
744: 		return true;
745: 	}
746: 
747: 	// update average bytes per line
748: 	double bytes_per_line = bytes_in_chunk / (double)options.sample_chunk_size;
749: 	bytes_per_line_avg = ((bytes_per_line_avg * (sample_chunk_idx)) + bytes_per_line) / (sample_chunk_idx + 1);
750: 
751: 	// if none of the previous conditions were met, we can jump
752: 	idx_t partition_size = (idx_t)round(file_handle->FileSize() / (double)options.sample_chunks);
753: 
754: 	// calculate offset to end of the current partition
755: 	int64_t offset = partition_size - bytes_in_chunk - remaining_bytes_in_buffer;
756: 	auto current_pos = file_handle->SeekPosition();
757: 
758: 	if (current_pos + offset < file_handle->FileSize()) {
759: 		// set position in stream and clear failure bits
760: 		file_handle->Seek(current_pos + offset);
761: 
762: 		// estimate linenr
763: 		linenr += (idx_t)round((offset + remaining_bytes_in_buffer) / bytes_per_line_avg);
764: 		linenr_estimated = true;
765: 	} else {
766: 		// seek backwards from the end in last chunk and hope to catch the end of the file
767: 		// TODO: actually it would be good to make sure that the end of file is being reached, because
768: 		// messy end-lines are quite common. For this case, however, we first need a skip_end detection anyways.
769: 		file_handle->Seek(file_handle->FileSize() - bytes_in_chunk);
770: 
771: 		// estimate linenr
772: 		linenr = (idx_t)round((file_handle->FileSize() - bytes_in_chunk) / bytes_per_line_avg);
773: 		linenr_estimated = true;
774: 	}
775: 
776: 	// reset buffers and parse chunk
777: 	ResetBuffer();
778: 
779: 	// seek beginning of next line
780: 	// FIXME: if this jump ends up in a quoted linebreak, we will have a problem
781: 	string read_line = file_handle->ReadLine();
782: 	linenr++;
783: 
784: 	sample_chunk_idx++;
785: 
786: 	return true;
787: }
788: 
789: void BufferedCSVReader::SetDateFormat(const string &format_specifier, const LogicalTypeId &sql_type) {
790: 	options.has_format[sql_type] = true;
791: 	auto &date_format = options.date_format[sql_type];
792: 	date_format.format_specifier = format_specifier;
793: 	StrTimeFormat::ParseFormatSpecifier(date_format.format_specifier, date_format);
794: }
795: 
796: bool BufferedCSVReader::TryCastValue(const Value &value, const LogicalType &sql_type) {
797: 	if (options.has_format[LogicalTypeId::DATE] && sql_type.id() == LogicalTypeId::DATE) {
798: 		date_t result;
799: 		string error_message;
800: 		return options.date_format[LogicalTypeId::DATE].TryParseDate(string_t(StringValue::Get(value)), result,
801: 		                                                             error_message);
802: 	} else if (options.has_format[LogicalTypeId::TIMESTAMP] && sql_type.id() == LogicalTypeId::TIMESTAMP) {
803: 		timestamp_t result;
804: 		string error_message;
805: 		return options.date_format[LogicalTypeId::TIMESTAMP].TryParseTimestamp(string_t(StringValue::Get(value)),
806: 		                                                                       result, error_message);
807: 	} else {
808: 		Value new_value;
809: 		string error_message;
810: 		return value.DefaultTryCastAs(sql_type, new_value, &error_message, true);
811: 	}
812: }
813: 
814: struct TryCastDateOperator {
815: 	static bool Operation(BufferedCSVReaderOptions &options, string_t input, date_t &result, string &error_message) {
816: 		return options.date_format[LogicalTypeId::DATE].TryParseDate(input, result, error_message);
817: 	}
818: };
819: 
820: struct TryCastTimestampOperator {
821: 	static bool Operation(BufferedCSVReaderOptions &options, string_t input, timestamp_t &result,
822: 	                      string &error_message) {
823: 		return options.date_format[LogicalTypeId::TIMESTAMP].TryParseTimestamp(input, result, error_message);
824: 	}
825: };
826: 
827: template <class OP, class T>
828: static bool TemplatedTryCastDateVector(BufferedCSVReaderOptions &options, Vector &input_vector, Vector &result_vector,
829:                                        idx_t count, string &error_message) {
830: 	D_ASSERT(input_vector.GetType().id() == LogicalTypeId::VARCHAR);
831: 	bool all_converted = true;
832: 	UnaryExecutor::Execute<string_t, T>(input_vector, result_vector, count, [&](string_t input) {
833: 		T result;
834: 		if (!OP::Operation(options, input, result, error_message)) {
835: 			all_converted = false;
836: 		}
837: 		return result;
838: 	});
839: 	return all_converted;
840: }
841: 
842: bool TryCastDateVector(BufferedCSVReaderOptions &options, Vector &input_vector, Vector &result_vector, idx_t count,
843:                        string &error_message) {
844: 	return TemplatedTryCastDateVector<TryCastDateOperator, date_t>(options, input_vector, result_vector, count,
845: 	                                                               error_message);
846: }
847: 
848: bool TryCastTimestampVector(BufferedCSVReaderOptions &options, Vector &input_vector, Vector &result_vector, idx_t count,
849:                             string &error_message) {
850: 	return TemplatedTryCastDateVector<TryCastTimestampOperator, timestamp_t>(options, input_vector, result_vector,
851: 	                                                                         count, error_message);
852: }
853: 
854: bool BufferedCSVReader::TryCastVector(Vector &parse_chunk_col, idx_t size, const LogicalType &sql_type) {
855: 	// try vector-cast from string to sql_type
856: 	Vector dummy_result(sql_type);
857: 	if (options.has_format[LogicalTypeId::DATE] && sql_type == LogicalTypeId::DATE) {
858: 		// use the date format to cast the chunk
859: 		string error_message;
860: 		return TryCastDateVector(options, parse_chunk_col, dummy_result, size, error_message);
861: 	} else if (options.has_format[LogicalTypeId::TIMESTAMP] && sql_type == LogicalTypeId::TIMESTAMP) {
862: 		// use the timestamp format to cast the chunk
863: 		string error_message;
864: 		return TryCastTimestampVector(options, parse_chunk_col, dummy_result, size, error_message);
865: 	} else {
866: 		// target type is not varchar: perform a cast
867: 		string error_message;
868: 		return VectorOperations::DefaultTryCast(parse_chunk_col, dummy_result, size, &error_message, true);
869: 	}
870: }
871: 
872: enum class QuoteRule : uint8_t { QUOTES_RFC = 0, QUOTES_OTHER = 1, NO_QUOTES = 2 };
873: 
874: void BufferedCSVReader::DetectDialect(const vector<LogicalType> &requested_types,
875:                                       BufferedCSVReaderOptions &original_options,
876:                                       vector<BufferedCSVReaderOptions> &info_candidates, idx_t &best_num_cols) {
877: 	// set up the candidates we consider for delimiter and quote rules based on user input
878: 	vector<string> delim_candidates;
879: 	vector<QuoteRule> quoterule_candidates;
880: 	vector<vector<string>> quote_candidates_map;
881: 	vector<vector<string>> escape_candidates_map = {{""}, {"\\"}, {""}};
882: 
883: 	if (options.has_delimiter) {
884: 		// user provided a delimiter: use that delimiter
885: 		delim_candidates = {options.delimiter};
886: 	} else {
887: 		// no delimiter provided: try standard/common delimiters
888: 		delim_candidates = {",", "|", ";", "\t"};
889: 	}
890: 	if (options.has_quote) {
891: 		// user provided quote: use that quote rule
892: 		quote_candidates_map = {{options.quote}, {options.quote}, {options.quote}};
893: 	} else {
894: 		// no quote rule provided: use standard/common quotes
895: 		quote_candidates_map = {{"\""}, {"\"", "'"}, {""}};
896: 	}
897: 	if (options.has_escape) {
898: 		// user provided escape: use that escape rule
899: 		if (options.escape.empty()) {
900: 			quoterule_candidates = {QuoteRule::QUOTES_RFC};
901: 		} else {
902: 			quoterule_candidates = {QuoteRule::QUOTES_OTHER};
903: 		}
904: 		escape_candidates_map[static_cast<uint8_t>(quoterule_candidates[0])] = {options.escape};
905: 	} else {
906: 		// no escape provided: try standard/common escapes
907: 		quoterule_candidates = {QuoteRule::QUOTES_RFC, QuoteRule::QUOTES_OTHER, QuoteRule::NO_QUOTES};
908: 	}
909: 
910: 	idx_t best_consistent_rows = 0;
911: 	for (auto quoterule : quoterule_candidates) {
912: 		const auto &quote_candidates = quote_candidates_map[static_cast<uint8_t>(quoterule)];
913: 		for (const auto &quote : quote_candidates) {
914: 			for (const auto &delim : delim_candidates) {
915: 				const auto &escape_candidates = escape_candidates_map[static_cast<uint8_t>(quoterule)];
916: 				for (const auto &escape : escape_candidates) {
917: 					BufferedCSVReaderOptions sniff_info = original_options;
918: 					sniff_info.delimiter = delim;
919: 					sniff_info.quote = quote;
920: 					sniff_info.escape = escape;
921: 
922: 					options = sniff_info;
923: 					PrepareComplexParser();
924: 
925: 					JumpToBeginning(original_options.skip_rows);
926: 					sniffed_column_counts.clear();
927: 
928: 					if (!TryParseCSV(ParserMode::SNIFFING_DIALECT)) {
929: 						continue;
930: 					}
931: 
932: 					idx_t start_row = original_options.skip_rows;
933: 					idx_t consistent_rows = 0;
934: 					idx_t num_cols = 0;
935: 
936: 					for (idx_t row = 0; row < sniffed_column_counts.size(); row++) {
937: 						if (sniffed_column_counts[row] == num_cols) {
938: 							consistent_rows++;
939: 						} else {
940: 							num_cols = sniffed_column_counts[row];
941: 							start_row = row + original_options.skip_rows;
942: 							consistent_rows = 1;
943: 						}
944: 					}
945: 
946: 					// some logic
947: 					bool more_values = (consistent_rows > best_consistent_rows && num_cols >= best_num_cols);
948: 					bool single_column_before = best_num_cols < 2 && num_cols > best_num_cols;
949: 					bool rows_consistent =
950: 					    start_row + consistent_rows - original_options.skip_rows == sniffed_column_counts.size();
951: 					bool more_than_one_row = (consistent_rows > 1);
952: 					bool more_than_one_column = (num_cols > 1);
953: 					bool start_good = !info_candidates.empty() && (start_row <= info_candidates.front().skip_rows);
954: 
955: 					if (!requested_types.empty() && requested_types.size() != num_cols) {
956: 						continue;
957: 					} else if ((more_values || single_column_before) && rows_consistent) {
958: 						sniff_info.skip_rows = start_row;
959: 						sniff_info.num_cols = num_cols;
960: 						best_consistent_rows = consistent_rows;
961: 						best_num_cols = num_cols;
962: 
963: 						info_candidates.clear();
964: 						info_candidates.push_back(sniff_info);
965: 					} else if (more_than_one_row && more_than_one_column && start_good && rows_consistent) {
966: 						bool same_quote_is_candidate = false;
967: 						for (auto &info_candidate : info_candidates) {
968: 							if (quote.compare(info_candidate.quote) == 0) {
969: 								same_quote_is_candidate = true;
970: 							}
971: 						}
972: 						if (!same_quote_is_candidate) {
973: 							sniff_info.skip_rows = start_row;
974: 							sniff_info.num_cols = num_cols;
975: 							info_candidates.push_back(sniff_info);
976: 						}
977: 					}
978: 				}
979: 			}
980: 		}
981: 	}
982: }
983: 
984: void BufferedCSVReader::DetectCandidateTypes(const vector<LogicalType> &type_candidates,
985:                                              const map<LogicalTypeId, vector<const char *>> &format_template_candidates,
986:                                              const vector<BufferedCSVReaderOptions> &info_candidates,
987:                                              BufferedCSVReaderOptions &original_options, idx_t best_num_cols,
988:                                              vector<vector<LogicalType>> &best_sql_types_candidates,
989:                                              std::map<LogicalTypeId, vector<string>> &best_format_candidates,
990:                                              DataChunk &best_header_row) {
991: 	BufferedCSVReaderOptions best_options;
992: 	idx_t min_varchar_cols = best_num_cols + 1;
993: 
994: 	// check which info candidate leads to minimum amount of non-varchar columns...
995: 	for (const auto &t : format_template_candidates) {
996: 		best_format_candidates[t.first].clear();
997: 	}
998: 	for (auto &info_candidate : info_candidates) {
999: 		options = info_candidate;
1000: 		vector<vector<LogicalType>> info_sql_types_candidates(options.num_cols, type_candidates);
1001: 		std::map<LogicalTypeId, bool> has_format_candidates;
1002: 		std::map<LogicalTypeId, vector<string>> format_candidates;
1003: 		for (const auto &t : format_template_candidates) {
1004: 			has_format_candidates[t.first] = false;
1005: 			format_candidates[t.first].clear();
1006: 		}
1007: 
1008: 		// set all sql_types to VARCHAR so we can do datatype detection based on VARCHAR values
1009: 		sql_types.clear();
1010: 		sql_types.assign(options.num_cols, LogicalType::VARCHAR);
1011: 
1012: 		// jump to beginning and skip potential header
1013: 		JumpToBeginning(options.skip_rows, true);
1014: 		DataChunk header_row;
1015: 		header_row.Initialize(allocator, sql_types);
1016: 		parse_chunk.Copy(header_row);
1017: 
1018: 		if (header_row.size() == 0) {
1019: 			continue;
1020: 		}
1021: 
1022: 		// init parse chunk and read csv with info candidate
1023: 		InitParseChunk(sql_types.size());
1024: 		ParseCSV(ParserMode::SNIFFING_DATATYPES);
1025: 		for (idx_t row_idx = 0; row_idx <= parse_chunk.size(); row_idx++) {
1026: 			bool is_header_row = row_idx == 0;
1027: 			idx_t row = row_idx - 1;
1028: 			for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
1029: 				auto &col_type_candidates = info_sql_types_candidates[col];
1030: 				while (col_type_candidates.size() > 1) {
1031: 					const auto &sql_type = col_type_candidates.back();
1032: 					// try cast from string to sql_type
1033: 					Value dummy_val;
1034: 					if (is_header_row) {
1035: 						dummy_val = header_row.GetValue(col, 0);
1036: 					} else {
1037: 						dummy_val = parse_chunk.GetValue(col, row);
1038: 					}
1039: 					// try formatting for date types if the user did not specify one and it starts with numeric values.
1040: 					string separator;
1041: 					if (has_format_candidates.count(sql_type.id()) && !original_options.has_format[sql_type.id()] &&
1042: 					    StartsWithNumericDate(separator, StringValue::Get(dummy_val))) {
1043: 						// generate date format candidates the first time through
1044: 						auto &type_format_candidates = format_candidates[sql_type.id()];
1045: 						const auto had_format_candidates = has_format_candidates[sql_type.id()];
1046: 						if (!has_format_candidates[sql_type.id()]) {
1047: 							has_format_candidates[sql_type.id()] = true;
1048: 							// order by preference
1049: 							auto entry = format_template_candidates.find(sql_type.id());
1050: 							if (entry != format_template_candidates.end()) {
1051: 								const auto &format_template_list = entry->second;
1052: 								for (const auto &t : format_template_list) {
1053: 									const auto format_string = GenerateDateFormat(separator, t);
1054: 									// don't parse ISO 8601
1055: 									if (format_string.find("%Y-%m-%d") == string::npos) {
1056: 										type_format_candidates.emplace_back(format_string);
1057: 									}
1058: 								}
1059: 							}
1060: 							//	initialise the first candidate
1061: 							options.has_format[sql_type.id()] = true;
1062: 							//	all formats are constructed to be valid
1063: 							SetDateFormat(type_format_candidates.back(), sql_type.id());
1064: 						}
1065: 						// check all formats and keep the first one that works
1066: 						StrpTimeFormat::ParseResult result;
1067: 						auto save_format_candidates = type_format_candidates;
1068: 						while (!type_format_candidates.empty()) {
1069: 							//	avoid using exceptions for flow control...
1070: 							auto &current_format = options.date_format[sql_type.id()];
1071: 							if (current_format.Parse(StringValue::Get(dummy_val), result)) {
1072: 								break;
1073: 							}
1074: 							//	doesn't work - move to the next one
1075: 							type_format_candidates.pop_back();
1076: 							options.has_format[sql_type.id()] = (!type_format_candidates.empty());
1077: 							if (!type_format_candidates.empty()) {
1078: 								SetDateFormat(type_format_candidates.back(), sql_type.id());
1079: 							}
1080: 						}
1081: 						//	if none match, then this is not a value of type sql_type,
1082: 						if (type_format_candidates.empty()) {
1083: 							//	so restore the candidates that did work.
1084: 							//	or throw them out if they were generated by this value.
1085: 							if (had_format_candidates) {
1086: 								type_format_candidates.swap(save_format_candidates);
1087: 								if (!type_format_candidates.empty()) {
1088: 									SetDateFormat(type_format_candidates.back(), sql_type.id());
1089: 								}
1090: 							} else {
1091: 								has_format_candidates[sql_type.id()] = false;
1092: 							}
1093: 						}
1094: 					}
1095: 					// try cast from string to sql_type
1096: 					if (TryCastValue(dummy_val, sql_type)) {
1097: 						break;
1098: 					} else {
1099: 						col_type_candidates.pop_back();
1100: 					}
1101: 				}
1102: 			}
1103: 			// reset type detection, because first row could be header,
1104: 			// but only do it if csv has more than one line (including header)
1105: 			if (parse_chunk.size() > 0 && is_header_row) {
1106: 				info_sql_types_candidates = vector<vector<LogicalType>>(options.num_cols, type_candidates);
1107: 				for (auto &f : format_candidates) {
1108: 					f.second.clear();
1109: 				}
1110: 				for (auto &h : has_format_candidates) {
1111: 					h.second = false;
1112: 				}
1113: 			}
1114: 		}
1115: 
1116: 		idx_t varchar_cols = 0;
1117: 		for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
1118: 			auto &col_type_candidates = info_sql_types_candidates[col];
1119: 			// check number of varchar columns
1120: 			const auto &col_type = col_type_candidates.back();
1121: 			if (col_type == LogicalType::VARCHAR) {
1122: 				varchar_cols++;
1123: 			}
1124: 		}
1125: 
1126: 		// it's good if the dialect creates more non-varchar columns, but only if we sacrifice < 30% of best_num_cols.
1127: 		if (varchar_cols < min_varchar_cols && parse_chunk.ColumnCount() > (best_num_cols * 0.7)) {
1128: 			// we have a new best_options candidate
1129: 			best_options = info_candidate;
1130: 			min_varchar_cols = varchar_cols;
1131: 			best_sql_types_candidates = info_sql_types_candidates;
1132: 			best_format_candidates = format_candidates;
1133: 			best_header_row.Destroy();
1134: 			auto header_row_types = header_row.GetTypes();
1135: 			best_header_row.Initialize(allocator, header_row_types);
1136: 			header_row.Copy(best_header_row);
1137: 		}
1138: 	}
1139: 
1140: 	options = best_options;
1141: 	for (const auto &best : best_format_candidates) {
1142: 		if (!best.second.empty()) {
1143: 			SetDateFormat(best.second.back(), best.first);
1144: 		}
1145: 	}
1146: }
1147: 
1148: void BufferedCSVReader::DetectHeader(const vector<vector<LogicalType>> &best_sql_types_candidates,
1149:                                      const DataChunk &best_header_row) {
1150: 	// information for header detection
1151: 	bool first_row_consistent = true;
1152: 	bool first_row_nulls = false;
1153: 
1154: 	// check if header row is all null and/or consistent with detected column data types
1155: 	first_row_nulls = true;
1156: 	for (idx_t col = 0; col < best_sql_types_candidates.size(); col++) {
1157: 		auto dummy_val = best_header_row.GetValue(col, 0);
1158: 		if (!dummy_val.IsNull()) {
1159: 			first_row_nulls = false;
1160: 		}
1161: 
1162: 		// try cast to sql_type of column
1163: 		const auto &sql_type = best_sql_types_candidates[col].back();
1164: 		if (!TryCastValue(dummy_val, sql_type)) {
1165: 			first_row_consistent = false;
1166: 		}
1167: 	}
1168: 
1169: 	// update parser info, and read, generate & set col_names based on previous findings
1170: 	if (((!first_row_consistent || first_row_nulls) && !options.has_header) || (options.has_header && options.header)) {
1171: 		options.header = true;
1172: 		case_insensitive_map_t<idx_t> name_collision_count;
1173: 		// get header names from CSV
1174: 		for (idx_t col = 0; col < options.num_cols; col++) {
1175: 			const auto &val = best_header_row.GetValue(col, 0);
1176: 			string col_name = val.ToString();
1177: 
1178: 			// generate name if field is empty
1179: 			if (col_name.empty() || val.IsNull()) {
1180: 				col_name = GenerateColumnName(options.num_cols, col);
1181: 			}
1182: 
1183: 			// normalize names or at least trim whitespace
1184: 			if (options.normalize_names) {
1185: 				col_name = NormalizeColumnName(col_name);
1186: 			} else {
1187: 				col_name = TrimWhitespace(col_name);
1188: 			}
1189: 
1190: 			// avoid duplicate header names
1191: 			const string col_name_raw = col_name;
1192: 			while (name_collision_count.find(col_name) != name_collision_count.end()) {
1193: 				name_collision_count[col_name] += 1;
1194: 				col_name = col_name + "_" + to_string(name_collision_count[col_name]);
1195: 			}
1196: 
1197: 			col_names.push_back(col_name);
1198: 			name_collision_count[col_name] = 0;
1199: 		}
1200: 
1201: 	} else {
1202: 		options.header = false;
1203: 		for (idx_t col = 0; col < options.num_cols; col++) {
1204: 			string column_name = GenerateColumnName(options.num_cols, col);
1205: 			col_names.push_back(column_name);
1206: 		}
1207: 	}
1208: }
1209: 
1210: vector<LogicalType> BufferedCSVReader::RefineTypeDetection(const vector<LogicalType> &type_candidates,
1211:                                                            const vector<LogicalType> &requested_types,
1212:                                                            vector<vector<LogicalType>> &best_sql_types_candidates,
1213:                                                            map<LogicalTypeId, vector<string>> &best_format_candidates) {
1214: 	// for the type refine we set the SQL types to VARCHAR for all columns
1215: 	sql_types.clear();
1216: 	sql_types.assign(options.num_cols, LogicalType::VARCHAR);
1217: 
1218: 	vector<LogicalType> detected_types;
1219: 
1220: 	// if data types were provided, exit here if number of columns does not match
1221: 	if (!requested_types.empty()) {
1222: 		if (requested_types.size() != options.num_cols) {
1223: 			throw InvalidInputException(
1224: 			    "Error while determining column types: found %lld columns but expected %d. (%s)", options.num_cols,
1225: 			    requested_types.size(), options.ToString());
1226: 		} else {
1227: 			detected_types = requested_types;
1228: 		}
1229: 	} else if (options.all_varchar) {
1230: 		// return all types varchar
1231: 		detected_types = sql_types;
1232: 	} else {
1233: 		// jump through the rest of the file and continue to refine the sql type guess
1234: 		while (JumpToNextSample()) {
1235: 			InitParseChunk(sql_types.size());
1236: 			// if jump ends up a bad line, we just skip this chunk
1237: 			if (!TryParseCSV(ParserMode::SNIFFING_DATATYPES)) {
1238: 				continue;
1239: 			}
1240: 			for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
1241: 				vector<LogicalType> &col_type_candidates = best_sql_types_candidates[col];
1242: 				while (col_type_candidates.size() > 1) {
1243: 					const auto &sql_type = col_type_candidates.back();
1244: 					//	narrow down the date formats
1245: 					if (best_format_candidates.count(sql_type.id())) {
1246: 						auto &best_type_format_candidates = best_format_candidates[sql_type.id()];
1247: 						auto save_format_candidates = best_type_format_candidates;
1248: 						while (!best_type_format_candidates.empty()) {
1249: 							if (TryCastVector(parse_chunk.data[col], parse_chunk.size(), sql_type)) {
1250: 								break;
1251: 							}
1252: 							//	doesn't work - move to the next one
1253: 							best_type_format_candidates.pop_back();
1254: 							options.has_format[sql_type.id()] = (!best_type_format_candidates.empty());
1255: 							if (!best_type_format_candidates.empty()) {
1256: 								SetDateFormat(best_type_format_candidates.back(), sql_type.id());
1257: 							}
1258: 						}
1259: 						//	if none match, then this is not a column of type sql_type,
1260: 						if (best_type_format_candidates.empty()) {
1261: 							//	so restore the candidates that did work.
1262: 							best_type_format_candidates.swap(save_format_candidates);
1263: 							if (!best_type_format_candidates.empty()) {
1264: 								SetDateFormat(best_type_format_candidates.back(), sql_type.id());
1265: 							}
1266: 						}
1267: 					}
1268: 
1269: 					if (TryCastVector(parse_chunk.data[col], parse_chunk.size(), sql_type)) {
1270: 						break;
1271: 					} else {
1272: 						col_type_candidates.pop_back();
1273: 					}
1274: 				}
1275: 			}
1276: 
1277: 			if (!jumping_samples) {
1278: 				if ((sample_chunk_idx)*options.sample_chunk_size <= options.buffer_size) {
1279: 					// cache parse chunk
1280: 					// create a new chunk and fill it with the remainder
1281: 					auto chunk = make_unique<DataChunk>();
1282: 					auto parse_chunk_types = parse_chunk.GetTypes();
1283: 					chunk->Move(parse_chunk);
1284: 					cached_chunks.push(move(chunk));
1285: 				} else {
1286: 					while (!cached_chunks.empty()) {
1287: 						cached_chunks.pop();
1288: 					}
1289: 				}
1290: 			}
1291: 		}
1292: 
1293: 		// set sql types
1294: 		for (auto &best_sql_types_candidate : best_sql_types_candidates) {
1295: 			LogicalType d_type = best_sql_types_candidate.back();
1296: 			if (best_sql_types_candidate.size() == type_candidates.size()) {
1297: 				d_type = LogicalType::VARCHAR;
1298: 			}
1299: 			detected_types.push_back(d_type);
1300: 		}
1301: 	}
1302: 
1303: 	return detected_types;
1304: }
1305: 
1306: vector<LogicalType> BufferedCSVReader::SniffCSV(const vector<LogicalType> &requested_types) {
1307: 	for (auto &type : requested_types) {
1308: 		// auto detect for blobs not supported: there may be invalid UTF-8 in the file
1309: 		if (type.id() == LogicalTypeId::BLOB) {
1310: 			return requested_types;
1311: 		}
1312: 	}
1313: 
1314: 	// #######
1315: 	// ### dialect detection
1316: 	// #######
1317: 	BufferedCSVReaderOptions original_options = options;
1318: 	vector<BufferedCSVReaderOptions> info_candidates;
1319: 	idx_t best_num_cols = 0;
1320: 
1321: 	DetectDialect(requested_types, original_options, info_candidates, best_num_cols);
1322: 
1323: 	// if no dialect candidate was found, then file was most likely empty and we throw an exception
1324: 	if (info_candidates.empty()) {
1325: 		throw InvalidInputException(
1326: 		    "Error in file \"%s\": CSV options could not be auto-detected. Consider setting parser options manually.",
1327: 		    options.file_path);
1328: 	}
1329: 
1330: 	// #######
1331: 	// ### type detection (initial)
1332: 	// #######
1333: 	// type candidates, ordered by descending specificity (~ from high to low)
1334: 	vector<LogicalType> type_candidates = {
1335: 	    LogicalType::VARCHAR, LogicalType::TIMESTAMP,
1336: 	    LogicalType::DATE,    LogicalType::TIME,
1337: 	    LogicalType::DOUBLE,  /* LogicalType::FLOAT,*/ LogicalType::BIGINT,
1338: 	    LogicalType::INTEGER, /*LogicalType::SMALLINT, LogicalType::TINYINT,*/ LogicalType::BOOLEAN,
1339: 	    LogicalType::SQLNULL};
1340: 	// format template candidates, ordered by descending specificity (~ from high to low)
1341: 	std::map<LogicalTypeId, vector<const char *>> format_template_candidates = {
1342: 	    {LogicalTypeId::DATE, {"%m-%d-%Y", "%m-%d-%y", "%d-%m-%Y", "%d-%m-%y", "%Y-%m-%d", "%y-%m-%d"}},
1343: 	    {LogicalTypeId::TIMESTAMP,
1344: 	     {"%Y-%m-%d %H:%M:%S.%f", "%m-%d-%Y %I:%M:%S %p", "%m-%d-%y %I:%M:%S %p", "%d-%m-%Y %H:%M:%S",
1345: 	      "%d-%m-%y %H:%M:%S", "%Y-%m-%d %H:%M:%S", "%y-%m-%d %H:%M:%S"}},
1346: 	};
1347: 	vector<vector<LogicalType>> best_sql_types_candidates;
1348: 	map<LogicalTypeId, vector<string>> best_format_candidates;
1349: 	DataChunk best_header_row;
1350: 	DetectCandidateTypes(type_candidates, format_template_candidates, info_candidates, original_options, best_num_cols,
1351: 	                     best_sql_types_candidates, best_format_candidates, best_header_row);
1352: 
1353: 	// #######
1354: 	// ### header detection
1355: 	// #######
1356: 	options.num_cols = best_num_cols;
1357: 	DetectHeader(best_sql_types_candidates, best_header_row);
1358: 
1359: 	// #######
1360: 	// ### type detection (refining)
1361: 	// #######
1362: 	return RefineTypeDetection(type_candidates, requested_types, best_sql_types_candidates, best_format_candidates);
1363: }
1364: 
1365: bool BufferedCSVReader::TryParseComplexCSV(DataChunk &insert_chunk, string &error_message) {
1366: 	// used for parsing algorithm
1367: 	bool finished_chunk = false;
1368: 	idx_t column = 0;
1369: 	vector<idx_t> escape_positions;
1370: 	bool has_quotes = false;
1371: 	uint8_t delimiter_pos = 0, escape_pos = 0, quote_pos = 0;
1372: 	idx_t offset = 0;
1373: 
1374: 	// read values into the buffer (if any)
1375: 	if (position >= buffer_size) {
1376: 		if (!ReadBuffer(start)) {
1377: 			return true;
1378: 		}
1379: 	}
1380: 	// start parsing the first value
1381: 	start = position;
1382: 	goto value_start;
1383: value_start:
1384: 	/* state: value_start */
1385: 	// this state parses the first characters of a value
1386: 	offset = 0;
1387: 	delimiter_pos = 0;
1388: 	quote_pos = 0;
1389: 	do {
1390: 		idx_t count = 0;
1391: 		for (; position < buffer_size; position++) {
1392: 			quote_search.Match(quote_pos, buffer[position]);
1393: 			delimiter_search.Match(delimiter_pos, buffer[position]);
1394: 			count++;
1395: 			if (delimiter_pos == options.delimiter.size()) {
1396: 				// found a delimiter, add the value
1397: 				offset = options.delimiter.size() - 1;
1398: 				goto add_value;
1399: 			} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1400: 				// found a newline, add the row
1401: 				goto add_row;
1402: 			}
1403: 			if (count > quote_pos) {
1404: 				// did not find a quote directly at the start of the value, stop looking for the quote now
1405: 				goto normal;
1406: 			}
1407: 			if (quote_pos == options.quote.size()) {
1408: 				// found a quote, go to quoted loop and skip the initial quote
1409: 				start += options.quote.size();
1410: 				goto in_quotes;
1411: 			}
1412: 		}
1413: 	} while (ReadBuffer(start));
1414: 	// file ends while scanning for quote/delimiter, go to final state
1415: 	goto final_state;
1416: normal:
1417: 	/* state: normal parsing state */
1418: 	// this state parses the remainder of a non-quoted value until we reach a delimiter or newline
1419: 	position++;
1420: 	do {
1421: 		for (; position < buffer_size; position++) {
1422: 			delimiter_search.Match(delimiter_pos, buffer[position]);
1423: 			if (delimiter_pos == options.delimiter.size()) {
1424: 				offset = options.delimiter.size() - 1;
1425: 				goto add_value;
1426: 			} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1427: 				goto add_row;
1428: 			}
1429: 		}
1430: 	} while (ReadBuffer(start));
1431: 	goto final_state;
1432: add_value:
1433: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions, has_quotes);
1434: 	// increase position by 1 and move start to the new position
1435: 	offset = 0;
1436: 	has_quotes = false;
1437: 	start = ++position;
1438: 	if (position >= buffer_size && !ReadBuffer(start)) {
1439: 		// file ends right after delimiter, go to final state
1440: 		goto final_state;
1441: 	}
1442: 	goto value_start;
1443: add_row : {
1444: 	// check type of newline (\r or \n)
1445: 	bool carriage_return = buffer[position] == '\r';
1446: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions, has_quotes);
1447: 	finished_chunk = AddRow(insert_chunk, column);
1448: 	// increase position by 1 and move start to the new position
1449: 	offset = 0;
1450: 	has_quotes = false;
1451: 	start = ++position;
1452: 	if (position >= buffer_size && !ReadBuffer(start)) {
1453: 		// file ends right after newline, go to final state
1454: 		goto final_state;
1455: 	}
1456: 	if (carriage_return) {
1457: 		// \r newline, go to special state that parses an optional \n afterwards
1458: 		goto carriage_return;
1459: 	} else {
1460: 		// \n newline, move to value start
1461: 		if (finished_chunk) {
1462: 			return true;
1463: 		}
1464: 		goto value_start;
1465: 	}
1466: }
1467: in_quotes:
1468: 	/* state: in_quotes */
1469: 	// this state parses the remainder of a quoted value
1470: 	quote_pos = 0;
1471: 	escape_pos = 0;
1472: 	has_quotes = true;
1473: 	position++;
1474: 	do {
1475: 		for (; position < buffer_size; position++) {
1476: 			quote_search.Match(quote_pos, buffer[position]);
1477: 			escape_search.Match(escape_pos, buffer[position]);
1478: 			if (quote_pos == options.quote.size()) {
1479: 				goto unquote;
1480: 			} else if (escape_pos == options.escape.size()) {
1481: 				escape_positions.push_back(position - start - (options.escape.size() - 1));
1482: 				goto handle_escape;
1483: 			}
1484: 		}
1485: 	} while (ReadBuffer(start));
1486: 	// still in quoted state at the end of the file, error:
1487: 	error_message = StringUtil::Format("Error in file \"%s\" on line %s: unterminated quotes. (%s)", options.file_path,
1488: 	                                   GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1489: 	return false;
1490: unquote:
1491: 	/* state: unquote */
1492: 	// this state handles the state directly after we unquote
1493: 	// in this state we expect either another quote (entering the quoted state again, and escaping the quote)
1494: 	// or a delimiter/newline, ending the current value and moving on to the next value
1495: 	delimiter_pos = 0;
1496: 	quote_pos = 0;
1497: 	position++;
1498: 	if (position >= buffer_size && !ReadBuffer(start)) {
1499: 		// file ends right after unquote, go to final state
1500: 		offset = options.quote.size();
1501: 		goto final_state;
1502: 	}
1503: 	if (StringUtil::CharacterIsNewline(buffer[position])) {
1504: 		// quote followed by newline, add row
1505: 		offset = options.quote.size();
1506: 		goto add_row;
1507: 	}
1508: 	do {
1509: 		idx_t count = 0;
1510: 		for (; position < buffer_size; position++) {
1511: 			quote_search.Match(quote_pos, buffer[position]);
1512: 			delimiter_search.Match(delimiter_pos, buffer[position]);
1513: 			count++;
1514: 			if (count > delimiter_pos && count > quote_pos) {
1515: 				error_message = StringUtil::Format(
1516: 				    "Error in file \"%s\" on line %s: quote should be followed by end of value, end "
1517: 				    "of row or another quote. (%s)",
1518: 				    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1519: 				return false;
1520: 			}
1521: 			if (delimiter_pos == options.delimiter.size()) {
1522: 				// quote followed by delimiter, add value
1523: 				offset = options.quote.size() + options.delimiter.size() - 1;
1524: 				goto add_value;
1525: 			} else if (quote_pos == options.quote.size() &&
1526: 			           (options.escape.empty() || options.escape == options.quote)) {
1527: 				// quote followed by quote, go back to quoted state and add to escape
1528: 				escape_positions.push_back(position - start - (options.quote.size() - 1));
1529: 				goto in_quotes;
1530: 			}
1531: 		}
1532: 	} while (ReadBuffer(start));
1533: 	error_message = StringUtil::Format(
1534: 	    "Error in file \"%s\" on line %s: quote should be followed by end of value, end of row or another quote. (%s)",
1535: 	    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1536: 	return false;
1537: handle_escape:
1538: 	escape_pos = 0;
1539: 	quote_pos = 0;
1540: 	position++;
1541: 	do {
1542: 		idx_t count = 0;
1543: 		for (; position < buffer_size; position++) {
1544: 			quote_search.Match(quote_pos, buffer[position]);
1545: 			escape_search.Match(escape_pos, buffer[position]);
1546: 			count++;
1547: 			if (count > escape_pos && count > quote_pos) {
1548: 				error_message = StringUtil::Format(
1549: 				    "Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)",
1550: 				    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1551: 				return false;
1552: 			}
1553: 			if (quote_pos == options.quote.size() || escape_pos == options.escape.size()) {
1554: 				// found quote or escape: move back to quoted state
1555: 				goto in_quotes;
1556: 			}
1557: 		}
1558: 	} while (ReadBuffer(start));
1559: 	error_message =
1560: 	    StringUtil::Format("Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)",
1561: 	                       options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1562: 	return false;
1563: carriage_return:
1564: 	/* state: carriage_return */
1565: 	// this stage optionally skips a newline (\n) character, which allows \r\n to be interpreted as a single line
1566: 	if (buffer[position] == '\n') {
1567: 		// newline after carriage return: skip
1568: 		start = ++position;
1569: 		if (position >= buffer_size && !ReadBuffer(start)) {
1570: 			// file ends right after newline, go to final state
1571: 			goto final_state;
1572: 		}
1573: 	}
1574: 	if (finished_chunk) {
1575: 		return true;
1576: 	}
1577: 	goto value_start;
1578: final_state:
1579: 	if (finished_chunk) {
1580: 		return true;
1581: 	}
1582: 	if (column > 0 || position > start) {
1583: 		// remaining values to be added to the chunk
1584: 		AddValue(buffer.get() + start, position - start - offset, column, escape_positions, has_quotes);
1585: 		finished_chunk = AddRow(insert_chunk, column);
1586: 	}
1587: 	// final stage, only reached after parsing the file is finished
1588: 	// flush the parsed chunk and finalize parsing
1589: 	if (mode == ParserMode::PARSING) {
1590: 		Flush(insert_chunk);
1591: 	}
1592: 
1593: 	end_of_file_reached = true;
1594: 	return true;
1595: }
1596: 
1597: bool BufferedCSVReader::TryParseSimpleCSV(DataChunk &insert_chunk, string &error_message) {
1598: 	// used for parsing algorithm
1599: 	bool finished_chunk = false;
1600: 	idx_t column = 0;
1601: 	idx_t offset = 0;
1602: 	bool has_quotes = false;
1603: 	vector<idx_t> escape_positions;
1604: 
1605: 	// read values into the buffer (if any)
1606: 	if (position >= buffer_size) {
1607: 		if (!ReadBuffer(start)) {
1608: 			return true;
1609: 		}
1610: 	}
1611: 	// start parsing the first value
1612: 	goto value_start;
1613: value_start:
1614: 	offset = 0;
1615: 	/* state: value_start */
1616: 	// this state parses the first character of a value
1617: 	if (buffer[position] == options.quote[0]) {
1618: 		// quote: actual value starts in the next position
1619: 		// move to in_quotes state
1620: 		start = position + 1;
1621: 		goto in_quotes;
1622: 	} else {
1623: 		// no quote, move to normal parsing state
1624: 		start = position;
1625: 		goto normal;
1626: 	}
1627: normal:
1628: 	/* state: normal parsing state */
1629: 	// this state parses the remainder of a non-quoted value until we reach a delimiter or newline
1630: 	do {
1631: 		for (; position < buffer_size; position++) {
1632: 			if (buffer[position] == options.delimiter[0]) {
1633: 				// delimiter: end the value and add it to the chunk
1634: 				goto add_value;
1635: 			} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1636: 				// newline: add row
1637: 				goto add_row;
1638: 			}
1639: 		}
1640: 	} while (ReadBuffer(start));
1641: 	// file ends during normal scan: go to end state
1642: 	goto final_state;
1643: add_value:
1644: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions, has_quotes);
1645: 	// increase position by 1 and move start to the new position
1646: 	offset = 0;
1647: 	has_quotes = false;
1648: 	start = ++position;
1649: 	if (position >= buffer_size && !ReadBuffer(start)) {
1650: 		// file ends right after delimiter, go to final state
1651: 		goto final_state;
1652: 	}
1653: 	goto value_start;
1654: add_row : {
1655: 	// check type of newline (\r or \n)
1656: 	bool carriage_return = buffer[position] == '\r';
1657: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions, has_quotes);
1658: 	finished_chunk = AddRow(insert_chunk, column);
1659: 	// increase position by 1 and move start to the new position
1660: 	offset = 0;
1661: 	has_quotes = false;
1662: 	start = ++position;
1663: 	if (position >= buffer_size && !ReadBuffer(start)) {
1664: 		// file ends right after delimiter, go to final state
1665: 		goto final_state;
1666: 	}
1667: 	if (carriage_return) {
1668: 		// \r newline, go to special state that parses an optional \n afterwards
1669: 		goto carriage_return;
1670: 	} else {
1671: 		// \n newline, move to value start
1672: 		if (finished_chunk) {
1673: 			return true;
1674: 		}
1675: 		goto value_start;
1676: 	}
1677: }
1678: in_quotes:
1679: 	/* state: in_quotes */
1680: 	// this state parses the remainder of a quoted value
1681: 	has_quotes = true;
1682: 	position++;
1683: 	do {
1684: 		for (; position < buffer_size; position++) {
1685: 			if (buffer[position] == options.quote[0]) {
1686: 				// quote: move to unquoted state
1687: 				goto unquote;
1688: 			} else if (buffer[position] == options.escape[0]) {
1689: 				// escape: store the escaped position and move to handle_escape state
1690: 				escape_positions.push_back(position - start);
1691: 				goto handle_escape;
1692: 			}
1693: 		}
1694: 	} while (ReadBuffer(start));
1695: 	// still in quoted state at the end of the file, error:
1696: 	throw InvalidInputException("Error in file \"%s\" on line %s: unterminated quotes. (%s)", options.file_path,
1697: 	                            GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1698: unquote:
1699: 	/* state: unquote */
1700: 	// this state handles the state directly after we unquote
1701: 	// in this state we expect either another quote (entering the quoted state again, and escaping the quote)
1702: 	// or a delimiter/newline, ending the current value and moving on to the next value
1703: 	position++;
1704: 	if (position >= buffer_size && !ReadBuffer(start)) {
1705: 		// file ends right after unquote, go to final state
1706: 		offset = 1;
1707: 		goto final_state;
1708: 	}
1709: 	if (buffer[position] == options.quote[0] && (options.escape.empty() || options.escape[0] == options.quote[0])) {
1710: 		// escaped quote, return to quoted state and store escape position
1711: 		escape_positions.push_back(position - start);
1712: 		goto in_quotes;
1713: 	} else if (buffer[position] == options.delimiter[0]) {
1714: 		// delimiter, add value
1715: 		offset = 1;
1716: 		goto add_value;
1717: 	} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1718: 		offset = 1;
1719: 		goto add_row;
1720: 	} else {
1721: 		error_message = StringUtil::Format(
1722: 		    "Error in file \"%s\" on line %s: quote should be followed by end of value, end of "
1723: 		    "row or another quote. (%s)",
1724: 		    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1725: 		return false;
1726: 	}
1727: handle_escape:
1728: 	/* state: handle_escape */
1729: 	// escape should be followed by a quote or another escape character
1730: 	position++;
1731: 	if (position >= buffer_size && !ReadBuffer(start)) {
1732: 		error_message = StringUtil::Format(
1733: 		    "Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)", options.file_path,
1734: 		    GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1735: 		return false;
1736: 	}
1737: 	if (buffer[position] != options.quote[0] && buffer[position] != options.escape[0]) {
1738: 		error_message = StringUtil::Format(
1739: 		    "Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)", options.file_path,
1740: 		    GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1741: 		return false;
1742: 	}
1743: 	// escape was followed by quote or escape, go back to quoted state
1744: 	goto in_quotes;
1745: carriage_return:
1746: 	/* state: carriage_return */
1747: 	// this stage optionally skips a newline (\n) character, which allows \r\n to be interpreted as a single line
1748: 	if (buffer[position] == '\n') {
1749: 		// newline after carriage return: skip
1750: 		// increase position by 1 and move start to the new position
1751: 		start = ++position;
1752: 		if (position >= buffer_size && !ReadBuffer(start)) {
1753: 			// file ends right after delimiter, go to final state
1754: 			goto final_state;
1755: 		}
1756: 	}
1757: 	if (finished_chunk) {
1758: 		return true;
1759: 	}
1760: 	goto value_start;
1761: final_state:
1762: 	if (finished_chunk) {
1763: 		return true;
1764: 	}
1765: 
1766: 	if (column > 0 || position > start) {
1767: 		// remaining values to be added to the chunk
1768: 		AddValue(buffer.get() + start, position - start - offset, column, escape_positions, has_quotes);
1769: 		finished_chunk = AddRow(insert_chunk, column);
1770: 	}
1771: 
1772: 	// final stage, only reached after parsing the file is finished
1773: 	// flush the parsed chunk and finalize parsing
1774: 	if (mode == ParserMode::PARSING) {
1775: 		Flush(insert_chunk);
1776: 	}
1777: 
1778: 	end_of_file_reached = true;
1779: 	return true;
1780: }
1781: 
1782: bool BufferedCSVReader::ReadBuffer(idx_t &start) {
1783: 	auto old_buffer = move(buffer);
1784: 
1785: 	// the remaining part of the last buffer
1786: 	idx_t remaining = buffer_size - start;
1787: 
1788: 	bool large_buffers = mode == ParserMode::PARSING && !file_handle->OnDiskFile() && file_handle->CanSeek();
1789: 	idx_t buffer_read_size = large_buffers ? INITIAL_BUFFER_SIZE_LARGE : INITIAL_BUFFER_SIZE;
1790: 
1791: 	while (remaining > buffer_read_size) {
1792: 		buffer_read_size *= 2;
1793: 	}
1794: 
1795: 	// Check line length
1796: 	if (remaining > options.maximum_line_size) {
1797: 		throw InvalidInputException("Maximum line size of %llu bytes exceeded!", options.maximum_line_size);
1798: 	}
1799: 
1800: 	buffer = unique_ptr<char[]>(new char[buffer_read_size + remaining + 1]);
1801: 	buffer_size = remaining + buffer_read_size;
1802: 	if (remaining > 0) {
1803: 		// remaining from last buffer: copy it here
1804: 		memcpy(buffer.get(), old_buffer.get() + start, remaining);
1805: 	}
1806: 	idx_t read_count = file_handle->Read(buffer.get() + remaining, buffer_read_size);
1807: 
1808: 	bytes_in_chunk += read_count;
1809: 	buffer_size = remaining + read_count;
1810: 	buffer[buffer_size] = '\0';
1811: 	if (old_buffer) {
1812: 		cached_buffers.push_back(move(old_buffer));
1813: 	}
1814: 	start = 0;
1815: 	position = remaining;
1816: 	if (!bom_checked) {
1817: 		bom_checked = true;
1818: 		if (read_count >= 3 && buffer[0] == '\xEF' && buffer[1] == '\xBB' && buffer[2] == '\xBF') {
1819: 			position += 3;
1820: 		}
1821: 	}
1822: 
1823: 	return read_count > 0;
1824: }
1825: 
1826: void BufferedCSVReader::ParseCSV(DataChunk &insert_chunk) {
1827: 	// if no auto-detect or auto-detect with jumping samples, we have nothing cached and start from the beginning
1828: 	if (cached_chunks.empty()) {
1829: 		cached_buffers.clear();
1830: 	} else {
1831: 		auto &chunk = cached_chunks.front();
1832: 		parse_chunk.Move(*chunk);
1833: 		cached_chunks.pop();
1834: 		Flush(insert_chunk);
1835: 		return;
1836: 	}
1837: 
1838: 	string error_message;
1839: 	if (!TryParseCSV(ParserMode::PARSING, insert_chunk, error_message)) {
1840: 		throw InvalidInputException(error_message);
1841: 	}
1842: }
1843: 
1844: bool BufferedCSVReader::TryParseCSV(ParserMode mode) {
1845: 	DataChunk dummy_chunk;
1846: 	string error_message;
1847: 	return TryParseCSV(mode, dummy_chunk, error_message);
1848: }
1849: 
1850: void BufferedCSVReader::ParseCSV(ParserMode mode) {
1851: 	DataChunk dummy_chunk;
1852: 	string error_message;
1853: 	if (!TryParseCSV(mode, dummy_chunk, error_message)) {
1854: 		throw InvalidInputException(error_message);
1855: 	}
1856: }
1857: 
1858: bool BufferedCSVReader::TryParseCSV(ParserMode parser_mode, DataChunk &insert_chunk, string &error_message) {
1859: 	mode = parser_mode;
1860: 
1861: 	if (options.quote.size() <= 1 && options.escape.size() <= 1 && options.delimiter.size() == 1) {
1862: 		return TryParseSimpleCSV(insert_chunk, error_message);
1863: 	} else {
1864: 		return TryParseComplexCSV(insert_chunk, error_message);
1865: 	}
1866: }
1867: 
1868: void BufferedCSVReader::AddValue(char *str_val, idx_t length, idx_t &column, vector<idx_t> &escape_positions,
1869:                                  bool has_quotes) {
1870: 	if (length == 0 && column == 0) {
1871: 		row_empty = true;
1872: 	} else {
1873: 		row_empty = false;
1874: 	}
1875: 
1876: 	if (!sql_types.empty() && column == sql_types.size() && length == 0) {
1877: 		// skip a single trailing delimiter in last column
1878: 		return;
1879: 	}
1880: 	if (mode == ParserMode::SNIFFING_DIALECT) {
1881: 		column++;
1882: 		return;
1883: 	}
1884: 	if (column >= sql_types.size()) {
1885: 		if (options.ignore_errors) {
1886: 			error_column_overflow = true;
1887: 			return;
1888: 		} else {
1889: 			throw InvalidInputException("Error on line %s: expected %lld values per row, but got more. (%s)",
1890: 			                            GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(),
1891: 			                            options.ToString());
1892: 		}
1893: 	}
1894: 
1895: 	// insert the line number into the chunk
1896: 	idx_t row_entry = parse_chunk.size();
1897: 
1898: 	str_val[length] = '\0';
1899: 
1900: 	// test against null string, but only if the value was not quoted
1901: 	if ((!has_quotes || sql_types[column].id() != LogicalTypeId::VARCHAR) && !options.force_not_null[column] &&
1902: 	    strcmp(options.null_str.c_str(), str_val) == 0) {
1903: 		FlatVector::SetNull(parse_chunk.data[column], row_entry, true);
1904: 	} else {
1905: 		auto &v = parse_chunk.data[column];
1906: 		auto parse_data = FlatVector::GetData<string_t>(v);
1907: 		if (!escape_positions.empty()) {
1908: 			// remove escape characters (if any)
1909: 			string old_val = str_val;
1910: 			string new_val = "";
1911: 			idx_t prev_pos = 0;
1912: 			for (idx_t i = 0; i < escape_positions.size(); i++) {
1913: 				idx_t next_pos = escape_positions[i];
1914: 				new_val += old_val.substr(prev_pos, next_pos - prev_pos);
1915: 
1916: 				if (options.escape.empty() || options.escape == options.quote) {
1917: 					prev_pos = next_pos + options.quote.size();
1918: 				} else {
1919: 					prev_pos = next_pos + options.escape.size();
1920: 				}
1921: 			}
1922: 			new_val += old_val.substr(prev_pos, old_val.size() - prev_pos);
1923: 			escape_positions.clear();
1924: 			parse_data[row_entry] = StringVector::AddStringOrBlob(v, string_t(new_val));
1925: 		} else {
1926: 			parse_data[row_entry] = string_t(str_val, length);
1927: 		}
1928: 	}
1929: 
1930: 	// move to the next column
1931: 	column++;
1932: }
1933: 
1934: bool BufferedCSVReader::AddRow(DataChunk &insert_chunk, idx_t &column) {
1935: 	linenr++;
1936: 
1937: 	if (row_empty) {
1938: 		row_empty = false;
1939: 		if (sql_types.size() != 1) {
1940: 			column = 0;
1941: 			return false;
1942: 		}
1943: 	}
1944: 
1945: 	// Error forwarded by 'ignore_errors' - originally encountered in 'AddValue'
1946: 	if (error_column_overflow) {
1947: 		D_ASSERT(options.ignore_errors);
1948: 		error_column_overflow = false;
1949: 		column = 0;
1950: 		return false;
1951: 	}
1952: 
1953: 	if (column < sql_types.size() && mode != ParserMode::SNIFFING_DIALECT) {
1954: 		if (options.ignore_errors) {
1955: 			column = 0;
1956: 			return false;
1957: 		} else {
1958: 			throw InvalidInputException("Error on line %s: expected %lld values per row, but got %d. (%s)",
1959: 			                            GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(), column,
1960: 			                            options.ToString());
1961: 		}
1962: 	}
1963: 
1964: 	if (mode == ParserMode::SNIFFING_DIALECT) {
1965: 		sniffed_column_counts.push_back(column);
1966: 
1967: 		if (sniffed_column_counts.size() == options.sample_chunk_size) {
1968: 			return true;
1969: 		}
1970: 	} else {
1971: 		parse_chunk.SetCardinality(parse_chunk.size() + 1);
1972: 	}
1973: 
1974: 	if (mode == ParserMode::PARSING_HEADER) {
1975: 		return true;
1976: 	}
1977: 
1978: 	if (mode == ParserMode::SNIFFING_DATATYPES && parse_chunk.size() == options.sample_chunk_size) {
1979: 		return true;
1980: 	}
1981: 
1982: 	if (mode == ParserMode::PARSING && parse_chunk.size() == STANDARD_VECTOR_SIZE) {
1983: 		Flush(insert_chunk);
1984: 		return true;
1985: 	}
1986: 
1987: 	column = 0;
1988: 	return false;
1989: }
1990: 
1991: void BufferedCSVReader::SetNullUnionCols(DataChunk &insert_chunk) {
1992: 	for (idx_t col = 0; col < insert_nulls_idx.size(); ++col) {
1993: 		insert_chunk.data[insert_nulls_idx[col]].SetVectorType(VectorType::CONSTANT_VECTOR);
1994: 		ConstantVector::SetNull(insert_chunk.data[insert_nulls_idx[col]], true);
1995: 	}
1996: }
1997: 
1998: void BufferedCSVReader::Flush(DataChunk &insert_chunk) {
1999: 	if (parse_chunk.size() == 0) {
2000: 		return;
2001: 	}
2002: 
2003: 	bool conversion_error_ignored = false;
2004: 
2005: 	// convert the columns in the parsed chunk to the types of the table
2006: 	insert_chunk.SetCardinality(parse_chunk);
2007: 	for (idx_t col_idx = 0; col_idx < sql_types.size(); col_idx++) {
2008: 		if (sql_types[col_idx].id() == LogicalTypeId::VARCHAR) {
2009: 			// target type is varchar: no need to convert
2010: 			// just test that all strings are valid utf-8 strings
2011: 			auto parse_data = FlatVector::GetData<string_t>(parse_chunk.data[col_idx]);
2012: 			for (idx_t i = 0; i < parse_chunk.size(); i++) {
2013: 				if (!FlatVector::IsNull(parse_chunk.data[col_idx], i)) {
2014: 					auto s = parse_data[i];
2015: 					auto utf_type = Utf8Proc::Analyze(s.GetDataUnsafe(), s.GetSize());
2016: 					if (utf_type == UnicodeType::INVALID) {
2017: 						string col_name = to_string(col_idx);
2018: 						if (col_idx < col_names.size()) {
2019: 							col_name = "\"" + col_names[col_idx] + "\"";
2020: 						}
2021: 						throw InvalidInputException("Error in file \"%s\" between line %llu and %llu in column \"%s\": "
2022: 						                            "file is not valid UTF8. Parser options: %s",
2023: 						                            options.file_path, linenr - parse_chunk.size(), linenr, col_name,
2024: 						                            options.ToString());
2025: 					}
2026: 				}
2027: 			}
2028: 			insert_chunk.data[insert_cols_idx[col_idx]].Reference(parse_chunk.data[col_idx]);
2029: 		} else {
2030: 			string error_message;
2031: 			bool success;
2032: 			if (options.has_format[LogicalTypeId::DATE] && sql_types[col_idx].id() == LogicalTypeId::DATE) {
2033: 				// use the date format to cast the chunk
2034: 				success =
2035: 				    TryCastDateVector(options, parse_chunk.data[col_idx], insert_chunk.data[insert_cols_idx[col_idx]],
2036: 				                      parse_chunk.size(), error_message);
2037: 			} else if (options.has_format[LogicalTypeId::TIMESTAMP] &&
2038: 			           sql_types[col_idx].id() == LogicalTypeId::TIMESTAMP) {
2039: 				// use the date format to cast the chunk
2040: 				success = TryCastTimestampVector(options, parse_chunk.data[col_idx],
2041: 				                                 insert_chunk.data[insert_cols_idx[col_idx]], parse_chunk.size(),
2042: 				                                 error_message);
2043: 			} else {
2044: 				// target type is not varchar: perform a cast
2045: 				success = VectorOperations::DefaultTryCast(parse_chunk.data[col_idx],
2046: 				                                           insert_chunk.data[insert_cols_idx[col_idx]],
2047: 				                                           parse_chunk.size(), &error_message);
2048: 			}
2049: 			if (success) {
2050: 				continue;
2051: 			}
2052: 			if (options.ignore_errors) {
2053: 				conversion_error_ignored = true;
2054: 				continue;
2055: 			}
2056: 			string col_name = to_string(col_idx);
2057: 			if (col_idx < col_names.size()) {
2058: 				col_name = "\"" + col_names[col_idx] + "\"";
2059: 			}
2060: 
2061: 			if (options.auto_detect) {
2062: 				throw InvalidInputException("%s in column %s, between line %llu and %llu. Parser "
2063: 				                            "options: %s. Consider either increasing the sample size "
2064: 				                            "(SAMPLE_SIZE=X [X rows] or SAMPLE_SIZE=-1 [all rows]), "
2065: 				                            "or skipping column conversion (ALL_VARCHAR=1)",
2066: 				                            error_message, col_name, linenr - parse_chunk.size() + 1, linenr,
2067: 				                            options.ToString());
2068: 			} else {
2069: 				throw InvalidInputException("%s between line %llu and %llu in column %s. Parser options: %s ",
2070: 				                            error_message, linenr - parse_chunk.size(), linenr, col_name,
2071: 				                            options.ToString());
2072: 			}
2073: 		}
2074: 	}
2075: 	if (conversion_error_ignored) {
2076: 		D_ASSERT(options.ignore_errors);
2077: 		SelectionVector succesful_rows;
2078: 		succesful_rows.Initialize(parse_chunk.size());
2079: 		idx_t sel_size = 0;
2080: 
2081: 		for (idx_t row_idx = 0; row_idx < parse_chunk.size(); row_idx++) {
2082: 			bool failed = false;
2083: 			for (idx_t column_idx = 0; column_idx < sql_types.size(); column_idx++) {
2084: 
2085: 				auto &inserted_column = insert_chunk.data[column_idx];
2086: 				auto &parsed_column = parse_chunk.data[column_idx];
2087: 
2088: 				bool was_already_null = FlatVector::IsNull(parsed_column, row_idx);
2089: 				if (!was_already_null && FlatVector::IsNull(inserted_column, row_idx)) {
2090: 					failed = true;
2091: 					break;
2092: 				}
2093: 			}
2094: 			if (!failed) {
2095: 				succesful_rows.set_index(sel_size++, row_idx);
2096: 			}
2097: 		}
2098: 		insert_chunk.Slice(succesful_rows, sel_size);
2099: 	}
2100: 	parse_chunk.Reset();
2101: }
2102: } // namespace duckdb
[end of src/execution/operator/persistent/buffered_csv_reader.cpp]
[start of src/include/duckdb/main/prepared_statement_data.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/prepared_statement_data.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/enums/statement_type.hpp"
12: #include "duckdb/common/types/value.hpp"
13: #include "duckdb/common/unordered_map.hpp"
14: #include "duckdb/common/unordered_set.hpp"
15: #include "duckdb/common/winapi.hpp"
16: #include "duckdb/planner/expression/bound_parameter_data.hpp"
17: 
18: namespace duckdb {
19: class CatalogEntry;
20: class ClientContext;
21: class PhysicalOperator;
22: class SQLStatement;
23: 
24: class PreparedStatementData {
25: public:
26: 	DUCKDB_API explicit PreparedStatementData(StatementType type);
27: 	DUCKDB_API ~PreparedStatementData();
28: 
29: 	StatementType statement_type;
30: 	//! The unbound SQL statement that was prepared
31: 	unique_ptr<SQLStatement> unbound_statement;
32: 	//! The fully prepared physical plan of the prepared statement
33: 	unique_ptr<PhysicalOperator> plan;
34: 	//! The map of parameter index to the actual value entry
35: 	bound_parameter_map_t value_map;
36: 
37: 	//! The result names of the transaction
38: 	vector<string> names;
39: 	//! The result types of the transaction
40: 	vector<LogicalType> types;
41: 
42: 	//! The statement properties
43: 	StatementProperties properties;
44: 
45: 	//! The catalog version of when the prepared statement was bound
46: 	//! If this version is lower than the current catalog version, we have to rebind the prepared statement
47: 	idx_t catalog_version;
48: 
49: public:
50: 	void CheckParameterCount(idx_t parameter_count);
51: 	//! Whether or not the prepared statement data requires the query to rebound for the given parameters
52: 	bool RequireRebind(ClientContext &context, const vector<Value> &values);
53: 	//! Bind a set of values to the prepared statement data
54: 	DUCKDB_API void Bind(vector<Value> values);
55: 	//! Get the expected SQL Type of the bound parameter
56: 	DUCKDB_API LogicalType GetType(idx_t param_index);
57: };
58: 
59: } // namespace duckdb
[end of src/include/duckdb/main/prepared_statement_data.hpp]
[start of src/main/capi/prepared-c.cpp]
1: #include "duckdb/main/capi_internal.hpp"
2: #include "duckdb/main/query_result.hpp"
3: #include "duckdb/main/prepared_statement_data.hpp"
4: 
5: using duckdb::Connection;
6: using duckdb::date_t;
7: using duckdb::dtime_t;
8: using duckdb::hugeint_t;
9: using duckdb::MaterializedQueryResult;
10: using duckdb::PreparedStatementWrapper;
11: using duckdb::QueryResultType;
12: using duckdb::timestamp_t;
13: using duckdb::Value;
14: 
15: duckdb_state duckdb_prepare(duckdb_connection connection, const char *query,
16:                             duckdb_prepared_statement *out_prepared_statement) {
17: 	if (!connection || !query || !out_prepared_statement) {
18: 		return DuckDBError;
19: 	}
20: 	auto wrapper = new PreparedStatementWrapper();
21: 	Connection *conn = (Connection *)connection;
22: 	wrapper->statement = conn->Prepare(query);
23: 	*out_prepared_statement = (duckdb_prepared_statement)wrapper;
24: 	return !wrapper->statement->HasError() ? DuckDBSuccess : DuckDBError;
25: }
26: 
27: const char *duckdb_prepare_error(duckdb_prepared_statement prepared_statement) {
28: 	auto wrapper = (PreparedStatementWrapper *)prepared_statement;
29: 	if (!wrapper || !wrapper->statement || !wrapper->statement->HasError()) {
30: 		return nullptr;
31: 	}
32: 	return wrapper->statement->error.Message().c_str();
33: }
34: 
35: idx_t duckdb_nparams(duckdb_prepared_statement prepared_statement) {
36: 	auto wrapper = (PreparedStatementWrapper *)prepared_statement;
37: 	if (!wrapper || !wrapper->statement || wrapper->statement->HasError()) {
38: 		return 0;
39: 	}
40: 	return wrapper->statement->n_param;
41: }
42: 
43: duckdb_type duckdb_param_type(duckdb_prepared_statement prepared_statement, idx_t param_idx) {
44: 	auto wrapper = (PreparedStatementWrapper *)prepared_statement;
45: 	if (!wrapper || !wrapper->statement || wrapper->statement->HasError()) {
46: 		return DUCKDB_TYPE_INVALID;
47: 	}
48: 	auto entry = wrapper->statement->data->value_map.find(param_idx);
49: 	if (entry == wrapper->statement->data->value_map.end()) {
50: 		return DUCKDB_TYPE_INVALID;
51: 	}
52: 	return ConvertCPPTypeToC(entry->second->return_type);
53: }
54: 
55: duckdb_state duckdb_clear_bindings(duckdb_prepared_statement prepared_statement) {
56: 	auto wrapper = (PreparedStatementWrapper *)prepared_statement;
57: 	if (!wrapper || !wrapper->statement || wrapper->statement->HasError()) {
58: 		return DuckDBError;
59: 	}
60: 	wrapper->values.clear();
61: 	return DuckDBSuccess;
62: }
63: 
64: static duckdb_state duckdb_bind_value(duckdb_prepared_statement prepared_statement, idx_t param_idx, Value val) {
65: 	auto wrapper = (PreparedStatementWrapper *)prepared_statement;
66: 	if (!wrapper || !wrapper->statement || wrapper->statement->HasError()) {
67: 		return DuckDBError;
68: 	}
69: 	if (param_idx <= 0 || param_idx > wrapper->statement->n_param) {
70: 		return DuckDBError;
71: 	}
72: 	if (param_idx > wrapper->values.size()) {
73: 		wrapper->values.resize(param_idx);
74: 	}
75: 	wrapper->values[param_idx - 1] = val;
76: 	return DuckDBSuccess;
77: }
78: 
79: duckdb_state duckdb_bind_boolean(duckdb_prepared_statement prepared_statement, idx_t param_idx, bool val) {
80: 	return duckdb_bind_value(prepared_statement, param_idx, Value::BOOLEAN(val));
81: }
82: 
83: duckdb_state duckdb_bind_int8(duckdb_prepared_statement prepared_statement, idx_t param_idx, int8_t val) {
84: 	return duckdb_bind_value(prepared_statement, param_idx, Value::TINYINT(val));
85: }
86: 
87: duckdb_state duckdb_bind_int16(duckdb_prepared_statement prepared_statement, idx_t param_idx, int16_t val) {
88: 	return duckdb_bind_value(prepared_statement, param_idx, Value::SMALLINT(val));
89: }
90: 
91: duckdb_state duckdb_bind_int32(duckdb_prepared_statement prepared_statement, idx_t param_idx, int32_t val) {
92: 	return duckdb_bind_value(prepared_statement, param_idx, Value::INTEGER(val));
93: }
94: 
95: duckdb_state duckdb_bind_int64(duckdb_prepared_statement prepared_statement, idx_t param_idx, int64_t val) {
96: 	return duckdb_bind_value(prepared_statement, param_idx, Value::BIGINT(val));
97: }
98: 
99: duckdb_state duckdb_bind_hugeint(duckdb_prepared_statement prepared_statement, idx_t param_idx, duckdb_hugeint val) {
100: 	hugeint_t internal;
101: 	internal.lower = val.lower;
102: 	internal.upper = val.upper;
103: 	return duckdb_bind_value(prepared_statement, param_idx, Value::HUGEINT(internal));
104: }
105: 
106: duckdb_state duckdb_bind_uint8(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint8_t val) {
107: 	return duckdb_bind_value(prepared_statement, param_idx, Value::UTINYINT(val));
108: }
109: 
110: duckdb_state duckdb_bind_uint16(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint16_t val) {
111: 	return duckdb_bind_value(prepared_statement, param_idx, Value::USMALLINT(val));
112: }
113: 
114: duckdb_state duckdb_bind_uint32(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint32_t val) {
115: 	return duckdb_bind_value(prepared_statement, param_idx, Value::UINTEGER(val));
116: }
117: 
118: duckdb_state duckdb_bind_uint64(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint64_t val) {
119: 	return duckdb_bind_value(prepared_statement, param_idx, Value::UBIGINT(val));
120: }
121: 
122: duckdb_state duckdb_bind_float(duckdb_prepared_statement prepared_statement, idx_t param_idx, float val) {
123: 	return duckdb_bind_value(prepared_statement, param_idx, Value::FLOAT(val));
124: }
125: 
126: duckdb_state duckdb_bind_double(duckdb_prepared_statement prepared_statement, idx_t param_idx, double val) {
127: 	return duckdb_bind_value(prepared_statement, param_idx, Value::DOUBLE(val));
128: }
129: 
130: duckdb_state duckdb_bind_date(duckdb_prepared_statement prepared_statement, idx_t param_idx, duckdb_date val) {
131: 	return duckdb_bind_value(prepared_statement, param_idx, Value::DATE(date_t(val.days)));
132: }
133: 
134: duckdb_state duckdb_bind_time(duckdb_prepared_statement prepared_statement, idx_t param_idx, duckdb_time val) {
135: 	return duckdb_bind_value(prepared_statement, param_idx, Value::TIME(dtime_t(val.micros)));
136: }
137: 
138: duckdb_state duckdb_bind_timestamp(duckdb_prepared_statement prepared_statement, idx_t param_idx,
139:                                    duckdb_timestamp val) {
140: 	return duckdb_bind_value(prepared_statement, param_idx, Value::TIMESTAMP(timestamp_t(val.micros)));
141: }
142: 
143: duckdb_state duckdb_bind_interval(duckdb_prepared_statement prepared_statement, idx_t param_idx, duckdb_interval val) {
144: 	return duckdb_bind_value(prepared_statement, param_idx, Value::INTERVAL(val.months, val.days, val.micros));
145: }
146: 
147: duckdb_state duckdb_bind_varchar(duckdb_prepared_statement prepared_statement, idx_t param_idx, const char *val) {
148: 	try {
149: 		return duckdb_bind_value(prepared_statement, param_idx, Value(val));
150: 	} catch (...) {
151: 		return DuckDBError;
152: 	}
153: }
154: 
155: duckdb_state duckdb_bind_varchar_length(duckdb_prepared_statement prepared_statement, idx_t param_idx, const char *val,
156:                                         idx_t length) {
157: 	try {
158: 		return duckdb_bind_value(prepared_statement, param_idx, Value(std::string(val, length)));
159: 	} catch (...) {
160: 		return DuckDBError;
161: 	}
162: }
163: 
164: duckdb_state duckdb_bind_blob(duckdb_prepared_statement prepared_statement, idx_t param_idx, const void *data,
165:                               idx_t length) {
166: 	return duckdb_bind_value(prepared_statement, param_idx, Value::BLOB((duckdb::const_data_ptr_t)data, length));
167: }
168: 
169: duckdb_state duckdb_bind_null(duckdb_prepared_statement prepared_statement, idx_t param_idx) {
170: 	return duckdb_bind_value(prepared_statement, param_idx, Value());
171: }
172: 
173: duckdb_state duckdb_execute_prepared(duckdb_prepared_statement prepared_statement, duckdb_result *out_result) {
174: 	auto wrapper = (PreparedStatementWrapper *)prepared_statement;
175: 	if (!wrapper || !wrapper->statement || wrapper->statement->HasError()) {
176: 		return DuckDBError;
177: 	}
178: 	auto result = wrapper->statement->Execute(wrapper->values, false);
179: 	return duckdb_translate_result(move(result), out_result);
180: }
181: 
182: void duckdb_destroy_prepare(duckdb_prepared_statement *prepared_statement) {
183: 	if (!prepared_statement) {
184: 		return;
185: 	}
186: 	auto wrapper = (PreparedStatementWrapper *)*prepared_statement;
187: 	if (wrapper) {
188: 		delete wrapper;
189: 	}
190: 	*prepared_statement = nullptr;
191: }
[end of src/main/capi/prepared-c.cpp]
[start of src/main/prepared_statement_data.cpp]
1: #include "duckdb/main/prepared_statement_data.hpp"
2: #include "duckdb/execution/physical_operator.hpp"
3: #include "duckdb/parser/sql_statement.hpp"
4: 
5: namespace duckdb {
6: 
7: PreparedStatementData::PreparedStatementData(StatementType type) : statement_type(type) {
8: }
9: 
10: PreparedStatementData::~PreparedStatementData() {
11: }
12: 
13: void PreparedStatementData::CheckParameterCount(idx_t parameter_count) {
14: 	const auto required = properties.parameter_count;
15: 	if (parameter_count != required) {
16: 		throw BinderException("Parameter/argument count mismatch for prepared statement. Expected %llu, got %llu",
17: 		                      required, parameter_count);
18: 	}
19: }
20: 
21: bool PreparedStatementData::RequireRebind(ClientContext &context, const vector<Value> &values) {
22: 	CheckParameterCount(values.size());
23: 	if (!unbound_statement) {
24: 		// no unbound statement!? cannot rebind?
25: 		return false;
26: 	}
27: 	if (!properties.bound_all_parameters) {
28: 		// parameters not yet bound: query always requires a rebind
29: 		return true;
30: 	}
31: 	auto &catalog = Catalog::GetCatalog(context);
32: 	if (catalog.GetCatalogVersion() != catalog_version) {
33: 		//! context is out of bounds
34: 		return true;
35: 	}
36: 	for (auto &it : value_map) {
37: 		const idx_t i = it.first - 1;
38: 		if (values[i].type() != it.second->return_type) {
39: 			return true;
40: 		}
41: 	}
42: 	return false;
43: }
44: 
45: void PreparedStatementData::Bind(vector<Value> values) {
46: 	// set parameters
47: 	D_ASSERT(!unbound_statement || unbound_statement->n_param == properties.parameter_count);
48: 	CheckParameterCount(values.size());
49: 
50: 	// bind the required values
51: 	for (auto &it : value_map) {
52: 		const idx_t i = it.first - 1;
53: 		if (i >= values.size()) {
54: 			throw BinderException("Could not find parameter with index %llu", i + 1);
55: 		}
56: 		D_ASSERT(it.second);
57: 		if (!values[i].DefaultTryCastAs(it.second->return_type)) {
58: 			throw BinderException(
59: 			    "Type mismatch for binding parameter with index %llu, expected type %s but got type %s", i + 1,
60: 			    it.second->return_type.ToString().c_str(), values[i].type().ToString().c_str());
61: 		}
62: 		it.second->value = values[i];
63: 	}
64: }
65: 
66: LogicalType PreparedStatementData::GetType(idx_t param_idx) {
67: 	auto it = value_map.find(param_idx);
68: 	if (it == value_map.end()) {
69: 		throw BinderException("Could not find parameter with index %llu", param_idx);
70: 	}
71: 	return it->second->return_type;
72: }
73: 
74: } // namespace duckdb
[end of src/main/prepared_statement_data.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: