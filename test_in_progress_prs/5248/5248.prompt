You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
HTTP 403 (Access Denied) when using read_parquet() with glob matching >1000 S3 objects
### What happens?

DuckDB throws an error when I do a glob-style query that matches over 1000 S3 objects.

Here's the query I'm using:

```
SELECT *
FROM read_parquet('s3://prod-ingestion-github-repositories/2022-09_*');
```


And here is the error message I'm getting:

```
Invalid Error: HTTP GET error on 'https://prod-ingestion-github-repositories.s3.amazonaws.com/?continuation-token=1ZU0D5N%2Bv2Dm9hhTi2JQvSAphFHiPRSnPegvosjQD3klo92WJYFwF8yrnJdtFhDHfsIka/QOCYNdQQTHC6f/3H8mudjEJscJydhNUTBJevcvcZlqgunsNdyhSHXsAZLbc&encoding-type=url&list-type=2&prefix=2022-09_' (HTTP 403)
```

I confirmed that the same query works if the glob pattern is modified to match fewer objects. E.g., the following returns the expected result:

```
SELECT *
FROM read_parquet('s3://prod-ingestion-github-repositories/2022-09_06-*');
```


### To Reproduce

Tough for me to provide the exact dataset, but the general gist is:

1) Create N parquet files in S3 with the same prefix, where N is some number >1000
2) Query them with `read_parquet('your-prefix-*')`

### OS:

OSX

### DuckDB Version:

0.5.1

### DuckDB Client:

Python

### Full Name:

Nathan Gould

### Affiliation:

Endeavor Labs LLC

### Have you tried this on the latest `master` branch?

- [X] I agree

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] I agree

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
[end of README.md]
[start of .github/workflows/LinuxRelease.yml]
1: name: LinuxRelease
2: on:
3:   push:
4:     paths-ignore:
5:       - '**.md'
6:   pull_request:
7:     paths-ignore:
8:       - '**.md'
9:       - 'tools/nodejs/**'
10:       - 'tools/juliapkg/**'
11:       - 'tools/pythonpkg/**'
12:       - 'tools/rpkg/**'
13:       - '.github/workflows/**'
14:       - '!.github/workflows/LinuxRelease.yml'
15: 
16: concurrency:
17:   group: ${{ github.workflow }}-${{ github.ref }}-${{ github.head_ref || '' }}-${{ github.base_ref || '' }}-${{ github.ref != 'refs/heads/master' || github.sha }}
18:   cancel-in-progress: true
19: 
20: env:
21:   GH_TOKEN: ${{ secrets.GH_TOKEN }}
22:   TWINE_PASSWORD: ${{ secrets.TWINE_PASSWORD }}
23:   AWS_ACCESS_KEY_ID: AKIAVBLKPL2ZW2T7TYFQ
24:   AWS_SECRET_ACCESS_KEY: ${{ secrets.NODE_PRE_GYP_SECRETACCESSKEY }}
25:   NODE_AUTH_TOKEN: ${{secrets.NODE_AUTH_TOKEN}}
26: 
27: jobs:
28:  linux-release-64:
29:     name: Linux (64 Bit)
30:     runs-on: ubuntu-latest
31:     container: ubuntu:16.04
32:     env:
33:       GEN: ninja
34:       BUILD_BENCHMARK: 1
35:       BUILD_ICU: 1
36:       BUILD_INET: 1
37:       BUILD_TPCH: 1
38:       BUILD_FTS: 1
39:       BUILD_JDBC: 1
40:       BUILD_JSON: 1
41:       BUILD_EXCEL: 1
42:       BUILD_ODBC: 1
43:       BUILD_JEMALLOC: 1
44:       TREAT_WARNINGS_AS_ERRORS: 1
45:       FORCE_WARN_UNUSED: 1
46:       STATIC_LIBCPP: 1
47: 
48:     steps:
49:     - uses: actions/checkout@v3
50:       with:
51:         fetch-depth: 0
52:     - uses: ./.github/actions/ubuntu_16_setup
53: 
54:     - name: Build
55:       shell: bash
56:       run: make
57: 
58:     - name: Test
59:       shell: bash
60:       run: make allunit
61: 
62:     - name: Tools Tests
63:       shell: bash
64:       run: |
65:         python3.7 tools/shell/shell-test.py build/release/duckdb
66:         java -cp build/release/tools/jdbc/duckdb_jdbc.jar org.duckdb.test.TestDuckDBJDBC
67: 
68:     - name: Examples
69:       shell: bash
70:       run: |
71:         (cd examples/embedded-c; make)
72:         (cd examples/embedded-c++; make)
73:         (cd examples/jdbc; make; make maven)
74:         build/release/benchmark/benchmark_runner benchmark/tpch/sf1/q01.benchmark
75:         build/release/duckdb -c "COPY (SELECT 42) TO '/dev/stdout' (FORMAT PARQUET)" | cat
76: 
77:     - name: Deploy
78:       shell: bash
79:       run: |
80:         python3.7 scripts/amalgamation.py
81:         zip -j duckdb_cli-linux-amd64.zip build/release/duckdb
82:         zip -j libduckdb-linux-amd64.zip build/release/src/libduckdb*.so src/amalgamation/duckdb.hpp src/include/duckdb.h
83:         zip -j libduckdb-src.zip src/amalgamation/duckdb.hpp src/amalgamation/duckdb.cpp src/include/duckdb.h
84:         zip -j duckdb_odbc-linux-amd64.zip build/release/tools/odbc/libduckdb_odbc.so tools/odbc/linux_setup/unixodbc_setup.sh
85:         python3.7 scripts/asset-upload-gha.py libduckdb-src.zip libduckdb-linux-amd64.zip duckdb_cli-linux-amd64.zip  duckdb_jdbc-linux-amd64.jar=build/release/tools/jdbc/duckdb_jdbc.jar duckdb_odbc-linux-amd64.zip
86: 
87:     - uses: actions/upload-artifact@v2
88:       with:
89:         name: duckdb-binaries-linux
90:         path: |
91:           libduckdb-linux-amd64.zip
92:           duckdb_cli-linux-amd64.zip
93:           build/release/tools/jdbc/duckdb_jdbc.jar
94: 
95: 
96:  linux-release-aarch64:
97:    name: Linux (AARCH64)
98:    runs-on: ubuntu-latest
99:    needs: linux-release-64
100:    container: ubuntu:18.04 # cross compiler not available in 16
101:    env:
102:      GEN: ninja
103:      BUILD_BENCHMARK: 1
104:      BUILD_ICU: 1
105:      BUILD_INET: 1
106:      BUILD_TPCH: 1
107:      BUILD_FTS: 1
108:      BUILD_JSON: 1
109:      BUILD_EXCEL: 1
110:      BUILD_JEMALLOC: 1
111:      TREAT_WARNINGS_AS_ERRORS: 1
112:      FORCE_WARN_UNUSED: 1
113:      STATIC_LIBCPP: 1
114:      BUILD_ODBC: 1
115:      ODBC_CONFIG: ../../build/unixodbc/build/bin/odbc_config
116:      BUILD_JDBC: 1
117:      OVERRIDE_JDBC_OS_ARCH: arm64
118:      JAVA_HOME: ../../jdk8u345-b01
119: 
120:    steps:
121:      - uses: actions/checkout@v3
122:        with:
123:          fetch-depth: 0
124:      - uses: ./.github/actions/ubuntu_16_setup
125: 
126:      - name: Install Stuff
127:        shell: bash
128:        run: |
129:          apt-get update -y -qq
130:          apt-get install -y -qq gcc-aarch64-linux-gnu g++-aarch64-linux-gnu
131:          curl -L https://github.com/adoptium/temurin8-binaries/releases/download/jdk8u345-b01/OpenJDK8U-jdk_x64_linux_hotspot_8u345b01.tar.gz | tar xvz
132: 
133:      - name: Install unixODBC
134:        shell: bash
135:        run: | # we need an x86 odbc_config tool to run cmake. fun.
136:          apt-get remove -y unixodbc unixodbc-dev
137:          CC=gcc ./scripts/install_unixodbc.sh
138:          cp build/unixodbc/build/bin/odbc_config .
139:          CC=aarch64-linux-gnu-gcc ./scripts/install_unixodbc.sh --host aarch64-linux-gnu
140:          cp odbc_config build/unixodbc/build/bin/odbc_config
141: 
142:      - name: Build
143:        shell: bash
144:        run: CC=aarch64-linux-gnu-gcc CXX=aarch64-linux-gnu-g++ make
145: 
146:      - name: Deploy
147:        shell: bash
148:        run: |
149:          python3.7 scripts/amalgamation.py
150:          zip -j duckdb_cli-linux-aarch64.zip build/release/duckdb
151:          zip -j duckdb_odbc-linux-aarch64.zip build/release/tools/odbc/libduckdb_odbc.so
152:          zip -j libduckdb-linux-aarch64.zip build/release/src/libduckdb*.so src/amalgamation/duckdb.hpp src/include/duckdb.h
153:          python3.7 scripts/asset-upload-gha.py libduckdb-linux-aarch64.zip duckdb_cli-linux-aarch64.zip duckdb_odbc-linux-aarch64.zip duckdb_jdbc-linux-aarch64.jar=build/release/tools/jdbc/duckdb_jdbc.jar
154: 
155:      - uses: actions/upload-artifact@v2
156:        with:
157:          name: duckdb-binaries-linux-aarch64
158:          path: |
159:            libduckdb-linux-aarch64.zip
160:            duckdb_odbc-linux-aarch64.zip
161:            duckdb_cli-linux-aarch64.zip
162:            build/release/tools/jdbc/duckdb_jdbc.jar
163: 
164:  linux-extensions-64:
165:     name: Linux Extensions (64 Bit)
166:     runs-on: ubuntu-latest
167:     container: ubuntu:16.04
168:     needs: linux-release-64
169: 
170:     steps:
171:     - uses: actions/checkout@v3
172:       with:
173:         fetch-depth: 0
174:     - uses: ./.github/actions/ubuntu_16_setup
175:       with:
176:         openssl: 1
177: 
178:     - uses: ./.github/actions/build_extensions
179:       with:
180:         post_install: rm build/release/src/libduckdb*
181:         deploy_as: linux_amd64
182:         s3_id: ${{ secrets.S3_ID }}
183:         s3_key: ${{ secrets.S3_KEY }}
184:         signing_pk: ${{ secrets.DUCKDB_EXTENSION_SIGNING_PK }}
185: 
186:     - uses: actions/upload-artifact@v2
187:       with:
188:         name: linux-extensions-64
189:         path: |
190:           build/release/extension/*/*.duckdb_extension
191: 
192:  linux-extensions-64-aarch64:
193:     name: Linux Extensions (AARCH64)
194:     runs-on: ubuntu-latest
195:     container: ubuntu:18.04 # cross compiler not available in 16
196:     needs: linux-release-64
197: 
198:     steps:
199:       - uses: actions/checkout@v3
200:         with:
201:           fetch-depth: 0
202: 
203:       - uses: ./.github/actions/ubuntu_16_setup
204:         with:
205:           openssl: 1
206:           aarch64_cross_compile: 1
207: 
208:       - name: Configure OpenSSL path
209:         shell: bash
210:         run: |
211:           export OPENSSL_ROOT_DIR=`pwd`/build/openssl/build
212:           echo "OPENSSL_ROOT_DIR=$OPENSSL_ROOT_DIR" >> $GITHUB_ENV
213: 
214:       - uses: ./.github/actions/build_extensions
215:         with:
216:           openssl_path: ${{ env.OPENSSL_ROOT_DIR }}
217:           deploy_as: linux_aarch64
218:           treat_warn_as_error: 0
219:           s3_id: ${{ secrets.S3_ID }}
220:           s3_key: ${{ secrets.S3_KEY }}
221:           signing_pk: ${{ secrets.DUCKDB_EXTENSION_SIGNING_PK }}
222:           aarch64_cross_compile: 1
223:           run_tests: 0 # Cannot run tests here due to cross-compiling
224: 
225:       - uses: actions/upload-artifact@v2
226:         with:
227:           name: linux-extensions-64-aarch64
228:           path: |
229:             build/release/extension/*/*.duckdb_extension
230: 
231:  check-load-install-extensions:
232:     name: Checks extension functions
233:     runs-on: ubuntu-20.04
234:     needs: linux-extensions-64
235: 
236:     env:
237:       CC: gcc-10
238:       CXX: g++-10
239:       GEN: ninja
240: 
241:     steps:
242:     - uses: actions/checkout@v3
243:       with:
244:         fetch-depth: 0
245: 
246:     - uses: actions/setup-python@v2
247:       with:
248:         python-version: '3.9'
249: 
250:     - name: Install
251:       shell: bash
252:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
253: 
254:     - name: Setup Ccache
255:       uses: hendrikmuhs/ccache-action@main
256:       with:
257:         key: ${{ github.job }}
258: 
259:     - name: Build
260:       shell: bash
261:       run: |
262:         mkdir -p build/release
263:         cd build/release
264:         cmake -G "Ninja" -DCMAKE_BUILD_TYPE=Release ../..
265:         cmake --build . --config Release
266: 
267:     - uses: actions/download-artifact@v3
268:       with:
269:         name: linux-extensions-64
270:         path: /tmp
271: 
272:     - name: Check if extension_functions.hpp is up to date
273:       shell: bash
274:       run: |
275:         python scripts/generate_extensions_function.py --validate
276: 
277:  linux-release-32:
278:     name: Linux (32 Bit)
279:     runs-on: ubuntu-latest
280:     container: ubuntu:16.04
281:     needs: linux-release-64
282:     env:
283:       GEN: ninja
284: 
285:     steps:
286:     - uses: actions/checkout@v3
287:     - uses: ./.github/actions/ubuntu_16_setup
288: 
289:     - name: Build
290:       shell: bash
291:       run: |
292:         mkdir -p build/release
293:         (cd build/release && cmake -DSTATIC_LIBCPP=1 -DJDBC_DRIVER=1 -DBUILD_ICU_EXTENSION=1 -DBUILD_PARQUET_EXTENSION=1 -DBUILD_FTS_EXTENSION=1 -DBUILD_JSON_EXTENSION=1 -DBUILD_EXCEL_EXTENSION=1 -DFORCE_32_BIT=1 -DCMAKE_BUILD_TYPE=Release ../.. && cmake --build .)
294: 
295:     - name: Test
296:       shell: bash
297:       run: build/release/test/unittest "*"
298: 
299:     - name: Deploy
300:       shell: bash
301:       run: |
302:         python3.7 scripts/amalgamation.py
303:         zip -j duckdb_cli-linux-i386.zip build/release/duckdb
304:         zip -j libduckdb-linux-i386.zip build/release/src/libduckdb*.so src/amalgamation/duckdb.hpp src/include/duckdb.h
305:         python3.7 scripts/asset-upload-gha.py libduckdb-linux-i386.zip duckdb_cli-linux-i386.zip duckdb_jdbc-linux-i386.jar=build/release/tools/jdbc/duckdb_jdbc.jar
306: 
307:     - uses: actions/upload-artifact@v2
308:       with:
309:         name: duckdb-binaries-linux
310:         path: |
311:           libduckdb-linux-i386.zip
312:           duckdb_cli-linux-i386.zip
313:     - uses: actions/upload-artifact@v2
314:       with:
315:         name: duckdb-binaries-linux-32bit
316:         path: |
317:           build/release/tools/jdbc/duckdb_jdbc.jar
318: 
319: 
320:  linux-rpi:
321:     name: Linux (Raspberry Pi)
322:     runs-on: ubuntu-20.04
323:     needs: linux-release-64
324:     steps:
325:     - uses: actions/checkout@v3
326:       with:
327:         fetch-depth: 0
328: 
329:     - uses: actions/setup-python@v2
330:       with:
331:         python-version: '3.7'
332: 
333:     - name: Install
334:       shell: bash
335:       run: |
336:         sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
337:         git clone https://github.com/raspberrypi/tools --depth=1 rpi-tools
338: 
339:     - name: Setup Ccache
340:       uses: hendrikmuhs/ccache-action@main
341:       with:
342:         key: ${{ github.job }}
343: 
344:     - name: Build
345:       shell: bash
346:       run: |
347:         export TOOLCHAIN=`pwd`/rpi-tools
348:         mkdir -p build/release
349:         cd build/release
350:         cmake -G Ninja -DBUILD_TPCH_EXTENSION=1 -DBUILD_TPCDS_EXTENSION=1 -DDUCKDB_RPI_TOOLCHAIN_PREFIX=$TOOLCHAIN -DBUILD_UNITTESTS=0 -DCMAKE_TOOLCHAIN_FILE=../../scripts/raspberry-pi-cmake-toolchain.cmake ../../
351:         cmake --build .
352:         file duckdb
353: 
354:     - name: Deploy
355:       shell: bash
356:       run: |
357:         python scripts/amalgamation.py
358:         zip -j duckdb_cli-linux-rpi.zip build/release/duckdb
359:         zip -j libduckdb-linux-rpi.zip build/release/src/libduckdb*.so src/amalgamation/duckdb.hpp src/include/duckdb.h
360:         python scripts/asset-upload-gha.py libduckdb-linux-rpi.zip duckdb_cli-linux-rpi.zip
361: 
362:     - uses: actions/upload-artifact@v2
363:       with:
364:         name: duckdb-binaries-rpi
365:         path: |
366:           libduckdb-linux-rpi.zip
367:           duckdb_cli-linux-rpi.zip
368: 
369: 
370:  old-gcc:
371:     name: GCC 4.8
372:     runs-on: ubuntu-18.04
373:     needs: linux-release-64
374: 
375:     env:
376:       CC: gcc-4.8
377:       CXX: g++-4.8
378: 
379:     steps:
380:     - uses: actions/checkout@v3
381:       with:
382:         fetch-depth: 0
383: 
384:     - uses: actions/setup-python@v2
385:       with:
386:         python-version: '3.7'
387: 
388:     - name: Install
389:       shell: bash
390:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq g++-4.8 binutils
391: 
392:     - name: Setup Ccache
393:       uses: hendrikmuhs/ccache-action@main
394:       with:
395:         key: ${{ github.job }}
396: 
397:     - name: Build
398:       shell: bash
399:       run: make release
400: 
401:     - name: Test
402:       shell: bash
403:       run: make allunit
404: 
405:  release-assert:
406:     name: Release Assertions
407:     runs-on: ubuntu-20.04
408:     needs: linux-release-64
409:     env:
410:       CC: gcc-10
411:       CXX: g++-10
412:       GEN: ninja
413:       BUILD_ICU: 1
414:       BUILD_INET: 1
415:       BUILD_TPCH: 1
416:       BUILD_TPCDS: 1
417:       BUILD_FTS: 1
418:       BUILD_EXCEL: 1
419:       BUILD_VISUALIZER: 1
420:       BUILD_JSON: 1
421:       BUILD_JEMALLOC: 1
422:       DISABLE_SANITIZER: 1
423:       CRASH_ON_ASSERT: 1
424: 
425:     steps:
426:     - uses: actions/checkout@v3
427:       with:
428:         fetch-depth: 0
429: 
430:     - name: Install
431:       shell: bash
432:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
433: 
434:     - name: Setup Ccache
435:       uses: hendrikmuhs/ccache-action@main
436:       with:
437:         key: ${{ github.job }}
438: 
439:     - name: Build
440:       shell: bash
441:       run: make relassert
442: 
443:     - name: Test
444:       shell: bash
445:       run: |
446:           python3 scripts/run_tests_one_by_one.py build/relassert/test/unittest "*"
447: 
448: 
449:  vector-sizes:
450:     name: Vector Sizes
451:     runs-on: ubuntu-20.04
452:     needs: linux-release-64
453:     env:
454:       CC: gcc-10
455:       CXX: g++-10
456: 
457:     steps:
458:     - uses: actions/checkout@v3
459:       with:
460:         fetch-depth: 0
461: 
462:     - uses: actions/setup-python@v2
463:       with:
464:         python-version: '3.7'
465: 
466:     - name: Setup Ccache
467:       uses: hendrikmuhs/ccache-action@main
468:       with:
469:         key: ${{ github.job }}
470: 
471:     - name: Test
472:       shell: bash
473:       run: python scripts/test_vector_sizes.py
474: 
475:  linux-wasm-release:
476:     name: WebAssembly Release
477:     runs-on: ubuntu-20.04
478:     needs: linux-release-64
479:     steps:
480:     - uses: actions/checkout@v3
481:       with:
482:         fetch-depth: 0
483: 
484:     - name: Build Amalgamation
485:       shell: bash
486:       run: python scripts/amalgamation.py
487: 
488:     - name: Setup
489:       shell: bash
490:       run: ./scripts/wasm_configure.sh
491: 
492:     - name: Setup Ccache
493:       uses: hendrikmuhs/ccache-action@main
494:       with:
495:         key: ${{ github.job }}
496: 
497:     - name: Build Library Module
498:       shell: bash
499:       run: ./scripts/wasm_build_lib.sh Release
500: 
501:     - name: Build Test Module
502:       shell: bash
503:       run: ./scripts/wasm_build_test.sh Release
504: 
505:     - name: Test WASM Module
506:       shell: bash
507:       run: node ./test/wasm/hello_wasm_test.js
508: 
509:     - name: Package
510:       shell: bash
511:       run: |
512:         zip -j duckdb-wasm32-nothreads.zip ./.wasm/build/duckdb.wasm
513:         python scripts/asset-upload-gha.py duckdb-wasm32-nothreads.zip
514: 
515:     - uses: actions/upload-artifact@v2
516:       with:
517:         name: duckdb-wasm32-nothreads
518:         path: |
519:           duckdb-wasm32-nothreads.zip
520: 
521:  symbol-leakage:
522:     name: Symbol Leakage
523:     runs-on: ubuntu-20.04
524:     needs: linux-release-64
525: 
526:     steps:
527:     - uses: actions/checkout@v3
528:       with:
529:         fetch-depth: 0
530: 
531:     - uses: actions/setup-python@v2
532:       with:
533:         python-version: '3.7'
534: 
535:     - name: Setup Ccache
536:       uses: hendrikmuhs/ccache-action@main
537:       with:
538:         key: ${{ github.job }}
539: 
540:     - name: Build
541:       shell: bash
542:       run: make
543: 
544:     - name: Symbol Leakage Test
545:       shell: bash
546:       run: python3.7 scripts/exported_symbols_check.py build/release/src/libduckdb*.so
547: 
548:  linux-httpfs:
549:     name: Linux HTTPFS
550:     runs-on: ubuntu-20.04
551:     needs: linux-release-64
552:     env:
553:       BUILD_VISUALIZER: 1
554:       BUILD_HTTPFS: 1
555:       S3_TEST_SERVER_AVAILABLE: 1
556:       GEN: ninja
557: 
558:     steps:
559:     - uses: actions/checkout@v3
560:       with:
561:         fetch-depth: 0
562: 
563:     - uses: actions/setup-python@v2
564:       with:
565:         python-version: '3.7'
566: 
567:     - name: Install Ninja
568:       shell: bash
569:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
570: 
571:     - name: Setup Ccache
572:       uses: hendrikmuhs/ccache-action@main
573:       with:
574:         key: ${{ github.job }}
575: 
576:     - name: Build
577:       shell: bash
578:       run: make
579: 
580:     - name: Start S3/HTTP test server
581:       shell: bash
582:       run: |
583:         sudo ./scripts/install_s3_test_server.sh
584:         ./scripts/run_s3_test_server.sh
585:         sleep 60
586: 
587:     - name: Test
588:       shell: bash
589:       run: |
590:         python3 scripts/get_test_list.py --file-contains "require httpfs" > test.list
591:         build/release/test/unittest -f test.list
592: 
593:  amalgamation-tests:
594:     name: Amalgamation Tests
595:     needs: linux-release-64
596:     runs-on: ubuntu-20.04
597:     env:
598:       CC: clang
599:       CXX: clang++
600: 
601:     steps:
602:     - uses: actions/checkout@v3
603:       with:
604:         fetch-depth: 0
605: 
606:     - uses: actions/setup-python@v2
607:       with:
608:         python-version: '3.7'
609: 
610:     - name: Install LLVM and Clang
611:       uses: KyleMayes/install-llvm-action@v1
612:       with:
613:         version: "10.0"
614: 
615:     - name: Generate Amalgamation
616:       shell: bash
617:       run:  |
618:           python scripts/amalgamation.py --extended
619:           python scripts/parquet_amalgamation.py
620:           clang++ -std=c++17 -Isrc/amalgamation src/amalgamation/parquet-amalgamation.cpp src/amalgamation/duckdb.cpp -emit-llvm -S -O0
[end of .github/workflows/LinuxRelease.yml]
[start of extension/httpfs/s3fs.cpp]
1: #include "s3fs.hpp"
2: 
3: #include "crypto.hpp"
4: #include "duckdb.hpp"
5: #ifndef DUCKDB_AMALGAMATION
6: #include "duckdb/common/thread.hpp"
7: #include "duckdb/common/types/timestamp.hpp"
8: #include "duckdb/function/scalar/strftime.hpp"
9: #endif
10: 
11: #include <duckdb/function/scalar/string_functions.hpp>
12: #include <duckdb/storage/buffer_manager.hpp>
13: #include <iostream>
14: #include <thread>
15: 
16: namespace duckdb {
17: 
18: static HeaderMap create_s3_header(string url, string query, string host, string service, string method,
19:                                   const S3AuthParams &auth_params, string date_now = "", string datetime_now = "",
20:                                   string payload_hash = "", string content_type = "") {
21: 
22: 	HeaderMap res;
23: 	res["Host"] = host;
24: 	// If access key is not set, we don't set the headers at all to allow accessing public files through s3 urls
25: 	if (auth_params.secret_access_key.empty() && auth_params.access_key_id.empty()) {
26: 		return res;
27: 	}
28: 
29: 	if (payload_hash == "") {
30: 		payload_hash = "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"; // Empty payload hash
31: 	}
32: 
33: 	// we can pass date/time but this is mostly useful in testing. normally we just get the current datetime here.
34: 	if (datetime_now.empty()) {
35: 		auto timestamp = Timestamp::GetCurrentTimestamp();
36: 		date_now = StrfTimeFormat::Format(timestamp, "%Y%m%d");
37: 		datetime_now = StrfTimeFormat::Format(timestamp, "%Y%m%dT%H%M%SZ");
38: 	}
39: 
40: 	res["x-amz-date"] = datetime_now;
41: 	res["x-amz-content-sha256"] = payload_hash;
42: 	if (auth_params.session_token.length() > 0) {
43: 		res["x-amz-security-token"] = auth_params.session_token;
44: 	}
45: 
46: 	string signed_headers = "";
47: 	hash_bytes canonical_request_hash;
48: 	hash_str canonical_request_hash_str;
49: 	if (content_type.length() > 0) {
50: 		signed_headers += "content-type;";
51: 	}
52: 	signed_headers += "host;x-amz-content-sha256;x-amz-date";
53: 	if (auth_params.session_token.length() > 0) {
54: 		signed_headers += ";x-amz-security-token";
55: 	}
56: 	auto canonical_request = method + "\n" + S3FileSystem::UrlEncode(url) + "\n" + query;
57: 	if (content_type.length() > 0) {
58: 		canonical_request += "\ncontent-type:" + content_type;
59: 	}
60: 	canonical_request += "\nhost:" + host + "\nx-amz-content-sha256:" + payload_hash + "\nx-amz-date:" + datetime_now;
61: 	if (auth_params.session_token.length() > 0) {
62: 		canonical_request += "\nx-amz-security-token:" + auth_params.session_token;
63: 	}
64: 
65: 	canonical_request += "\n\n" + signed_headers + "\n" + payload_hash;
66: 	sha256(canonical_request.c_str(), canonical_request.length(), canonical_request_hash);
67: 
68: 	hex256(canonical_request_hash, canonical_request_hash_str);
69: 	auto string_to_sign = "AWS4-HMAC-SHA256\n" + datetime_now + "\n" + date_now + "/" + auth_params.region + "/" +
70: 	                      service + "/aws4_request\n" + string((char *)canonical_request_hash_str, sizeof(hash_str));
71: 	// compute signature
72: 	hash_bytes k_date, k_region, k_service, signing_key, signature;
73: 	hash_str signature_str;
74: 	auto sign_key = "AWS4" + auth_params.secret_access_key;
75: 	hmac256(date_now, sign_key.c_str(), sign_key.length(), k_date);
76: 	hmac256(auth_params.region, k_date, k_region);
77: 	hmac256(service, k_region, k_service);
78: 	hmac256("aws4_request", k_service, signing_key);
79: 	hmac256(string_to_sign, signing_key, signature);
80: 	hex256(signature, signature_str);
81: 
82: 	res["Authorization"] = "AWS4-HMAC-SHA256 Credential=" + auth_params.access_key_id + "/" + date_now + "/" +
83: 	                       auth_params.region + "/" + service + "/aws4_request, SignedHeaders=" + signed_headers +
84: 	                       ", Signature=" + string((char *)signature_str, sizeof(hash_str));
85: 
86: 	return res;
87: }
88: 
89: static unique_ptr<duckdb_httplib_openssl::Headers> initialize_http_headers(HeaderMap &header_map) {
90: 	auto headers = make_unique<duckdb_httplib_openssl::Headers>();
91: 	for (auto &entry : header_map) {
92: 		headers->insert(entry);
93: 	}
94: 	return headers;
95: }
96: 
97: string S3FileSystem::UrlDecode(string input) {
98: 	string result;
99: 	result.reserve(input.size());
100: 	char ch;
101: 	replace(input.begin(), input.end(), '+', ' ');
102: 	for (idx_t i = 0; i < input.length(); i++) {
103: 		if (int(input[i]) == 37) {
104: 			int ii;
105: 			sscanf(input.substr(i + 1, 2).c_str(), "%x", &ii);
106: 			ch = static_cast<char>(ii);
107: 			result += ch;
108: 			i += 2;
109: 		} else {
110: 			result += input[i];
111: 		}
112: 	}
113: 	return result;
114: }
115: 
116: string S3FileSystem::UrlEncode(const string &input, bool encode_slash) {
117: 	// https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html
118: 	static const char *hex_digit = "0123456789ABCDEF";
119: 	string result;
120: 	result.reserve(input.size());
121: 	for (idx_t i = 0; i < input.length(); i++) {
122: 		char ch = input[i];
123: 		if ((ch >= 'A' && ch <= 'Z') || (ch >= 'a' && ch <= 'z') || (ch >= '0' && ch <= '9') || ch == '_' ||
124: 		    ch == '-' || ch == '~' || ch == '.') {
125: 			result += ch;
126: 		} else if (ch == '/') {
127: 			if (encode_slash) {
128: 				result += string("%2F");
129: 			} else {
130: 				result += ch;
131: 			}
132: 		} else {
133: 			result += string("%");
134: 			result += hex_digit[static_cast<unsigned char>(ch) >> 4];
135: 			result += hex_digit[static_cast<unsigned char>(ch) & 15];
136: 		}
137: 	}
138: 	return result;
139: }
140: 
141: S3AuthParams S3AuthParams::ReadFrom(FileOpener *opener) {
142: 	string region;
143: 	string access_key_id;
144: 	string secret_access_key;
145: 	string session_token;
146: 	string endpoint;
147: 	string url_style;
148: 	bool use_ssl;
149: 	Value value;
150: 
151: 	if (opener->TryGetCurrentSetting("s3_region", value)) {
152: 		region = value.ToString();
153: 	}
154: 
155: 	if (opener->TryGetCurrentSetting("s3_access_key_id", value)) {
156: 		access_key_id = value.ToString();
157: 	}
158: 
159: 	if (opener->TryGetCurrentSetting("s3_secret_access_key", value)) {
160: 		secret_access_key = value.ToString();
161: 	}
162: 
163: 	if (opener->TryGetCurrentSetting("s3_session_token", value)) {
164: 		session_token = value.ToString();
165: 	}
166: 
167: 	if (opener->TryGetCurrentSetting("s3_endpoint", value)) {
168: 		endpoint = value.ToString();
169: 	} else {
170: 		endpoint = "s3.amazonaws.com";
171: 	}
172: 
173: 	if (opener->TryGetCurrentSetting("s3_url_style", value)) {
174: 		auto val_str = value.ToString();
175: 		if (!(val_str == "vhost" || val_str != "path" || val_str != "")) {
176: 			throw std::runtime_error(
177: 			    "Incorrect setting found for s3_url_style, allowed values are: 'path' and 'vhost'");
178: 		}
179: 		url_style = val_str;
180: 	} else {
181: 		url_style = "vhost";
182: 	}
183: 
184: 	if (opener->TryGetCurrentSetting("s3_use_ssl", value)) {
185: 		use_ssl = value.GetValue<bool>();
186: 	} else {
187: 		use_ssl = true;
188: 	}
189: 
190: 	return {region, access_key_id, secret_access_key, session_token, endpoint, url_style, use_ssl};
191: }
192: 
193: S3ConfigParams S3ConfigParams::ReadFrom(FileOpener *opener) {
194: 	uint64_t uploader_max_filesize;
195: 	uint64_t max_parts_per_file;
196: 	uint64_t max_upload_threads;
197: 	Value value;
198: 
199: 	if (opener->TryGetCurrentSetting("s3_uploader_max_filesize", value)) {
200: 		uploader_max_filesize = DBConfig::ParseMemoryLimit(value.GetValue<string>());
201: 	} else {
202: 		uploader_max_filesize = S3ConfigParams::DEFAULT_MAX_FILESIZE;
203: 	}
204: 
205: 	if (opener->TryGetCurrentSetting("s3_uploader_max_parts_per_file", value)) {
206: 		max_parts_per_file = value.GetValue<uint64_t>();
207: 	} else {
208: 		max_parts_per_file = S3ConfigParams::DEFAULT_MAX_PARTS_PER_FILE; // AWS Default
209: 	}
210: 
211: 	if (opener->TryGetCurrentSetting("s3_uploader_thread_limit", value)) {
212: 		max_upload_threads = value.GetValue<uint64_t>();
213: 	} else {
214: 		max_upload_threads = S3ConfigParams::DEFAULT_MAX_UPLOAD_THREADS;
215: 	}
216: 
217: 	return {uploader_max_filesize, max_parts_per_file, max_upload_threads};
218: }
219: 
220: void S3FileHandle::Close() {
221: 	auto &s3fs = (S3FileSystem &)file_system;
222: 	if ((flags & FileFlags::FILE_FLAGS_WRITE) && !upload_finalized) {
223: 		s3fs.FlushAllBuffers(*this);
224: 		s3fs.FinalizeMultipartUpload(*this);
225: 	}
226: }
227: 
228: void S3FileHandle::InitializeClient() {
229: 	auto parsed_url = S3FileSystem::S3UrlParse(path, this->auth_params);
230: 
231: 	string proto_host_port = parsed_url.http_proto + parsed_url.host;
232: 	http_client = HTTPFileSystem::GetClient(this->http_params, proto_host_port.c_str());
233: }
234: 
235: // Opens the multipart upload and returns the ID
236: string S3FileSystem::InitializeMultipartUpload(S3FileHandle &file_handle) {
237: 	auto &s3fs = (S3FileSystem &)file_handle.file_system;
238: 
239: 	// AWS response is around 300~ chars in docs so this should be enough to not need a resize
240: 	idx_t response_buffer_len = 1000;
241: 	auto response_buffer = unique_ptr<char[]> {new char[response_buffer_len]};
242: 
243: 	string query_param = "?" + UrlEncode("uploads") + "=";
244: 	auto res = s3fs.PostRequest(file_handle, file_handle.stripped_path + query_param, {}, response_buffer,
245: 	                            response_buffer_len, nullptr, 0);
246: 	string result(response_buffer.get(), response_buffer_len);
247: 
248: 	auto open_tag_pos = result.find("<UploadId>", 0);
249: 	auto close_tag_pos = result.find("</UploadId>", open_tag_pos);
250: 
251: 	if (open_tag_pos == string::npos || close_tag_pos == string::npos) {
252: 		throw std::runtime_error("Unexpected response while initializing S3 multipart upload");
253: 	}
254: 
255: 	open_tag_pos += 10; // Skip open tag
256: 
257: 	return result.substr(open_tag_pos, close_tag_pos - open_tag_pos);
258: }
259: 
260: void S3FileSystem::UploadBuffer(S3FileHandle &file_handle, shared_ptr<S3WriteBuffer> write_buffer) {
261: 	auto &s3fs = (S3FileSystem &)file_handle.file_system;
262: 
263: 	string query_param = S3FileSystem::UrlEncode("partNumber") + "=" + to_string(write_buffer->part_no + 1) + "&" +
264: 	                     S3FileSystem::UrlEncode("uploadId") + "=" +
265: 	                     S3FileSystem::UrlEncode(file_handle.multipart_upload_id, true);
266: 	unique_ptr<ResponseWrapper> res;
267: 
268: 	bool success = false;
269: 	string last_error = "";
270: 	auto time_at_start = duration_cast<milliseconds>(system_clock::now().time_since_epoch()).count();
271: 
272: 	// Retry loop to make large uploads resilient to brief connection issues
273: 	while (true) {
274: 		try {
275: 			res = s3fs.PutRequest(file_handle, file_handle.stripped_path + "?" + query_param, {},
276: 			                      (char *)write_buffer->Ptr(), write_buffer->idx);
277: 			if (res->code == 200) {
278: 				success = true;
279: 				break;
280: 			} else {
281: 				last_error = res->error + " (HTTP code " + to_string(res->code) + ")";
282: 			}
283: 		} catch (std::runtime_error &e) {
284: 			if (strncmp(e.what(), "HTTP PUT error", 14) != 0) {
285: 				throw e;
286: 			}
287: 			last_error = e.what();
288: 		}
289: 
290: 		// If there are no parts uploaded yet, failing immediately makes more sense than waiting for the time-out
291: 		if (file_handle.parts_uploaded.load() == 0) {
292: 			break;
293: 		}
294: 
295: 		auto current_time = duration_cast<milliseconds>(system_clock::now().time_since_epoch()).count();
296: 		if ((uint64_t)(current_time - time_at_start) > file_handle.http_params.timeout) {
297: 			break;
298: 		}
299: 
300: 		std::this_thread::sleep_for(milliseconds(MULTIPART_UPLOAD_WAIT_BETWEEN_RETRIES_MS + 1));
301: 	}
302: 
303: 	if (!success) {
304: 		throw std::runtime_error("Unable to connect to URL \"" + file_handle.path + "\"(last attempt failed with: \"" +
305: 		                         last_error + "\")");
306: 	}
307: 
308: 	auto etag_lookup = res->headers.find("ETag");
309: 	if (etag_lookup == res->headers.end()) {
310: 		throw std::runtime_error("Unexpected reponse when uploading part to S3");
311: 	}
312: 
313: 	// Insert etag
314: 	file_handle.part_etags_lock.lock();
315: 	file_handle.part_etags.insert(std::pair<uint16_t, string>(write_buffer->part_no, etag_lookup->second));
316: 	file_handle.part_etags_lock.unlock();
317: 
318: 	file_handle.parts_uploaded++;
319: 
320: 	// Free up space for another thread to acquire an S3WriteBuffer
321: 	write_buffer.reset();
322: 	s3fs.buffers_available++;
323: 	s3fs.buffers_available_cv.notify_one();
324: 
325: 	// Update uploads in progress
326: 	file_handle.uploads_in_progress--;
327: 	file_handle.uploads_in_progress_cv.notify_one();
328: }
329: 
330: void S3FileSystem::FlushBuffer(S3FileHandle &file_handle, shared_ptr<S3WriteBuffer> write_buffer) {
331: 	if (write_buffer->idx == 0) {
332: 		return;
333: 	}
334: 
335: 	auto uploading = write_buffer->uploading.load();
336: 	if (uploading) {
337: 		return;
338: 	}
339: 	bool can_upload = write_buffer->uploading.compare_exchange_strong(uploading, true);
340: 	if (!can_upload) {
341: 		return;
342: 	}
343: 
344: 	file_handle.write_buffers_lock.lock();
345: 	file_handle.write_buffers.erase(write_buffer->part_no);
346: 	file_handle.write_buffers_lock.unlock();
347: 	file_handle.uploads_in_progress++;
348: 
349: 	thread upload_thread(UploadBuffer, std::ref(file_handle), write_buffer);
350: 	upload_thread.detach();
351: }
352: 
353: // Note that FlushAll currently does not allow to continue writing afterwards. Therefore, FinalizeMultipartUpload should
354: // be called right after it!
355: // TODO: we can fix this by keeping the last partially written buffer in memory and allow reuploading it with new data.
356: void S3FileSystem::FlushAllBuffers(S3FileHandle &file_handle) {
357: 	//  Collect references to all buffers to check
358: 	vector<shared_ptr<S3WriteBuffer>> to_flush;
359: 	file_handle.write_buffers_lock.lock();
360: 	for (auto &item : file_handle.write_buffers) {
361: 		to_flush.push_back(item.second);
362: 	}
363: 	file_handle.write_buffers_lock.unlock();
364: 
365: 	// Flush all buffers that aren't already uploading
366: 	for (auto &write_buffer : to_flush) {
367: 		if (!write_buffer->uploading) {
368: 			FlushBuffer(file_handle, write_buffer);
369: 		}
370: 	}
371: 	unique_lock<mutex> lck(file_handle.uploads_in_progress_lock);
372: 	file_handle.uploads_in_progress_cv.wait(lck,
373: 	                                        [&file_handle] { return file_handle.uploads_in_progress.load() == 0; });
374: }
375: 
376: void S3FileSystem::FinalizeMultipartUpload(S3FileHandle &file_handle) {
377: 	auto &s3fs = (S3FileSystem &)file_handle.file_system;
378: 
379: 	std::stringstream ss;
380: 	ss << "<CompleteMultipartUpload xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">";
381: 
382: 	auto parts = file_handle.parts_uploaded.load();
383: 	for (auto i = 0; i < parts; i++) {
384: 		auto etag_lookup = file_handle.part_etags.find(i);
385: 		if (etag_lookup == file_handle.part_etags.end()) {
386: 			throw std::runtime_error("Unknown part number");
387: 		}
388: 		ss << "<Part><ETag>" << etag_lookup->second << "</ETag><PartNumber>" << i + 1 << "</PartNumber></Part>";
389: 	}
390: 	ss << "</CompleteMultipartUpload>";
391: 	string body = ss.str();
392: 
393: 	// Response is around ~400 in AWS docs so this should be enough to not need a resize
394: 	idx_t response_buffer_len = 1000;
395: 	auto response_buffer = unique_ptr<char[]> {new char[response_buffer_len]};
396: 
397: 	string query_param = "?" + UrlEncode("uploadId") + "=" + file_handle.multipart_upload_id;
398: 	auto res = s3fs.PostRequest(file_handle, file_handle.stripped_path + query_param, {}, response_buffer,
399: 	                            response_buffer_len, (char *)body.c_str(), body.length());
400: 	string result(response_buffer.get(), response_buffer_len);
401: 
402: 	auto open_tag_pos = result.find("<CompleteMultipartUploadResult", 0);
403: 	if (open_tag_pos == string::npos) {
404: 		throw std::runtime_error("Unexpected response during S3 multipart upload finalization");
405: 	}
406: 	file_handle.upload_finalized = true;
407: }
408: 
409: shared_ptr<S3WriteBuffer> S3FileSystem::GetBuffer(S3FileHandle &file_handle, uint16_t write_buffer_idx) {
410: 	auto &s3fs = (S3FileSystem &)file_handle.file_system;
411: 	// Check if write buffer already exists
412: 	{
413: 		unique_lock<mutex> lck(file_handle.write_buffers_lock);
414: 		auto lookup_result = file_handle.write_buffers.find(write_buffer_idx);
415: 		if (lookup_result != file_handle.write_buffers.end()) {
416: 			shared_ptr<S3WriteBuffer> buffer = lookup_result->second;
417: 			return buffer;
418: 		}
419: 	}
420: 
421: 	// Wait for a buffer to become available
422: 	{
423: 		unique_lock<mutex> lck(s3fs.buffers_available_lock);
424: 		s3fs.buffers_available_cv.wait(lck, [&s3fs] { return s3fs.buffers_available > 0; });
425: 		s3fs.buffers_available--;
426: 	}
427: 
428: 	// Try to allocate a buffer from the buffer manager
429: 	BufferHandle duckdb_buffer;
430: 	bool set_waiting_for_memory = false;
431: 
432: 	while (true) {
433: 		try {
434: 			duckdb_buffer = buffer_manager.Allocate(file_handle.part_size);
435: 
436: 			if (set_waiting_for_memory) {
437: 				threads_waiting_for_memory--;
438: 			}
439: 			break;
440: 		} catch (OutOfMemoryException &e) {
441: 			if (!set_waiting_for_memory) {
442: 				threads_waiting_for_memory++;
443: 				set_waiting_for_memory = true;
444: 			}
445: 			auto buffers_available = s3fs.buffers_available.load();
446: 
447: 			if (buffers_available >= file_handle.config_params.max_upload_threads - threads_waiting_for_memory) {
448: 				// There exist no upload write buffers that can release more memory. We really ran out of memory here.
449: 				throw e;
450: 			} else {
451: 
452: 				// Wait for more buffers to become available before trying again
453: 				{
454: 					unique_lock<mutex> lck(s3fs.buffers_available_lock);
455: 					s3fs.buffers_available_cv.wait(
456: 					    lck, [&s3fs, &buffers_available] { return s3fs.buffers_available > buffers_available; });
457: 				}
458: 			}
459: 		}
460: 	}
461: 
462: 	auto new_write_buffer = make_shared<S3WriteBuffer>(write_buffer_idx * file_handle.part_size, file_handle.part_size,
463: 	                                                   move(duckdb_buffer));
464: 
465: 	{
466: 		unique_lock<mutex> lck(file_handle.write_buffers_lock);
467: 		auto lookup_result = file_handle.write_buffers.find(write_buffer_idx);
468: 
469: 		// Check if other thread has created the same buffer, if so we return theirs and drop ours.
470: 		if (lookup_result != file_handle.write_buffers.end()) {
471: 			// write_buffer_idx << std::endl;
472: 			shared_ptr<S3WriteBuffer> write_buffer = lookup_result->second;
473: 			file_handle.write_buffers_lock.unlock();
474: 			return write_buffer;
475: 		}
476: 		file_handle.write_buffers.insert(pair<uint16_t, shared_ptr<S3WriteBuffer>>(write_buffer_idx, new_write_buffer));
477: 	}
478: 
479: 	return new_write_buffer;
480: }
481: 
482: void S3FileSystem::GetQueryParam(const string &key, string &param, duckdb_httplib_openssl::Params &query_params) {
483: 	auto found_param = query_params.find(key);
484: 	if (found_param != query_params.end()) {
485: 		param = found_param->second;
486: 		query_params.erase(found_param);
487: 	}
488: }
489: 
490: void S3FileSystem::ReadQueryParams(const string &url_query_param, S3AuthParams &params) {
491: 	if (url_query_param.empty())
492: 		return;
493: 
494: 	duckdb_httplib_openssl::Params query_params;
495: 	duckdb_httplib_openssl::detail::parse_query_text(url_query_param, query_params);
496: 
497: 	GetQueryParam("s3_region", params.region, query_params);
498: 	GetQueryParam("s3_access_key_id", params.access_key_id, query_params);
499: 	GetQueryParam("s3_secret_access_key", params.secret_access_key, query_params);
500: 	GetQueryParam("s3_session_token", params.session_token, query_params);
501: 	GetQueryParam("s3_endpoint", params.endpoint, query_params);
502: 	GetQueryParam("s3_url_style", params.url_style, query_params);
503: 	auto found_param = query_params.find("s3_use_ssl");
504: 	if (found_param != query_params.end()) {
505: 		if (found_param->second == "true") {
506: 			params.use_ssl = true;
507: 		} else if (found_param->second == "false") {
508: 			params.use_ssl = false;
509: 		} else {
510: 			throw std::runtime_error("Incorrect setting found for s3_use_ssl, allowed values are: 'true' or 'false'");
511: 		}
512: 		query_params.erase(found_param);
513: 	}
514: 	if (!query_params.empty()) {
515: 		throw std::runtime_error(
516: 		    "Invalid query parameters found. Supported parameters are:\n's3_region', 's3_access_key_id', "
517: 		    "'s3_secret_access_key', 's3_session_token',\n's3_endpoint', 's3_url_style', 's3_use_ssl'");
518: 	}
519: }
520: 
521: ParsedS3Url S3FileSystem::S3UrlParse(string url, S3AuthParams &params) {
522: 	string http_proto, host, bucket, path, query_param;
523: 
524: 	if (url.rfind("s3://", 0) != 0) {
525: 		throw std::runtime_error("URL needs to start with s3://");
526: 	}
527: 	auto slash_pos = url.find('/', 5);
528: 	if (slash_pos == string::npos) {
529: 		throw std::runtime_error("URL needs to contain a '/' after the host");
530: 	}
531: 	bucket = url.substr(5, slash_pos - 5);
532: 	if (bucket.empty()) {
533: 		throw std::runtime_error("URL needs to contain a bucket name");
534: 	}
535: 
536: 	auto question_pos = url.find_last_of('?');
537: 
538: 	// See https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html
539: 	if (params.url_style == "path") {
540: 		path = "/" + bucket;
541: 	} else {
542: 		path = "";
543: 	}
544: 
545: 	if (question_pos == string::npos) {
546: 		path += url.substr(slash_pos);
547: 		query_param = "";
548: 	} else {
549: 		path += url.substr(slash_pos, question_pos - slash_pos);
550: 		query_param = url.substr(question_pos + 1);
551: 	}
552: 	if (path.empty()) {
553: 		throw std::runtime_error("URL needs to contain key");
554: 	}
555: 
556: 	if (params.url_style == "vhost" || params.url_style == "") {
557: 		host = bucket + "." + params.endpoint;
558: 	} else {
559: 		host = params.endpoint;
560: 	}
561: 
562: 	http_proto = params.use_ssl ? "https://" : "http://";
563: 
564: 	return {http_proto, host, bucket, path, query_param};
565: }
566: 
567: string S3FileSystem::GetPayloadHash(char *buffer, idx_t buffer_len) {
568: 	if (buffer_len > 0) {
569: 		hash_bytes payload_hash_bytes;
570: 		hash_str payload_hash_str;
571: 		sha256(buffer, buffer_len, payload_hash_bytes);
572: 		hex256(payload_hash_bytes, payload_hash_str);
573: 		return string((char *)payload_hash_str, sizeof(payload_hash_str));
574: 	} else {
575: 		return "";
576: 	}
577: }
578: 
579: static string get_full_s3_url(S3AuthParams &auth_params, ParsedS3Url parsed_url) {
580: 	string full_url = parsed_url.http_proto + parsed_url.host + parsed_url.path;
581: 
582: 	if (!parsed_url.query_param.empty()) {
583: 		full_url += "?" + parsed_url.query_param;
584: 	}
585: 	return full_url;
586: }
587: 
588: unique_ptr<ResponseWrapper> S3FileSystem::PostRequest(FileHandle &handle, string url, HeaderMap header_map,
589:                                                       unique_ptr<char[]> &buffer_out, idx_t &buffer_out_len,
590:                                                       char *buffer_in, idx_t buffer_in_len) {
591: 	auto auth_params = static_cast<S3FileHandle &>(handle).auth_params;
592: 	auto parsed_url = S3UrlParse(url, auth_params);
593: 
594: 	string full_url = get_full_s3_url(auth_params, parsed_url);
595: 	string post_url;
596: 	auto payload_hash = GetPayloadHash(buffer_in, buffer_in_len);
597: 
598: 	auto headers = create_s3_header(parsed_url.path, parsed_url.query_param, parsed_url.host, "s3", "POST", auth_params,
599: 	                                "", "", payload_hash, "application/octet-stream");
600: 
601: 	return HTTPFileSystem::PostRequest(handle, full_url, headers, buffer_out, buffer_out_len, buffer_in, buffer_in_len);
602: }
603: 
604: unique_ptr<ResponseWrapper> S3FileSystem::PutRequest(FileHandle &handle, string url, HeaderMap header_map,
605:                                                      char *buffer_in, idx_t buffer_in_len) {
606: 	auto auth_params = static_cast<S3FileHandle &>(handle).auth_params;
607: 	auto parsed_url = S3UrlParse(url, auth_params);
608: 	string full_url = get_full_s3_url(auth_params, parsed_url);
609: 	auto content_type = "application/octet-stream";
610: 	auto payload_hash = GetPayloadHash(buffer_in, buffer_in_len);
611: 
612: 	auto headers = create_s3_header(parsed_url.path, parsed_url.query_param, parsed_url.host, "s3", "PUT", auth_params,
613: 	                                "", "", payload_hash, content_type);
614: 	return HTTPFileSystem::PutRequest(handle, full_url, headers, buffer_in, buffer_in_len);
615: }
616: 
617: unique_ptr<ResponseWrapper> S3FileSystem::HeadRequest(FileHandle &handle, string url, HeaderMap header_map) {
618: 	auto auth_params = static_cast<S3FileHandle &>(handle).auth_params;
619: 	url = url.substr(0, url.find_last_of('?'));
620: 	auto parsed_url = S3UrlParse(url, auth_params);
621: 	string full_url = get_full_s3_url(auth_params, parsed_url);
622: 	auto headers = create_s3_header(parsed_url.path, parsed_url.query_param, parsed_url.host, "s3", "HEAD", auth_params,
623: 	                                "", "", "", "");
624: 	return HTTPFileSystem::HeadRequest(handle, full_url, headers);
625: }
626: 
627: unique_ptr<ResponseWrapper> S3FileSystem::GetRangeRequest(FileHandle &handle, string url, HeaderMap header_map,
628:                                                           idx_t file_offset, char *buffer_out, idx_t buffer_out_len) {
629: 	auto auth_params = static_cast<S3FileHandle &>(handle).auth_params;
630: 	url = url.substr(0, url.find_last_of('?'));
631: 	auto parsed_url = S3UrlParse(url, auth_params);
632: 	string full_url = get_full_s3_url(auth_params, parsed_url);
633: 	auto headers = create_s3_header(parsed_url.path, parsed_url.query_param, parsed_url.host, "s3", "GET", auth_params,
634: 	                                "", "", "", "");
635: 	return HTTPFileSystem::GetRangeRequest(handle, full_url, headers, file_offset, buffer_out, buffer_out_len);
636: }
637: 
638: unique_ptr<HTTPFileHandle> S3FileSystem::CreateHandle(const string &path, const string &query_param, uint8_t flags,
639:                                                       FileLockType lock, FileCompressionType compression,
640:                                                       FileOpener *opener) {
641: 	if (!opener) {
642: 		throw std::runtime_error("CreateHandle called on S3FileSystem without FileOpener");
643: 	}
644: 	auto s3authparams = S3AuthParams::ReadFrom(opener);
645: 	ReadQueryParams(query_param, s3authparams);
646: 	string full_path = query_param.empty() ? path : path + "?" + query_param;
647: 
648: 	return duckdb::make_unique<S3FileHandle>(*this, full_path, path, flags,
649: 	                                         opener ? HTTPParams::ReadFrom(opener) : HTTPParams(), s3authparams,
650: 	                                         S3ConfigParams::ReadFrom(opener));
651: }
652: 
653: // this computes the signature from https://czak.pl/2015/09/15/s3-rest-api-with-curl.html
654: void S3FileSystem::Verify() {
655: 
656: 	S3AuthParams auth_params = {
657: 	    "us-east-1", "AKIAIOSFODNN7EXAMPLE", "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY", "", "", "", true};
658: 	auto test_header = create_s3_header("/", "", "my-precious-bucket.s3.amazonaws.com", "s3", "GET", auth_params,
659: 	                                    "20150915", "20150915T124500Z");
660: 	if (test_header["Authorization"] !=
661: 	    "AWS4-HMAC-SHA256 Credential=AKIAIOSFODNN7EXAMPLE/20150915/us-east-1/s3/aws4_request, "
662: 	    "SignedHeaders=host;x-amz-content-sha256;x-amz-date, "
663: 	    "Signature=182072eb53d85c36b2d791a1fa46a12d23454ec1e921b02075c23aee40166d5a") {
664: 		throw std::runtime_error("test fail");
665: 	}
666: 
667: 	if (UrlEncode("/category=Books/") != "/category%3DBooks/") {
668: 		throw std::runtime_error("test fail");
669: 	}
670: 	if (UrlEncode("/?category=Books&title=Ducks Retreat/") != "/%3Fcategory%3DBooks%26title%3DDucks%20Retreat/") {
671: 		throw std::runtime_error("test fail");
672: 	}
673: 	if (UrlEncode("/?category=Books&title=Ducks Retreat/", true) !=
674: 	    "%2F%3Fcategory%3DBooks%26title%3DDucks%20Retreat%2F") {
675: 		throw std::runtime_error("test fail");
676: 	}
677: 	// AWS_SECRET_ACCESS_KEY="vs1BZPxSL2qVARBSg5vCMKJsavCoEPlo/HSHRaVe" AWS_ACCESS_KEY_ID="ASIAYSPIOYDTHTBIITVC"
678: 	// AWS_SESSION_TOKEN="IQoJb3JpZ2luX2VjENX//////////wEaCWV1LXdlc3QtMSJHMEUCIQDfjzs9BYHrEXDMU/NR+PHV1uSTr7CSVSQdjKSfiPRLdgIgCCztF0VMbi9+uHHAfBVKhV4t9MlUrQg3VAOIsLxrWyoqlAIIHRAAGgw1ODk0MzQ4OTY2MTQiDOGl2DsYxENcKCbh+irxARe91faI+hwUhT60sMGRFg0GWefKnPclH4uRFzczrDOcJlAAaQRJ7KOsT8BrJlrY1jSgjkO7PkVjPp92vi6lJX77bg99MkUTJActiOKmd84XvAE5bFc/jFbqechtBjXzopAPkKsGuaqAhCenXnFt6cwq+LZikv/NJGVw7TRphLV+Aq9PSL9XwdzIgsW2qXwe1c3rxDNj53yStRZHVggdxJ0OgHx5v040c98gFphzSULHyg0OY6wmCMTYcswpb4kO2IIi6AiD9cY25TlwPKRKPi5CdBsTPnyTeW62u7PvwK0fTSy4ZuJUuGKQnH2cKmCXquEwoOHEiQY6nQH9fzY/EDGHMRxWWhxu0HiqIfsuFqC7GS0p0ToKQE+pzNsvVwMjZc+KILIDDQpdCWRIwu53I5PZy2Cvk+3y4XLvdZKQCsAKqeOc4c94UAS4NmUT7mCDOuRV0cLBVM8F0JYBGrUxyI+YoIvHhQWmnRLuKgTb5PkF7ZWrXBHFWG5/tZDOvBbbaCWTlRCL9b0Vpg5+BM/81xd8jChP4w83"
679: 	// aws --region eu-west-1 --debug s3 ls my-precious-bucket 2>&1 | less
680: 	string canonical_query_string = "delimiter=%2F&encoding-type=url&list-type=2&prefix="; // aws s3 ls <bucket>
681: 
682: 	S3AuthParams auth_params2 = {
683: 	    "eu-west-1",
684: 	    "ASIAYSPIOYDTHTBIITVC",
685: 	    "vs1BZPxSL2qVARBSg5vCMKJsavCoEPlo/HSHRaVe",
686: 	    "IQoJb3JpZ2luX2VjENX//////////wEaCWV1LXdlc3QtMSJHMEUCIQDfjzs9BYHrEXDMU/"
687: 	    "NR+PHV1uSTr7CSVSQdjKSfiPRLdgIgCCztF0VMbi9+"
688: 	    "uHHAfBVKhV4t9MlUrQg3VAOIsLxrWyoqlAIIHRAAGgw1ODk0MzQ4OTY2MTQiDOGl2DsYxENcKCbh+irxARe91faI+"
689: 	    "hwUhT60sMGRFg0GWefKnPclH4uRFzczrDOcJlAAaQRJ7KOsT8BrJlrY1jSgjkO7PkVjPp92vi6lJX77bg99MkUTJA"
690: 	    "ctiOKmd84XvAE5bFc/jFbqechtBjXzopAPkKsGuaqAhCenXnFt6cwq+LZikv/"
691: 	    "NJGVw7TRphLV+"
692: 	    "Aq9PSL9XwdzIgsW2qXwe1c3rxDNj53yStRZHVggdxJ0OgHx5v040c98gFphzSULHyg0OY6wmCMTYcswpb4kO2IIi6"
693: 	    "AiD9cY25TlwPKRKPi5CdBsTPnyTeW62u7PvwK0fTSy4ZuJUuGKQnH2cKmCXquEwoOHEiQY6nQH9fzY/"
694: 	    "EDGHMRxWWhxu0HiqIfsuFqC7GS0p0ToKQE+pzNsvVwMjZc+KILIDDQpdCWRIwu53I5PZy2Cvk+"
695: 	    "3y4XLvdZKQCsAKqeOc4c94UAS4NmUT7mCDOuRV0cLBVM8F0JYBGrUxyI+"
696: 	    "YoIvHhQWmnRLuKgTb5PkF7ZWrXBHFWG5/tZDOvBbbaCWTlRCL9b0Vpg5+BM/81xd8jChP4w83",
697: 	    "",
698: 	    "",
699: 	    true};
700: 	auto test_header2 = create_s3_header("/", canonical_query_string, "my-precious-bucket.s3.eu-west-1.amazonaws.com",
701: 	                                     "s3", "GET", auth_params2, "20210904", "20210904T121746Z");
702: 	if (test_header2["Authorization"] !=
703: 	    "AWS4-HMAC-SHA256 Credential=ASIAYSPIOYDTHTBIITVC/20210904/eu-west-1/s3/aws4_request, "
704: 	    "SignedHeaders=host;x-amz-content-sha256;x-amz-date;x-amz-security-token, "
705: 	    "Signature=4d9d6b59d7836b6485f6ad822de97be40287da30347d83042ea7fbed530dc4c0") {
706: 		throw std::runtime_error("test fail");
707: 	}
708: 
709: 	S3AuthParams auth_params3 = {"eu-west-1", "S3RVER", "S3RVER", "", "", "", true};
710: 	auto test_header3 =
711: 	    create_s3_header("/correct_auth_test.csv", "", "test-bucket-ceiveran.s3.amazonaws.com", "s3", "PUT",
712: 	                     auth_params3, "20220121", "20220121T141452Z",
713: 	                     "28a0cf6ac5c4cb73793091fe6ecc6a68bf90855ac9186158748158f50241bb0c", "text/data;charset=utf-8");
714: 	if (test_header3["Authorization"] != "AWS4-HMAC-SHA256 Credential=S3RVER/20220121/eu-west-1/s3/aws4_request, "
715: 	                                     "SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date, "
716: 	                                     "Signature=5d9a6cbfaa78a6d0f2ab7df0445e2f1cc9c80cd3655ac7de9e7219c036f23f02") {
717: 		throw std::runtime_error("test3 fail");
718: 	}
719: 
720: 	// bug #4082
721: 	S3AuthParams auth_params4 = {
722: 	    "auto", "asdf", "asdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdfasdf", "", "", "", true};
723: 	create_s3_header("/", "", "exampple.com", "s3", "GET", auth_params4);
724: 
725: 	if (UrlEncode("/category=Books/") != "/category%3DBooks/") {
726: 		throw std::runtime_error("test fail");
727: 	}
728: 	if (UrlEncode("/?category=Books&title=Ducks Retreat/") != "/%3Fcategory%3DBooks%26title%3DDucks%20Retreat/") {
729: 		throw std::runtime_error("test fail");
730: 	}
731: 	if (UrlEncode("/?category=Books&title=Ducks Retreat/", true) !=
732: 	    "%2F%3Fcategory%3DBooks%26title%3DDucks%20Retreat%2F") {
733: 		throw std::runtime_error("test fail");
734: 	}
735: 
736: 	// TODO add a test that checks the signing for path-style
737: }
738: 
739: unique_ptr<ResponseWrapper> S3FileHandle::Initialize() {
740: 	auto res = HTTPFileHandle::Initialize();
741: 
742: 	auto &s3fs = (S3FileSystem &)file_system;
743: 
744: 	if (flags & FileFlags::FILE_FLAGS_WRITE) {
745: 		auto aws_minimum_part_size = 5242880; // 5 MiB https://docs.aws.amazon.com/AmazonS3/latest/userguide/qfacts.html
746: 		auto max_part_count = config_params.max_parts_per_file;
747: 		auto required_part_size = config_params.max_file_size / max_part_count;
748: 		auto minimum_part_size = MaxValue<idx_t>(aws_minimum_part_size, required_part_size);
749: 
750: 		// Round part size up to multiple of BLOCK_SIZE
751: 		part_size = ((minimum_part_size + Storage::BLOCK_SIZE - 1) / Storage::BLOCK_SIZE) * Storage::BLOCK_SIZE;
752: 		D_ASSERT(part_size * max_part_count >= config_params.max_file_size);
753: 
754: 		multipart_upload_id = s3fs.InitializeMultipartUpload(*this);
755: 
756: 		// Threads are limited by limiting the amount of write buffers available, since each
757: 		s3fs.buffers_available = config_params.max_upload_threads;
758: 		uploads_in_progress = 0;
759: 		parts_uploaded = 0;
760: 		upload_finalized = false;
761: 	}
762: 
763: 	return res;
764: }
765: 
766: bool S3FileSystem::CanHandleFile(const string &fpath) {
767: 	return fpath.rfind("s3://", 0) == 0;
768: }
769: 
770: void S3FileSystem::FileSync(FileHandle &handle) {
771: 	auto &s3fh = (S3FileHandle &)handle;
772: 	if (!s3fh.upload_finalized) {
773: 		FlushAllBuffers(s3fh);
774: 		FinalizeMultipartUpload(s3fh);
775: 	}
776: }
777: 
778: void S3FileSystem::Write(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) {
779: 	auto &s3fh = (S3FileHandle &)handle;
780: 	if (!(s3fh.flags & FileFlags::FILE_FLAGS_WRITE)) {
781: 		throw InternalException("Write called on file not opened in write mode");
782: 	}
783: 	int64_t bytes_written = 0;
784: 
785: 	while (bytes_written < nr_bytes) {
786: 		auto curr_location = location + bytes_written;
787: 
788: 		if (curr_location != s3fh.file_offset) {
789: 			throw InternalException("Non-sequential write not supported!");
790: 		}
791: 
792: 		// Find buffer for writing
793: 		auto write_buffer_idx = curr_location / s3fh.part_size;
794: 
795: 		// Get write buffer, may block until buffer is available
796: 		auto write_buffer = GetBuffer(s3fh, write_buffer_idx);
797: 
798: 		// Writing to buffer
799: 		auto idx_to_write = curr_location - write_buffer->buffer_start;
800: 		auto bytes_to_write = MinValue<idx_t>(nr_bytes - bytes_written, s3fh.part_size - idx_to_write);
801: 		memcpy((char *)write_buffer->Ptr() + idx_to_write, (char *)buffer + bytes_written, bytes_to_write);
802: 		write_buffer->idx += bytes_to_write;
803: 
804: 		// Flush to HTTP if full
805: 		if (write_buffer->idx >= s3fh.part_size) {
806: 			FlushBuffer(s3fh, write_buffer);
807: 		}
808: 		s3fh.file_offset += bytes_to_write;
809: 		bytes_written += bytes_to_write;
810: 	}
811: }
812: 
813: vector<string> S3FileSystem::Glob(const string &glob_pattern, FileOpener *opener) {
814: 	if (opener == nullptr) {
815: 		throw InternalException("Cannot S3 Glob without FileOpener");
816: 	}
817: 	// AWS matches on prefix, not glob pattern so we take a substring until the first wildcard char for the aws calls
818: 	auto first_wildcard_pos = glob_pattern.find_first_of("*?[\\");
819: 	if (first_wildcard_pos == string::npos) {
820: 		return {glob_pattern};
821: 	}
822: 
823: 	auto dot_pos = glob_pattern.find('.');
824: 	if (glob_pattern[first_wildcard_pos] == '?' &&
825: 	    first_wildcard_pos > dot_pos) { // a '?' after the '.' so assuming query parameters
826: 		return {glob_pattern};
827: 	}
828: 
829: 	string shared_path = glob_pattern.substr(0, first_wildcard_pos);
830: 
831: 	auto s3_auth_params = S3AuthParams::ReadFrom(opener);
832: 	auto http_params = HTTPParams::ReadFrom(opener);
833: 
834: 	// Parse pattern
835: 	auto parsed_url = S3UrlParse(glob_pattern, s3_auth_params);
836: 
837: 	// Do main listobjectsv2 request
838: 	vector<string> s3_keys;
839: 	string main_continuation_token = "";
840: 
841: 	// Main paging loop
842: 	do {
843: 		// main listobject call, may
844: 		string response_str =
845: 		    AWSListObjectV2::Request(shared_path, http_params, s3_auth_params, main_continuation_token);
846: 		main_continuation_token = AWSListObjectV2::ParseContinuationToken(response_str);
847: 		AWSListObjectV2::ParseKey(response_str, s3_keys);
848: 
849: 		// Repeat requests until the keys of all common prefixes are parsed.
850: 		auto common_prefixes = AWSListObjectV2::ParseCommonPrefix(response_str);
851: 		while (!common_prefixes.empty()) {
852: 			auto prefix_path = "s3://" + parsed_url.bucket + '/' + common_prefixes.back();
853: 			common_prefixes.pop_back();
854: 
855: 			// TODO we could optimize here by doing a match on the prefix, if it doesn't match we can skip this prefix
856: 			// Paging loop for common prefix requests
857: 			string common_prefix_continuation_token = "";
858: 			do {
859: 				auto prefix_res = AWSListObjectV2::Request(prefix_path, http_params, s3_auth_params,
860: 				                                           common_prefix_continuation_token);
861: 				AWSListObjectV2::ParseKey(prefix_res, s3_keys);
862: 				auto more_prefixes = AWSListObjectV2::ParseCommonPrefix(prefix_res);
863: 				common_prefixes.insert(common_prefixes.end(), more_prefixes.begin(), more_prefixes.end());
864: 				common_prefix_continuation_token = AWSListObjectV2::ParseContinuationToken(prefix_res);
865: 			} while (!common_prefix_continuation_token.empty());
866: 		}
867: 	} while (!main_continuation_token.empty());
868: 
869: 	auto pattern_trimmed = parsed_url.path.substr(1);
870: 
871: 	// Trim the bucket prefix for path-style urls
872: 	if (s3_auth_params.url_style == "path") {
873: 		pattern_trimmed = pattern_trimmed.substr(parsed_url.bucket.length() + 1);
874: 	}
875: 
876: 	// if a ? char was present, we re-add it here as the url parsing will have trimmed it.
877: 	if (parsed_url.query_param != "") {
878: 		pattern_trimmed += '?' + parsed_url.query_param;
879: 	}
880: 
881: 	vector<string> result;
882: 	for (const auto &s3_key : s3_keys) {
883: 
884: 		auto is_match = LikeFun::Glob(s3_key.data(), s3_key.length(), pattern_trimmed.data(), pattern_trimmed.length());
885: 
886: 		if (is_match) {
887: 			auto result_full_url = "s3://" + parsed_url.bucket + "/" + s3_key;
888: 			result.push_back(result_full_url);
889: 		}
890: 	}
891: 	return result;
892: }
893: 
894: string AWSListObjectV2::Request(string &path, HTTPParams &http_params, S3AuthParams &s3_auth_params,
895:                                 string &continuation_token, bool use_delimiter) {
896: 	auto parsed_url = S3FileSystem::S3UrlParse(path, s3_auth_params);
897: 
898: 	// Construct the ListObjectsV2 call
899: 	string req_path;
900: 	if (s3_auth_params.url_style == "path") {
901: 		req_path = "/" + parsed_url.bucket + "/";
902: 	} else {
903: 		req_path = "/";
904: 	}
905: 
906: 	string prefix = parsed_url.path.substr(1);
907: 
908: 	// Trim the bucket prefix for path-style urls
909: 	if (s3_auth_params.url_style == "path") {
910: 		prefix = prefix.substr(parsed_url.bucket.length() + 1);
911: 	}
912: 
913: 	string req_params = "";
914: 	if (!continuation_token.empty()) {
915: 		req_params += "continuation-token=" + S3FileSystem::UrlEncode(continuation_token);
916: 		req_params += "&";
917: 	}
918: 	req_params += "encoding-type=url&list-type=2";
919: 	req_params += "&prefix=" + S3FileSystem::UrlEncode(prefix, true);
920: 
921: 	if (use_delimiter) {
922: 		req_params += "&delimiter=%2F";
923: 	}
924: 
925: 	string listobjectv2_url = parsed_url.http_proto + parsed_url.host + req_path + "?" + req_params;
926: 
927: 	auto header_map =
928: 	    create_s3_header(req_path, req_params, parsed_url.host, "s3", "GET", s3_auth_params, "", "", "", "");
929: 	auto headers = initialize_http_headers(header_map);
930: 
931: 	auto client = S3FileSystem::GetClient(
932: 	    http_params, (parsed_url.http_proto + parsed_url.host).c_str()); // Get requests use fresh connection
933: 	std::stringstream response;
934: 	auto res = client->Get(
935: 	    listobjectv2_url.c_str(), *headers,
936: 	    [&](const duckdb_httplib_openssl::Response &response) {
937: 		    if (response.status >= 400) {
938: 			    std::cerr << response.reason << std::endl;
939: 			    throw std::runtime_error("HTTP GET error on '" + listobjectv2_url + "' (HTTP " +
940: 			                             std::to_string(response.status) + ")");
941: 		    }
942: 		    return true;
943: 	    },
944: 	    [&](const char *data, size_t data_length) {
945: 		    response << string(data, data_length);
946: 		    return true;
947: 	    });
948: 	if (res.error() != duckdb_httplib_openssl::Error::Success) {
949: 		throw std::runtime_error("HTTP GET error on '" + listobjectv2_url + "' (Error code " +
950: 		                         to_string((int)res.error()) + ")");
951: 	}
952: 
953: 	return response.str();
954: }
955: 
956: void AWSListObjectV2::ParseKey(string &aws_response, vector<string> &result) {
957: 	idx_t cur_pos = 0;
958: 	while (true) {
959: 		auto next_open_tag_pos = aws_response.find("<Key>", cur_pos);
960: 		if (next_open_tag_pos == string::npos) {
961: 			break;
962: 		} else {
963: 			auto next_close_tag_pos = aws_response.find("</Key>", next_open_tag_pos + 5);
964: 			if (next_close_tag_pos == string::npos) {
965: 				throw InternalException("Failed to parse S3 result");
966: 			}
967: 			auto parsed_path = S3FileSystem::UrlDecode(
968: 			    aws_response.substr(next_open_tag_pos + 5, next_close_tag_pos - next_open_tag_pos - 5));
969: 			if (parsed_path.back() != '/') {
970: 				result.push_back(parsed_path);
971: 			}
972: 			cur_pos = next_close_tag_pos + 6;
973: 		}
974: 	}
975: }
976: 
977: string AWSListObjectV2::ParseContinuationToken(string &aws_response) {
978: 
979: 	auto open_tag_pos = aws_response.find("<NextContinuationToken>");
980: 	if (open_tag_pos == string::npos) {
981: 		return "";
982: 	} else {
983: 		auto close_tag_pos = aws_response.find("</NextContinuationToken>", open_tag_pos + 23);
984: 		if (close_tag_pos == string::npos) {
985: 			throw InternalException("Failed to parse S3 result");
986: 		}
987: 		return aws_response.substr(open_tag_pos + 23, close_tag_pos - open_tag_pos - 23);
988: 	}
989: }
990: 
991: vector<string> AWSListObjectV2::ParseCommonPrefix(string &aws_response) {
992: 	vector<string> s3_prefixes;
993: 	idx_t cur_pos = 0;
994: 	while (true) {
995: 		cur_pos = aws_response.find("<CommonPrefixes>", cur_pos);
996: 		if (cur_pos == string::npos) {
997: 			break;
998: 		}
999: 		auto next_open_tag_pos = aws_response.find("<Prefix>", cur_pos);
1000: 		if (next_open_tag_pos == string::npos) {
1001: 			throw InternalException("Parsing error while parsing s3 listobject result");
1002: 		} else {
1003: 			auto next_close_tag_pos = aws_response.find("</Prefix>", next_open_tag_pos + 8);
1004: 			if (next_close_tag_pos == string::npos) {
1005: 				throw InternalException("Failed to parse S3 result");
1006: 			}
1007: 			auto parsed_path = aws_response.substr(next_open_tag_pos + 8, next_close_tag_pos - next_open_tag_pos - 8);
1008: 			s3_prefixes.push_back(parsed_path);
1009: 			cur_pos = next_close_tag_pos + 6;
1010: 		}
1011: 	}
1012: 	return s3_prefixes;
1013: }
1014: 
1015: } // namespace duckdb
[end of extension/httpfs/s3fs.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: