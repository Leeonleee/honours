You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Using "with duckdb.connect" causes silent crashes on invalid SQL (and perhaps other kinds of errors)
### What happens?

Using `with duckdb.connect` causes silent crashes on invalid SQL (and perhaps other kinds of errors).

### To Reproduce

This:

```
import ducckdb

print(duckdb.__version__)

with duckdb.connect() as con:
  print('before')
  con.execute('select 1')
  print('after')

with duckdb.connect() as con:
  print('before')
  con.execute('invalid')
  print('after')
```

produces this output:

```
0.5.1
before
after
before
```

There's a missing `after` for the second `with` block and there are no exceptions shown, it fails silently.

### OS:

linux

### DuckDB Version:

0.5.1

### DuckDB Client:

python

### Full Name:

Cebaa

### Affiliation:

None

### Have you tried this on the latest `master` branch?

- [X] I agree

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] I agree

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
[end of README.md]
[start of tools/pythonpkg/src/pyconnection.cpp]
1: #include "duckdb_python/pyconnection.hpp"
2: 
3: #include "datetime.h" // from Python
4: #include "duckdb/common/arrow/arrow.hpp"
5: #include "duckdb/common/printer.hpp"
6: #include "duckdb/common/types.hpp"
7: #include "duckdb/common/types/vector.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/main/config.hpp"
10: #include "duckdb/main/db_instance_cache.hpp"
11: #include "duckdb/main/extension_helper.hpp"
12: #include "duckdb/parser/expression/constant_expression.hpp"
13: #include "duckdb/parser/expression/function_expression.hpp"
14: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
15: #include "duckdb/parser/parser.hpp"
16: #include "duckdb/parser/tableref/table_function_ref.hpp"
17: #include "duckdb_python/arrow_array_stream.hpp"
18: #include "duckdb_python/map.hpp"
19: #include "duckdb_python/pandas_scan.hpp"
20: #include "duckdb_python/pyrelation.hpp"
21: #include "duckdb_python/pyresult.hpp"
22: #include "duckdb_python/python_conversion.hpp"
23: 
24: #include <random>
25: 
26: namespace duckdb {
27: 
28: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::default_connection = nullptr;
29: DBInstanceCache instance_cache;
30: shared_ptr<PythonImportCache> DuckDBPyConnection::import_cache = nullptr;
31: 
32: void DuckDBPyConnection::Initialize(py::handle &m) {
33: 	py::class_<DuckDBPyConnection, shared_ptr<DuckDBPyConnection>>(m, "DuckDBPyConnection", py::module_local())
34: 	    .def("cursor", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection")
35: 	    .def("duplicate", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection")
36: 	    .def("execute", &DuckDBPyConnection::Execute,
37: 	         "Execute the given SQL query, optionally using prepared statements with parameters set", py::arg("query"),
38: 	         py::arg("parameters") = py::none(), py::arg("multiple_parameter_sets") = false)
39: 	    .def("executemany", &DuckDBPyConnection::ExecuteMany,
40: 	         "Execute the given prepared statement multiple times using the list of parameter sets in parameters",
41: 	         py::arg("query"), py::arg("parameters") = py::none())
42: 	    .def("close", &DuckDBPyConnection::Close, "Close the connection")
43: 	    .def("fetchone", &DuckDBPyConnection::FetchOne, "Fetch a single row from a result following execute")
44: 	    .def("fetchmany", &DuckDBPyConnection::FetchMany, "Fetch the next set of rows from a result following execute",
45: 	         py::arg("size") = 1)
46: 	    .def("fetchall", &DuckDBPyConnection::FetchAll, "Fetch all rows from a result following execute")
47: 	    .def("fetchnumpy", &DuckDBPyConnection::FetchNumpy, "Fetch a result as list of NumPy arrays following execute")
48: 	    .def("fetchdf", &DuckDBPyConnection::FetchDF, "Fetch a result as Data.Frame following execute()", py::kw_only(),
49: 	         py::arg("date_as_object") = false)
50: 	    .def("fetch_df", &DuckDBPyConnection::FetchDF, "Fetch a result as Data.Frame following execute()",
51: 	         py::kw_only(), py::arg("date_as_object") = false)
52: 	    .def("fetch_df_chunk", &DuckDBPyConnection::FetchDFChunk,
53: 	         "Fetch a chunk of the result as Data.Frame following execute()", py::arg("vectors_per_chunk") = 1,
54: 	         py::kw_only(), py::arg("date_as_object") = false)
55: 	    .def("df", &DuckDBPyConnection::FetchDF, "Fetch a result as Data.Frame following execute()", py::kw_only(),
56: 	         py::arg("date_as_object") = false)
57: 	    .def("fetch_arrow_table", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()",
58: 	         py::arg("chunk_size") = 1000000)
59: 	    .def("fetch_record_batch", &DuckDBPyConnection::FetchRecordBatchReader,
60: 	         "Fetch an Arrow RecordBatchReader following execute()", py::arg("chunk_size") = 1000000)
61: 	    .def("arrow", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()",
62: 	         py::arg("chunk_size") = 1000000)
63: 	    .def("begin", &DuckDBPyConnection::Begin, "Start a new transaction")
64: 	    .def("commit", &DuckDBPyConnection::Commit, "Commit changes performed within a transaction")
65: 	    .def("rollback", &DuckDBPyConnection::Rollback, "Roll back changes performed within a transaction")
66: 	    .def("append", &DuckDBPyConnection::Append, "Append the passed Data.Frame to the named table",
67: 	         py::arg("table_name"), py::arg("df"))
68: 	    .def("register", &DuckDBPyConnection::RegisterPythonObject,
69: 	         "Register the passed Python Object value for querying with a view", py::arg("view_name"),
70: 	         py::arg("python_object"))
71: 	    .def("unregister", &DuckDBPyConnection::UnregisterPythonObject, "Unregister the view name",
72: 	         py::arg("view_name"))
73: 	    .def("table", &DuckDBPyConnection::Table, "Create a relation object for the name'd table",
74: 	         py::arg("table_name"))
75: 	    .def("view", &DuckDBPyConnection::View, "Create a relation object for the name'd view", py::arg("view_name"))
76: 	    .def("values", &DuckDBPyConnection::Values, "Create a relation object from the passed values",
77: 	         py::arg("values"))
78: 	    .def("table_function", &DuckDBPyConnection::TableFunction,
79: 	         "Create a relation object from the name'd table function with given parameters", py::arg("name"),
80: 	         py::arg("parameters") = py::none())
81: 	    .def("from_query", &DuckDBPyConnection::FromQuery, "Create a relation object from the given SQL query",
82: 	         py::arg("query"), py::arg("alias") = "query_relation")
83: 	    .def("query", &DuckDBPyConnection::RunQuery,
84: 	         "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, "
85: 	         "otherwise run the query as-is.",
86: 	         py::arg("query"), py::arg("alias") = "query_relation")
87: 	    .def("from_df", &DuckDBPyConnection::FromDF, "Create a relation object from the Data.Frame in df",
88: 	         py::arg("df") = py::none())
89: 	    .def("from_arrow", &DuckDBPyConnection::FromArrow, "Create a relation object from an Arrow object",
90: 	         py::arg("arrow_object"))
91: 	    .def("df", &DuckDBPyConnection::FromDF,
92: 	         "Create a relation object from the Data.Frame in df. This is an alias of from_df", py::arg("df"))
93: 	    .def("from_csv_auto", &DuckDBPyConnection::FromCsvAuto,
94: 	         "Create a relation object from the CSV file in file_name", py::arg("file_name"))
95: 	    .def("from_parquet", &DuckDBPyConnection::FromParquet,
96: 	         "Create a relation object from the Parquet file in file_name", py::arg("file_name"),
97: 	         py::arg("binary_as_string") = false)
98: 	    .def("from_substrait", &DuckDBPyConnection::FromSubstrait, "Create a query object from protobuf plan",
99: 	         py::arg("proto"))
100: 	    .def("get_substrait", &DuckDBPyConnection::GetSubstrait, "Serialize a query to protobuf", py::arg("query"))
101: 	    .def("get_substrait_json", &DuckDBPyConnection::GetSubstraitJSON,
102: 	         "Serialize a query to protobuf on the JSON format", py::arg("query"))
103: 	    .def("get_table_names", &DuckDBPyConnection::GetTableNames, "Extract the required table names from a query",
104: 	         py::arg("query"))
105: 	    .def("__enter__", &DuckDBPyConnection::Enter)
106: 	    .def("__exit__", &DuckDBPyConnection::Exit, py::arg("exc_type"), py::arg("exc"), py::arg("traceback"))
107: 	    .def_property_readonly("description", &DuckDBPyConnection::GetDescription,
108: 	                           "Get result set attributes, mainly column names")
109: 	    .def("install_extension", &DuckDBPyConnection::InstallExtension, "Install an extension by name",
110: 	         py::arg("extension"), py::kw_only(), py::arg("force_install") = false)
111: 	    .def("load_extension", &DuckDBPyConnection::LoadExtension, "Load an installed extension", py::arg("extension"));
112: 
113: 	PyDateTime_IMPORT;
114: 	DuckDBPyConnection::ImportCache();
115: }
116: 
117: DuckDBPyConnection *DuckDBPyConnection::ExecuteMany(const string &query, py::object params) {
118: 	if (params.is_none()) {
119: 		params = py::list();
120: 	}
121: 	Execute(query, std::move(params), true);
122: 	return this;
123: }
124: 
125: static unique_ptr<QueryResult> CompletePendingQuery(PendingQueryResult &pending_query) {
126: 	PendingExecutionResult execution_result;
127: 	do {
128: 		execution_result = pending_query.ExecuteTask();
129: 	} while (execution_result == PendingExecutionResult::RESULT_NOT_READY);
130: 	if (execution_result == PendingExecutionResult::EXECUTION_ERROR) {
131: 		pending_query.ThrowError();
132: 	}
133: 	return pending_query.Execute();
134: }
135: 
136: DuckDBPyConnection *DuckDBPyConnection::Execute(const string &query, py::object params, bool many) {
137: 	if (!connection) {
138: 		throw ConnectionException("Connection has already been closed");
139: 	}
140: 	if (params.is_none()) {
141: 		params = py::list();
142: 	}
143: 	result = nullptr;
144: 	unique_ptr<PreparedStatement> prep;
145: 	{
146: 		py::gil_scoped_release release;
147: 		unique_lock<std::mutex> lock(py_connection_lock);
148: 
149: 		auto statements = connection->ExtractStatements(query);
150: 		if (statements.empty()) {
151: 			// no statements to execute
152: 			return this;
153: 		}
154: 		// if there are multiple statements, we directly execute the statements besides the last one
155: 		// we only return the result of the last statement to the user, unless one of the previous statements fails
156: 		for (idx_t i = 0; i + 1 < statements.size(); i++) {
157: 			auto pending_query = connection->PendingQuery(move(statements[i]));
158: 			auto res = CompletePendingQuery(*pending_query);
159: 
160: 			if (res->HasError()) {
161: 				res->ThrowError();
162: 			}
163: 		}
164: 
165: 		prep = connection->Prepare(move(statements.back()));
166: 		if (prep->HasError()) {
167: 			prep->error.Throw();
168: 		}
169: 	}
170: 
171: 	// this is a list of a list of parameters in executemany
172: 	py::list params_set;
173: 	if (!many) {
174: 		params_set = py::list(1);
175: 		params_set[0] = params;
176: 	} else {
177: 		params_set = params;
178: 	}
179: 
180: 	for (pybind11::handle single_query_params : params_set) {
181: 		if (prep->n_param != py::len(single_query_params)) {
182: 			throw InvalidInputException("Prepared statement needs %d parameters, %d given", prep->n_param,
183: 			                            py::len(single_query_params));
184: 		}
185: 		auto args = DuckDBPyConnection::TransformPythonParamList(single_query_params);
186: 		auto res = make_unique<DuckDBPyResult>();
187: 		{
188: 			py::gil_scoped_release release;
189: 			unique_lock<std::mutex> lock(py_connection_lock);
190: 			auto pending_query = prep->PendingQuery(args);
191: 			res->result = CompletePendingQuery(*pending_query);
192: 
193: 			if (res->result->HasError()) {
194: 				res->result->ThrowError();
195: 			}
196: 		}
197: 
198: 		if (!many) {
199: 			result = move(res);
200: 		}
201: 	}
202: 	return this;
203: }
204: 
205: DuckDBPyConnection *DuckDBPyConnection::Append(const string &name, DataFrame value) {
206: 	RegisterPythonObject("__append_df", std::move(value));
207: 	return Execute("INSERT INTO \"" + name + "\" SELECT * FROM __append_df");
208: }
209: 
210: DuckDBPyConnection *DuckDBPyConnection::RegisterPythonObject(const string &name, py::object python_object) {
211: 	if (!connection) {
212: 		throw ConnectionException("Connection has already been closed");
213: 	}
214: 
215: 	if (DuckDBPyConnection::IsPandasDataframe(python_object)) {
216: 		auto new_df = PandasScanFunction::PandasReplaceCopiedNames(python_object);
217: 		{
218: 			py::gil_scoped_release release;
219: 			temporary_views[name] = connection->TableFunction("pandas_scan", {Value::POINTER((uintptr_t)new_df.ptr())})
220: 			                            ->CreateView(name, true, true);
221: 		}
222: 
223: 		// keep a reference
224: 		vector<shared_ptr<ExternalDependency>> dependencies;
225: 		dependencies.push_back(make_shared<PythonDependencies>(make_unique<RegisteredObject>(python_object),
226: 		                                                       make_unique<RegisteredObject>(new_df)));
227: 		connection->context->external_dependencies[name] = move(dependencies);
228: 	} else if (IsAcceptedArrowObject(python_object)) {
229: 		auto stream_factory =
230: 		    make_unique<PythonTableArrowArrayStreamFactory>(python_object.ptr(), connection->context->config);
231: 		auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
232: 		auto stream_factory_get_schema = PythonTableArrowArrayStreamFactory::GetSchema;
233: 		{
234: 			py::gil_scoped_release release;
235: 			temporary_views[name] =
236: 			    connection
237: 			        ->TableFunction("arrow_scan", {Value::POINTER((uintptr_t)stream_factory.get()),
238: 			                                       Value::POINTER((uintptr_t)stream_factory_produce),
239: 			                                       Value::POINTER((uintptr_t)stream_factory_get_schema)})
240: 			        ->CreateView(name, true, true);
241: 		}
242: 		vector<shared_ptr<ExternalDependency>> dependencies;
243: 		dependencies.push_back(
244: 		    make_shared<PythonDependencies>(make_unique<RegisteredArrow>(move(stream_factory), python_object)));
245: 		connection->context->external_dependencies[name] = move(dependencies);
246: 	} else {
247: 		auto py_object_type = string(py::str(python_object.get_type().attr("__name__")));
248: 		throw InvalidInputException("Python Object %s not suitable to be registered as a view", py_object_type);
249: 	}
250: 	return this;
251: }
252: 
253: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromQuery(const string &query, const string &alias) {
254: 	if (!connection) {
255: 		throw ConnectionException("Connection has already been closed");
256: 	}
257: 	const char *duckdb_query_error = R"(duckdb.from_query cannot be used to run arbitrary SQL queries.
258: It can only be used to run individual SELECT statements, and converts the result of that SELECT
259: statement into a Relation object.
260: Use duckdb.query to run arbitrary SQL queries.)";
261: 	return make_unique<DuckDBPyRelation>(connection->RelationFromQuery(query, alias, duckdb_query_error));
262: }
263: 
264: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::RunQuery(const string &query, const string &alias) {
265: 	if (!connection) {
266: 		throw ConnectionException("Connection has already been closed");
267: 	}
268: 	Parser parser(connection->context->GetParserOptions());
269: 	parser.ParseQuery(query);
270: 	if (parser.statements.size() == 1 && parser.statements[0]->type == StatementType::SELECT_STATEMENT) {
271: 		return make_unique<DuckDBPyRelation>(connection->RelationFromQuery(
272: 		    unique_ptr_cast<SQLStatement, SelectStatement>(move(parser.statements[0])), alias));
273: 	}
274: 	Execute(query);
275: 	if (result) {
276: 		FetchAll();
277: 	}
278: 	return nullptr;
279: }
280: 
281: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Table(const string &tname) {
282: 	if (!connection) {
283: 		throw ConnectionException("Connection has already been closed");
284: 	}
285: 	auto qualified_name = QualifiedName::Parse(tname);
286: 	if (qualified_name.schema.empty()) {
287: 		qualified_name.schema = DEFAULT_SCHEMA;
288: 	}
289: 	return make_unique<DuckDBPyRelation>(connection->Table(qualified_name.schema, qualified_name.name));
290: }
291: 
292: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Values(py::object params) {
293: 	if (!connection) {
294: 		throw ConnectionException("Connection has already been closed");
295: 	}
296: 	if (params.is_none()) {
297: 		params = py::list();
298: 	}
299: 	if (!py::hasattr(params, "__len__")) {
300: 		throw InvalidInputException("Type of object passed to parameter 'values' must be iterable");
301: 	}
302: 	vector<vector<Value>> values {DuckDBPyConnection::TransformPythonParamList(std::move(params))};
303: 	return make_unique<DuckDBPyRelation>(connection->Values(values));
304: }
305: 
306: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::View(const string &vname) {
307: 	if (!connection) {
308: 		throw ConnectionException("Connection has already been closed");
309: 	}
310: 	// First check our temporary view
311: 	if (temporary_views.find(vname) != temporary_views.end()) {
312: 		return make_unique<DuckDBPyRelation>(temporary_views[vname]);
313: 	}
314: 	return make_unique<DuckDBPyRelation>(connection->View(vname));
315: }
316: 
317: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::TableFunction(const string &fname, py::object params) {
318: 	if (params.is_none()) {
319: 		params = py::list();
320: 	}
321: 	if (!connection) {
322: 		throw ConnectionException("Connection has already been closed");
323: 	}
324: 
325: 	return make_unique<DuckDBPyRelation>(
326: 	    connection->TableFunction(fname, DuckDBPyConnection::TransformPythonParamList(std::move(params))));
327: }
328: 
329: static std::string GenerateRandomName() {
330: 	std::random_device rd;
331: 	std::mt19937 gen(rd());
332: 	std::uniform_int_distribution<> dis(0, 15);
333: 
334: 	std::stringstream ss;
335: 	int i;
336: 	ss << std::hex;
337: 	for (i = 0; i < 16; i++) {
338: 		ss << dis(gen);
339: 	}
340: 	return ss.str();
341: }
342: 
343: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromDF(const DataFrame &value) {
344: 	if (!connection) {
345: 		throw ConnectionException("Connection has already been closed");
346: 	}
347: 	string name = "df_" + GenerateRandomName();
348: 	auto new_df = PandasScanFunction::PandasReplaceCopiedNames(value);
349: 	vector<Value> params;
350: 	params.emplace_back(Value::POINTER((uintptr_t)new_df.ptr()));
351: 	auto rel = make_unique<DuckDBPyRelation>(connection->TableFunction("pandas_scan", params)->Alias(name));
352: 	rel->rel->extra_dependencies =
353: 	    make_unique<PythonDependencies>(make_unique<RegisteredObject>(value), make_unique<RegisteredObject>(new_df));
354: 	return rel;
355: }
356: 
357: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromCsvAuto(const string &filename) {
358: 	if (!connection) {
359: 		throw ConnectionException("Connection has already been closed");
360: 	}
361: 	vector<Value> params;
362: 	params.emplace_back(filename);
363: 	return make_unique<DuckDBPyRelation>(connection->TableFunction("read_csv_auto", params)->Alias(filename));
364: }
365: 
366: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquet(const string &filename, bool binary_as_string) {
367: 	if (!connection) {
368: 		throw ConnectionException("Connection has already been closed");
369: 	}
370: 	vector<Value> params;
371: 	params.emplace_back(filename);
372: 	named_parameter_map_t named_parameters({{"binary_as_string", Value::BOOLEAN(binary_as_string)}});
373: 	return make_unique<DuckDBPyRelation>(
374: 	    connection->TableFunction("parquet_scan", params, named_parameters)->Alias(filename));
375: }
376: 
377: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromArrow(py::object &arrow_object) {
378: 	if (!connection) {
379: 		throw ConnectionException("Connection has already been closed");
380: 	}
381: 	py::gil_scoped_acquire acquire;
382: 	string name = "arrow_object_" + GenerateRandomName();
383: 	if (!IsAcceptedArrowObject(arrow_object)) {
384: 		auto py_object_type = string(py::str(arrow_object.get_type().attr("__name__")));
385: 		throw InvalidInputException("Python Object Type %s is not an accepted Arrow Object.", py_object_type);
386: 	}
387: 	auto stream_factory =
388: 	    make_unique<PythonTableArrowArrayStreamFactory>(arrow_object.ptr(), connection->context->config);
389: 
390: 	auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
391: 	auto stream_factory_get_schema = PythonTableArrowArrayStreamFactory::GetSchema;
392: 
393: 	auto rel = make_unique<DuckDBPyRelation>(
394: 	    connection
395: 	        ->TableFunction("arrow_scan", {Value::POINTER((uintptr_t)stream_factory.get()),
396: 	                                       Value::POINTER((uintptr_t)stream_factory_produce),
397: 	                                       Value::POINTER((uintptr_t)stream_factory_get_schema)})
398: 	        ->Alias(name));
399: 	rel->rel->extra_dependencies =
400: 	    make_unique<PythonDependencies>(make_unique<RegisteredArrow>(move(stream_factory), arrow_object));
401: 	return rel;
402: }
403: 
404: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromSubstrait(py::bytes &proto) {
405: 	if (!connection) {
406: 		throw ConnectionException("Connection has already been closed");
407: 	}
408: 	string name = "substrait_" + GenerateRandomName();
409: 	vector<Value> params;
410: 	params.emplace_back(Value::BLOB_RAW(proto));
411: 	return make_unique<DuckDBPyRelation>(connection->TableFunction("from_substrait", params)->Alias(name));
412: }
413: 
414: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::GetSubstrait(const string &query) {
415: 	if (!connection) {
416: 		throw ConnectionException("Connection has already been closed");
417: 	}
418: 	vector<Value> params;
419: 	params.emplace_back(query);
420: 	return make_unique<DuckDBPyRelation>(connection->TableFunction("get_substrait", params)->Alias(query));
421: }
422: 
423: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::GetSubstraitJSON(const string &query) {
424: 	if (!connection) {
425: 		throw ConnectionException("Connection has already been closed");
426: 	}
427: 	vector<Value> params;
428: 	params.emplace_back(query);
429: 	return make_unique<DuckDBPyRelation>(connection->TableFunction("get_substrait_json", params)->Alias(query));
430: }
431: 
432: unordered_set<string> DuckDBPyConnection::GetTableNames(const string &query) {
433: 	if (!connection) {
434: 		throw ConnectionException("Connection has already been closed");
435: 	}
436: 	return connection->GetTableNames(query);
437: }
438: 
439: DuckDBPyConnection *DuckDBPyConnection::UnregisterPythonObject(const string &name) {
440: 	connection->context->external_dependencies.erase(name);
441: 	temporary_views.erase(name);
442: 	py::gil_scoped_release release;
443: 	if (connection) {
444: 		connection->Query("DROP VIEW \"" + name + "\"");
445: 	}
446: 	return this;
447: }
448: 
449: DuckDBPyConnection *DuckDBPyConnection::Begin() {
450: 	Execute("BEGIN TRANSACTION");
451: 	return this;
452: }
453: 
454: DuckDBPyConnection *DuckDBPyConnection::Commit() {
455: 	if (connection->context->transaction.IsAutoCommit()) {
456: 		return this;
457: 	}
458: 	Execute("COMMIT");
459: 	return this;
460: }
461: 
462: DuckDBPyConnection *DuckDBPyConnection::Rollback() {
463: 	Execute("ROLLBACK");
464: 	return this;
465: }
466: 
467: py::object DuckDBPyConnection::GetDescription() {
468: 	if (!result) {
469: 		return py::none();
470: 	}
471: 	return result->Description();
472: }
473: 
474: void DuckDBPyConnection::Close() {
475: 	result = nullptr;
476: 	connection = nullptr;
477: 	database = nullptr;
478: 	for (auto &cur : cursors) {
479: 		cur->Close();
480: 	}
481: 	cursors.clear();
482: }
483: 
484: void DuckDBPyConnection::InstallExtension(const string &extension, bool force_install) {
485: 	ExtensionHelper::InstallExtension(*connection->context, extension, force_install);
486: }
487: 
488: void DuckDBPyConnection::LoadExtension(const string &extension) {
489: 	ExtensionHelper::LoadExternalExtension(*connection->context, extension);
490: }
491: 
492: // cursor() is stupid
493: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Cursor() {
494: 	auto res = make_shared<DuckDBPyConnection>();
495: 	res->database = database;
496: 	res->connection = make_unique<Connection>(*res->database);
497: 	cursors.push_back(res);
498: 	return res;
499: }
500: 
501: // these should be functions on the result but well
502: py::object DuckDBPyConnection::FetchOne() {
503: 	if (!result) {
504: 		throw InvalidInputException("No open result set");
505: 	}
506: 	return result->Fetchone();
507: }
508: 
509: py::list DuckDBPyConnection::FetchMany(idx_t size) {
510: 	if (!result) {
511: 		throw InvalidInputException("No open result set");
512: 	}
513: 	return result->Fetchmany(size);
514: }
515: 
516: py::list DuckDBPyConnection::FetchAll() {
517: 	if (!result) {
518: 		throw InvalidInputException("No open result set");
519: 	}
520: 	return result->Fetchall();
521: }
522: 
523: py::dict DuckDBPyConnection::FetchNumpy() {
524: 	if (!result) {
525: 		throw InvalidInputException("No open result set");
526: 	}
527: 	return result->FetchNumpyInternal();
528: }
529: DataFrame DuckDBPyConnection::FetchDF(bool date_as_object) {
530: 	if (!result) {
531: 		throw InvalidInputException("No open result set");
532: 	}
533: 	return result->FetchDF(date_as_object);
534: }
535: 
536: DataFrame DuckDBPyConnection::FetchDFChunk(const idx_t vectors_per_chunk, bool date_as_object) const {
537: 	if (!result) {
538: 		throw InvalidInputException("No open result set");
539: 	}
540: 	return result->FetchDFChunk(vectors_per_chunk, date_as_object);
541: }
542: 
543: duckdb::pyarrow::Table DuckDBPyConnection::FetchArrow(idx_t chunk_size) {
544: 	if (!result) {
545: 		throw InvalidInputException("No open result set");
546: 	}
547: 	return result->FetchArrowTable(chunk_size);
548: }
549: 
550: duckdb::pyarrow::RecordBatchReader DuckDBPyConnection::FetchRecordBatchReader(const idx_t chunk_size) const {
551: 	if (!result) {
552: 		throw InvalidInputException("No open result set");
553: 	}
554: 	return result->FetchRecordBatchReader(chunk_size);
555: }
556: static unique_ptr<TableFunctionRef> TryReplacement(py::dict &dict, py::str &table_name, ClientConfig &config,
557:                                                    py::object &current_frame) {
558: 	if (!dict.contains(table_name)) {
559: 		// not present in the globals
560: 		return nullptr;
561: 	}
562: 	auto entry = dict[table_name];
563: 	auto table_function = make_unique<TableFunctionRef>();
564: 	vector<unique_ptr<ParsedExpression>> children;
565: 	if (DuckDBPyConnection::IsPandasDataframe(entry)) {
566: 		string name = "df_" + GenerateRandomName();
567: 		auto new_df = PandasScanFunction::PandasReplaceCopiedNames(entry);
568: 		children.push_back(make_unique<ConstantExpression>(Value::POINTER((uintptr_t)new_df.ptr())));
569: 		table_function->function = make_unique<FunctionExpression>("pandas_scan", move(children));
570: 		table_function->external_dependency = make_unique<PythonDependencies>(make_unique<RegisteredObject>(entry),
571: 		                                                                      make_unique<RegisteredObject>(new_df));
572: 	} else if (DuckDBPyConnection::IsAcceptedArrowObject(entry)) {
573: 		string name = "arrow_" + GenerateRandomName();
574: 		auto stream_factory = make_unique<PythonTableArrowArrayStreamFactory>(entry.ptr(), config);
575: 		auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
576: 		auto stream_factory_get_schema = PythonTableArrowArrayStreamFactory::GetSchema;
577: 
578: 		children.push_back(make_unique<ConstantExpression>(Value::POINTER((uintptr_t)stream_factory.get())));
579: 		children.push_back(make_unique<ConstantExpression>(Value::POINTER((uintptr_t)stream_factory_produce)));
580: 		children.push_back(make_unique<ConstantExpression>(Value::POINTER((uintptr_t)stream_factory_get_schema)));
581: 
582: 		table_function->function = make_unique<FunctionExpression>("arrow_scan", move(children));
583: 		table_function->external_dependency =
584: 		    make_unique<PythonDependencies>(make_unique<RegisteredArrow>(move(stream_factory), entry));
585: 	} else {
586: 		std::string location = py::cast<py::str>(current_frame.attr("f_code").attr("co_filename"));
587: 		location += ":";
588: 		location += py::cast<py::str>(current_frame.attr("f_lineno"));
589: 		std::string cpp_table_name = table_name;
590: 		auto py_object_type = string(py::str(entry.get_type().attr("__name__")));
591: 
592: 		throw InvalidInputException(
593: 		    "Python Object \"%s\" of type \"%s\" found on line \"%s\" not suitable for replacement scans.\nMake sure "
594: 		    "that \"%s\" is either a pandas.DataFrame, or pyarrow Table, FileSystemDataset, InMemoryDataset, "
595: 		    "RecordBatchReader, or Scanner",
596: 		    cpp_table_name, py_object_type, location, cpp_table_name);
597: 	}
598: 	return table_function;
599: }
600: 
601: static unique_ptr<TableFunctionRef> ScanReplacement(ClientContext &context, const string &table_name,
602:                                                     ReplacementScanData *data) {
603: 	py::gil_scoped_acquire acquire;
604: 	auto py_table_name = py::str(table_name);
605: 	// Here we do an exhaustive search on the frame lineage
606: 	auto current_frame = py::module::import("inspect").attr("currentframe")();
607: 	while (hasattr(current_frame, "f_locals")) {
608: 		auto local_dict = py::reinterpret_borrow<py::dict>(current_frame.attr("f_locals"));
609: 		// search local dictionary
610: 		if (local_dict) {
611: 			auto result = TryReplacement(local_dict, py_table_name, context.config, current_frame);
612: 			if (result) {
613: 				return result;
614: 			}
615: 		}
616: 		// search global dictionary
617: 		auto global_dict = py::reinterpret_borrow<py::dict>(current_frame.attr("f_globals"));
618: 		if (global_dict) {
619: 			auto result = TryReplacement(global_dict, py_table_name, context.config, current_frame);
620: 			if (result) {
621: 				return result;
622: 			}
623: 		}
624: 		current_frame = current_frame.attr("f_back");
625: 	}
626: 	// Not found :(
627: 	return nullptr;
628: }
629: 
630: unordered_map<string, string> TransformPyConfigDict(const py::dict &py_config_dict) {
631: 	unordered_map<string, string> config_dict;
632: 	for (auto &kv : py_config_dict) {
633: 		auto key = py::str(kv.first);
634: 		auto val = py::str(kv.second);
635: 		config_dict[key] = val;
636: 	}
637: 	return config_dict;
638: }
639: 
640: void CreateNewInstance(DuckDBPyConnection &res, const string &database, DBConfig &config) {
641: 	// We don't cache unnamed memory instances (i.e., :memory:)
642: 	bool cache_instance = database != ":memory:" && !database.empty();
643: 	res.database = instance_cache.CreateInstance(database, config, cache_instance);
644: 	res.connection = make_unique<Connection>(*res.database);
645: 	auto &context = *res.connection->context;
646: 	PandasScanFunction scan_fun;
647: 	CreateTableFunctionInfo scan_info(scan_fun);
648: 	MapFunction map_fun;
649: 	CreateTableFunctionInfo map_info(map_fun);
650: 	auto &catalog = Catalog::GetCatalog(context);
651: 	context.transaction.BeginTransaction();
652: 	catalog.CreateTableFunction(context, &scan_info);
653: 	catalog.CreateTableFunction(context, &map_info);
654: 	context.transaction.Commit();
655: 	auto &db_config = res.database->instance->config;
656: 	db_config.AddExtensionOption("pandas_analyze_sample",
657: 	                             "The maximum number of rows to sample when analyzing a pandas object column.",
658: 	                             LogicalType::UBIGINT);
659: 	db_config.options.set_variables["pandas_analyze_sample"] = Value::UBIGINT(1000);
660: 	if (db_config.options.enable_external_access) {
661: 		db_config.replacement_scans.emplace_back(ScanReplacement);
662: 	}
663: }
664: 
665: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Connect(const string &database, bool read_only,
666:                                                            py::object config_options) {
667: 	if (config_options.is_none()) {
668: 		config_options = py::dict();
669: 	}
670: 	auto res = make_shared<DuckDBPyConnection>();
671: 	if (!py::isinstance<py::dict>(config_options)) {
672: 		throw InvalidInputException("Type of object passed to parameter 'config' has to be <dict>");
673: 	}
674: 	py::dict py_config_dict = config_options;
675: 	auto config_dict = TransformPyConfigDict(py_config_dict);
676: 	DBConfig config(config_dict, read_only);
677: 
678: 	res->database = instance_cache.GetInstance(database, config);
679: 	if (!res->database) {
680: 		//! No cached database, we must create a new instance
681: 		CreateNewInstance(*res, database, config);
682: 		return res;
683: 	}
684: 	res->connection = make_unique<Connection>(*res->database);
685: 	return res;
686: }
687: 
688: vector<Value> DuckDBPyConnection::TransformPythonParamList(py::handle params) {
689: 	vector<Value> args;
690: 	args.reserve(py::len(params));
691: 
692: 	for (auto param : params) {
693: 		args.emplace_back(TransformPythonValue(param, LogicalType::UNKNOWN, false));
694: 	}
695: 	return args;
696: }
697: 
698: DuckDBPyConnection *DuckDBPyConnection::DefaultConnection() {
699: 	if (!default_connection) {
700: 		py::dict config_dict;
701: 		default_connection = DuckDBPyConnection::Connect(":memory:", false, config_dict);
702: 	}
703: 	return default_connection.get();
704: }
705: 
706: PythonImportCache *DuckDBPyConnection::ImportCache() {
707: 	if (!import_cache) {
708: 		import_cache = make_shared<PythonImportCache>();
709: 	}
710: 	return import_cache.get();
711: }
712: 
713: DuckDBPyConnection *DuckDBPyConnection::Enter() {
714: 	return this;
715: }
716: 
717: bool DuckDBPyConnection::Exit(DuckDBPyConnection &self, const py::object &exc_type, const py::object &exc,
718:                               const py::object &traceback) {
719: 	self.Close();
720: 	return true;
721: }
722: 
723: void DuckDBPyConnection::Cleanup() {
724: 	default_connection.reset();
725: 	import_cache.reset();
726: }
727: 
728: bool DuckDBPyConnection::IsPandasDataframe(const py::object &object) {
729: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
730: 	return import_cache.pandas.DataFrame.IsInstance(object);
731: }
732: 
733: bool DuckDBPyConnection::IsAcceptedArrowObject(const py::object &object) {
734: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
735: 	return import_cache.arrow.lib.Table.IsInstance(object) ||
736: 	       import_cache.arrow.lib.RecordBatchReader.IsInstance(object) ||
737: 	       import_cache.arrow.dataset.FileSystemDataset.IsInstance(object) ||
738: 	       import_cache.arrow.dataset.InMemoryDataset.IsInstance(object) ||
739: 	       import_cache.arrow.dataset.Scanner.IsInstance(object);
740: }
741: 
742: unique_lock<std::mutex> DuckDBPyConnection::AcquireConnectionLock() {
743: 	// we first release the gil and then acquire the connection lock
744: 	unique_lock<std::mutex> lock(py_connection_lock, std::defer_lock);
745: 	{
746: 		py::gil_scoped_release release;
747: 		lock.lock();
748: 	}
749: 	return lock;
750: }
751: 
752: } // namespace duckdb
[end of tools/pythonpkg/src/pyconnection.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: