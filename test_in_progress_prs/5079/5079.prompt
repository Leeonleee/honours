You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Convert window payloads to DataChunks
The input argument payloads for computing window functions are `ChunkCollection`s, which we want to eliminate. With the removal of standard vector size limits from `DataChunk` it makes more sense to just build a partition-sized `DataChunk`.  In future, we hope to reduce partition sizes to a minimum, which should make this even more valuable.

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
[end of README.md]
[start of .editorconfig]
1: # Unix-style newlines with a newline ending every file
2: [*.{c,cpp,h,hpp}]
3: end_of_line = lf
4: insert_final_newline = true
5: indent_style = tab
6: tab_width = 4
7: indent_size = tab
8: trim_trailing_whitespace = true
9: charset = utf-8
10: max_line_length = 120
11: x-soft-wrap-text = true
12: x-soft-wrap-mode = CharacterWidth
13: x-soft-wrap-limit = 120
14: x-show-invisibles = false
15: x-show-spaces = false
16: 
17: [*.{test,test_slow,test_coverage,benchmark}]
18: end_of_line = lf
19: insert_final_newline = true
20: indent_style = tab
21: tab_width = 4
22: indent_size = tab
23: trim_trailing_whitespace = true
24: charset = utf-8
25: x-soft-wrap-text = false
26: 
27: [Makefile]
28: end_of_line = lf
29: insert_final_newline = true
30: indent_style = tab
31: tab_width = 4
32: indent_size = tab
33: trim_trailing_whitespace = true
34: charset = utf-8
35: x-soft-wrap-text = false
36: 
37: [*keywords.list]
38: insert_final_newline = false
[end of .editorconfig]
[start of extension/parquet/column_reader.cpp]
1: #include "column_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "utf8proc_wrapper.hpp"
4: #include "parquet_reader.hpp"
5: 
6: #include "boolean_column_reader.hpp"
7: #include "cast_column_reader.hpp"
8: #include "generated_column_reader.hpp"
9: #include "callback_column_reader.hpp"
10: #include "parquet_decimal_utils.hpp"
11: #include "list_column_reader.hpp"
12: #include "string_column_reader.hpp"
13: #include "struct_column_reader.hpp"
14: #include "templated_column_reader.hpp"
15: 
16: #include "snappy.h"
17: #include "miniz_wrapper.hpp"
18: #include "zstd.h"
19: #include <iostream>
20: 
21: #include "duckdb.hpp"
22: #ifndef DUCKDB_AMALGAMATION
23: #include "duckdb/common/types/blob.hpp"
24: #include "duckdb/common/types/chunk_collection.hpp"
25: #endif
26: 
27: namespace duckdb {
28: 
29: using duckdb_parquet::format::CompressionCodec;
30: using duckdb_parquet::format::ConvertedType;
31: using duckdb_parquet::format::Encoding;
32: using duckdb_parquet::format::PageType;
33: using duckdb_parquet::format::Type;
34: 
35: const uint32_t ParquetDecodeUtils::BITPACK_MASKS[] = {
36:     0,       1,       3,        7,        15,       31,        63,        127,       255,        511,       1023,
37:     2047,    4095,    8191,     16383,    32767,    65535,     131071,    262143,    524287,     1048575,   2097151,
38:     4194303, 8388607, 16777215, 33554431, 67108863, 134217727, 268435455, 536870911, 1073741823, 2147483647};
39: 
40: const uint8_t ParquetDecodeUtils::BITPACK_DLEN = 8;
41: 
42: ColumnReader::ColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
43:                            idx_t max_define_p, idx_t max_repeat_p)
44:     : schema(schema_p), file_idx(file_idx_p), max_define(max_define_p), max_repeat(max_repeat_p), reader(reader),
45:       type(move(type_p)), page_rows_available(0) {
46: 
47: 	// dummies for Skip()
48: 	dummy_define.resize(reader.allocator, STANDARD_VECTOR_SIZE);
49: 	dummy_repeat.resize(reader.allocator, STANDARD_VECTOR_SIZE);
50: }
51: 
52: ColumnReader::~ColumnReader() {
53: }
54: 
55: ParquetReader &ColumnReader::Reader() {
56: 	return reader;
57: }
58: 
59: const LogicalType &ColumnReader::Type() const {
60: 	return type;
61: }
62: 
63: const SchemaElement &ColumnReader::Schema() const {
64: 	return schema;
65: }
66: 
67: idx_t ColumnReader::FileIdx() const {
68: 	return file_idx;
69: }
70: 
71: idx_t ColumnReader::MaxDefine() const {
72: 	return max_define;
73: }
74: 
75: idx_t ColumnReader::MaxRepeat() const {
76: 	return max_repeat;
77: }
78: 
79: void ColumnReader::RegisterPrefetch(ThriftFileTransport &transport, bool allow_merge) {
80: 	if (chunk) {
81: 		uint64_t size = chunk->meta_data.total_compressed_size;
82: 		transport.RegisterPrefetch(FileOffset(), size, allow_merge);
83: 	}
84: }
85: 
86: uint64_t ColumnReader::TotalCompressedSize() {
87: 	if (!chunk) {
88: 		return 0;
89: 	}
90: 
91: 	return chunk->meta_data.total_compressed_size;
92: }
93: 
94: // Note: It's not trivial to determine where all Column data is stored. Chunk->file_offset
95: // apparently is not the first page of the data. Therefore we determine the address of the first page by taking the
96: // minimum of all page offsets.
97: idx_t ColumnReader::FileOffset() const {
98: 	if (!chunk) {
99: 		throw std::runtime_error("FileOffset called on ColumnReader with no chunk");
100: 	}
101: 	auto min_offset = NumericLimits<idx_t>::Maximum();
102: 	if (chunk->meta_data.__isset.dictionary_page_offset) {
103: 		min_offset = MinValue<idx_t>(min_offset, chunk->meta_data.dictionary_page_offset);
104: 	}
105: 	if (chunk->meta_data.__isset.index_page_offset) {
106: 		min_offset = MinValue<idx_t>(min_offset, chunk->meta_data.index_page_offset);
107: 	}
108: 	min_offset = MinValue<idx_t>(min_offset, chunk->meta_data.data_page_offset);
109: 
110: 	return min_offset;
111: }
112: 
113: idx_t ColumnReader::GroupRowsAvailable() {
114: 	return group_rows_available;
115: }
116: 
117: unique_ptr<BaseStatistics> ColumnReader::Stats(const std::vector<ColumnChunk> &columns) {
118: 	if (Type().id() == LogicalTypeId::LIST || Type().id() == LogicalTypeId::STRUCT ||
119: 	    Type().id() == LogicalTypeId::MAP) {
120: 		return nullptr;
121: 	}
122: 	return ParquetStatisticsUtils::TransformColumnStatistics(Schema(), Type(), columns[file_idx]);
123: }
124: 
125: void ColumnReader::Plain(shared_ptr<ByteBuffer> plain_data, uint8_t *defines, idx_t num_values, // NOLINT
126:                          parquet_filter_t &filter, idx_t result_offset, Vector &result) {
127: 	throw NotImplementedException("Plain");
128: }
129: 
130: void ColumnReader::Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) { // NOLINT
131: 	throw NotImplementedException("Dictionary");
132: }
133: 
134: void ColumnReader::Offsets(uint32_t *offsets, uint8_t *defines, idx_t num_values, parquet_filter_t &filter,
135:                            idx_t result_offset, Vector &result) {
136: 	throw NotImplementedException("Offsets");
137: }
138: 
139: void ColumnReader::DictReference(Vector &result) {
140: }
141: void ColumnReader::PlainReference(shared_ptr<ByteBuffer>, Vector &result) { // NOLINT
142: }
143: 
144: void ColumnReader::InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) {
145: 	D_ASSERT(file_idx < columns.size());
146: 	chunk = &columns[file_idx];
147: 	protocol = &protocol_p;
148: 	D_ASSERT(chunk);
149: 	D_ASSERT(chunk->__isset.meta_data);
150: 
151: 	if (chunk->__isset.file_path) {
152: 		throw std::runtime_error("Only inlined data files are supported (no references)");
153: 	}
154: 
155: 	// ugh. sometimes there is an extra offset for the dict. sometimes it's wrong.
156: 	chunk_read_offset = chunk->meta_data.data_page_offset;
157: 	if (chunk->meta_data.__isset.dictionary_page_offset && chunk->meta_data.dictionary_page_offset >= 4) {
158: 		// this assumes the data pages follow the dict pages directly.
159: 		chunk_read_offset = chunk->meta_data.dictionary_page_offset;
160: 	}
161: 	group_rows_available = chunk->meta_data.num_values;
162: }
163: 
164: void ColumnReader::PrepareRead(parquet_filter_t &filter) {
165: 	dict_decoder.reset();
166: 	defined_decoder.reset();
167: 	block.reset();
168: 
169: 	PageHeader page_hdr;
170: 	page_hdr.read(protocol);
171: 
172: 	switch (page_hdr.type) {
173: 	case PageType::DATA_PAGE_V2:
174: 		PreparePageV2(page_hdr);
175: 		PrepareDataPage(page_hdr);
176: 		break;
177: 	case PageType::DATA_PAGE:
178: 		PreparePage(page_hdr);
179: 		PrepareDataPage(page_hdr);
180: 		break;
181: 	case PageType::DICTIONARY_PAGE:
182: 		PreparePage(page_hdr);
183: 		Dictionary(move(block), page_hdr.dictionary_page_header.num_values);
184: 		break;
185: 	default:
186: 		break; // ignore INDEX page type and any other custom extensions
187: 	}
188: }
189: 
190: void ColumnReader::PreparePageV2(PageHeader &page_hdr) {
191: 	D_ASSERT(page_hdr.type == PageType::DATA_PAGE_V2);
192: 
193: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
194: 
195: 	block = make_shared<ResizeableBuffer>(reader.allocator, page_hdr.uncompressed_page_size + 1);
196: 
197: 	if (chunk->meta_data.codec == CompressionCodec::UNCOMPRESSED) {
198: 		if (page_hdr.compressed_page_size != page_hdr.uncompressed_page_size) {
199: 			throw std::runtime_error("Page size mismatch");
200: 		}
201: 		trans.read((uint8_t *)block->ptr, page_hdr.compressed_page_size);
202: 		return;
203: 	}
204: 
205: 	// copy repeats & defines as-is because FOR SOME REASON they are uncompressed
206: 	auto uncompressed_bytes = page_hdr.data_page_header_v2.repetition_levels_byte_length +
207: 	                          page_hdr.data_page_header_v2.definition_levels_byte_length;
208: 	trans.read((uint8_t *)block->ptr, uncompressed_bytes);
209: 
210: 	auto compressed_bytes = page_hdr.compressed_page_size - uncompressed_bytes;
211: 	ResizeableBuffer compressed_buffer(reader.allocator, compressed_bytes);
212: 	trans.read((uint8_t *)compressed_buffer.ptr, compressed_bytes);
213: 
214: 	DecompressInternal(chunk->meta_data.codec, (const char *)compressed_buffer.ptr, compressed_bytes,
215: 	                   (char *)block->ptr + uncompressed_bytes, page_hdr.uncompressed_page_size - uncompressed_bytes);
216: }
217: 
218: void ColumnReader::PreparePage(PageHeader &page_hdr) {
219: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
220: 
221: 	block = make_shared<ResizeableBuffer>(reader.allocator, page_hdr.uncompressed_page_size + 1);
222: 
223: 	if (chunk->meta_data.codec == CompressionCodec::UNCOMPRESSED) {
224: 		if (page_hdr.compressed_page_size != page_hdr.uncompressed_page_size) {
225: 			throw std::runtime_error("Page size mismatch");
226: 		}
227: 		trans.read((uint8_t *)block->ptr, page_hdr.compressed_page_size);
228: 		return;
229: 	}
230: 
231: 	ResizeableBuffer compressed_buffer(reader.allocator, page_hdr.compressed_page_size + 1);
232: 	trans.read((uint8_t *)compressed_buffer.ptr, page_hdr.compressed_page_size);
233: 
234: 	DecompressInternal(chunk->meta_data.codec, (const char *)compressed_buffer.ptr, page_hdr.compressed_page_size,
235: 	                   (char *)block->ptr, page_hdr.uncompressed_page_size);
236: }
237: 
238: void ColumnReader::DecompressInternal(CompressionCodec::type codec, const char *src, idx_t src_size, char *dst,
239:                                       idx_t dst_size) {
240: 	switch (codec) {
241: 	case CompressionCodec::UNCOMPRESSED:
242: 		throw InternalException("Parquet data unexpectedly uncompressed");
243: 	case CompressionCodec::GZIP: {
244: 		MiniZStream s;
245: 		s.Decompress(src, src_size, dst, dst_size);
246: 		break;
247: 	}
248: 	case CompressionCodec::SNAPPY: {
249: 		{
250: 			size_t uncompressed_size = 0;
251: 			auto res = duckdb_snappy::GetUncompressedLength(src, src_size, &uncompressed_size);
252: 			if (!res) {
253: 				throw std::runtime_error("Snappy decompression failure");
254: 			}
255: 			if (uncompressed_size != (size_t)dst_size) {
256: 				throw std::runtime_error("Snappy decompression failure: Uncompressed data size mismatch");
257: 			}
258: 		}
259: 		auto res = duckdb_snappy::RawUncompress(src, src_size, dst);
260: 		if (!res) {
261: 			throw std::runtime_error("Snappy decompression failure");
262: 		}
263: 		break;
264: 	}
265: 	case CompressionCodec::ZSTD: {
266: 		auto res = duckdb_zstd::ZSTD_decompress(dst, dst_size, src, src_size);
267: 		if (duckdb_zstd::ZSTD_isError(res) || res != (size_t)dst_size) {
268: 			throw std::runtime_error("ZSTD Decompression failure");
269: 		}
270: 		break;
271: 	}
272: 	default: {
273: 		std::stringstream codec_name;
274: 		codec_name << codec;
275: 		throw std::runtime_error("Unsupported compression codec \"" + codec_name.str() +
276: 		                         "\". Supported options are uncompressed, gzip, snappy or zstd");
277: 		break;
278: 	}
279: 	}
280: }
281: 
282: void ColumnReader::PrepareDataPage(PageHeader &page_hdr) {
283: 	if (page_hdr.type == PageType::DATA_PAGE && !page_hdr.__isset.data_page_header) {
284: 		throw std::runtime_error("Missing data page header from data page");
285: 	}
286: 	if (page_hdr.type == PageType::DATA_PAGE_V2 && !page_hdr.__isset.data_page_header_v2) {
287: 		throw std::runtime_error("Missing data page header from data page v2");
288: 	}
289: 
290: 	page_rows_available = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.num_values
291: 	                                                           : page_hdr.data_page_header_v2.num_values;
292: 	auto page_encoding = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.encoding
293: 	                                                          : page_hdr.data_page_header_v2.encoding;
294: 
295: 	if (HasRepeats()) {
296: 		uint32_t rep_length = page_hdr.type == PageType::DATA_PAGE
297: 		                          ? block->read<uint32_t>()
298: 		                          : page_hdr.data_page_header_v2.repetition_levels_byte_length;
299: 		block->available(rep_length);
300: 		repeated_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, rep_length,
301: 		                                             RleBpDecoder::ComputeBitWidth(max_repeat));
302: 		block->inc(rep_length);
303: 	}
304: 
305: 	if (HasDefines()) {
306: 		uint32_t def_length = page_hdr.type == PageType::DATA_PAGE
307: 		                          ? block->read<uint32_t>()
308: 		                          : page_hdr.data_page_header_v2.definition_levels_byte_length;
309: 		block->available(def_length);
310: 		defined_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, def_length,
311: 		                                            RleBpDecoder::ComputeBitWidth(max_define));
312: 		block->inc(def_length);
313: 	}
314: 
315: 	switch (page_encoding) {
316: 	case Encoding::RLE_DICTIONARY:
317: 	case Encoding::PLAIN_DICTIONARY: {
318: 		// where is it otherwise??
319: 		auto dict_width = block->read<uint8_t>();
320: 		// TODO somehow dict_width can be 0 ?
321: 		dict_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, block->len, dict_width);
322: 		block->inc(block->len);
323: 		break;
324: 	}
325: 	case Encoding::DELTA_BINARY_PACKED: {
326: 		dbp_decoder = make_unique<DbpDecoder>((const uint8_t *)block->ptr, block->len);
327: 		block->inc(block->len);
328: 		break;
329: 	}
330: 		/*
331: 	case Encoding::DELTA_BYTE_ARRAY: {
332: 		dbp_decoder = make_unique<DbpDecoder>((const uint8_t *)block->ptr, block->len);
333: 		auto prefix_buffer = make_shared<ResizeableBuffer>();
334: 		prefix_buffer->resize(reader.allocator, sizeof(uint32_t) * page_hdr.data_page_header_v2.num_rows);
335: 
336: 		auto suffix_buffer = make_shared<ResizeableBuffer>();
337: 		suffix_buffer->resize(reader.allocator, sizeof(uint32_t) * page_hdr.data_page_header_v2.num_rows);
338: 
339: 		dbp_decoder->GetBatch<uint32_t>(prefix_buffer->ptr, page_hdr.data_page_header_v2.num_rows);
340: 		auto buffer_after_prefixes = dbp_decoder->BufferPtr();
341: 
342: 		dbp_decoder = make_unique<DbpDecoder>((const uint8_t *)buffer_after_prefixes.ptr, buffer_after_prefixes.len);
343: 		dbp_decoder->GetBatch<uint32_t>(suffix_buffer->ptr, page_hdr.data_page_header_v2.num_rows);
344: 
345: 		auto string_buffer = dbp_decoder->BufferPtr();
346: 
347: 		for (idx_t i = 0 ; i < page_hdr.data_page_header_v2.num_rows; i++) {
348: 		    auto suffix_length = (uint32_t*) suffix_buffer->ptr;
349: 		    string str( suffix_length[i] + 1, '\0');
350: 		    string_buffer.copy_to((char*) str.data(), suffix_length[i]);
351: 		    printf("%s\n", str.c_str());
352: 		}
353: 		throw std::runtime_error("eek");
354: 
355: 
356: 		// This is also known as incremental encoding or front compression: for each element in a sequence of strings,
357: 		// store the prefix length of the previous entry plus the suffix. This is stored as a sequence of delta-encoded
358: 		// prefix lengths (DELTA_BINARY_PACKED), followed by the suffixes encoded as delta length byte arrays
359: 		// (DELTA_LENGTH_BYTE_ARRAY). DELTA_LENGTH_BYTE_ARRAY: The encoded data would be DeltaEncoding(5, 5, 6, 6)
360: 		// "HelloWorldFoobarABCDEF"
361: 
362: 		// TODO actually do something here
363: 		break;
364: 	}
365: 		 */
366: 	case Encoding::PLAIN:
367: 		// nothing to do here, will be read directly below
368: 		break;
369: 
370: 	default:
371: 		throw std::runtime_error("Unsupported page encoding");
372: 	}
373: }
374: 
375: idx_t ColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
376:                          Vector &result) {
377: 	// we need to reset the location because multiple column readers share the same protocol
378: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
379: 	trans.SetLocation(chunk_read_offset);
380: 
381: 	// Perform any skips that were not applied yet.
382: 	if (pending_skips > 0) {
383: 		ApplyPendingSkips(pending_skips);
384: 	}
385: 
386: 	idx_t result_offset = 0;
387: 	auto to_read = num_values;
388: 
389: 	while (to_read > 0) {
390: 		while (page_rows_available == 0) {
391: 			PrepareRead(filter);
392: 		}
393: 
394: 		D_ASSERT(block);
395: 		auto read_now = MinValue<idx_t>(to_read, page_rows_available);
396: 
397: 		D_ASSERT(read_now <= STANDARD_VECTOR_SIZE);
398: 
399: 		if (HasRepeats()) {
400: 			D_ASSERT(repeated_decoder);
401: 			repeated_decoder->GetBatch<uint8_t>((char *)repeat_out + result_offset, read_now);
402: 		}
403: 
404: 		if (HasDefines()) {
405: 			D_ASSERT(defined_decoder);
406: 			defined_decoder->GetBatch<uint8_t>((char *)define_out + result_offset, read_now);
407: 		}
408: 
409: 		idx_t null_count = 0;
410: 
411: 		if ((dict_decoder || dbp_decoder) && HasDefines()) {
412: 			// we need the null count because the dictionary offsets have no entries for nulls
413: 			for (idx_t i = 0; i < read_now; i++) {
414: 				if (define_out[i + result_offset] != max_define) {
415: 					null_count++;
416: 				}
417: 			}
418: 		}
419: 
420: 		if (dict_decoder) {
421: 			offset_buffer.resize(reader.allocator, sizeof(uint32_t) * (read_now - null_count));
422: 			dict_decoder->GetBatch<uint32_t>(offset_buffer.ptr, read_now - null_count);
423: 			DictReference(result);
424: 			Offsets((uint32_t *)offset_buffer.ptr, define_out, read_now, filter, result_offset, result);
425: 		} else if (dbp_decoder) {
426: 			// TODO keep this in the state
427: 			auto read_buf = make_shared<ResizeableBuffer>();
428: 
429: 			switch (type.id()) {
430: 			case LogicalTypeId::INTEGER:
431: 				read_buf->resize(reader.allocator, sizeof(int32_t) * (read_now - null_count));
432: 				dbp_decoder->GetBatch<int32_t>(read_buf->ptr, read_now - null_count);
433: 
434: 				break;
435: 			case LogicalTypeId::BIGINT:
436: 				read_buf->resize(reader.allocator, sizeof(int64_t) * (read_now - null_count));
437: 				dbp_decoder->GetBatch<int64_t>(read_buf->ptr, read_now - null_count);
438: 				break;
439: 
440: 			default:
441: 				throw std::runtime_error("DELTA_BINARY_PACKED should only be INT32 or INT64");
442: 			}
443: 			// Plain() will put NULLs in the right place
444: 			Plain(read_buf, define_out, read_now, filter, result_offset, result);
445: 		} else {
446: 			PlainReference(block, result);
447: 			Plain(block, define_out, read_now, filter, result_offset, result);
448: 		}
449: 
450: 		result_offset += read_now;
451: 		page_rows_available -= read_now;
452: 		to_read -= read_now;
453: 	}
454: 	group_rows_available -= num_values;
455: 	chunk_read_offset = trans.GetLocation();
456: 
457: 	return num_values;
458: }
459: 
460: void ColumnReader::Skip(idx_t num_values) {
461: 	pending_skips += num_values;
462: }
463: 
464: void ColumnReader::ApplyPendingSkips(idx_t num_values) {
465: 	pending_skips -= num_values;
466: 
467: 	dummy_define.zero();
468: 	dummy_repeat.zero();
469: 
470: 	// TODO this can be optimized, for example we dont actually have to bitunpack offsets
471: 	Vector dummy_result(type, nullptr);
472: 
473: 	idx_t remaining = num_values;
474: 	idx_t read = 0;
475: 
476: 	while (remaining) {
477: 		idx_t to_read = MinValue<idx_t>(remaining, STANDARD_VECTOR_SIZE);
478: 		read += Read(to_read, none_filter, (uint8_t *)dummy_define.ptr, (uint8_t *)dummy_repeat.ptr, dummy_result);
479: 		remaining -= to_read;
480: 	}
481: 
482: 	if (read != num_values) {
483: 		throw std::runtime_error("Row count mismatch when skipping rows");
484: 	}
485: }
486: 
487: //===--------------------------------------------------------------------===//
488: // String Column Reader
489: //===--------------------------------------------------------------------===//
490: StringColumnReader::StringColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
491:                                        idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p)
492:     : TemplatedColumnReader<string_t, StringParquetValueConversion>(reader, move(type_p), schema_p, schema_idx_p,
493:                                                                     max_define_p, max_repeat_p) {
494: 	fixed_width_string_length = 0;
495: 	if (schema_p.type == Type::FIXED_LEN_BYTE_ARRAY) {
496: 		D_ASSERT(schema_p.__isset.type_length);
497: 		fixed_width_string_length = schema_p.type_length;
498: 	}
499: }
500: 
501: uint32_t StringColumnReader::VerifyString(const char *str_data, uint32_t str_len) {
502: 	if (Type() != LogicalTypeId::VARCHAR) {
503: 		return str_len;
504: 	}
505: 	// verify if a string is actually UTF8, and if there are no null bytes in the middle of the string
506: 	// technically Parquet should guarantee this, but reality is often disappointing
507: 	UnicodeInvalidReason reason;
508: 	size_t pos;
509: 	auto utf_type = Utf8Proc::Analyze(str_data, str_len, &reason, &pos);
510: 	if (utf_type == UnicodeType::INVALID) {
511: 		if (reason == UnicodeInvalidReason::NULL_BYTE) {
512: 			// for null bytes we just truncate the string
513: 			return pos;
514: 		}
515: 		throw InvalidInputException("Invalid string encoding found in Parquet file: value \"" +
516: 		                            Blob::ToString(string_t(str_data, str_len)) + "\" is not valid UTF8!");
517: 	}
518: 	return str_len;
519: }
520: 
521: void StringColumnReader::Dictionary(shared_ptr<ByteBuffer> data, idx_t num_entries) {
522: 	dict = move(data);
523: 	dict_strings = unique_ptr<string_t[]>(new string_t[num_entries]);
524: 	for (idx_t dict_idx = 0; dict_idx < num_entries; dict_idx++) {
525: 		uint32_t str_len;
526: 		if (fixed_width_string_length == 0) {
527: 			// variable length string: read from dictionary
528: 			str_len = dict->read<uint32_t>();
529: 		} else {
530: 			// fixed length string
531: 			str_len = fixed_width_string_length;
532: 		}
533: 		dict->available(str_len);
534: 
535: 		auto actual_str_len = VerifyString(dict->ptr, str_len);
536: 		dict_strings[dict_idx] = string_t(dict->ptr, actual_str_len);
537: 		dict->inc(str_len);
538: 	}
539: }
540: 
541: class ParquetStringVectorBuffer : public VectorBuffer {
542: public:
543: 	explicit ParquetStringVectorBuffer(shared_ptr<ByteBuffer> buffer_p)
544: 	    : VectorBuffer(VectorBufferType::OPAQUE_BUFFER), buffer(move(buffer_p)) {
545: 	}
546: 
547: private:
548: 	shared_ptr<ByteBuffer> buffer;
549: };
550: 
551: void StringColumnReader::DictReference(Vector &result) {
552: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(dict));
553: }
554: void StringColumnReader::PlainReference(shared_ptr<ByteBuffer> plain_data, Vector &result) {
555: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(move(plain_data)));
556: }
557: 
558: string_t StringParquetValueConversion::DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
559: 	auto &dict_strings = ((StringColumnReader &)reader).dict_strings;
560: 	return dict_strings[offset];
561: }
562: 
563: string_t StringParquetValueConversion::PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
564: 	auto &scr = ((StringColumnReader &)reader);
565: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
566: 	plain_data.available(str_len);
567: 	auto actual_str_len = ((StringColumnReader &)reader).VerifyString(plain_data.ptr, str_len);
568: 	auto ret_str = string_t(plain_data.ptr, actual_str_len);
569: 	plain_data.inc(str_len);
570: 	return ret_str;
571: }
572: 
573: void StringParquetValueConversion::PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
574: 	auto &scr = ((StringColumnReader &)reader);
575: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
576: 	plain_data.inc(str_len);
577: }
578: 
579: //===--------------------------------------------------------------------===//
580: // List Column Reader
581: //===--------------------------------------------------------------------===//
582: idx_t ListColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
583:                              Vector &result_out) {
584: 	idx_t result_offset = 0;
585: 	auto result_ptr = FlatVector::GetData<list_entry_t>(result_out);
586: 	auto &result_mask = FlatVector::Validity(result_out);
587: 
588: 	if (pending_skips > 0) {
589: 		ApplyPendingSkips(pending_skips);
590: 	}
591: 
592: 	D_ASSERT(ListVector::GetListSize(result_out) == 0);
593: 	// if an individual list is longer than STANDARD_VECTOR_SIZE we actually have to loop the child read to fill it
594: 	bool finished = false;
595: 	while (!finished) {
596: 		idx_t child_actual_num_values = 0;
597: 
598: 		// check if we have any overflow from a previous read
599: 		if (overflow_child_count == 0) {
600: 			// we don't: read elements from the child reader
601: 			child_defines.zero();
602: 			child_repeats.zero();
603: 			// we don't know in advance how many values to read because of the beautiful repetition/definition setup
604: 			// we just read (up to) a vector from the child column, and see if we have read enough
605: 			// if we have not read enough, we read another vector
606: 			// if we have read enough, we leave any unhandled elements in the overflow vector for a subsequent read
607: 			auto child_req_num_values =
608: 			    MinValue<idx_t>(STANDARD_VECTOR_SIZE, child_column_reader->GroupRowsAvailable());
609: 			read_vector.ResetFromCache(read_cache);
610: 			child_actual_num_values = child_column_reader->Read(child_req_num_values, child_filter, child_defines_ptr,
611: 			                                                    child_repeats_ptr, read_vector);
612: 		} else {
613: 			// we do: use the overflow values
614: 			child_actual_num_values = overflow_child_count;
615: 			overflow_child_count = 0;
616: 		}
617: 
618: 		if (child_actual_num_values == 0) {
619: 			// no more elements available: we are done
620: 			break;
621: 		}
622: 		read_vector.Verify(child_actual_num_values);
623: 		idx_t current_chunk_offset = ListVector::GetListSize(result_out);
624: 
625: 		// hard-won piece of code this, modify at your own risk
626: 		// the intuition is that we have to only collapse values into lists that are repeated *on this level*
627: 		// the rest is pretty much handed up as-is as a single-valued list or NULL
628: 		idx_t child_idx;
629: 		for (child_idx = 0; child_idx < child_actual_num_values; child_idx++) {
630: 			if (child_repeats_ptr[child_idx] == max_repeat) {
631: 				// value repeats on this level, append
632: 				D_ASSERT(result_offset > 0);
633: 				result_ptr[result_offset - 1].length++;
634: 				continue;
635: 			}
636: 
637: 			if (result_offset >= num_values) {
638: 				// we ran out of output space
639: 				finished = true;
640: 				break;
641: 			}
642: 			if (child_defines_ptr[child_idx] >= max_define) {
643: 				// value has been defined down the stack, hence its NOT NULL
644: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
645: 				result_ptr[result_offset].length = 1;
646: 			} else if (child_defines_ptr[child_idx] == max_define - 1) {
647: 				// empty list
648: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
649: 				result_ptr[result_offset].length = 0;
650: 			} else {
651: 				// value is NULL somewhere up the stack
652: 				result_mask.SetInvalid(result_offset);
653: 				result_ptr[result_offset].offset = 0;
654: 				result_ptr[result_offset].length = 0;
655: 			}
656: 
657: 			repeat_out[result_offset] = child_repeats_ptr[child_idx];
658: 			define_out[result_offset] = child_defines_ptr[child_idx];
659: 
660: 			result_offset++;
661: 		}
662: 		// actually append the required elements to the child list
663: 		ListVector::Append(result_out, read_vector, child_idx);
664: 
665: 		// we have read more values from the child reader than we can fit into the result for this read
666: 		// we have to pass everything from child_idx to child_actual_num_values into the next call
667: 		if (child_idx < child_actual_num_values && result_offset == num_values) {
668: 			read_vector.Slice(read_vector, child_idx);
669: 			overflow_child_count = child_actual_num_values - child_idx;
670: 			read_vector.Verify(overflow_child_count);
671: 
672: 			// move values in the child repeats and defines *backward* by child_idx
673: 			for (idx_t repdef_idx = 0; repdef_idx < overflow_child_count; repdef_idx++) {
674: 				child_defines_ptr[repdef_idx] = child_defines_ptr[child_idx + repdef_idx];
675: 				child_repeats_ptr[repdef_idx] = child_repeats_ptr[child_idx + repdef_idx];
676: 			}
677: 		}
678: 	}
679: 	result_out.Verify(result_offset);
680: 	return result_offset;
681: }
682: 
683: ListColumnReader::ListColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
684:                                    idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,
685:                                    unique_ptr<ColumnReader> child_column_reader_p)
686:     : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
687:       child_column_reader(move(child_column_reader_p)), read_cache(reader.allocator, ListType::GetChildType(Type())),
688:       read_vector(read_cache), overflow_child_count(0) {
689: 
690: 	child_defines.resize(reader.allocator, STANDARD_VECTOR_SIZE);
691: 	child_repeats.resize(reader.allocator, STANDARD_VECTOR_SIZE);
692: 	child_defines_ptr = (uint8_t *)child_defines.ptr;
693: 	child_repeats_ptr = (uint8_t *)child_repeats.ptr;
694: 
695: 	child_filter.set();
696: }
697: 
698: void ListColumnReader::ApplyPendingSkips(idx_t num_values) {
699: 	pending_skips -= num_values;
700: 
701: 	auto define_out = unique_ptr<uint8_t[]>(new uint8_t[num_values]);
702: 	auto repeat_out = unique_ptr<uint8_t[]>(new uint8_t[num_values]);
703: 
704: 	idx_t remaining = num_values;
705: 	idx_t read = 0;
706: 
707: 	while (remaining) {
708: 		Vector result_out(Type());
709: 		parquet_filter_t filter;
710: 		idx_t to_read = MinValue<idx_t>(remaining, STANDARD_VECTOR_SIZE);
711: 		read += Read(to_read, filter, define_out.get(), repeat_out.get(), result_out);
712: 		remaining -= to_read;
713: 	}
714: 
715: 	if (read != num_values) {
716: 		throw InternalException("Not all skips done!");
717: 	}
718: }
719: 
720: //===--------------------------------------------------------------------===//
721: // Generated Constant Column Reader
722: //===--------------------------------------------------------------------===//
723: GeneratedConstantColumnReader::GeneratedConstantColumnReader(ParquetReader &reader, LogicalType type_p,
724:                                                              const SchemaElement &schema_p, idx_t schema_idx_p,
725:                                                              idx_t max_define_p, idx_t max_repeat_p, Value constant_p)
726:     : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
727:       constant(move(constant_p)) {
728: }
729: idx_t GeneratedConstantColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out,
730:                                           uint8_t *repeat_out, Vector &result) {
731: 	result.SetValue(0, constant);
732: 	result.SetVectorType(VectorType::CONSTANT_VECTOR);
733: 	return num_values;
734: }
735: 
736: //===--------------------------------------------------------------------===//
737: // Cast Column Reader
738: //===--------------------------------------------------------------------===//
739: CastColumnReader::CastColumnReader(unique_ptr<ColumnReader> child_reader_p, LogicalType target_type_p)
740:     : ColumnReader(child_reader_p->Reader(), move(target_type_p), child_reader_p->Schema(), child_reader_p->FileIdx(),
741:                    child_reader_p->MaxDefine(), child_reader_p->MaxRepeat()),
742:       child_reader(move(child_reader_p)) {
743: 	vector<LogicalType> intermediate_types {child_reader->Type()};
744: 	intermediate_chunk.Initialize(reader.allocator, intermediate_types);
745: }
746: 
747: unique_ptr<BaseStatistics> CastColumnReader::Stats(const std::vector<ColumnChunk> &columns) {
748: 	// casting stats is not supported (yet)
749: 	return nullptr;
750: }
751: 
752: void CastColumnReader::InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) {
753: 	child_reader->InitializeRead(columns, protocol_p);
754: }
755: 
756: idx_t CastColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
757:                              Vector &result) {
758: 	intermediate_chunk.Reset();
759: 	auto &intermediate_vector = intermediate_chunk.data[0];
760: 
761: 	auto amount = child_reader->Read(num_values, filter, define_out, repeat_out, intermediate_vector);
762: 	if (!filter.all()) {
763: 		// work-around for filters: set all values that are filtered to NULL to prevent the cast from failing on
764: 		// uninitialized data
765: 		intermediate_vector.Flatten(amount);
766: 		auto &validity = FlatVector::Validity(intermediate_vector);
767: 		for (idx_t i = 0; i < amount; i++) {
768: 			if (!filter[i]) {
769: 				validity.SetInvalid(i);
770: 			}
771: 		}
772: 	}
773: 	VectorOperations::DefaultCast(intermediate_vector, result, amount);
774: 	return amount;
775: }
776: 
777: void CastColumnReader::Skip(idx_t num_values) {
778: 	child_reader->Skip(num_values);
779: }
780: 
781: idx_t CastColumnReader::GroupRowsAvailable() {
782: 	return child_reader->GroupRowsAvailable();
783: }
784: 
785: //===--------------------------------------------------------------------===//
786: // Struct Column Reader
787: //===--------------------------------------------------------------------===//
788: StructColumnReader::StructColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
789:                                        idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,
790:                                        vector<unique_ptr<ColumnReader>> child_readers_p)
791:     : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
792:       child_readers(move(child_readers_p)) {
793: 	D_ASSERT(type.InternalType() == PhysicalType::STRUCT);
794: }
795: 
796: ColumnReader *StructColumnReader::GetChildReader(idx_t child_idx) {
797: 	return child_readers[child_idx].get();
798: }
799: 
800: void StructColumnReader::InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) {
801: 	for (auto &child : child_readers) {
802: 		child->InitializeRead(columns, protocol_p);
803: 	}
804: }
805: 
806: idx_t StructColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
807:                                Vector &result) {
808: 	auto &struct_entries = StructVector::GetEntries(result);
809: 	D_ASSERT(StructType::GetChildTypes(Type()).size() == struct_entries.size());
810: 
811: 	if (pending_skips > 0) {
812: 		ApplyPendingSkips(pending_skips);
813: 	}
814: 
815: 	idx_t read_count = num_values;
816: 	for (idx_t i = 0; i < struct_entries.size(); i++) {
817: 		auto child_num_values = child_readers[i]->Read(num_values, filter, define_out, repeat_out, *struct_entries[i]);
818: 		if (i == 0) {
819: 			read_count = child_num_values;
820: 		} else if (read_count != child_num_values) {
821: 			throw std::runtime_error("Struct child row count mismatch");
822: 		}
823: 	}
824: 	// set the validity mask for this level
825: 	auto &validity = FlatVector::Validity(result);
826: 	for (idx_t i = 0; i < read_count; i++) {
827: 		if (define_out[i] < max_define) {
828: 			validity.SetInvalid(i);
829: 		}
830: 	}
831: 
832: 	return read_count;
833: }
834: 
835: void StructColumnReader::Skip(idx_t num_values) {
836: 	for (auto &child_reader : child_readers) {
837: 		child_reader->Skip(num_values);
838: 	}
839: }
840: 
841: void StructColumnReader::RegisterPrefetch(ThriftFileTransport &transport, bool allow_merge) {
842: 	for (auto &child : child_readers) {
843: 		child->RegisterPrefetch(transport, allow_merge);
844: 	}
845: }
846: 
847: uint64_t StructColumnReader::TotalCompressedSize() {
848: 	uint64_t size = 0;
849: 	for (auto &child : child_readers) {
850: 		size += child->TotalCompressedSize();
851: 	}
852: 	return size;
853: }
854: 
855: static bool TypeHasExactRowCount(const LogicalType &type) {
856: 	switch (type.id()) {
857: 	case LogicalTypeId::LIST:
858: 	case LogicalTypeId::MAP:
859: 		return false;
860: 	case LogicalTypeId::STRUCT:
861: 		for (auto &kv : StructType::GetChildTypes(type)) {
862: 			if (TypeHasExactRowCount(kv.second)) {
863: 				return true;
864: 			}
865: 		}
866: 		return false;
867: 	default:
868: 		return true;
869: 	}
870: }
871: 
872: idx_t StructColumnReader::GroupRowsAvailable() {
873: 	for (idx_t i = 0; i < child_readers.size(); i++) {
874: 		if (TypeHasExactRowCount(child_readers[i]->Type())) {
875: 			return child_readers[i]->GroupRowsAvailable();
876: 		}
877: 	}
878: 	return child_readers[0]->GroupRowsAvailable();
879: }
880: 
881: //===--------------------------------------------------------------------===//
882: // Decimal Column Reader
883: //===--------------------------------------------------------------------===//
884: template <class DUCKDB_PHYSICAL_TYPE, bool FIXED_LENGTH>
885: struct DecimalParquetValueConversion {
886: 	static DUCKDB_PHYSICAL_TYPE DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
887: 		auto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)dict.ptr;
888: 		return dict_ptr[offset];
889: 	}
890: 
891: 	static DUCKDB_PHYSICAL_TYPE PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
892: 		idx_t byte_len;
893: 		if (FIXED_LENGTH) {
894: 			byte_len = (idx_t)reader.Schema().type_length; /* sure, type length needs to be a signed int */
895: 		} else {
896: 			byte_len = plain_data.read<uint32_t>();
897: 		}
898: 		plain_data.available(byte_len);
899: 		auto res =
900: 		    ParquetDecimalUtils::ReadDecimalValue<DUCKDB_PHYSICAL_TYPE>((const_data_ptr_t)plain_data.ptr, byte_len);
901: 
902: 		plain_data.inc(byte_len);
903: 		return res;
904: 	}
905: 
906: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
907: 		uint32_t decimal_len = FIXED_LENGTH ? reader.Schema().type_length : plain_data.read<uint32_t>();
908: 		plain_data.inc(decimal_len);
909: 	}
910: };
911: 
912: template <class DUCKDB_PHYSICAL_TYPE, bool FIXED_LENGTH>
913: class DecimalColumnReader
914:     : public TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE,
915:                                    DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>> {
916: 
917: public:
918: 	DecimalColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, // NOLINT
919: 	                    idx_t file_idx_p, idx_t max_define_p, idx_t max_repeat_p)
920: 	    : TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE,
921: 	                            DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>>(
922: 	          reader, move(type_p), schema_p, file_idx_p, max_define_p, max_repeat_p) {};
923: 
924: protected:
925: 	void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) { // NOLINT
926: 		this->dict = make_shared<ResizeableBuffer>(this->reader.allocator, num_entries * sizeof(DUCKDB_PHYSICAL_TYPE));
927: 		auto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)this->dict->ptr;
928: 		for (idx_t i = 0; i < num_entries; i++) {
929: 			dict_ptr[i] =
930: 			    DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>::PlainRead(*dictionary_data, *this);
931: 		}
932: 	}
933: };
934: 
935: template <bool FIXED_LENGTH>
936: static unique_ptr<ColumnReader> CreateDecimalReaderInternal(ParquetReader &reader, const LogicalType &type_p,
937:                                                             const SchemaElement &schema_p, idx_t file_idx_p,
938:                                                             idx_t max_define, idx_t max_repeat) {
939: 	switch (type_p.InternalType()) {
940: 	case PhysicalType::INT16:
941: 		return make_unique<DecimalColumnReader<int16_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,
942: 		                                                               max_repeat);
943: 	case PhysicalType::INT32:
944: 		return make_unique<DecimalColumnReader<int32_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,
945: 		                                                               max_repeat);
946: 	case PhysicalType::INT64:
947: 		return make_unique<DecimalColumnReader<int64_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,
948: 		                                                               max_repeat);
949: 	case PhysicalType::INT128:
950: 		return make_unique<DecimalColumnReader<hugeint_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p,
951: 		                                                                 max_define, max_repeat);
952: 	default:
953: 		throw InternalException("Unrecognized type for Decimal");
954: 	}
955: }
956: 
957: unique_ptr<ColumnReader> ParquetDecimalUtils::CreateReader(ParquetReader &reader, const LogicalType &type_p,
958:                                                            const SchemaElement &schema_p, idx_t file_idx_p,
959:                                                            idx_t max_define, idx_t max_repeat) {
960: 	if (schema_p.__isset.type_length) {
961: 		return CreateDecimalReaderInternal<true>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
962: 	} else {
963: 		return CreateDecimalReaderInternal<false>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
964: 	}
965: }
966: 
967: //===--------------------------------------------------------------------===//
968: // UUID Column Reader
969: //===--------------------------------------------------------------------===//
970: struct UUIDValueConversion {
971: 	static hugeint_t DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
972: 		auto dict_ptr = (hugeint_t *)dict.ptr;
973: 		return dict_ptr[offset];
974: 	}
975: 
976: 	static hugeint_t ReadParquetUUID(const_data_ptr_t input) {
977: 		hugeint_t result;
978: 		result.lower = 0;
979: 		uint64_t unsigned_upper = 0;
980: 		for (idx_t i = 0; i < sizeof(uint64_t); i++) {
981: 			unsigned_upper <<= 8;
982: 			unsigned_upper += input[i];
983: 		}
984: 		for (idx_t i = sizeof(uint64_t); i < sizeof(hugeint_t); i++) {
985: 			result.lower <<= 8;
986: 			result.lower += input[i];
987: 		}
988: 		result.upper = unsigned_upper;
989: 		result.upper ^= (int64_t(1) << 63);
990: 		return result;
991: 	}
992: 
993: 	static hugeint_t PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
994: 		idx_t byte_len = sizeof(hugeint_t);
995: 		plain_data.available(byte_len);
996: 		auto res = ReadParquetUUID((const_data_ptr_t)plain_data.ptr);
997: 
998: 		plain_data.inc(byte_len);
999: 		return res;
1000: 	}
1001: 
1002: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
1003: 		plain_data.inc(sizeof(hugeint_t));
1004: 	}
1005: };
1006: 
1007: class UUIDColumnReader : public TemplatedColumnReader<hugeint_t, UUIDValueConversion> {
1008: 
1009: public:
1010: 	UUIDColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
1011: 	                 idx_t max_define_p, idx_t max_repeat_p)
1012: 	    : TemplatedColumnReader<hugeint_t, UUIDValueConversion>(reader, move(type_p), schema_p, file_idx_p,
1013: 	                                                            max_define_p, max_repeat_p) {};
1014: 
1015: protected:
1016: 	void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) { // NOLINT
1017: 		this->dict = make_shared<ResizeableBuffer>(this->reader.allocator, num_entries * sizeof(hugeint_t));
1018: 		auto dict_ptr = (hugeint_t *)this->dict->ptr;
1019: 		for (idx_t i = 0; i < num_entries; i++) {
1020: 			dict_ptr[i] = UUIDValueConversion::PlainRead(*dictionary_data, *this);
1021: 		}
1022: 	}
1023: };
1024: 
1025: //===--------------------------------------------------------------------===//
1026: // Interval Column Reader
1027: //===--------------------------------------------------------------------===//
1028: struct IntervalValueConversion {
1029: 	static constexpr const idx_t PARQUET_INTERVAL_SIZE = 12;
1030: 
1031: 	static interval_t DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
1032: 		auto dict_ptr = (interval_t *)dict.ptr;
1033: 		return dict_ptr[offset];
1034: 	}
1035: 
1036: 	static interval_t ReadParquetInterval(const_data_ptr_t input) {
1037: 		interval_t result;
1038: 		result.months = Load<uint32_t>(input);
1039: 		result.days = Load<uint32_t>(input + sizeof(uint32_t));
1040: 		result.micros = int64_t(Load<uint32_t>(input + sizeof(uint32_t) * 2)) * 1000;
1041: 		return result;
1042: 	}
1043: 
1044: 	static interval_t PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
1045: 		idx_t byte_len = PARQUET_INTERVAL_SIZE;
1046: 		plain_data.available(byte_len);
1047: 		auto res = ReadParquetInterval((const_data_ptr_t)plain_data.ptr);
1048: 
1049: 		plain_data.inc(byte_len);
1050: 		return res;
1051: 	}
1052: 
1053: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
1054: 		plain_data.inc(PARQUET_INTERVAL_SIZE);
1055: 	}
1056: };
1057: 
1058: class IntervalColumnReader : public TemplatedColumnReader<interval_t, IntervalValueConversion> {
1059: 
1060: public:
1061: 	IntervalColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
1062: 	                     idx_t max_define_p, idx_t max_repeat_p)
1063: 	    : TemplatedColumnReader<interval_t, IntervalValueConversion>(reader, move(type_p), schema_p, file_idx_p,
1064: 	                                                                 max_define_p, max_repeat_p) {};
1065: 
1066: protected:
1067: 	void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) override { // NOLINT
1068: 		this->dict = make_shared<ResizeableBuffer>(this->reader.allocator, num_entries * sizeof(interval_t));
1069: 		auto dict_ptr = (interval_t *)this->dict->ptr;
1070: 		for (idx_t i = 0; i < num_entries; i++) {
1071: 			dict_ptr[i] = IntervalValueConversion::PlainRead(*dictionary_data, *this);
1072: 		}
1073: 	}
1074: };
1075: 
1076: //===--------------------------------------------------------------------===//
1077: // Create Column Reader
1078: //===--------------------------------------------------------------------===//
1079: template <class T>
1080: unique_ptr<ColumnReader> CreateDecimalReader(ParquetReader &reader, const LogicalType &type_p,
1081:                                              const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
1082:                                              idx_t max_repeat) {
1083: 	switch (type_p.InternalType()) {
1084: 	case PhysicalType::INT16:
1085: 		return make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<T>>>(
1086: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1087: 	case PhysicalType::INT32:
1088: 		return make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<T>>>(
1089: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1090: 	case PhysicalType::INT64:
1091: 		return make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<T>>>(
1092: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1093: 	default:
1094: 		throw NotImplementedException("Unimplemented internal type for CreateDecimalReader");
1095: 	}
1096: }
1097: 
1098: unique_ptr<ColumnReader> ColumnReader::CreateReader(ParquetReader &reader, const LogicalType &type_p,
1099:                                                     const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
1100:                                                     idx_t max_repeat) {
1101: 	switch (type_p.id()) {
1102: 	case LogicalTypeId::BOOLEAN:
1103: 		return make_unique<BooleanColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1104: 	case LogicalTypeId::UTINYINT:
1105: 		return make_unique<TemplatedColumnReader<uint8_t, TemplatedParquetValueConversion<uint32_t>>>(
1106: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1107: 	case LogicalTypeId::USMALLINT:
1108: 		return make_unique<TemplatedColumnReader<uint16_t, TemplatedParquetValueConversion<uint32_t>>>(
1109: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1110: 	case LogicalTypeId::UINTEGER:
1111: 		return make_unique<TemplatedColumnReader<uint32_t, TemplatedParquetValueConversion<uint32_t>>>(
1112: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1113: 	case LogicalTypeId::UBIGINT:
1114: 		return make_unique<TemplatedColumnReader<uint64_t, TemplatedParquetValueConversion<uint64_t>>>(
1115: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1116: 	case LogicalTypeId::TINYINT:
1117: 		return make_unique<TemplatedColumnReader<int8_t, TemplatedParquetValueConversion<int32_t>>>(
1118: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1119: 	case LogicalTypeId::SMALLINT:
1120: 		return make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<int32_t>>>(
1121: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1122: 	case LogicalTypeId::INTEGER:
1123: 		return make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<int32_t>>>(
1124: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1125: 	case LogicalTypeId::BIGINT:
1126: 		return make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<int64_t>>>(
1127: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1128: 	case LogicalTypeId::FLOAT:
1129: 		return make_unique<TemplatedColumnReader<float, TemplatedParquetValueConversion<float>>>(
1130: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1131: 	case LogicalTypeId::DOUBLE:
1132: 		return make_unique<TemplatedColumnReader<double, TemplatedParquetValueConversion<double>>>(
1133: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1134: 	case LogicalTypeId::TIMESTAMP:
1135: 		switch (schema_p.type) {
1136: 		case Type::INT96:
1137: 			return make_unique<CallbackColumnReader<Int96, timestamp_t, ImpalaTimestampToTimestamp>>(
1138: 			    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1139: 		case Type::INT64:
1140: 			if (schema_p.__isset.logicalType && schema_p.logicalType.__isset.TIMESTAMP) {
1141: 				if (schema_p.logicalType.TIMESTAMP.unit.__isset.MILLIS) {
1142: 					return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMsToTimestamp>>(
1143: 					    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1144: 				} else if (schema_p.logicalType.TIMESTAMP.unit.__isset.MICROS) {
1145: 					return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMicrosToTimestamp>>(
1146: 					    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1147: 				} else if (schema_p.logicalType.TIMESTAMP.unit.__isset.NANOS) {
1148: 					return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampNsToTimestamp>>(
1149: 					    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1150: 				}
1151: 
1152: 			} else if (schema_p.__isset.converted_type) {
1153: 				switch (schema_p.converted_type) {
1154: 				case ConvertedType::TIMESTAMP_MICROS:
1155: 					return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMicrosToTimestamp>>(
1156: 					    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1157: 				case ConvertedType::TIMESTAMP_MILLIS:
1158: 					return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMsToTimestamp>>(
1159: 					    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1160: 				default:
1161: 					break;
1162: 				}
1163: 			}
1164: 		default:
1165: 			break;
1166: 		}
1167: 		break;
1168: 	case LogicalTypeId::DATE:
1169: 		return make_unique<CallbackColumnReader<int32_t, date_t, ParquetIntToDate>>(reader, type_p, schema_p,
1170: 		                                                                            file_idx_p, max_define, max_repeat);
1171: 	case LogicalTypeId::TIME:
1172: 		return make_unique<CallbackColumnReader<int64_t, dtime_t, ParquetIntToTime>>(
1173: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1174: 	case LogicalTypeId::BLOB:
1175: 	case LogicalTypeId::VARCHAR:
1176: 		return make_unique<StringColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1177: 	case LogicalTypeId::DECIMAL:
1178: 		// we have to figure out what kind of int we need
1179: 		switch (schema_p.type) {
1180: 		case Type::INT32:
1181: 			return CreateDecimalReader<int32_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1182: 		case Type::INT64:
1183: 			return CreateDecimalReader<int64_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1184: 		case Type::BYTE_ARRAY:
1185: 		case Type::FIXED_LEN_BYTE_ARRAY:
1186: 			return ParquetDecimalUtils::CreateReader(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1187: 		default:
1188: 			throw NotImplementedException("Unrecognized Parquet type for Decimal");
1189: 		}
1190: 		break;
1191: 	case LogicalTypeId::UUID:
1192: 		return make_unique<UUIDColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1193: 	case LogicalTypeId::INTERVAL:
1194: 		return make_unique<IntervalColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1195: 	default:
1196: 		break;
1197: 	}
1198: 	throw NotImplementedException(type_p.ToString());
1199: }
1200: 
1201: } // namespace duckdb
[end of extension/parquet/column_reader.cpp]
[start of src/common/types/validity_mask.cpp]
1: #include "duckdb/common/types/validity_mask.hpp"
2: 
3: namespace duckdb {
4: 
5: ValidityData::ValidityData(idx_t count) : TemplatedValidityData(count) {
6: }
7: ValidityData::ValidityData(const ValidityMask &original, idx_t count)
8:     : TemplatedValidityData(original.GetData(), count) {
9: }
10: 
11: void ValidityMask::Combine(const ValidityMask &other, idx_t count) {
12: 	if (other.AllValid()) {
13: 		// X & 1 = X
14: 		return;
15: 	}
16: 	if (AllValid()) {
17: 		// 1 & Y = Y
18: 		Initialize(other);
19: 		return;
20: 	}
21: 	if (validity_mask == other.validity_mask) {
22: 		// X & X == X
23: 		return;
24: 	}
25: 	// have to merge
26: 	// create a new validity mask that contains the combined mask
27: 	auto owned_data = move(validity_data);
28: 	auto data = GetData();
29: 	auto other_data = other.GetData();
30: 
31: 	Initialize(count);
32: 	auto result_data = GetData();
33: 
34: 	auto entry_count = ValidityData::EntryCount(count);
35: 	for (idx_t entry_idx = 0; entry_idx < entry_count; entry_idx++) {
36: 		result_data[entry_idx] = data[entry_idx] & other_data[entry_idx];
37: 	}
38: }
39: 
40: // LCOV_EXCL_START
41: string ValidityMask::ToString(idx_t count) const {
42: 	string result = "Validity Mask (" + to_string(count) + ") [";
43: 	for (idx_t i = 0; i < count; i++) {
44: 		result += RowIsValid(i) ? "." : "X";
45: 	}
46: 	result += "]";
47: 	return result;
48: }
49: // LCOV_EXCL_STOP
50: 
51: void ValidityMask::Resize(idx_t old_size, idx_t new_size) {
52: 	D_ASSERT(new_size >= old_size);
53: 	if (validity_mask) {
54: 		auto new_size_count = EntryCount(new_size);
55: 		auto old_size_count = EntryCount(old_size);
56: 		auto new_validity_data = make_buffer<ValidityBuffer>(new_size);
57: 		auto new_owned_data = new_validity_data->owned_data.get();
58: 		for (idx_t entry_idx = 0; entry_idx < old_size_count; entry_idx++) {
59: 			new_owned_data[entry_idx] = validity_mask[entry_idx];
60: 		}
61: 		for (idx_t entry_idx = old_size_count; entry_idx < new_size_count; entry_idx++) {
62: 			new_owned_data[entry_idx] = ValidityData::MAX_ENTRY;
63: 		}
64: 		validity_data = move(new_validity_data);
65: 		validity_mask = validity_data->owned_data.get();
66: 	} else {
67: 		Initialize(new_size);
68: 	}
69: }
70: 
71: void ValidityMask::Slice(const ValidityMask &other, idx_t offset) {
72: 	if (other.AllValid()) {
73: 		validity_mask = nullptr;
74: 		validity_data.reset();
75: 		return;
76: 	}
77: 	if (offset == 0) {
78: 		Initialize(other);
79: 		return;
80: 	}
81: 	ValidityMask new_mask(STANDARD_VECTOR_SIZE);
82: 
83: // FIXME THIS NEEDS FIXING!
84: #if 1
85: 	for (idx_t i = offset; i < STANDARD_VECTOR_SIZE; i++) {
86: 		new_mask.Set(i - offset, other.RowIsValid(i));
87: 	}
88: 	Initialize(new_mask);
89: #else
90: 	// first shift the "whole" units
91: 	idx_t entire_units = offset / BITS_PER_VALUE;
92: 	idx_t sub_units = offset - entire_units * BITS_PER_VALUE;
93: 	if (entire_units > 0) {
94: 		idx_t validity_idx;
95: 		for (validity_idx = 0; validity_idx + entire_units < STANDARD_ENTRY_COUNT; validity_idx++) {
96: 			new_mask.validity_mask[validity_idx] = other.validity_mask[validity_idx + entire_units];
97: 		}
98: 	}
99: 	// now we shift the remaining sub units
100: 	// this gets a bit more complicated because we have to shift over the borders of the entries
101: 	// e.g. suppose we have 2 entries of length 4 and we left-shift by two
102: 	// 0101|1010
103: 	// a regular left-shift of both gets us:
104: 	// 0100|1000
105: 	// we then OR the overflow (right-shifted by BITS_PER_VALUE - offset) together to get the correct result
106: 	// 0100|1000 ->
107: 	// 0110|1000
108: 	if (sub_units > 0) {
109: 		idx_t validity_idx;
110: 		for (validity_idx = 0; validity_idx + 1 < STANDARD_ENTRY_COUNT; validity_idx++) {
111: 			new_mask.validity_mask[validity_idx] =
112: 			    (other.validity_mask[validity_idx] >> sub_units) |
113: 			    (other.validity_mask[validity_idx + 1] << (BITS_PER_VALUE - sub_units));
114: 		}
115: 		new_mask.validity_mask[validity_idx] >>= sub_units;
116: 	}
117: #ifdef DEBUG
118: 	for (idx_t i = offset; i < STANDARD_VECTOR_SIZE; i++) {
119: 		D_ASSERT(new_mask.RowIsValid(i - offset) == other.RowIsValid(i));
120: 	}
121: 	Initialize(new_mask);
122: #endif
123: #endif
124: }
125: 
126: } // namespace duckdb
[end of src/common/types/validity_mask.cpp]
[start of src/common/types/vector.cpp]
1: #include "duckdb/common/types/vector.hpp"
2: 
3: #include "duckdb/common/algorithm.hpp"
4: #include "duckdb/common/assert.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/operator/comparison_operators.hpp"
7: #include "duckdb/common/pair.hpp"
8: #include "duckdb/common/printer.hpp"
9: #include "duckdb/common/serializer.hpp"
10: #include "duckdb/common/types/null_value.hpp"
11: #include "duckdb/common/types/sel_cache.hpp"
12: #include "duckdb/common/types/vector_cache.hpp"
13: #include "duckdb/common/vector_operations/vector_operations.hpp"
14: #include "duckdb/storage/buffer/buffer_handle.hpp"
15: #include "duckdb/function/scalar/nested_functions.hpp"
16: #include "duckdb/storage/string_uncompressed.hpp"
17: #include "duckdb/common/types/value.hpp"
18: #include "duckdb/common/fsst.hpp"
19: #include "fsst.h"
20: 
21: #include <cstring> // strlen() on Solaris
22: 
23: namespace duckdb {
24: 
25: Vector::Vector(LogicalType type_p, bool create_data, bool zero_data, idx_t capacity)
26:     : vector_type(VectorType::FLAT_VECTOR), type(move(type_p)), data(nullptr) {
27: 	if (create_data) {
28: 		Initialize(zero_data, capacity);
29: 	}
30: }
31: 
32: Vector::Vector(LogicalType type_p, idx_t capacity) : Vector(move(type_p), true, false, capacity) {
33: }
34: 
35: Vector::Vector(LogicalType type_p, data_ptr_t dataptr)
36:     : vector_type(VectorType::FLAT_VECTOR), type(move(type_p)), data(dataptr) {
37: 	if (dataptr && !type.IsValid()) {
38: 		throw InternalException("Cannot create a vector of type INVALID!");
39: 	}
40: }
41: 
42: Vector::Vector(const VectorCache &cache) : type(cache.GetType()) {
43: 	ResetFromCache(cache);
44: }
45: 
46: Vector::Vector(Vector &other) : type(other.type) {
47: 	Reference(other);
48: }
49: 
50: Vector::Vector(Vector &other, const SelectionVector &sel, idx_t count) : type(other.type) {
51: 	Slice(other, sel, count);
52: }
53: 
54: Vector::Vector(Vector &other, idx_t offset) : type(other.type) {
55: 	Slice(other, offset);
56: }
57: 
58: Vector::Vector(const Value &value) : type(value.type()) {
59: 	Reference(value);
60: }
61: 
62: Vector::Vector(Vector &&other) noexcept
63:     : vector_type(other.vector_type), type(move(other.type)), data(other.data), validity(move(other.validity)),
64:       buffer(move(other.buffer)), auxiliary(move(other.auxiliary)) {
65: }
66: 
67: void Vector::Reference(const Value &value) {
68: 	D_ASSERT(GetType().id() == value.type().id());
69: 	this->vector_type = VectorType::CONSTANT_VECTOR;
70: 	buffer = VectorBuffer::CreateConstantVector(value.type());
71: 	auto internal_type = value.type().InternalType();
72: 	if (internal_type == PhysicalType::STRUCT) {
73: 		auto struct_buffer = make_unique<VectorStructBuffer>();
74: 		auto &child_types = StructType::GetChildTypes(value.type());
75: 		auto &child_vectors = struct_buffer->GetChildren();
76: 		auto &value_children = StructValue::GetChildren(value);
77: 		for (idx_t i = 0; i < child_types.size(); i++) {
78: 			auto vector = make_unique<Vector>(value.IsNull() ? Value(child_types[i].second) : value_children[i]);
79: 			child_vectors.push_back(move(vector));
80: 		}
81: 		auxiliary = move(struct_buffer);
82: 		if (value.IsNull()) {
83: 			SetValue(0, value);
84: 		}
85: 	} else if (internal_type == PhysicalType::LIST) {
86: 		auto list_buffer = make_unique<VectorListBuffer>(value.type());
87: 		auxiliary = move(list_buffer);
88: 		data = buffer->GetData();
89: 		SetValue(0, value);
90: 	} else {
91: 		auxiliary.reset();
92: 		data = buffer->GetData();
93: 		SetValue(0, value);
94: 	}
95: }
96: 
97: void Vector::Reference(Vector &other) {
98: 	D_ASSERT(other.GetType() == GetType());
99: 	Reinterpret(other);
100: }
101: 
102: void Vector::ReferenceAndSetType(Vector &other) {
103: 	type = other.GetType();
104: 	Reference(other);
105: }
106: 
107: void Vector::Reinterpret(Vector &other) {
108: 	vector_type = other.vector_type;
109: 	AssignSharedPointer(buffer, other.buffer);
110: 	AssignSharedPointer(auxiliary, other.auxiliary);
111: 	data = other.data;
112: 	validity = other.validity;
113: }
114: 
115: void Vector::ResetFromCache(const VectorCache &cache) {
116: 	cache.ResetFromCache(*this);
117: }
118: 
119: void Vector::Slice(Vector &other, idx_t offset) {
120: 	if (other.GetVectorType() == VectorType::CONSTANT_VECTOR) {
121: 		Reference(other);
122: 		return;
123: 	}
124: 	D_ASSERT(other.GetVectorType() == VectorType::FLAT_VECTOR);
125: 
126: 	auto internal_type = GetType().InternalType();
127: 	if (internal_type == PhysicalType::STRUCT) {
128: 		Vector new_vector(GetType());
129: 		auto &entries = StructVector::GetEntries(new_vector);
130: 		auto &other_entries = StructVector::GetEntries(other);
131: 		D_ASSERT(entries.size() == other_entries.size());
132: 		for (idx_t i = 0; i < entries.size(); i++) {
133: 			entries[i]->Slice(*other_entries[i], offset);
134: 		}
135: 		if (offset > 0) {
136: 			new_vector.validity.Slice(other.validity, offset);
137: 		} else {
138: 			new_vector.validity = other.validity;
139: 		}
140: 		Reference(new_vector);
141: 	} else {
142: 		Reference(other);
143: 		if (offset > 0) {
144: 			data = data + GetTypeIdSize(internal_type) * offset;
145: 			validity.Slice(other.validity, offset);
146: 		}
147: 	}
148: }
149: 
150: void Vector::Slice(Vector &other, const SelectionVector &sel, idx_t count) {
151: 	Reference(other);
152: 	Slice(sel, count);
153: }
154: 
155: void Vector::Slice(const SelectionVector &sel, idx_t count) {
156: 	if (GetVectorType() == VectorType::CONSTANT_VECTOR) {
157: 		// dictionary on a constant is just a constant
158: 		return;
159: 	}
160: 	if (GetVectorType() == VectorType::DICTIONARY_VECTOR) {
161: 		// already a dictionary, slice the current dictionary
162: 		auto &current_sel = DictionaryVector::SelVector(*this);
163: 		auto sliced_dictionary = current_sel.Slice(sel, count);
164: 		buffer = make_buffer<DictionaryBuffer>(move(sliced_dictionary));
165: 		if (GetType().InternalType() == PhysicalType::STRUCT) {
166: 			auto &child_vector = DictionaryVector::Child(*this);
167: 
168: 			Vector new_child(child_vector);
169: 			new_child.auxiliary = make_buffer<VectorStructBuffer>(new_child, sel, count);
170: 			auxiliary = make_buffer<VectorChildBuffer>(move(new_child));
171: 		}
172: 		return;
173: 	}
174: 
175: 	if (GetVectorType() == VectorType::FSST_VECTOR) {
176: 		Flatten(sel, count);
177: 		return;
178: 	}
179: 
180: 	Vector child_vector(*this);
181: 	auto internal_type = GetType().InternalType();
182: 	if (internal_type == PhysicalType::STRUCT) {
183: 		child_vector.auxiliary = make_buffer<VectorStructBuffer>(*this, sel, count);
184: 	}
185: 	auto child_ref = make_buffer<VectorChildBuffer>(move(child_vector));
186: 	auto dict_buffer = make_buffer<DictionaryBuffer>(sel);
187: 	vector_type = VectorType::DICTIONARY_VECTOR;
188: 	buffer = move(dict_buffer);
189: 	auxiliary = move(child_ref);
190: }
191: 
192: void Vector::Slice(const SelectionVector &sel, idx_t count, SelCache &cache) {
193: 	if (GetVectorType() == VectorType::DICTIONARY_VECTOR && GetType().InternalType() != PhysicalType::STRUCT) {
194: 		// dictionary vector: need to merge dictionaries
195: 		// check if we have a cached entry
196: 		auto &current_sel = DictionaryVector::SelVector(*this);
197: 		auto target_data = current_sel.data();
198: 		auto entry = cache.cache.find(target_data);
199: 		if (entry != cache.cache.end()) {
200: 			// cached entry exists: use that
201: 			this->buffer = make_buffer<DictionaryBuffer>(((DictionaryBuffer &)*entry->second).GetSelVector());
202: 			vector_type = VectorType::DICTIONARY_VECTOR;
203: 		} else {
204: 			Slice(sel, count);
205: 			cache.cache[target_data] = this->buffer;
206: 		}
207: 	} else {
208: 		Slice(sel, count);
209: 	}
210: }
211: 
212: void Vector::Initialize(bool zero_data, idx_t capacity) {
213: 	auxiliary.reset();
214: 	validity.Reset();
215: 	auto &type = GetType();
216: 	auto internal_type = type.InternalType();
217: 	if (internal_type == PhysicalType::STRUCT) {
218: 		auto struct_buffer = make_unique<VectorStructBuffer>(type, capacity);
219: 		auxiliary = move(struct_buffer);
220: 	} else if (internal_type == PhysicalType::LIST) {
221: 		auto list_buffer = make_unique<VectorListBuffer>(type, capacity);
222: 		auxiliary = move(list_buffer);
223: 	}
224: 	auto type_size = GetTypeIdSize(internal_type);
225: 	if (type_size > 0) {
226: 		buffer = VectorBuffer::CreateStandardVector(type, capacity);
227: 		data = buffer->GetData();
228: 		if (zero_data) {
229: 			memset(data, 0, capacity * type_size);
230: 		}
231: 	}
232: 	if (capacity > STANDARD_VECTOR_SIZE) {
233: 		validity.Resize(STANDARD_VECTOR_SIZE, capacity);
234: 	}
235: }
236: 
237: struct DataArrays {
238: 	Vector &vec;
239: 	data_ptr_t data;
240: 	VectorBuffer *buffer;
241: 	idx_t type_size;
242: 	bool is_nested;
243: 	DataArrays(Vector &vec, data_ptr_t data, VectorBuffer *buffer, idx_t type_size, bool is_nested)
244: 	    : vec(vec), data(data), buffer(buffer), type_size(type_size), is_nested(is_nested) {};
245: };
246: 
247: void FindChildren(std::vector<DataArrays> &to_resize, VectorBuffer &auxiliary) {
248: 	if (auxiliary.GetBufferType() == VectorBufferType::LIST_BUFFER) {
249: 		auto &buffer = (VectorListBuffer &)auxiliary;
250: 		auto &child = buffer.GetChild();
251: 		auto data = child.GetData();
252: 		if (!data) {
253: 			//! Nested type
254: 			DataArrays arrays(child, data, child.GetBuffer().get(), GetTypeIdSize(child.GetType().InternalType()),
255: 			                  true);
256: 			to_resize.emplace_back(arrays);
257: 			FindChildren(to_resize, *child.GetAuxiliary());
258: 		} else {
259: 			DataArrays arrays(child, data, child.GetBuffer().get(), GetTypeIdSize(child.GetType().InternalType()),
260: 			                  false);
261: 			to_resize.emplace_back(arrays);
262: 		}
263: 	} else if (auxiliary.GetBufferType() == VectorBufferType::STRUCT_BUFFER) {
264: 		auto &buffer = (VectorStructBuffer &)auxiliary;
265: 		auto &children = buffer.GetChildren();
266: 		for (auto &child : children) {
267: 			auto data = child->GetData();
268: 			if (!data) {
269: 				//! Nested type
270: 				DataArrays arrays(*child, data, child->GetBuffer().get(),
271: 				                  GetTypeIdSize(child->GetType().InternalType()), true);
272: 				to_resize.emplace_back(arrays);
273: 				FindChildren(to_resize, *child->GetAuxiliary());
274: 			} else {
275: 				DataArrays arrays(*child, data, child->GetBuffer().get(),
276: 				                  GetTypeIdSize(child->GetType().InternalType()), false);
277: 				to_resize.emplace_back(arrays);
278: 			}
279: 		}
280: 	}
281: }
282: void Vector::Resize(idx_t cur_size, idx_t new_size) {
283: 	std::vector<DataArrays> to_resize;
284: 	if (!buffer) {
285: 		buffer = make_unique<VectorBuffer>(0);
286: 	}
287: 	if (!data) {
288: 		//! this is a nested structure
289: 		DataArrays arrays(*this, data, buffer.get(), GetTypeIdSize(GetType().InternalType()), true);
290: 		to_resize.emplace_back(arrays);
291: 		FindChildren(to_resize, *auxiliary);
292: 	} else {
293: 		DataArrays arrays(*this, data, buffer.get(), GetTypeIdSize(GetType().InternalType()), false);
294: 		to_resize.emplace_back(arrays);
295: 	}
296: 	for (auto &data_to_resize : to_resize) {
297: 		if (!data_to_resize.is_nested) {
298: 			auto new_data = unique_ptr<data_t[]>(new data_t[new_size * data_to_resize.type_size]);
299: 			memcpy(new_data.get(), data_to_resize.data, cur_size * data_to_resize.type_size * sizeof(data_t));
300: 			data_to_resize.buffer->SetData(move(new_data));
301: 			data_to_resize.vec.data = data_to_resize.buffer->GetData();
302: 		}
303: 		data_to_resize.vec.validity.Resize(cur_size, new_size);
304: 	}
305: }
306: 
307: void Vector::SetValue(idx_t index, const Value &val) {
308: 	if (GetVectorType() == VectorType::DICTIONARY_VECTOR) {
309: 		// dictionary: apply dictionary and forward to child
310: 		auto &sel_vector = DictionaryVector::SelVector(*this);
311: 		auto &child = DictionaryVector::Child(*this);
312: 		return child.SetValue(sel_vector.get_index(index), val);
313: 	}
314: 	if (val.type() != GetType()) {
315: 		SetValue(index, val.DefaultCastAs(GetType()));
316: 		return;
317: 	}
318: 	D_ASSERT(val.type().InternalType() == GetType().InternalType());
319: 
320: 	validity.EnsureWritable();
321: 	validity.Set(index, !val.IsNull());
322: 	if (val.IsNull() && GetType().InternalType() != PhysicalType::STRUCT) {
323: 		// for structs we still need to set the child-entries to NULL
324: 		// so we do not bail out yet
325: 		return;
326: 	}
327: 
328: 	switch (GetType().InternalType()) {
329: 	case PhysicalType::BOOL:
330: 		((bool *)data)[index] = val.GetValueUnsafe<bool>();
331: 		break;
332: 	case PhysicalType::INT8:
333: 		((int8_t *)data)[index] = val.GetValueUnsafe<int8_t>();
334: 		break;
335: 	case PhysicalType::INT16:
336: 		((int16_t *)data)[index] = val.GetValueUnsafe<int16_t>();
337: 		break;
338: 	case PhysicalType::INT32:
339: 		((int32_t *)data)[index] = val.GetValueUnsafe<int32_t>();
340: 		break;
341: 	case PhysicalType::INT64:
342: 		((int64_t *)data)[index] = val.GetValueUnsafe<int64_t>();
343: 		break;
344: 	case PhysicalType::INT128:
345: 		((hugeint_t *)data)[index] = val.GetValueUnsafe<hugeint_t>();
346: 		break;
347: 	case PhysicalType::UINT8:
348: 		((uint8_t *)data)[index] = val.GetValueUnsafe<uint8_t>();
349: 		break;
350: 	case PhysicalType::UINT16:
351: 		((uint16_t *)data)[index] = val.GetValueUnsafe<uint16_t>();
352: 		break;
353: 	case PhysicalType::UINT32:
354: 		((uint32_t *)data)[index] = val.GetValueUnsafe<uint32_t>();
355: 		break;
356: 	case PhysicalType::UINT64:
357: 		((uint64_t *)data)[index] = val.GetValueUnsafe<uint64_t>();
358: 		break;
359: 	case PhysicalType::FLOAT:
360: 		((float *)data)[index] = val.GetValueUnsafe<float>();
361: 		break;
362: 	case PhysicalType::DOUBLE:
363: 		((double *)data)[index] = val.GetValueUnsafe<double>();
364: 		break;
365: 	case PhysicalType::INTERVAL:
366: 		((interval_t *)data)[index] = val.GetValueUnsafe<interval_t>();
367: 		break;
368: 	case PhysicalType::VARCHAR:
369: 		((string_t *)data)[index] = StringVector::AddStringOrBlob(*this, StringValue::Get(val));
370: 		break;
371: 	case PhysicalType::STRUCT: {
372: 		D_ASSERT(GetVectorType() == VectorType::CONSTANT_VECTOR || GetVectorType() == VectorType::FLAT_VECTOR);
373: 
374: 		auto &children = StructVector::GetEntries(*this);
375: 		auto &val_children = StructValue::GetChildren(val);
376: 		D_ASSERT(val.IsNull() || children.size() == val_children.size());
377: 		for (size_t i = 0; i < children.size(); i++) {
378: 			auto &vec_child = children[i];
379: 			if (!val.IsNull()) {
380: 				auto &struct_child = val_children[i];
381: 				vec_child->SetValue(index, struct_child);
382: 			} else {
383: 				vec_child->SetValue(index, Value());
384: 			}
385: 		}
386: 		break;
387: 	}
388: 	case PhysicalType::LIST: {
389: 		auto offset = ListVector::GetListSize(*this);
390: 		auto &val_children = ListValue::GetChildren(val);
391: 		if (!val_children.empty()) {
392: 			for (idx_t i = 0; i < val_children.size(); i++) {
393: 				ListVector::PushBack(*this, val_children[i]);
394: 			}
395: 		}
396: 		//! now set the pointer
397: 		auto &entry = ((list_entry_t *)data)[index];
398: 		entry.length = val_children.size();
399: 		entry.offset = offset;
400: 		break;
401: 	}
402: 	default:
403: 		throw InternalException("Unimplemented type for Vector::SetValue");
404: 	}
405: }
406: 
407: Value Vector::GetValueInternal(const Vector &v_p, idx_t index_p) {
408: 	const Vector *vector = &v_p;
409: 	idx_t index = index_p;
410: 	bool finished = false;
411: 	while (!finished) {
412: 		switch (vector->GetVectorType()) {
413: 		case VectorType::CONSTANT_VECTOR:
414: 			index = 0;
415: 			finished = true;
416: 			break;
417: 		case VectorType::FLAT_VECTOR:
418: 			finished = true;
419: 			break;
420: 		case VectorType::FSST_VECTOR:
421: 			finished = true;
422: 			break;
423: 		// dictionary: apply dictionary and forward to child
424: 		case VectorType::DICTIONARY_VECTOR: {
425: 			auto &sel_vector = DictionaryVector::SelVector(*vector);
426: 			auto &child = DictionaryVector::Child(*vector);
427: 			vector = &child;
428: 			index = sel_vector.get_index(index);
429: 			break;
430: 		}
431: 		case VectorType::SEQUENCE_VECTOR: {
432: 			int64_t start, increment;
433: 			SequenceVector::GetSequence(*vector, start, increment);
434: 			return Value::Numeric(vector->GetType(), start + increment * index);
435: 		}
436: 		default:
437: 			throw InternalException("Unimplemented vector type for Vector::GetValue");
438: 		}
439: 	}
440: 	auto data = vector->data;
441: 	auto &validity = vector->validity;
442: 	auto &type = vector->GetType();
443: 
444: 	if (!validity.RowIsValid(index)) {
445: 		return Value(vector->GetType());
446: 	}
447: 
448: 	if (vector->GetVectorType() == VectorType::FSST_VECTOR) {
449: 		if (vector->GetType().InternalType() != PhysicalType::VARCHAR) {
450: 			throw InternalException("FSST Vector with non-string datatype found!");
451: 		}
452: 		auto str_compressed = ((string_t *)data)[index];
453: 		Value result =
454: 		    FSSTPrimitives::DecompressValue(FSSTVector::GetDecoder(const_cast<Vector &>(*vector)),
455: 		                                    (unsigned char *)str_compressed.GetDataUnsafe(), str_compressed.GetSize());
456: 		return result;
457: 	}
458: 
459: 	switch (vector->GetType().id()) {
460: 	case LogicalTypeId::BOOLEAN:
461: 		return Value::BOOLEAN(((bool *)data)[index]);
462: 	case LogicalTypeId::TINYINT:
463: 		return Value::TINYINT(((int8_t *)data)[index]);
464: 	case LogicalTypeId::SMALLINT:
465: 		return Value::SMALLINT(((int16_t *)data)[index]);
466: 	case LogicalTypeId::INTEGER:
467: 		return Value::INTEGER(((int32_t *)data)[index]);
468: 	case LogicalTypeId::DATE:
469: 		return Value::DATE(((date_t *)data)[index]);
470: 	case LogicalTypeId::TIME:
471: 		return Value::TIME(((dtime_t *)data)[index]);
472: 	case LogicalTypeId::TIME_TZ:
473: 		return Value::TIMETZ(((dtime_t *)data)[index]);
474: 	case LogicalTypeId::BIGINT:
475: 		return Value::BIGINT(((int64_t *)data)[index]);
476: 	case LogicalTypeId::UTINYINT:
477: 		return Value::UTINYINT(((uint8_t *)data)[index]);
478: 	case LogicalTypeId::USMALLINT:
479: 		return Value::USMALLINT(((uint16_t *)data)[index]);
480: 	case LogicalTypeId::UINTEGER:
481: 		return Value::UINTEGER(((uint32_t *)data)[index]);
482: 	case LogicalTypeId::UBIGINT:
483: 		return Value::UBIGINT(((uint64_t *)data)[index]);
484: 	case LogicalTypeId::TIMESTAMP:
485: 		return Value::TIMESTAMP(((timestamp_t *)data)[index]);
486: 	case LogicalTypeId::TIMESTAMP_NS:
487: 		return Value::TIMESTAMPNS(((timestamp_t *)data)[index]);
488: 	case LogicalTypeId::TIMESTAMP_MS:
489: 		return Value::TIMESTAMPMS(((timestamp_t *)data)[index]);
490: 	case LogicalTypeId::TIMESTAMP_SEC:
491: 		return Value::TIMESTAMPSEC(((timestamp_t *)data)[index]);
492: 	case LogicalTypeId::TIMESTAMP_TZ:
493: 		return Value::TIMESTAMPTZ(((timestamp_t *)data)[index]);
494: 	case LogicalTypeId::HUGEINT:
495: 		return Value::HUGEINT(((hugeint_t *)data)[index]);
496: 	case LogicalTypeId::UUID:
497: 		return Value::UUID(((hugeint_t *)data)[index]);
498: 	case LogicalTypeId::DECIMAL: {
499: 		auto width = DecimalType::GetWidth(type);
500: 		auto scale = DecimalType::GetScale(type);
501: 		switch (type.InternalType()) {
502: 		case PhysicalType::INT16:
503: 			return Value::DECIMAL(((int16_t *)data)[index], width, scale);
504: 		case PhysicalType::INT32:
505: 			return Value::DECIMAL(((int32_t *)data)[index], width, scale);
506: 		case PhysicalType::INT64:
507: 			return Value::DECIMAL(((int64_t *)data)[index], width, scale);
508: 		case PhysicalType::INT128:
509: 			return Value::DECIMAL(((hugeint_t *)data)[index], width, scale);
510: 		default:
511: 			throw InternalException("Physical type '%s' has a width bigger than 38, which is not supported",
512: 			                        TypeIdToString(type.InternalType()));
513: 		}
514: 	}
515: 	case LogicalTypeId::ENUM: {
516: 		switch (type.InternalType()) {
517: 		case PhysicalType::UINT8:
518: 			return Value::ENUM(((uint8_t *)data)[index], type);
519: 		case PhysicalType::UINT16:
520: 			return Value::ENUM(((uint16_t *)data)[index], type);
521: 		case PhysicalType::UINT32:
522: 			return Value::ENUM(((uint32_t *)data)[index], type);
523: 		case PhysicalType::UINT64: //  DEDUP_POINTER_ENUM
524: 			return Value::ENUM(((uint64_t *)data)[index], type);
525: 		default:
526: 			throw InternalException("ENUM can only have unsigned integers as physical types");
527: 		}
528: 	}
529: 	case LogicalTypeId::POINTER:
530: 		return Value::POINTER(((uintptr_t *)data)[index]);
531: 	case LogicalTypeId::FLOAT:
532: 		return Value::FLOAT(((float *)data)[index]);
533: 	case LogicalTypeId::DOUBLE:
534: 		return Value::DOUBLE(((double *)data)[index]);
535: 	case LogicalTypeId::INTERVAL:
536: 		return Value::INTERVAL(((interval_t *)data)[index]);
537: 	case LogicalTypeId::VARCHAR: {
538: 		auto str = ((string_t *)data)[index];
539: 		return Value(str.GetString());
540: 	}
541: 	case LogicalTypeId::JSON: {
542: 		auto str = ((string_t *)data)[index];
543: 		return Value::JSON(str.GetString());
544: 	}
545: 	case LogicalTypeId::AGGREGATE_STATE:
546: 	case LogicalTypeId::BLOB: {
547: 		auto str = ((string_t *)data)[index];
548: 		return Value::BLOB((const_data_ptr_t)str.GetDataUnsafe(), str.GetSize());
549: 	}
550: 	case LogicalTypeId::MAP: {
551: 		auto &child_entries = StructVector::GetEntries(*vector);
552: 		Value key = child_entries[0]->GetValue(index);
553: 		Value value = child_entries[1]->GetValue(index);
554: 		return Value::MAP(move(key), move(value));
555: 	}
556: 	case LogicalTypeId::STRUCT: {
557: 		// we can derive the value schema from the vector schema
558: 		auto &child_entries = StructVector::GetEntries(*vector);
559: 		child_list_t<Value> children;
560: 		for (idx_t child_idx = 0; child_idx < child_entries.size(); child_idx++) {
561: 			auto &struct_child = child_entries[child_idx];
562: 			children.push_back(make_pair(StructType::GetChildName(type, child_idx), struct_child->GetValue(index_p)));
563: 		}
564: 		return Value::STRUCT(move(children));
565: 	}
566: 	case LogicalTypeId::LIST: {
567: 		auto offlen = ((list_entry_t *)data)[index];
568: 		auto &child_vec = ListVector::GetEntry(*vector);
569: 		std::vector<Value> children;
570: 		for (idx_t i = offlen.offset; i < offlen.offset + offlen.length; i++) {
571: 			children.push_back(child_vec.GetValue(i));
572: 		}
573: 		return Value::LIST(ListType::GetChildType(type), move(children));
574: 	}
575: 	default:
576: 		throw InternalException("Unimplemented type for value access");
577: 	}
578: }
579: 
580: Value Vector::GetValue(const Vector &v_p, idx_t index_p) {
581: 	auto value = GetValueInternal(v_p, index_p);
582: 	// set the alias of the type to the correct value, if there is a type alias
583: 	if (v_p.GetType().HasAlias()) {
584: 		value.type().CopyAuxInfo(v_p.GetType());
585: 	}
586: 	if (v_p.GetType().id() != LogicalTypeId::AGGREGATE_STATE && value.type().id() != LogicalTypeId::AGGREGATE_STATE) {
587: 		D_ASSERT(v_p.GetType() == value.type());
588: 	}
589: 	return value;
590: }
591: 
592: Value Vector::GetValue(idx_t index) const {
593: 	return GetValue(*this, index);
594: }
595: 
596: // LCOV_EXCL_START
597: string VectorTypeToString(VectorType type) {
598: 	switch (type) {
599: 	case VectorType::FLAT_VECTOR:
600: 		return "FLAT";
601: 	case VectorType::FSST_VECTOR:
602: 		return "FSST";
603: 	case VectorType::SEQUENCE_VECTOR:
604: 		return "SEQUENCE";
605: 	case VectorType::DICTIONARY_VECTOR:
606: 		return "DICTIONARY";
607: 	case VectorType::CONSTANT_VECTOR:
608: 		return "CONSTANT";
609: 	default:
610: 		return "UNKNOWN";
611: 	}
612: }
613: 
614: string Vector::ToString(idx_t count) const {
615: 	string retval =
616: 	    VectorTypeToString(GetVectorType()) + " " + GetType().ToString() + ": " + to_string(count) + " = [ ";
617: 	switch (GetVectorType()) {
618: 	case VectorType::FLAT_VECTOR:
619: 	case VectorType::DICTIONARY_VECTOR:
620: 		for (idx_t i = 0; i < count; i++) {
621: 			retval += GetValue(i).ToString() + (i == count - 1 ? "" : ", ");
622: 		}
623: 		break;
624: 	case VectorType::FSST_VECTOR: {
625: 		for (idx_t i = 0; i < count; i++) {
626: 			string_t compressed_string = ((string_t *)data)[i];
627: 			Value val = FSSTPrimitives::DecompressValue(FSSTVector::GetDecoder(const_cast<Vector &>(*this)),
628: 			                                            (unsigned char *)compressed_string.GetDataUnsafe(),
629: 			                                            compressed_string.GetSize());
630: 			retval += GetValue(i).ToString() + (i == count - 1 ? "" : ", ");
631: 		}
632: 	} break;
633: 	case VectorType::CONSTANT_VECTOR:
634: 		retval += GetValue(0).ToString();
635: 		break;
636: 	case VectorType::SEQUENCE_VECTOR: {
637: 		int64_t start, increment;
638: 		SequenceVector::GetSequence(*this, start, increment);
639: 		for (idx_t i = 0; i < count; i++) {
640: 			retval += to_string(start + increment * i) + (i == count - 1 ? "" : ", ");
641: 		}
642: 		break;
643: 	}
644: 	default:
645: 		retval += "UNKNOWN VECTOR TYPE";
646: 		break;
647: 	}
648: 	retval += "]";
649: 	return retval;
650: }
651: 
652: void Vector::Print(idx_t count) {
653: 	Printer::Print(ToString(count));
654: }
655: 
656: string Vector::ToString() const {
657: 	string retval = VectorTypeToString(GetVectorType()) + " " + GetType().ToString() + ": (UNKNOWN COUNT) [ ";
658: 	switch (GetVectorType()) {
659: 	case VectorType::FLAT_VECTOR:
660: 	case VectorType::DICTIONARY_VECTOR:
661: 		break;
662: 	case VectorType::CONSTANT_VECTOR:
663: 		retval += GetValue(0).ToString();
664: 		break;
665: 	case VectorType::SEQUENCE_VECTOR: {
666: 		break;
667: 	}
668: 	default:
669: 		retval += "UNKNOWN VECTOR TYPE";
670: 		break;
671: 	}
672: 	retval += "]";
673: 	return retval;
674: }
675: 
676: void Vector::Print() {
677: 	Printer::Print(ToString());
678: }
679: // LCOV_EXCL_STOP
680: 
681: template <class T>
682: static void TemplatedFlattenConstantVector(data_ptr_t data, data_ptr_t old_data, idx_t count) {
683: 	auto constant = Load<T>(old_data);
684: 	auto output = (T *)data;
685: 	for (idx_t i = 0; i < count; i++) {
686: 		output[i] = constant;
687: 	}
688: }
689: 
690: void Vector::Flatten(idx_t count) {
691: 	switch (GetVectorType()) {
692: 	case VectorType::FLAT_VECTOR:
693: 		// already a flat vector
694: 		break;
695: 	case VectorType::FSST_VECTOR: {
696: 		// Even though count may only be a part of the vector, we need to flatten the whole thing due to the way
697: 		// ToUnifiedFormat uses flatten
698: 		idx_t total_count = FSSTVector::GetCount(*this);
699: 		// create vector to decompress into
700: 		Vector other(GetType(), total_count);
701: 		// now copy the data of this vector to the other vector, decompressing the strings in the process
702: 		VectorOperations::Copy(*this, other, total_count, 0, 0);
703: 		// create a reference to the data in the other vector
704: 		this->Reference(other);
705: 		break;
706: 	}
707: 	case VectorType::DICTIONARY_VECTOR: {
708: 		// create a new flat vector of this type
709: 		Vector other(GetType(), count);
710: 		// now copy the data of this vector to the other vector, removing the selection vector in the process
711: 		VectorOperations::Copy(*this, other, count, 0, 0);
712: 		// create a reference to the data in the other vector
713: 		this->Reference(other);
714: 		break;
715: 	}
716: 	case VectorType::CONSTANT_VECTOR: {
717: 		bool is_null = ConstantVector::IsNull(*this);
718: 		// allocate a new buffer for the vector
719: 		auto old_buffer = move(buffer);
720: 		auto old_data = data;
721: 		buffer = VectorBuffer::CreateStandardVector(type, MaxValue<idx_t>(STANDARD_VECTOR_SIZE, count));
722: 		data = buffer->GetData();
723: 		vector_type = VectorType::FLAT_VECTOR;
724: 		if (is_null) {
725: 			// constant NULL, set nullmask
726: 			validity.EnsureWritable();
727: 			validity.SetAllInvalid(count);
728: 			return;
729: 		}
730: 		// non-null constant: have to repeat the constant
731: 		switch (GetType().InternalType()) {
732: 		case PhysicalType::BOOL:
733: 			TemplatedFlattenConstantVector<bool>(data, old_data, count);
734: 			break;
735: 		case PhysicalType::INT8:
736: 			TemplatedFlattenConstantVector<int8_t>(data, old_data, count);
737: 			break;
738: 		case PhysicalType::INT16:
739: 			TemplatedFlattenConstantVector<int16_t>(data, old_data, count);
740: 			break;
741: 		case PhysicalType::INT32:
742: 			TemplatedFlattenConstantVector<int32_t>(data, old_data, count);
743: 			break;
744: 		case PhysicalType::INT64:
745: 			TemplatedFlattenConstantVector<int64_t>(data, old_data, count);
746: 			break;
747: 		case PhysicalType::UINT8:
748: 			TemplatedFlattenConstantVector<uint8_t>(data, old_data, count);
749: 			break;
750: 		case PhysicalType::UINT16:
751: 			TemplatedFlattenConstantVector<uint16_t>(data, old_data, count);
752: 			break;
753: 		case PhysicalType::UINT32:
754: 			TemplatedFlattenConstantVector<uint32_t>(data, old_data, count);
755: 			break;
756: 		case PhysicalType::UINT64:
757: 			TemplatedFlattenConstantVector<uint64_t>(data, old_data, count);
758: 			break;
759: 		case PhysicalType::INT128:
760: 			TemplatedFlattenConstantVector<hugeint_t>(data, old_data, count);
761: 			break;
762: 		case PhysicalType::FLOAT:
763: 			TemplatedFlattenConstantVector<float>(data, old_data, count);
764: 			break;
765: 		case PhysicalType::DOUBLE:
766: 			TemplatedFlattenConstantVector<double>(data, old_data, count);
767: 			break;
768: 		case PhysicalType::INTERVAL:
769: 			TemplatedFlattenConstantVector<interval_t>(data, old_data, count);
770: 			break;
771: 		case PhysicalType::VARCHAR:
772: 			TemplatedFlattenConstantVector<string_t>(data, old_data, count);
773: 			break;
774: 		case PhysicalType::LIST: {
775: 			TemplatedFlattenConstantVector<list_entry_t>(data, old_data, count);
776: 			break;
777: 		}
778: 		case PhysicalType::STRUCT: {
779: 			auto normalified_buffer = make_unique<VectorStructBuffer>();
780: 
781: 			auto &new_children = normalified_buffer->GetChildren();
782: 
783: 			auto &child_entries = StructVector::GetEntries(*this);
784: 			for (auto &child : child_entries) {
785: 				D_ASSERT(child->GetVectorType() == VectorType::CONSTANT_VECTOR);
786: 				auto vector = make_unique<Vector>(*child);
787: 				vector->Flatten(count);
788: 				new_children.push_back(move(vector));
789: 			}
790: 			auxiliary = move(normalified_buffer);
791: 		} break;
792: 		default:
793: 			throw InternalException("Unimplemented type for VectorOperations::Flatten");
794: 		}
795: 		break;
796: 	}
797: 	case VectorType::SEQUENCE_VECTOR: {
798: 		int64_t start, increment, sequence_count;
799: 		SequenceVector::GetSequence(*this, start, increment, sequence_count);
800: 
801: 		buffer = VectorBuffer::CreateStandardVector(GetType());
802: 		data = buffer->GetData();
803: 		VectorOperations::GenerateSequence(*this, sequence_count, start, increment);
804: 		break;
805: 	}
806: 	default:
807: 		throw InternalException("Unimplemented type for normalify");
808: 	}
809: }
810: 
811: void Vector::Flatten(const SelectionVector &sel, idx_t count) {
812: 	switch (GetVectorType()) {
813: 	case VectorType::FLAT_VECTOR:
814: 		// already a flat vector
815: 		break;
816: 	case VectorType::FSST_VECTOR: {
817: 		// create a new flat vector of this type
818: 		Vector other(GetType());
819: 		// copy the data of this vector to the other vector, removing compression and selection vector in the process
820: 		VectorOperations::Copy(*this, other, sel, count, 0, 0);
821: 		// create a reference to the data in the other vector
822: 		this->Reference(other);
823: 		break;
824: 	}
825: 	case VectorType::SEQUENCE_VECTOR: {
826: 		int64_t start, increment;
827: 		SequenceVector::GetSequence(*this, start, increment);
828: 
829: 		buffer = VectorBuffer::CreateStandardVector(GetType());
830: 		data = buffer->GetData();
831: 		VectorOperations::GenerateSequence(*this, count, sel, start, increment);
832: 		break;
833: 	}
834: 	default:
835: 		throw InternalException("Unimplemented type for normalify with selection vector");
836: 	}
837: }
838: 
839: void Vector::ToUnifiedFormat(idx_t count, UnifiedVectorFormat &data) {
840: 	switch (GetVectorType()) {
841: 	case VectorType::DICTIONARY_VECTOR: {
842: 		auto &sel = DictionaryVector::SelVector(*this);
843: 		auto &child = DictionaryVector::Child(*this);
844: 		if (child.GetVectorType() == VectorType::FLAT_VECTOR) {
845: 			data.sel = &sel;
846: 			data.data = FlatVector::GetData(child);
847: 			data.validity = FlatVector::Validity(child);
848: 		} else {
849: 			// dictionary with non-flat child: create a new reference to the child and normalify it
850: 			Vector child_vector(child);
851: 			child_vector.Flatten(sel, count);
852: 			auto new_aux = make_buffer<VectorChildBuffer>(move(child_vector));
853: 
854: 			data.sel = &sel;
855: 			data.data = FlatVector::GetData(new_aux->data);
856: 			data.validity = FlatVector::Validity(new_aux->data);
857: 			this->auxiliary = move(new_aux);
858: 		}
859: 		break;
860: 	}
861: 	case VectorType::CONSTANT_VECTOR:
862: 		data.sel = ConstantVector::ZeroSelectionVector(count, data.owned_sel);
863: 		data.data = ConstantVector::GetData(*this);
864: 		data.validity = ConstantVector::Validity(*this);
865: 		break;
866: 	default:
867: 		Flatten(count);
868: 		data.sel = FlatVector::IncrementalSelectionVector();
869: 		data.data = FlatVector::GetData(*this);
870: 		data.validity = FlatVector::Validity(*this);
871: 		break;
872: 	}
873: }
874: 
875: void Vector::Sequence(int64_t start, int64_t increment, idx_t count) {
876: 	this->vector_type = VectorType::SEQUENCE_VECTOR;
877: 	this->buffer = make_buffer<VectorBuffer>(sizeof(int64_t) * 3);
878: 	auto data = (int64_t *)buffer->GetData();
879: 	data[0] = start;
880: 	data[1] = increment;
881: 	data[2] = int64_t(count);
882: 	validity.Reset();
883: 	auxiliary.reset();
884: }
885: 
886: void Vector::Serialize(idx_t count, Serializer &serializer) {
887: 	auto &type = GetType();
888: 
889: 	UnifiedVectorFormat vdata;
890: 	ToUnifiedFormat(count, vdata);
891: 
892: 	const auto write_validity = (count > 0) && !vdata.validity.AllValid();
893: 	serializer.Write<bool>(write_validity);
894: 	if (write_validity) {
895: 		ValidityMask flat_mask(count);
896: 		for (idx_t i = 0; i < count; ++i) {
897: 			auto row_idx = vdata.sel->get_index(i);
898: 			flat_mask.Set(i, vdata.validity.RowIsValid(row_idx));
899: 		}
900: 		serializer.WriteData((const_data_ptr_t)flat_mask.GetData(), flat_mask.ValidityMaskSize(count));
901: 	}
902: 	if (TypeIsConstantSize(type.InternalType())) {
903: 		// constant size type: simple copy
904: 		idx_t write_size = GetTypeIdSize(type.InternalType()) * count;
905: 		auto ptr = unique_ptr<data_t[]>(new data_t[write_size]);
906: 		VectorOperations::WriteToStorage(*this, count, ptr.get());
907: 		serializer.WriteData(ptr.get(), write_size);
908: 	} else {
909: 		switch (type.InternalType()) {
910: 		case PhysicalType::VARCHAR: {
911: 			auto strings = (string_t *)vdata.data;
912: 			for (idx_t i = 0; i < count; i++) {
913: 				auto idx = vdata.sel->get_index(i);
914: 				auto source = !vdata.validity.RowIsValid(idx) ? NullValue<string_t>() : strings[idx];
915: 				serializer.WriteStringLen((const_data_ptr_t)source.GetDataUnsafe(), source.GetSize());
916: 			}
917: 			break;
918: 		}
919: 		case PhysicalType::STRUCT: {
920: 			Flatten(count);
921: 			auto &entries = StructVector::GetEntries(*this);
922: 			for (auto &entry : entries) {
923: 				entry->Serialize(count, serializer);
924: 			}
925: 			break;
926: 		}
927: 		case PhysicalType::LIST: {
928: 			auto &child = ListVector::GetEntry(*this);
929: 			auto list_size = ListVector::GetListSize(*this);
930: 
931: 			// serialize the list entries in a flat array
932: 			auto data = unique_ptr<list_entry_t[]>(new list_entry_t[count]);
933: 			auto source_array = (list_entry_t *)vdata.data;
934: 			for (idx_t i = 0; i < count; i++) {
935: 				auto idx = vdata.sel->get_index(i);
936: 				auto source = source_array[idx];
937: 				data[i].offset = source.offset;
938: 				data[i].length = source.length;
939: 			}
940: 
941: 			// write the list size
942: 			serializer.Write<idx_t>(list_size);
943: 			serializer.WriteData((data_ptr_t)data.get(), count * sizeof(list_entry_t));
944: 
945: 			child.Serialize(list_size, serializer);
946: 			break;
947: 		}
948: 		default:
949: 			throw InternalException("Unimplemented variable width type for Vector::Serialize!");
950: 		}
951: 	}
952: }
953: 
954: void Vector::Deserialize(idx_t count, Deserializer &source) {
955: 	auto &type = GetType();
956: 
957: 	auto &validity = FlatVector::Validity(*this);
958: 	validity.Reset();
959: 	const auto has_validity = source.Read<bool>();
960: 	if (has_validity) {
961: 		validity.Initialize(count);
962: 		source.ReadData((data_ptr_t)validity.GetData(), validity.ValidityMaskSize(count));
963: 	}
964: 
965: 	if (TypeIsConstantSize(type.InternalType())) {
966: 		// constant size type: read fixed amount of data from
967: 		auto column_size = GetTypeIdSize(type.InternalType()) * count;
968: 		auto ptr = unique_ptr<data_t[]>(new data_t[column_size]);
969: 		source.ReadData(ptr.get(), column_size);
970: 
971: 		VectorOperations::ReadFromStorage(ptr.get(), count, *this);
972: 	} else {
973: 		switch (type.InternalType()) {
974: 		case PhysicalType::VARCHAR: {
975: 			auto strings = FlatVector::GetData<string_t>(*this);
976: 			for (idx_t i = 0; i < count; i++) {
977: 				// read the strings
978: 				auto str = source.Read<string>();
979: 				// now add the string to the StringHeap of the vector
980: 				// and write the pointer into the vector
981: 				if (validity.RowIsValid(i)) {
982: 					strings[i] = StringVector::AddStringOrBlob(*this, str);
983: 				}
984: 			}
985: 			break;
986: 		}
987: 		case PhysicalType::STRUCT: {
988: 			auto &entries = StructVector::GetEntries(*this);
989: 			for (auto &entry : entries) {
990: 				entry->Deserialize(count, source);
991: 			}
992: 			break;
993: 		}
994: 		case PhysicalType::LIST: {
995: 			// read the list size
996: 			auto list_size = source.Read<idx_t>();
997: 			ListVector::Reserve(*this, list_size);
998: 			ListVector::SetListSize(*this, list_size);
999: 
1000: 			// read the list entry
1001: 			auto list_entries = FlatVector::GetData(*this);
1002: 			source.ReadData(list_entries, count * sizeof(list_entry_t));
1003: 
1004: 			// deserialize the child vector
1005: 			auto &child = ListVector::GetEntry(*this);
1006: 			child.Deserialize(list_size, source);
1007: 
1008: 			break;
1009: 		}
1010: 		default:
1011: 			throw InternalException("Unimplemented variable width type for Vector::Deserialize!");
1012: 		}
1013: 	}
1014: }
1015: 
1016: void Vector::SetVectorType(VectorType vector_type_p) {
1017: 	this->vector_type = vector_type_p;
1018: 	if (TypeIsConstantSize(GetType().InternalType()) &&
1019: 	    (GetVectorType() == VectorType::CONSTANT_VECTOR || GetVectorType() == VectorType::FLAT_VECTOR)) {
1020: 		auxiliary.reset();
1021: 	}
1022: 	if (vector_type == VectorType::CONSTANT_VECTOR && GetType().InternalType() == PhysicalType::STRUCT) {
1023: 		auto &entries = StructVector::GetEntries(*this);
1024: 		for (auto &entry : entries) {
1025: 			entry->SetVectorType(vector_type);
1026: 		}
1027: 	}
1028: }
1029: 
1030: void Vector::UTFVerify(const SelectionVector &sel, idx_t count) {
1031: #ifdef DEBUG
1032: 	if (count == 0) {
1033: 		return;
1034: 	}
1035: 	if (GetType().InternalType() == PhysicalType::VARCHAR) {
1036: 		// we just touch all the strings and let the sanitizer figure out if any
1037: 		// of them are deallocated/corrupt
1038: 		switch (GetVectorType()) {
1039: 		case VectorType::CONSTANT_VECTOR: {
1040: 			auto string = ConstantVector::GetData<string_t>(*this);
1041: 			if (!ConstantVector::IsNull(*this)) {
1042: 				string->Verify();
1043: 			}
1044: 			break;
1045: 		}
1046: 		case VectorType::FLAT_VECTOR: {
1047: 			auto strings = FlatVector::GetData<string_t>(*this);
1048: 			for (idx_t i = 0; i < count; i++) {
1049: 				auto oidx = sel.get_index(i);
1050: 				if (validity.RowIsValid(oidx)) {
1051: 					strings[oidx].Verify();
1052: 				}
1053: 			}
1054: 			break;
1055: 		}
1056: 		default:
1057: 			break;
1058: 		}
1059: 	}
1060: #endif
1061: }
1062: 
1063: void Vector::UTFVerify(idx_t count) {
1064: 	auto flat_sel = FlatVector::IncrementalSelectionVector();
1065: 
1066: 	UTFVerify(*flat_sel, count);
1067: }
1068: 
1069: void Vector::VerifyMap(Vector &vector_p, const SelectionVector &sel_p, idx_t count) {
1070: #ifdef DEBUG
1071: 	D_ASSERT(vector_p.GetType().id() == LogicalTypeId::MAP);
1072: 	auto valid_check = CheckMapValidity(vector_p, count, sel_p);
1073: 	D_ASSERT(valid_check == MapInvalidReason::VALID);
1074: #endif // DEBUG
1075: }
1076: 
1077: void Vector::Verify(Vector &vector_p, const SelectionVector &sel_p, idx_t count) {
1078: #ifdef DEBUG
1079: 	if (count == 0) {
1080: 		return;
1081: 	}
1082: 	Vector *vector = &vector_p;
1083: 	const SelectionVector *sel = &sel_p;
1084: 	SelectionVector owned_sel;
1085: 	auto &type = vector->GetType();
1086: 	auto vtype = vector->GetVectorType();
1087: 	if (vector->GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1088: 		auto &child = DictionaryVector::Child(*vector);
1089: 		D_ASSERT(child.GetVectorType() != VectorType::DICTIONARY_VECTOR);
1090: 		auto &dict_sel = DictionaryVector::SelVector(*vector);
1091: 		// merge the selection vectors and verify the child
1092: 		auto new_buffer = dict_sel.Slice(*sel, count);
1093: 		owned_sel.Initialize(new_buffer);
1094: 		sel = &owned_sel;
1095: 		vector = &child;
1096: 		vtype = vector->GetVectorType();
1097: 	}
1098: 	if (TypeIsConstantSize(type.InternalType()) &&
1099: 	    (vtype == VectorType::CONSTANT_VECTOR || vtype == VectorType::FLAT_VECTOR)) {
1100: 		D_ASSERT(!vector->auxiliary);
1101: 	}
1102: 	if (type.id() == LogicalTypeId::VARCHAR || type.id() == LogicalTypeId::JSON) {
1103: 		// verify that there are no '\0' bytes in string values
1104: 		switch (vtype) {
1105: 		case VectorType::FLAT_VECTOR: {
1106: 			auto &validity = FlatVector::Validity(*vector);
1107: 			auto strings = FlatVector::GetData<string_t>(*vector);
1108: 			for (idx_t i = 0; i < count; i++) {
1109: 				auto oidx = sel->get_index(i);
1110: 				if (validity.RowIsValid(oidx)) {
1111: 					strings[oidx].VerifyNull();
1112: 				}
1113: 			}
1114: 			break;
1115: 		}
1116: 		default:
1117: 			break;
1118: 		}
1119: 	}
1120: 
1121: 	if (type.InternalType() == PhysicalType::STRUCT) {
1122: 		auto &child_types = StructType::GetChildTypes(type);
1123: 		D_ASSERT(!child_types.empty());
1124: 		// create a selection vector of the non-null entries of the struct vector
1125: 		auto &children = StructVector::GetEntries(*vector);
1126: 		D_ASSERT(child_types.size() == children.size());
1127: 		for (idx_t child_idx = 0; child_idx < children.size(); child_idx++) {
1128: 			D_ASSERT(children[child_idx]->GetType() == child_types[child_idx].second);
1129: 			Vector::Verify(*children[child_idx], sel_p, count);
1130: 			if (vtype == VectorType::CONSTANT_VECTOR) {
1131: 				D_ASSERT(children[child_idx]->GetVectorType() == VectorType::CONSTANT_VECTOR);
1132: 				if (ConstantVector::IsNull(*vector)) {
1133: 					D_ASSERT(ConstantVector::IsNull(*children[child_idx]));
1134: 				}
1135: 			}
1136: 			if (vtype != VectorType::FLAT_VECTOR) {
1137: 				continue;
1138: 			}
1139: 			ValidityMask *child_validity;
1140: 			SelectionVector owned_child_sel;
1141: 			const SelectionVector *child_sel = &owned_child_sel;
1142: 			if (children[child_idx]->GetVectorType() == VectorType::FLAT_VECTOR) {
1143: 				child_sel = FlatVector::IncrementalSelectionVector();
1144: 				child_validity = &FlatVector::Validity(*children[child_idx]);
1145: 			} else if (children[child_idx]->GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1146: 				auto &child = DictionaryVector::Child(*children[child_idx]);
1147: 				if (child.GetVectorType() != VectorType::FLAT_VECTOR) {
1148: 					continue;
1149: 				}
1150: 				child_validity = &FlatVector::Validity(child);
1151: 				child_sel = &DictionaryVector::SelVector(*children[child_idx]);
1152: 			} else if (children[child_idx]->GetVectorType() == VectorType::CONSTANT_VECTOR) {
1153: 				child_sel = ConstantVector::ZeroSelectionVector(count, owned_child_sel);
1154: 				child_validity = &ConstantVector::Validity(*children[child_idx]);
1155: 			} else {
1156: 				continue;
1157: 			}
1158: 			// for any NULL entry in the struct, the child should be NULL as well
1159: 			auto &validity = FlatVector::Validity(*vector);
1160: 			for (idx_t i = 0; i < count; i++) {
1161: 				auto index = sel->get_index(i);
1162: 				if (!validity.RowIsValid(index)) {
1163: 					auto child_index = child_sel->get_index(sel_p.get_index(i));
1164: 					D_ASSERT(!child_validity->RowIsValid(child_index));
1165: 				}
1166: 			}
1167: 		}
1168: 		if (vector->GetType().id() == LogicalTypeId::MAP) {
1169: 			VerifyMap(*vector, *sel, count);
1170: 		}
1171: 	}
1172: 
1173: 	if (type.InternalType() == PhysicalType::LIST) {
1174: 		if (vtype == VectorType::CONSTANT_VECTOR) {
1175: 			if (!ConstantVector::IsNull(*vector)) {
1176: 				auto &child = ListVector::GetEntry(*vector);
1177: 				SelectionVector child_sel(ListVector::GetListSize(*vector));
1178: 				idx_t child_count = 0;
1179: 				auto le = ConstantVector::GetData<list_entry_t>(*vector);
1180: 				D_ASSERT(le->offset + le->length <= ListVector::GetListSize(*vector));
1181: 				for (idx_t k = 0; k < le->length; k++) {
1182: 					child_sel.set_index(child_count++, le->offset + k);
1183: 				}
1184: 				Vector::Verify(child, child_sel, child_count);
1185: 			}
1186: 		} else if (vtype == VectorType::FLAT_VECTOR) {
1187: 			auto &validity = FlatVector::Validity(*vector);
1188: 			auto &child = ListVector::GetEntry(*vector);
1189: 			auto child_size = ListVector::GetListSize(*vector);
1190: 			auto list_data = FlatVector::GetData<list_entry_t>(*vector);
1191: 			idx_t total_size = 0;
1192: 			for (idx_t i = 0; i < count; i++) {
1193: 				auto idx = sel->get_index(i);
1194: 				auto &le = list_data[idx];
1195: 				if (validity.RowIsValid(idx)) {
1196: 					D_ASSERT(le.offset + le.length <= child_size);
1197: 					total_size += le.length;
1198: 				}
1199: 			}
1200: 			SelectionVector child_sel(total_size);
1201: 			idx_t child_count = 0;
1202: 			for (idx_t i = 0; i < count; i++) {
1203: 				auto idx = sel->get_index(i);
1204: 				auto &le = list_data[idx];
1205: 				if (validity.RowIsValid(idx)) {
1206: 					D_ASSERT(le.offset + le.length <= child_size);
1207: 					for (idx_t k = 0; k < le.length; k++) {
1208: 						child_sel.set_index(child_count++, le.offset + k);
1209: 					}
1210: 				}
1211: 			}
1212: 			Vector::Verify(child, child_sel, child_count);
1213: 		}
1214: 	}
1215: #endif
1216: }
1217: 
1218: void Vector::Verify(idx_t count) {
1219: 	auto flat_sel = FlatVector::IncrementalSelectionVector();
1220: 	Verify(*this, *flat_sel, count);
1221: }
1222: 
1223: void FlatVector::SetNull(Vector &vector, idx_t idx, bool is_null) {
1224: 	D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
1225: 	vector.validity.Set(idx, !is_null);
1226: 	if (is_null && vector.GetType().InternalType() == PhysicalType::STRUCT) {
1227: 		// set all child entries to null as well
1228: 		auto &entries = StructVector::GetEntries(vector);
1229: 		for (auto &entry : entries) {
1230: 			FlatVector::SetNull(*entry, idx, is_null);
1231: 		}
1232: 	}
1233: }
1234: 
1235: void ConstantVector::SetNull(Vector &vector, bool is_null) {
1236: 	D_ASSERT(vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1237: 	vector.validity.Set(0, !is_null);
1238: 	if (is_null && vector.GetType().InternalType() == PhysicalType::STRUCT) {
1239: 		// set all child entries to null as well
1240: 		auto &entries = StructVector::GetEntries(vector);
1241: 		for (auto &entry : entries) {
1242: 			entry->SetVectorType(VectorType::CONSTANT_VECTOR);
1243: 			ConstantVector::SetNull(*entry, is_null);
1244: 		}
1245: 	}
1246: }
1247: 
1248: const SelectionVector *ConstantVector::ZeroSelectionVector(idx_t count, SelectionVector &owned_sel) {
1249: 	if (count <= STANDARD_VECTOR_SIZE) {
1250: 		return ConstantVector::ZeroSelectionVector();
1251: 	}
1252: 	owned_sel.Initialize(count);
1253: 	for (idx_t i = 0; i < count; i++) {
1254: 		owned_sel.set_index(i, 0);
1255: 	}
1256: 	return &owned_sel;
1257: }
1258: 
1259: void ConstantVector::Reference(Vector &vector, Vector &source, idx_t position, idx_t count) {
1260: 	auto &source_type = source.GetType();
1261: 	switch (source_type.InternalType()) {
1262: 	case PhysicalType::LIST: {
1263: 		// retrieve the list entry from the source vector
1264: 		UnifiedVectorFormat vdata;
1265: 		source.ToUnifiedFormat(count, vdata);
1266: 
1267: 		auto list_index = vdata.sel->get_index(position);
1268: 		if (!vdata.validity.RowIsValid(list_index)) {
1269: 			// list is null: create null value
1270: 			Value null_value(source_type);
1271: 			vector.Reference(null_value);
1272: 			break;
1273: 		}
1274: 
1275: 		auto list_data = (list_entry_t *)vdata.data;
1276: 		auto list_entry = list_data[list_index];
1277: 
1278: 		// add the list entry as the first element of "vector"
1279: 		// FIXME: we only need to allocate space for 1 tuple here
1280: 		auto target_data = FlatVector::GetData<list_entry_t>(vector);
1281: 		target_data[0] = list_entry;
1282: 
1283: 		// create a reference to the child list of the source vector
1284: 		auto &child = ListVector::GetEntry(vector);
1285: 		child.Reference(ListVector::GetEntry(source));
1286: 
1287: 		ListVector::SetListSize(vector, ListVector::GetListSize(source));
1288: 		vector.SetVectorType(VectorType::CONSTANT_VECTOR);
1289: 		break;
1290: 	}
1291: 	case PhysicalType::STRUCT: {
1292: 		UnifiedVectorFormat vdata;
1293: 		source.ToUnifiedFormat(count, vdata);
1294: 
1295: 		auto struct_index = vdata.sel->get_index(position);
1296: 		if (!vdata.validity.RowIsValid(struct_index)) {
1297: 			// null struct: create null value
1298: 			Value null_value(source_type);
1299: 			vector.Reference(null_value);
1300: 			break;
1301: 		}
1302: 
1303: 		// struct: pass constant reference into child entries
1304: 		auto &source_entries = StructVector::GetEntries(source);
1305: 		auto &target_entries = StructVector::GetEntries(vector);
1306: 		for (idx_t i = 0; i < source_entries.size(); i++) {
1307: 			ConstantVector::Reference(*target_entries[i], *source_entries[i], position, count);
1308: 		}
1309: 		vector.SetVectorType(VectorType::CONSTANT_VECTOR);
1310: 		break;
1311: 	}
1312: 	default:
1313: 		// default behavior: get a value from the vector and reference it
1314: 		// this is not that expensive for scalar types
1315: 		auto value = source.GetValue(position);
1316: 		vector.Reference(value);
1317: 		D_ASSERT(vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1318: 		break;
1319: 	}
1320: }
1321: 
1322: string_t StringVector::AddString(Vector &vector, const char *data, idx_t len) {
1323: 	return StringVector::AddString(vector, string_t(data, len));
1324: }
1325: 
1326: string_t StringVector::AddStringOrBlob(Vector &vector, const char *data, idx_t len) {
1327: 	return StringVector::AddStringOrBlob(vector, string_t(data, len));
1328: }
1329: 
1330: string_t StringVector::AddString(Vector &vector, const char *data) {
1331: 	return StringVector::AddString(vector, string_t(data, strlen(data)));
1332: }
1333: 
1334: string_t StringVector::AddString(Vector &vector, const string &data) {
1335: 	return StringVector::AddString(vector, string_t(data.c_str(), data.size()));
1336: }
1337: 
1338: string_t StringVector::AddString(Vector &vector, string_t data) {
1339: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::VARCHAR || vector.GetType().id() == LogicalTypeId::JSON);
1340: 	if (data.IsInlined()) {
1341: 		// string will be inlined: no need to store in string heap
1342: 		return data;
1343: 	}
1344: 	if (!vector.auxiliary) {
1345: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
1346: 	}
1347: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::STRING_BUFFER);
1348: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
1349: 	return string_buffer.AddString(data);
1350: }
1351: 
1352: string_t StringVector::AddStringOrBlob(Vector &vector, string_t data) {
1353: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1354: 	if (data.IsInlined()) {
1355: 		// string will be inlined: no need to store in string heap
1356: 		return data;
1357: 	}
1358: 	if (!vector.auxiliary) {
1359: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
1360: 	}
1361: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::STRING_BUFFER);
1362: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
1363: 	return string_buffer.AddBlob(data);
1364: }
1365: 
1366: string_t StringVector::EmptyString(Vector &vector, idx_t len) {
1367: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1368: 	if (len < string_t::INLINE_LENGTH) {
1369: 		return string_t(len);
1370: 	}
1371: 	if (!vector.auxiliary) {
1372: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
1373: 	}
1374: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::STRING_BUFFER);
1375: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
1376: 	return string_buffer.EmptyString(len);
1377: }
1378: 
1379: void StringVector::AddHandle(Vector &vector, BufferHandle handle) {
1380: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1381: 	if (!vector.auxiliary) {
1382: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
1383: 	}
1384: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
1385: 	string_buffer.AddHeapReference(make_buffer<ManagedVectorBuffer>(move(handle)));
1386: }
1387: 
1388: void StringVector::AddBuffer(Vector &vector, buffer_ptr<VectorBuffer> buffer) {
1389: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1390: 	D_ASSERT(buffer.get() != vector.auxiliary.get());
1391: 	if (!vector.auxiliary) {
1392: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
1393: 	}
1394: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
1395: 	string_buffer.AddHeapReference(move(buffer));
1396: }
1397: 
1398: void StringVector::AddHeapReference(Vector &vector, Vector &other) {
1399: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1400: 	D_ASSERT(other.GetType().InternalType() == PhysicalType::VARCHAR);
1401: 
1402: 	if (other.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1403: 		StringVector::AddHeapReference(vector, DictionaryVector::Child(other));
1404: 		return;
1405: 	}
1406: 	if (!other.auxiliary) {
1407: 		return;
1408: 	}
1409: 	StringVector::AddBuffer(vector, other.auxiliary);
1410: }
1411: 
1412: string_t FSSTVector::AddCompressedString(Vector &vector, const char *data, idx_t len) {
1413: 	return FSSTVector::AddCompressedString(vector, string_t(data, len));
1414: }
1415: 
1416: string_t FSSTVector::AddCompressedString(Vector &vector, string_t data) {
1417: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1418: 	if (data.IsInlined()) {
1419: 		// string will be inlined: no need to store in string heap
1420: 		return data;
1421: 	}
1422: 	if (!vector.auxiliary) {
1423: 		vector.auxiliary = make_buffer<VectorFSSTStringBuffer>();
1424: 	}
1425: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::FSST_BUFFER);
1426: 	auto &fsst_string_buffer = (VectorFSSTStringBuffer &)*vector.auxiliary;
1427: 	return fsst_string_buffer.AddBlob(data);
1428: }
1429: 
1430: void *FSSTVector::GetDecoder(const Vector &vector) {
1431: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1432: 	if (!vector.auxiliary) {
1433: 		throw InternalException("GetDecoder called on FSST Vector without registered buffer");
1434: 	}
1435: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::FSST_BUFFER);
1436: 	auto &fsst_string_buffer = (VectorFSSTStringBuffer &)*vector.auxiliary;
1437: 	return (duckdb_fsst_decoder_t *)fsst_string_buffer.GetDecoder();
1438: }
1439: 
1440: void FSSTVector::RegisterDecoder(Vector &vector, buffer_ptr<void> &duckdb_fsst_decoder) {
1441: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1442: 
1443: 	if (!vector.auxiliary) {
1444: 		vector.auxiliary = make_buffer<VectorFSSTStringBuffer>();
1445: 	}
1446: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::FSST_BUFFER);
1447: 
1448: 	auto &fsst_string_buffer = (VectorFSSTStringBuffer &)*vector.auxiliary;
1449: 	fsst_string_buffer.AddDecoder(duckdb_fsst_decoder);
1450: }
1451: 
1452: void FSSTVector::SetCount(Vector &vector, idx_t count) {
1453: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1454: 
1455: 	if (!vector.auxiliary) {
1456: 		vector.auxiliary = make_buffer<VectorFSSTStringBuffer>();
1457: 	}
1458: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::FSST_BUFFER);
1459: 
1460: 	auto &fsst_string_buffer = (VectorFSSTStringBuffer &)*vector.auxiliary;
1461: 	fsst_string_buffer.SetCount(count);
1462: }
1463: 
1464: idx_t FSSTVector::GetCount(Vector &vector) {
1465: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1466: 
1467: 	if (!vector.auxiliary) {
1468: 		vector.auxiliary = make_buffer<VectorFSSTStringBuffer>();
1469: 	}
1470: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::FSST_BUFFER);
1471: 
1472: 	auto &fsst_string_buffer = (VectorFSSTStringBuffer &)*vector.auxiliary;
1473: 	return fsst_string_buffer.GetCount();
1474: }
1475: 
1476: void FSSTVector::DecompressVector(const Vector &src, Vector &dst, idx_t src_offset, idx_t dst_offset, idx_t copy_count,
1477:                                   const SelectionVector *sel) {
1478: 	D_ASSERT(src.GetVectorType() == VectorType::FSST_VECTOR);
1479: 	D_ASSERT(dst.GetVectorType() == VectorType::FLAT_VECTOR);
1480: 	auto dst_mask = FlatVector::Validity(dst);
1481: 	auto ldata = FSSTVector::GetCompressedData<string_t>(src);
1482: 	auto tdata = FlatVector::GetData<string_t>(dst);
1483: 	for (idx_t i = 0; i < copy_count; i++) {
1484: 		auto source_idx = sel->get_index(src_offset + i);
1485: 		auto target_idx = dst_offset + i;
1486: 		string_t compressed_string = ldata[source_idx];
1487: 		if (dst_mask.RowIsValid(target_idx) && compressed_string.GetSize() > 0) {
1488: 			tdata[target_idx] = FSSTPrimitives::DecompressValue(FSSTVector::GetDecoder(src), dst,
1489: 			                                                    (unsigned char *)compressed_string.GetDataUnsafe(),
1490: 			                                                    compressed_string.GetSize());
1491: 		} else {
1492: 			tdata[target_idx] = string_t(nullptr, 0);
1493: 		}
1494: 	}
1495: }
1496: 
1497: Vector &MapVector::GetKeys(Vector &vector) {
1498: 	auto &entries = StructVector::GetEntries(vector);
1499: 	D_ASSERT(entries.size() == 2);
1500: 	return *entries[0];
1501: }
1502: Vector &MapVector::GetValues(Vector &vector) {
1503: 	auto &entries = StructVector::GetEntries(vector);
1504: 	D_ASSERT(entries.size() == 2);
1505: 	return *entries[1];
1506: }
1507: 
1508: const Vector &MapVector::GetKeys(const Vector &vector) {
1509: 	return GetKeys((Vector &)vector);
1510: }
1511: const Vector &MapVector::GetValues(const Vector &vector) {
1512: 	return GetValues((Vector &)vector);
1513: }
1514: 
1515: vector<unique_ptr<Vector>> &StructVector::GetEntries(Vector &vector) {
1516: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::STRUCT || vector.GetType().id() == LogicalTypeId::MAP);
1517: 	if (vector.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1518: 		auto &child = DictionaryVector::Child(vector);
1519: 		return StructVector::GetEntries(child);
1520: 	}
1521: 	D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR ||
1522: 	         vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1523: 	D_ASSERT(vector.auxiliary);
1524: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::STRUCT_BUFFER);
1525: 	return ((VectorStructBuffer *)vector.auxiliary.get())->GetChildren();
1526: }
1527: 
1528: const vector<unique_ptr<Vector>> &StructVector::GetEntries(const Vector &vector) {
1529: 	return GetEntries((Vector &)vector);
1530: }
1531: 
1532: const Vector &ListVector::GetEntry(const Vector &vector) {
1533: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::LIST);
1534: 	if (vector.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1535: 		auto &child = DictionaryVector::Child(vector);
1536: 		return ListVector::GetEntry(child);
1537: 	}
1538: 	D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR ||
1539: 	         vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1540: 	D_ASSERT(vector.auxiliary);
1541: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::LIST_BUFFER);
1542: 	return ((VectorListBuffer *)vector.auxiliary.get())->GetChild();
1543: }
1544: 
1545: Vector &ListVector::GetEntry(Vector &vector) {
1546: 	const Vector &cvector = vector;
1547: 	return const_cast<Vector &>(ListVector::GetEntry(cvector));
1548: }
1549: 
1550: void ListVector::Reserve(Vector &vector, idx_t required_capacity) {
1551: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::LIST);
1552: 	D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR ||
1553: 	         vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1554: 	D_ASSERT(vector.auxiliary);
1555: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::LIST_BUFFER);
1556: 	auto &child_buffer = *((VectorListBuffer *)vector.auxiliary.get());
1557: 	child_buffer.Reserve(required_capacity);
1558: }
1559: 
1560: template <class T>
1561: void TemplatedSearchInMap(Vector &list, T key, vector<idx_t> &offsets, bool is_key_null, idx_t offset, idx_t length) {
1562: 	auto &list_vector = ListVector::GetEntry(list);
1563: 	UnifiedVectorFormat vector_data;
1564: 	list_vector.ToUnifiedFormat(ListVector::GetListSize(list), vector_data);
1565: 	auto data = (T *)vector_data.data;
1566: 	auto validity_mask = vector_data.validity;
1567: 
1568: 	if (is_key_null) {
1569: 		for (idx_t i = offset; i < offset + length; i++) {
1570: 			if (!validity_mask.RowIsValid(i)) {
1571: 				offsets.push_back(i);
1572: 			}
1573: 		}
1574: 	} else {
1575: 		for (idx_t i = offset; i < offset + length; i++) {
1576: 			if (!validity_mask.RowIsValid(i)) {
1577: 				continue;
1578: 			}
1579: 			if (key == data[i]) {
1580: 				offsets.push_back(i);
1581: 			}
1582: 		}
1583: 	}
1584: }
1585: 
1586: template <class T>
1587: void TemplatedSearchInMap(Vector &list, const Value &key, vector<idx_t> &offsets, bool is_key_null, idx_t offset,
1588:                           idx_t length) {
1589: 	TemplatedSearchInMap<T>(list, key.template GetValueUnsafe<T>(), offsets, is_key_null, offset, length);
1590: }
1591: 
1592: void SearchStringInMap(Vector &list, const string &key, vector<idx_t> &offsets, bool is_key_null, idx_t offset,
1593:                        idx_t length) {
1594: 	auto &list_vector = ListVector::GetEntry(list);
1595: 	UnifiedVectorFormat vector_data;
1596: 	list_vector.ToUnifiedFormat(ListVector::GetListSize(list), vector_data);
1597: 	auto data = (string_t *)vector_data.data;
1598: 	auto validity_mask = vector_data.validity;
1599: 	if (is_key_null) {
1600: 		for (idx_t i = offset; i < offset + length; i++) {
1601: 			if (!validity_mask.RowIsValid(i)) {
1602: 				offsets.push_back(i);
1603: 			}
1604: 		}
1605: 	} else {
1606: 		string_t key_str_t(key);
1607: 		for (idx_t i = offset; i < offset + length; i++) {
1608: 			if (!validity_mask.RowIsValid(i)) {
1609: 				continue;
1610: 			}
1611: 			if (Equals::Operation<string_t>(data[i], key_str_t)) {
1612: 				offsets.push_back(i);
1613: 			}
1614: 		}
1615: 	}
1616: }
1617: 
1618: vector<idx_t> ListVector::Search(Vector &list, const Value &key, idx_t row) {
1619: 	vector<idx_t> offsets;
1620: 
1621: 	auto &list_vector = ListVector::GetEntry(list);
1622: 	auto &entry = ListVector::GetData(list)[row];
1623: 
1624: 	switch (list_vector.GetType().InternalType()) {
1625: 	case PhysicalType::BOOL:
1626: 	case PhysicalType::INT8:
1627: 		TemplatedSearchInMap<int8_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1628: 		break;
1629: 	case PhysicalType::INT16:
1630: 		TemplatedSearchInMap<int16_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1631: 		break;
1632: 	case PhysicalType::INT32:
1633: 		TemplatedSearchInMap<int32_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1634: 		break;
1635: 	case PhysicalType::INT64:
1636: 		TemplatedSearchInMap<int64_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1637: 		break;
1638: 	case PhysicalType::INT128:
1639: 		TemplatedSearchInMap<hugeint_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1640: 		break;
1641: 	case PhysicalType::UINT8:
1642: 		TemplatedSearchInMap<uint8_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1643: 		break;
1644: 	case PhysicalType::UINT16:
1645: 		TemplatedSearchInMap<uint16_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1646: 		break;
1647: 	case PhysicalType::UINT32:
1648: 		TemplatedSearchInMap<uint32_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1649: 		break;
1650: 	case PhysicalType::UINT64:
1651: 		TemplatedSearchInMap<uint64_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1652: 		break;
1653: 	case PhysicalType::FLOAT:
1654: 		TemplatedSearchInMap<float>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1655: 		break;
1656: 	case PhysicalType::DOUBLE:
1657: 		TemplatedSearchInMap<double>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1658: 		break;
1659: 	case PhysicalType::VARCHAR:
1660: 		SearchStringInMap(list, StringValue::Get(key), offsets, key.IsNull(), entry.offset, entry.length);
1661: 		break;
1662: 	default:
1663: 		throw InvalidTypeException(list.GetType().id(), "Invalid type for List Vector Search");
1664: 	}
1665: 	return offsets;
1666: }
1667: 
1668: Value ListVector::GetValuesFromOffsets(Vector &list, vector<idx_t> &offsets) {
1669: 	auto &child_vec = ListVector::GetEntry(list);
1670: 	vector<Value> list_values;
1671: 	list_values.reserve(offsets.size());
1672: 	for (auto &offset : offsets) {
1673: 		list_values.push_back(child_vec.GetValue(offset));
1674: 	}
1675: 	return Value::LIST(ListType::GetChildType(list.GetType()), move(list_values));
1676: }
1677: 
1678: idx_t ListVector::GetListSize(const Vector &vec) {
1679: 	if (vec.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1680: 		auto &child = DictionaryVector::Child(vec);
1681: 		return ListVector::GetListSize(child);
1682: 	}
1683: 	D_ASSERT(vec.auxiliary);
1684: 	return ((VectorListBuffer &)*vec.auxiliary).size;
1685: }
1686: 
1687: void ListVector::ReferenceEntry(Vector &vector, Vector &other) {
1688: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::LIST);
1689: 	D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR ||
1690: 	         vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1691: 	D_ASSERT(other.GetType().id() == LogicalTypeId::LIST);
1692: 	D_ASSERT(other.GetVectorType() == VectorType::FLAT_VECTOR || other.GetVectorType() == VectorType::CONSTANT_VECTOR);
1693: 	vector.auxiliary = other.auxiliary;
1694: }
1695: 
1696: void ListVector::SetListSize(Vector &vec, idx_t size) {
1697: 	if (vec.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1698: 		auto &child = DictionaryVector::Child(vec);
1699: 		ListVector::SetListSize(child, size);
1700: 	}
1701: 	((VectorListBuffer &)*vec.auxiliary).size = size;
1702: }
1703: 
1704: void ListVector::Append(Vector &target, const Vector &source, idx_t source_size, idx_t source_offset) {
1705: 	if (source_size - source_offset == 0) {
1706: 		//! Nothing to add
1707: 		return;
1708: 	}
1709: 	auto &target_buffer = (VectorListBuffer &)*target.auxiliary;
1710: 	target_buffer.Append(source, source_size, source_offset);
1711: }
1712: 
1713: void ListVector::Append(Vector &target, const Vector &source, const SelectionVector &sel, idx_t source_size,
1714:                         idx_t source_offset) {
1715: 	if (source_size - source_offset == 0) {
1716: 		//! Nothing to add
1717: 		return;
1718: 	}
1719: 	auto &target_buffer = (VectorListBuffer &)*target.auxiliary;
1720: 	target_buffer.Append(source, sel, source_size, source_offset);
1721: }
1722: 
1723: void ListVector::PushBack(Vector &target, const Value &insert) {
1724: 	auto &target_buffer = (VectorListBuffer &)*target.auxiliary;
1725: 	target_buffer.PushBack(insert);
1726: }
1727: 
1728: } // namespace duckdb
[end of src/common/types/vector.cpp]
[start of src/execution/operator/aggregate/physical_window.cpp]
1: #include "duckdb/execution/operator/aggregate/physical_window.hpp"
2: 
3: #include "duckdb/common/operator/cast_operators.hpp"
4: #include "duckdb/common/operator/comparison_operators.hpp"
5: #include "duckdb/common/row_operations/row_operations.hpp"
6: #include "duckdb/common/sort/sort.hpp"
7: #include "duckdb/common/types/chunk_collection.hpp"
8: #include "duckdb/common/types/row_data_collection_scanner.hpp"
9: #include "duckdb/common/vector_operations/vector_operations.hpp"
10: #include "duckdb/common/windows_undefs.hpp"
11: #include "duckdb/execution/expression_executor.hpp"
12: #include "duckdb/execution/partitionable_hashtable.hpp"
13: #include "duckdb/execution/window_segment_tree.hpp"
14: #include "duckdb/main/client_config.hpp"
15: #include "duckdb/main/config.hpp"
16: #include "duckdb/parallel/base_pipeline_event.hpp"
17: #include "duckdb/planner/expression/bound_reference_expression.hpp"
18: #include "duckdb/planner/expression/bound_window_expression.hpp"
19: 
20: #include <algorithm>
21: #include <cmath>
22: #include <numeric>
23: 
24: namespace duckdb {
25: 
26: using counts_t = std::vector<size_t>;
27: 
28: class WindowGlobalHashGroup {
29: public:
30: 	using GlobalSortStatePtr = unique_ptr<GlobalSortState>;
31: 	using LocalSortStatePtr = unique_ptr<LocalSortState>;
32: 	using Orders = vector<BoundOrderByNode>;
33: 	using Types = vector<LogicalType>;
34: 
35: 	WindowGlobalHashGroup(BufferManager &buffer_manager, const Orders &partitions, const Orders &orders,
36: 	                      const Types &payload_types, idx_t max_mem, bool external)
37: 	    : memory_per_thread(max_mem), count(0) {
38: 
39: 		RowLayout payload_layout;
40: 		payload_layout.Initialize(payload_types);
41: 		global_sort = make_unique<GlobalSortState>(buffer_manager, orders, payload_layout);
42: 		global_sort->external = external;
43: 
44: 		partition_layout = global_sort->sort_layout.GetPrefixComparisonLayout(partitions.size());
45: 	}
46: 
47: 	void Combine(LocalSortState &local_sort) {
48: 		global_sort->AddLocalState(local_sort);
49: 	}
50: 
51: 	void PrepareMergePhase() {
52: 		global_sort->PrepareMergePhase();
53: 	}
54: 
55: 	void ComputeMasks(ValidityMask &partition_mask, ValidityMask &order_mask);
56: 
57: 	const idx_t memory_per_thread;
58: 	GlobalSortStatePtr global_sort;
59: 	atomic<idx_t> count;
60: 
61: 	// Mask computation
62: 	SortLayout partition_layout;
63: };
64: 
65: void WindowGlobalHashGroup::ComputeMasks(ValidityMask &partition_mask, ValidityMask &order_mask) {
66: 	D_ASSERT(count > 0);
67: 
68: 	//	Set up a comparator for the partition subset
69: 	const auto partition_size = partition_layout.comparison_size;
70: 
71: 	SBIterator prev(*global_sort, ExpressionType::COMPARE_LESSTHAN);
72: 	SBIterator curr(*global_sort, ExpressionType::COMPARE_LESSTHAN);
73: 
74: 	partition_mask.SetValidUnsafe(0);
75: 	order_mask.SetValidUnsafe(0);
76: 	for (++curr; curr.GetIndex() < count; ++curr) {
77: 		//	Compare the partition subset first because if that differs, then so does the full ordering
78: 		int part_cmp = 0;
79: 		if (partition_layout.all_constant) {
80: 			part_cmp = FastMemcmp(prev.entry_ptr, curr.entry_ptr, partition_size);
81: 		} else {
82: 			part_cmp = Comparators::CompareTuple(prev.scan, curr.scan, prev.entry_ptr, curr.entry_ptr, partition_layout,
83: 			                                     prev.external);
84: 		}
85: 
86: 		if (part_cmp) {
87: 			partition_mask.SetValidUnsafe(curr.GetIndex());
88: 			order_mask.SetValidUnsafe(curr.GetIndex());
89: 		} else if (prev.Compare(curr)) {
90: 			order_mask.SetValidUnsafe(curr.GetIndex());
91: 		}
92: 		++prev;
93: 	}
94: }
95: 
96: //	Global sink state
97: class WindowGlobalSinkState : public GlobalSinkState {
98: public:
99: 	using HashGroupPtr = unique_ptr<WindowGlobalHashGroup>;
100: 	using Orders = vector<BoundOrderByNode>;
101: 	using Types = vector<LogicalType>;
102: 
103: 	WindowGlobalSinkState(const PhysicalWindow &op_p, ClientContext &context)
104: 	    : op(op_p), buffer_manager(BufferManager::GetBufferManager(context)), allocator(Allocator::Get(context)),
105: 	      partition_info((idx_t)TaskScheduler::GetScheduler(context).NumberOfThreads()), next_sort(0),
106: 	      memory_per_thread(0), count(0), mode(DBConfig::GetConfig(context).options.window_mode) {
107: 
108: 		D_ASSERT(op.select_list[0]->GetExpressionClass() == ExpressionClass::BOUND_WINDOW);
109: 		auto wexpr = reinterpret_cast<BoundWindowExpression *>(op.select_list[0].get());
110: 
111: 		// we sort by both 1) partition by expression list and 2) order by expressions
112: 		payload_types = op.children[0]->types;
113: 
114: 		partition_cols = wexpr->partitions.size();
115: 		for (idx_t prt_idx = 0; prt_idx < partition_cols; prt_idx++) {
116: 			auto &pexpr = wexpr->partitions[prt_idx];
117: 
118: 			if (wexpr->partitions_stats.empty() || !wexpr->partitions_stats[prt_idx]) {
119: 				orders.emplace_back(OrderType::ASCENDING, OrderByNullType::NULLS_FIRST, pexpr->Copy(), nullptr);
120: 			} else {
121: 				orders.emplace_back(OrderType::ASCENDING, OrderByNullType::NULLS_FIRST, pexpr->Copy(),
122: 				                    wexpr->partitions_stats[prt_idx]->Copy());
123: 			}
124: 			partitions.emplace_back(orders.back().Copy());
125: 		}
126: 
127: 		for (const auto &order : wexpr->orders) {
128: 			orders.emplace_back(order.Copy());
129: 		}
130: 
131: 		memory_per_thread = op.GetMaxThreadMemory(context);
132: 		external = ClientConfig::GetConfig(context).force_external;
133: 	}
134: 
135: 	WindowGlobalHashGroup *GetUngrouped() {
136: 		lock_guard<mutex> guard(lock);
137: 
138: 		if (!ungrouped) {
139: 			ungrouped = make_unique<WindowGlobalHashGroup>(buffer_manager, partitions, orders, payload_types,
140: 			                                               memory_per_thread, external);
141: 		}
142: 
143: 		return ungrouped.get();
144: 	}
145: 
146: 	//! Switch to hash grouping
147: 	size_t Group() {
148: 		lock_guard<mutex> guard(lock);
149: 		if (hash_groups.size() < partition_info.n_partitions) {
150: 			hash_groups.resize(partition_info.n_partitions);
151: 		}
152: 
153: 		return hash_groups.size();
154: 	}
155: 
156: 	idx_t GroupCount() const {
157: 		return std::accumulate(
158: 		    hash_groups.begin(), hash_groups.end(), 0,
159: 		    [&](const idx_t &n, const HashGroupPtr &group) { return n + (group ? idx_t(group->count) : 0); });
160: 	}
161: 
162: 	WindowGlobalHashGroup *GetHashGroup(idx_t group) {
163: 		lock_guard<mutex> guard(lock);
164: 		D_ASSERT(group < hash_groups.size());
165: 		auto &hash_group = hash_groups[group];
166: 		if (!hash_group) {
167: 			const auto maxmem = memory_per_thread / partition_info.n_partitions;
168: 			hash_group =
169: 			    make_unique<WindowGlobalHashGroup>(buffer_manager, partitions, orders, payload_types, maxmem, external);
170: 		}
171: 
172: 		return hash_group.get();
173: 	}
174: 
175: 	void Finalize();
176: 
177: 	size_t GetNextSortGroup() {
178: 		for (auto group = next_sort++; group < hash_groups.size(); group = next_sort++) {
179: 			// Only non-empty groups exist.
180: 			if (hash_groups[group]) {
181: 				return group;
182: 			}
183: 		}
184: 
185: 		return hash_groups.size();
186: 	}
187: 
188: 	const PhysicalWindow &op;
189: 	BufferManager &buffer_manager;
190: 	Allocator &allocator;
191: 	size_t partition_cols;
192: 	const RadixPartitionInfo partition_info;
193: 	mutex lock;
194: 
195: 	// Sorting
196: 	Orders partitions;
197: 	Orders orders;
198: 	Types payload_types;
199: 	HashGroupPtr ungrouped;
200: 	vector<HashGroupPtr> hash_groups;
201: 	bool external;
202: 	atomic<size_t> next_sort;
203: 
204: 	// OVER() (no sorting)
205: 	unique_ptr<RowDataCollection> rows;
206: 	unique_ptr<RowDataCollection> strings;
207: 
208: 	// Threading
209: 	idx_t memory_per_thread;
210: 	atomic<idx_t> count;
211: 	WindowAggregationMode mode;
212: };
213: 
214: //	Per-thread hash group
215: class WindowLocalHashGroup {
216: public:
217: 	using LocalSortStatePtr = unique_ptr<LocalSortState>;
218: 	using DataChunkPtr = unique_ptr<DataChunk>;
219: 
220: 	explicit WindowLocalHashGroup(WindowGlobalHashGroup &global_group_p) : global_group(global_group_p), count(0) {
221: 	}
222: 
223: 	bool SinkChunk(DataChunk &sort_chunk, DataChunk &payload_chunk);
224: 	void Combine();
225: 
226: 	WindowGlobalHashGroup &global_group;
227: 	LocalSortStatePtr local_sort;
228: 	idx_t count;
229: };
230: 
231: bool WindowLocalHashGroup::SinkChunk(DataChunk &sort_buffer, DataChunk &input_chunk) {
232: 	D_ASSERT(sort_buffer.size() == input_chunk.size());
233: 	count += input_chunk.size();
234: 	auto &global_sort = *global_group.global_sort;
235: 	if (!local_sort) {
236: 		local_sort = make_unique<LocalSortState>();
237: 		local_sort->Initialize(global_sort, global_sort.buffer_manager);
238: 	}
239: 
240: 	local_sort->SinkChunk(sort_buffer, input_chunk);
241: 
242: 	if (local_sort->SizeInBytes() >= global_group.memory_per_thread) {
243: 		local_sort->Sort(global_sort, true);
244: 	}
245: 
246: 	return (local_sort->SizeInBytes() >= global_group.memory_per_thread);
247: }
248: 
249: void WindowLocalHashGroup::Combine() {
250: 	if (local_sort) {
251: 		global_group.Combine(*local_sort);
252: 		global_group.count += count;
253: 		local_sort.reset();
254: 	}
255: }
256: 
257: //	Per-thread sink state
258: class WindowLocalSinkState : public LocalSinkState {
259: public:
260: 	using LocalHashGroupPtr = unique_ptr<WindowLocalHashGroup>;
261: 
262: 	WindowLocalSinkState(Allocator &allocator, const PhysicalWindow &op_p)
263: 	    : op(op_p), executor(allocator), count(0), hash_vector(LogicalTypeId::UBIGINT), sel(STANDARD_VECTOR_SIZE) {
264: 
265: 		D_ASSERT(op.select_list[0]->GetExpressionClass() == ExpressionClass::BOUND_WINDOW);
266: 		auto wexpr = reinterpret_cast<BoundWindowExpression *>(op.select_list[0].get());
267: 		partition_cols = wexpr->partitions.size();
268: 
269: 		// we sort by both 1) partition by expression list and 2) order by expressions
270: 		auto &payload_types = op.children[0]->types;
271: 		vector<LogicalType> over_types;
272: 		for (idx_t prt_idx = 0; prt_idx < wexpr->partitions.size(); prt_idx++) {
273: 			auto &pexpr = wexpr->partitions[prt_idx];
274: 			over_types.push_back(pexpr->return_type);
275: 			executor.AddExpression(*pexpr);
276: 		}
277: 
278: 		for (const auto &order : wexpr->orders) {
279: 			auto &oexpr = order.expression;
280: 			over_types.push_back(oexpr->return_type);
281: 			executor.AddExpression(*oexpr);
282: 		}
283: 
284: 		if (!over_types.empty()) {
285: 			over_chunk.Initialize(allocator, over_types);
286: 			over_subset.Initialize(allocator, over_types);
287: 		}
288: 
289: 		payload_chunk.Initialize(allocator, payload_types);
290: 		payload_subset.Initialize(allocator, payload_types);
291: 		payload_layout.Initialize(payload_types);
292: 	}
293: 
294: 	// Global state
295: 	const PhysicalWindow &op;
296: 
297: 	// Input
298: 	ExpressionExecutor executor;
299: 	DataChunk over_chunk;
300: 	DataChunk payload_chunk;
301: 	idx_t count;
302: 
303: 	// Grouping
304: 	idx_t partition_cols;
305: 	counts_t counts;
306: 	counts_t offsets;
307: 	Vector hash_vector;
308: 	SelectionVector sel;
309: 	DataChunk over_subset;
310: 	DataChunk payload_subset;
311: 	LocalHashGroupPtr ungrouped;
312: 	vector<LocalHashGroupPtr> hash_groups;
313: 
314: 	// OVER() (no sorting)
315: 	RowLayout payload_layout;
316: 	unique_ptr<RowDataCollection> rows;
317: 	unique_ptr<RowDataCollection> strings;
318: 
319: 	//! Switch to grouping the data
320: 	void Group(WindowGlobalSinkState &gstate);
321: 	//! Compute the OVER values
322: 	void Over(DataChunk &input_chunk);
323: 	//! Hash the data and group it
324: 	void Hash(WindowGlobalSinkState &gstate, DataChunk &input_chunk);
325: 	//! Sink an input chunk
326: 	void Sink(DataChunk &input_chunk, WindowGlobalSinkState &gstate);
327: 	//! Merge the state into the global state.
328: 	void Combine(WindowGlobalSinkState &gstate);
329: };
330: 
331: void WindowLocalSinkState::Over(DataChunk &input_chunk) {
332: 	if (over_chunk.ColumnCount() > 0) {
333: 		over_chunk.Reset();
334: 		executor.Execute(input_chunk, over_chunk);
335: 		over_chunk.Verify();
336: 	}
337: }
338: 
339: void WindowLocalSinkState::Hash(WindowGlobalSinkState &gstate, DataChunk &input_chunk) {
340: 	// There are three types of hash grouping:
341: 	// 1. No partitions (no sorting)
342: 	// 2. One group (sorting, but no hash grouping)
343: 	// 3. Multiple groups (sorting and hash grouping)
344: 	if (over_chunk.ColumnCount() == 0) {
345: 		return;
346: 	}
347: 
348: 	const auto count = over_chunk.size();
349: 	auto hashes = FlatVector::GetData<hash_t>(hash_vector);
350: 	if (hash_groups.empty()) {
351: 		// Ungrouped, so take them all
352: 		counts.resize(1, count);
353: 	} else {
354: 		// First pass: count bins sizes
355: 		counts.resize(0);
356: 		counts.resize(hash_groups.size(), 0);
357: 
358: 		VectorOperations::Hash(over_chunk.data[0], hash_vector, count);
359: 		for (idx_t prt_idx = 1; prt_idx < partition_cols; ++prt_idx) {
360: 			VectorOperations::CombineHash(hash_vector, over_chunk.data[prt_idx], count);
361: 		}
362: 
363: 		const auto &partition_info = gstate.partition_info;
364: 		if (hash_vector.GetVectorType() == VectorType::CONSTANT_VECTOR) {
365: 			const auto group = partition_info.GetHashPartition(hashes[0]);
366: 			counts[group] = count;
367: 			for (idx_t i = 0; i < count; ++i) {
368: 				sel.set_index(i, i);
369: 			}
370: 		} else {
371: 			for (idx_t i = 0; i < count; ++i) {
372: 				const auto group = partition_info.GetHashPartition(hashes[i]);
373: 				++counts[group];
374: 			}
375: 
376: 			// Second pass: Initialise offsets
377: 			offsets.resize(counts.size());
378: 			size_t offset = 0;
379: 			for (size_t c = 0; c < counts.size(); ++c) {
380: 				offsets[c] = offset;
381: 				offset += counts[c];
382: 			}
383: 
384: 			// Third pass: Build sequential selections
385: 			for (idx_t i = 0; i < count; ++i) {
386: 				const auto group = partition_info.GetHashPartition(hashes[i]);
387: 				auto &group_idx = offsets[group];
388: 				sel.set_index(group_idx++, i);
389: 			}
390: 		}
391: 	}
392: 
393: 	idx_t group_offset = 0;
394: 	for (size_t group = 0; group < counts.size(); ++group) {
395: 		const auto group_size = counts[group];
396: 		if (group_size) {
397: 			auto &local_group = hash_groups[group];
398: 			if (!local_group) {
399: 				auto global_group = gstate.GetHashGroup(group);
400: 				local_group = make_unique<WindowLocalHashGroup>(*global_group);
401: 			}
402: 
403: 			if (counts.size() == 1) {
404: 				local_group->SinkChunk(over_chunk, input_chunk);
405: 			} else {
406: 				SelectionVector psel(sel.data() + group_offset);
407: 				over_subset.Slice(over_chunk, psel, group_size);
408: 				payload_subset.Slice(input_chunk, psel, group_size);
409: 				local_group->SinkChunk(over_subset, payload_subset);
410: 				group_offset += group_size;
411: 			}
412: 		}
413: 	}
414: }
415: 
416: void WindowLocalSinkState::Group(WindowGlobalSinkState &gstate) {
417: 	if (!gstate.partition_cols) {
418: 		return;
419: 	}
420: 
421: 	if (!hash_groups.empty()) {
422: 		return;
423: 	}
424: 
425: 	hash_groups.resize(gstate.Group());
426: 
427: 	if (!ungrouped) {
428: 		return;
429: 	}
430: 
431: 	auto &payload_data = *ungrouped->local_sort->payload_data;
432: 	auto rows = payload_data.CloneEmpty(payload_data.keep_pinned);
433: 
434: 	auto &payload_heap = *ungrouped->local_sort->payload_heap;
435: 	auto heap = payload_heap.CloneEmpty(payload_heap.keep_pinned);
436: 
437: 	RowDataCollectionScanner::AlignHeapBlocks(*rows, *heap, payload_data, payload_heap, payload_layout);
438: 	RowDataCollectionScanner scanner(*rows, *heap, payload_layout, true);
439: 	while (scanner.Remaining()) {
440: 		payload_chunk.Reset();
441: 		scanner.Scan(payload_chunk);
442: 
443: 		Over(payload_chunk);
444: 		Hash(gstate, payload_chunk);
445: 	}
446: 
447: 	ungrouped.reset();
448: }
449: 
450: void WindowLocalSinkState::Sink(DataChunk &input_chunk, WindowGlobalSinkState &gstate) {
451: 	gstate.count += input_chunk.size();
452: 	count += input_chunk.size();
453: 
454: 	Over(input_chunk);
455: 
456: 	// OVER()
457: 	if (over_chunk.ColumnCount() == 0) {
458: 		//	No sorts, so build paged row chunks
459: 		if (!rows) {
460: 			const auto entry_size = payload_layout.GetRowWidth();
461: 			const auto capacity = MaxValue<idx_t>(STANDARD_VECTOR_SIZE, (Storage::BLOCK_SIZE / entry_size) + 1);
462: 			rows = make_unique<RowDataCollection>(gstate.buffer_manager, capacity, entry_size);
463: 			strings = make_unique<RowDataCollection>(gstate.buffer_manager, (idx_t)Storage::BLOCK_SIZE, 1, true);
464: 		}
465: 		const auto row_count = input_chunk.size();
466: 		const auto row_sel = FlatVector::IncrementalSelectionVector();
467: 		Vector addresses(LogicalType::POINTER);
468: 		auto key_locations = FlatVector::GetData<data_ptr_t>(addresses);
469: 		const auto prev_rows_blocks = rows->blocks.size();
470: 		auto handles = rows->Build(row_count, key_locations, nullptr, row_sel);
471: 		auto input_data = input_chunk.ToUnifiedFormat();
472: 		RowOperations::Scatter(input_chunk, input_data.get(), payload_layout, addresses, *strings, *row_sel, row_count);
473: 		// Mark that row blocks contain pointers (heap blocks are pinned)
474: 		if (!payload_layout.AllConstant()) {
475: 			D_ASSERT(strings->keep_pinned);
476: 			for (size_t i = prev_rows_blocks; i < rows->blocks.size(); ++i) {
477: 				rows->blocks[i]->block->SetSwizzling("WindowLocalSinkState::Sink");
478: 			}
479: 		}
480: 		return;
481: 	}
482: 
483: 	// Ungrouped
484: 	if (hash_groups.empty()) {
485: 		auto global_ungrouped = gstate.GetUngrouped();
486: 		if (!ungrouped) {
487: 			ungrouped = make_unique<WindowLocalHashGroup>(*global_ungrouped);
488: 		}
489: 
490: 		// If we pass our thread memory budget, then switch to hash grouping.
491: 		if (ungrouped->SinkChunk(over_chunk, input_chunk) || gstate.count > 100000) {
492: 			Group(gstate);
493: 		}
494: 		return;
495: 	}
496: 
497: 	// Grouped, so hash
498: 	Hash(gstate, input_chunk);
499: }
500: 
501: void WindowLocalSinkState::Combine(WindowGlobalSinkState &gstate) {
502: 	// OVER()
503: 	if (over_chunk.ColumnCount() == 0) {
504: 		// Only one partition again, so need a global lock.
505: 		lock_guard<mutex> glock(gstate.lock);
506: 		if (gstate.rows) {
507: 			if (rows) {
508: 				gstate.rows->Merge(*rows);
509: 				gstate.strings->Merge(*strings);
510: 				rows.reset();
511: 				strings.reset();
512: 			}
513: 		} else {
514: 			gstate.rows = move(rows);
515: 			gstate.strings = move(strings);
516: 		}
517: 		return;
518: 	}
519: 
520: 	// Ungrouped data
521: 	idx_t check = 0;
522: 	if (ungrouped) {
523: 		check += ungrouped->count;
524: 		ungrouped->Combine();
525: 		ungrouped.reset();
526: 	}
527: 
528: 	// Grouped data
529: 	for (auto &local_group : hash_groups) {
530: 		if (local_group) {
531: 			check += local_group->count;
532: 			local_group->Combine();
533: 			local_group.reset();
534: 		}
535: 	}
536: 
537: 	(void)check;
538: 	D_ASSERT(check == count);
539: }
540: 
541: void WindowGlobalSinkState::Finalize() {
542: 	if (!ungrouped) {
543: 		return;
544: 	}
545: 
546: 	if (hash_groups.empty()) {
547: 		hash_groups.emplace_back(move(ungrouped));
548: 		return;
549: 	}
550: 
551: 	//	If we have grouped data, merge the remaining ungrouped data into it.
552: 	//	This can happen if only SOME of the threads ended up regrouping.
553: 	//	The simplest thing to do is to fake a local thread state,
554: 	//	push the ungrouped data into it and then use that state to partition the data.
555: 	// 	This is probably OK because this situation will only happen
556: 	// 	with relatively small amounts of data.
557: 
558: 	//	Sort the data so we can scan it
559: 	auto &global_sort = *ungrouped->global_sort;
560: 	if (global_sort.sorted_blocks.empty()) {
561: 		return;
562: 	}
563: 
564: 	global_sort.PrepareMergePhase();
565: 	while (global_sort.sorted_blocks.size() > 1) {
566: 		global_sort.InitializeMergeRound();
567: 		MergeSorter merge_sorter(global_sort, global_sort.buffer_manager);
568: 		merge_sorter.PerformInMergeRound();
569: 		global_sort.CompleteMergeRound(true);
570: 	}
571: 
572: 	// 	Sink it into a temporary local sink state
573: 	auto lstate = make_unique<WindowLocalSinkState>(allocator, op);
574: 
575: 	//	Match the grouping.
576: 	lstate->Group(*this);
577: 
578: 	// Write into the state chunks directly to hash them
579: 	auto &payload_chunk = lstate->payload_chunk;
580: 
581: 	// 	Now scan the sorted data
582: 	PayloadScanner scanner(global_sort);
583: 	while (scanner.Remaining()) {
584: 		lstate->payload_chunk.Reset();
585: 		scanner.Scan(lstate->payload_chunk);
586: 		if (payload_chunk.size() == 0) {
587: 			break;
588: 		}
589: 		lstate->count += payload_chunk.size();
590: 
591: 		lstate->Over(payload_chunk);
592: 		lstate->Hash(*this, payload_chunk);
593: 	}
594: 
595: 	//	Merge the grouped data in.
596: 	lstate->Combine(*this);
597: }
598: 
599: // this implements a sorted window functions variant
600: PhysicalWindow::PhysicalWindow(vector<LogicalType> types, vector<unique_ptr<Expression>> select_list,
601:                                idx_t estimated_cardinality, PhysicalOperatorType type)
602:     : PhysicalOperator(type, move(types), estimated_cardinality), select_list(move(select_list)) {
603: }
604: 
605: static idx_t FindNextStart(const ValidityMask &mask, idx_t l, const idx_t r, idx_t &n) {
606: 	if (mask.AllValid()) {
607: 		auto start = MinValue(l + n - 1, r);
608: 		n -= MinValue(n, r - l);
609: 		return start;
610: 	}
611: 
612: 	while (l < r) {
613: 		//	If l is aligned with the start of a block, and the block is blank, then skip forward one block.
614: 		idx_t entry_idx;
615: 		idx_t shift;
616: 		mask.GetEntryIndex(l, entry_idx, shift);
617: 
618: 		const auto block = mask.GetValidityEntry(entry_idx);
619: 		if (mask.NoneValid(block) && !shift) {
620: 			l += ValidityMask::BITS_PER_VALUE;
621: 			continue;
622: 		}
623: 
624: 		// Loop over the block
625: 		for (; shift < ValidityMask::BITS_PER_VALUE && l < r; ++shift, ++l) {
626: 			if (mask.RowIsValid(block, shift) && --n == 0) {
627: 				return MinValue(l, r);
628: 			}
629: 		}
630: 	}
631: 
632: 	//	Didn't find a start so return the end of the range
633: 	return r;
634: }
635: 
636: static idx_t FindPrevStart(const ValidityMask &mask, const idx_t l, idx_t r, idx_t &n) {
637: 	if (mask.AllValid()) {
638: 		auto start = (r <= l + n) ? l : r - n;
639: 		n -= r - start;
640: 		return start;
641: 	}
642: 
643: 	while (l < r) {
644: 		// If r is aligned with the start of a block, and the previous block is blank,
645: 		// then skip backwards one block.
646: 		idx_t entry_idx;
647: 		idx_t shift;
648: 		mask.GetEntryIndex(r - 1, entry_idx, shift);
649: 
650: 		const auto block = mask.GetValidityEntry(entry_idx);
651: 		if (mask.NoneValid(block) && (shift + 1 == ValidityMask::BITS_PER_VALUE)) {
652: 			// r is nonzero (> l) and word aligned, so this will not underflow.
653: 			r -= ValidityMask::BITS_PER_VALUE;
654: 			continue;
655: 		}
656: 
657: 		// Loop backwards over the block
658: 		// shift is probing r-1 >= l >= 0
659: 		for (++shift; shift-- > 0; --r) {
660: 			if (mask.RowIsValid(block, shift) && --n == 0) {
661: 				return MaxValue(l, r - 1);
662: 			}
663: 		}
664: 	}
665: 
666: 	//	Didn't find a start so return the start of the range
667: 	return l;
668: }
669: 
670: static void PrepareInputExpressions(Expression **exprs, idx_t expr_count, ExpressionExecutor &executor,
671:                                     DataChunk &chunk) {
672: 	if (expr_count == 0) {
673: 		return;
674: 	}
675: 
676: 	vector<LogicalType> types;
677: 	for (idx_t expr_idx = 0; expr_idx < expr_count; ++expr_idx) {
678: 		types.push_back(exprs[expr_idx]->return_type);
679: 		executor.AddExpression(*exprs[expr_idx]);
680: 	}
681: 
682: 	if (!types.empty()) {
683: 		chunk.Initialize(executor.allocator, types);
684: 	}
685: }
686: 
687: static void PrepareInputExpression(Expression *expr, ExpressionExecutor &executor, DataChunk &chunk) {
688: 	PrepareInputExpressions(&expr, 1, executor, chunk);
689: }
690: 
691: struct WindowInputExpression {
692: 	WindowInputExpression(Expression *expr_p, Allocator &allocator)
693: 	    : expr(expr_p), ptype(PhysicalType::INVALID), scalar(true), executor(allocator) {
694: 		if (expr) {
695: 			PrepareInputExpression(expr, executor, chunk);
696: 			ptype = expr->return_type.InternalType();
697: 			scalar = expr->IsScalar();
698: 		}
699: 	}
700: 
701: 	void Execute(DataChunk &input_chunk) {
702: 		if (expr) {
703: 			chunk.Reset();
704: 			executor.Execute(input_chunk, chunk);
705: 			chunk.Verify();
706: 		}
707: 	}
708: 
709: 	template <typename T>
710: 	inline T GetCell(idx_t i) const {
711: 		D_ASSERT(!chunk.data.empty());
712: 		const auto data = FlatVector::GetData<T>(chunk.data[0]);
713: 		return data[scalar ? 0 : i];
714: 	}
715: 
716: 	inline bool CellIsNull(idx_t i) const {
717: 		D_ASSERT(!chunk.data.empty());
718: 		if (chunk.data[0].GetVectorType() == VectorType::CONSTANT_VECTOR) {
719: 			return ConstantVector::IsNull(chunk.data[0]);
720: 		}
721: 		return FlatVector::IsNull(chunk.data[0], i);
722: 	}
723: 
724: 	inline void CopyCell(Vector &target, idx_t target_offset) const {
725: 		D_ASSERT(!chunk.data.empty());
726: 		auto &source = chunk.data[0];
727: 		auto source_offset = scalar ? 0 : target_offset;
728: 		VectorOperations::Copy(source, target, source_offset + 1, source_offset, target_offset);
729: 	}
730: 
731: 	Expression *expr;
732: 	PhysicalType ptype;
733: 	bool scalar;
734: 	ExpressionExecutor executor;
735: 	DataChunk chunk;
736: };
737: 
738: struct WindowInputColumn {
739: 	WindowInputColumn(Expression *expr_p, Allocator &allocator, idx_t capacity_p)
740: 	    : input_expr(expr_p, allocator), count(0), capacity(capacity_p) {
741: 		if (input_expr.expr) {
742: 			target = make_unique<Vector>(input_expr.chunk.data[0].GetType(), capacity);
743: 		}
744: 	}
745: 
746: 	void Append(DataChunk &input_chunk) {
747: 		if (input_expr.expr && (!input_expr.scalar || !count)) {
748: 			input_expr.Execute(input_chunk);
749: 			auto &source = input_expr.chunk.data[0];
750: 			const auto source_count = input_expr.chunk.size();
751: 			D_ASSERT(count + source_count <= capacity);
752: 			VectorOperations::Copy(source, *target, source_count, 0, count);
753: 			count += source_count;
754: 		}
755: 	}
756: 
757: 	inline bool CellIsNull(idx_t i) {
758: 		D_ASSERT(target);
759: 		D_ASSERT(i < count);
760: 		return FlatVector::IsNull(*target, input_expr.scalar ? 0 : i);
761: 	}
762: 
763: 	template <typename T>
764: 	inline T GetCell(idx_t i) {
765: 		D_ASSERT(target);
766: 		D_ASSERT(i < count);
767: 		const auto data = FlatVector::GetData<T>(*target);
768: 		return data[input_expr.scalar ? 0 : i];
769: 	}
770: 
771: 	WindowInputExpression input_expr;
772: 
773: private:
774: 	unique_ptr<Vector> target;
775: 	idx_t count;
776: 	idx_t capacity;
777: };
778: 
779: static inline bool BoundaryNeedsPeer(const WindowBoundary &boundary) {
780: 	switch (boundary) {
781: 	case WindowBoundary::CURRENT_ROW_RANGE:
782: 	case WindowBoundary::EXPR_PRECEDING_RANGE:
783: 	case WindowBoundary::EXPR_FOLLOWING_RANGE:
784: 		return true;
785: 	default:
786: 		return false;
787: 	}
788: }
789: 
790: struct WindowBoundariesState {
791: 	static inline bool IsScalar(const unique_ptr<Expression> &expr) {
792: 		return expr ? expr->IsScalar() : true;
793: 	}
794: 
795: 	WindowBoundariesState(BoundWindowExpression *wexpr, const idx_t input_size)
796: 	    : type(wexpr->type), input_size(input_size), start_boundary(wexpr->start), end_boundary(wexpr->end),
797: 	      partition_count(wexpr->partitions.size()), order_count(wexpr->orders.size()),
798: 	      range_sense(wexpr->orders.empty() ? OrderType::INVALID : wexpr->orders[0].type),
799: 	      has_preceding_range(wexpr->start == WindowBoundary::EXPR_PRECEDING_RANGE ||
800: 	                          wexpr->end == WindowBoundary::EXPR_PRECEDING_RANGE),
801: 	      has_following_range(wexpr->start == WindowBoundary::EXPR_FOLLOWING_RANGE ||
802: 	                          wexpr->end == WindowBoundary::EXPR_FOLLOWING_RANGE),
803: 	      needs_peer(BoundaryNeedsPeer(wexpr->end) || wexpr->type == ExpressionType::WINDOW_CUME_DIST) {
804: 	}
805: 
806: 	void Update(const idx_t row_idx, WindowInputColumn &range_collection, const idx_t source_offset,
807: 	            WindowInputExpression &boundary_start, WindowInputExpression &boundary_end,
808: 	            const ValidityMask &partition_mask, const ValidityMask &order_mask);
809: 
810: 	// Cached lookups
811: 	const ExpressionType type;
812: 	const idx_t input_size;
813: 	const WindowBoundary start_boundary;
814: 	const WindowBoundary end_boundary;
815: 	const size_t partition_count;
816: 	const size_t order_count;
817: 	const OrderType range_sense;
818: 	const bool has_preceding_range;
819: 	const bool has_following_range;
820: 	const bool needs_peer;
821: 
822: 	idx_t partition_start = 0;
823: 	idx_t partition_end = 0;
824: 	idx_t peer_start = 0;
825: 	idx_t peer_end = 0;
826: 	idx_t valid_start = 0;
827: 	idx_t valid_end = 0;
828: 	int64_t window_start = -1;
829: 	int64_t window_end = -1;
830: 	bool is_same_partition = false;
831: 	bool is_peer = false;
832: };
833: 
834: static bool WindowNeedsRank(BoundWindowExpression *wexpr) {
835: 	return wexpr->type == ExpressionType::WINDOW_PERCENT_RANK || wexpr->type == ExpressionType::WINDOW_RANK ||
836: 	       wexpr->type == ExpressionType::WINDOW_RANK_DENSE || wexpr->type == ExpressionType::WINDOW_CUME_DIST;
837: }
838: 
839: template <typename T>
840: static T GetCell(ChunkCollection &collection, idx_t column, idx_t index) {
841: 	D_ASSERT(collection.ColumnCount() > column);
842: 	auto &chunk = collection.GetChunkForRow(index);
843: 	auto &source = chunk.data[column];
844: 	const auto source_offset = index % STANDARD_VECTOR_SIZE;
845: 	const auto data = FlatVector::GetData<T>(source);
846: 	return data[source_offset];
847: }
848: 
849: static bool CellIsNull(ChunkCollection &collection, idx_t column, idx_t index) {
850: 	D_ASSERT(collection.ColumnCount() > column);
851: 	auto &chunk = collection.GetChunkForRow(index);
852: 	auto &source = chunk.data[column];
853: 	const auto source_offset = index % STANDARD_VECTOR_SIZE;
854: 	return FlatVector::IsNull(source, source_offset);
855: }
856: 
857: template <typename T>
858: struct WindowColumnIterator {
859: 	using iterator = WindowColumnIterator<T>;
860: 	using iterator_category = std::forward_iterator_tag;
861: 	using difference_type = std::ptrdiff_t;
862: 	using value_type = T;
863: 	using reference = T;
864: 	using pointer = idx_t;
865: 
866: 	explicit WindowColumnIterator(WindowInputColumn &coll_p, pointer pos_p = 0) : coll(&coll_p), pos(pos_p) {
867: 	}
868: 
869: 	inline reference operator*() const {
870: 		return coll->GetCell<T>(pos);
871: 	}
872: 	inline explicit operator pointer() const {
873: 		return pos;
874: 	}
875: 
876: 	inline iterator &operator++() {
877: 		++pos;
878: 		return *this;
879: 	}
880: 	inline iterator operator++(int) {
881: 		auto result = *this;
882: 		++(*this);
883: 		return result;
884: 	}
885: 
886: 	friend inline bool operator==(const iterator &a, const iterator &b) {
887: 		return a.pos == b.pos;
888: 	}
889: 	friend inline bool operator!=(const iterator &a, const iterator &b) {
890: 		return a.pos != b.pos;
891: 	}
892: 
893: private:
894: 	WindowInputColumn *coll;
895: 	pointer pos;
896: };
897: 
898: template <typename T, typename OP>
899: struct OperationCompare : public std::function<bool(T, T)> {
900: 	inline bool operator()(const T &lhs, const T &val) const {
901: 		return OP::template Operation(lhs, val);
902: 	}
903: };
904: 
905: template <typename T, typename OP, bool FROM>
906: static idx_t FindTypedRangeBound(WindowInputColumn &over, const idx_t order_begin, const idx_t order_end,
907:                                  WindowInputExpression &boundary, const idx_t boundary_row) {
908: 	D_ASSERT(!boundary.CellIsNull(boundary_row));
909: 	const auto val = boundary.GetCell<T>(boundary_row);
910: 
911: 	OperationCompare<T, OP> comp;
912: 	WindowColumnIterator<T> begin(over, order_begin);
913: 	WindowColumnIterator<T> end(over, order_end);
914: 	if (FROM) {
915: 		return idx_t(std::lower_bound(begin, end, val, comp));
916: 	} else {
917: 		return idx_t(std::upper_bound(begin, end, val, comp));
918: 	}
919: }
920: 
921: template <typename OP, bool FROM>
922: static idx_t FindRangeBound(WindowInputColumn &over, const idx_t order_begin, const idx_t order_end,
923:                             WindowInputExpression &boundary, const idx_t expr_idx) {
924: 	D_ASSERT(boundary.chunk.ColumnCount() == 1);
925: 	D_ASSERT(boundary.chunk.data[0].GetType().InternalType() == over.input_expr.ptype);
926: 
927: 	switch (over.input_expr.ptype) {
928: 	case PhysicalType::INT8:
929: 		return FindTypedRangeBound<int8_t, OP, FROM>(over, order_begin, order_end, boundary, expr_idx);
930: 	case PhysicalType::INT16:
931: 		return FindTypedRangeBound<int16_t, OP, FROM>(over, order_begin, order_end, boundary, expr_idx);
932: 	case PhysicalType::INT32:
933: 		return FindTypedRangeBound<int32_t, OP, FROM>(over, order_begin, order_end, boundary, expr_idx);
934: 	case PhysicalType::INT64:
935: 		return FindTypedRangeBound<int64_t, OP, FROM>(over, order_begin, order_end, boundary, expr_idx);
936: 	case PhysicalType::UINT8:
937: 		return FindTypedRangeBound<uint8_t, OP, FROM>(over, order_begin, order_end, boundary, expr_idx);
938: 	case PhysicalType::UINT16:
939: 		return FindTypedRangeBound<uint16_t, OP, FROM>(over, order_begin, order_end, boundary, expr_idx);
940: 	case PhysicalType::UINT32:
941: 		return FindTypedRangeBound<uint32_t, OP, FROM>(over, order_begin, order_end, boundary, expr_idx);
942: 	case PhysicalType::UINT64:
943: 		return FindTypedRangeBound<uint64_t, OP, FROM>(over, order_begin, order_end, boundary, expr_idx);
944: 	case PhysicalType::INT128:
945: 		return FindTypedRangeBound<hugeint_t, OP, FROM>(over, order_begin, order_end, boundary, expr_idx);
946: 	case PhysicalType::FLOAT:
947: 		return FindTypedRangeBound<float, OP, FROM>(over, order_begin, order_end, boundary, expr_idx);
948: 	case PhysicalType::DOUBLE:
949: 		return FindTypedRangeBound<double, OP, FROM>(over, order_begin, order_end, boundary, expr_idx);
950: 	case PhysicalType::INTERVAL:
951: 		return FindTypedRangeBound<interval_t, OP, FROM>(over, order_begin, order_end, boundary, expr_idx);
952: 	default:
953: 		throw InternalException("Unsupported column type for RANGE");
954: 	}
955: }
956: 
957: template <bool FROM>
958: static idx_t FindOrderedRangeBound(WindowInputColumn &over, const OrderType range_sense, const idx_t order_begin,
959:                                    const idx_t order_end, WindowInputExpression &boundary, const idx_t expr_idx) {
960: 	switch (range_sense) {
961: 	case OrderType::ASCENDING:
962: 		return FindRangeBound<LessThan, FROM>(over, order_begin, order_end, boundary, expr_idx);
963: 	case OrderType::DESCENDING:
964: 		return FindRangeBound<GreaterThan, FROM>(over, order_begin, order_end, boundary, expr_idx);
965: 	default:
966: 		throw InternalException("Unsupported ORDER BY sense for RANGE");
967: 	}
968: }
969: 
970: void WindowBoundariesState::Update(const idx_t row_idx, WindowInputColumn &range_collection, const idx_t expr_idx,
971:                                    WindowInputExpression &boundary_start, WindowInputExpression &boundary_end,
972:                                    const ValidityMask &partition_mask, const ValidityMask &order_mask) {
973: 
974: 	auto &bounds = *this;
975: 	if (bounds.partition_count + bounds.order_count > 0) {
976: 
977: 		// determine partition and peer group boundaries to ultimately figure out window size
978: 		bounds.is_same_partition = !partition_mask.RowIsValidUnsafe(row_idx);
979: 		bounds.is_peer = !order_mask.RowIsValidUnsafe(row_idx);
980: 
981: 		// when the partition changes, recompute the boundaries
982: 		if (!bounds.is_same_partition) {
983: 			bounds.partition_start = row_idx;
984: 			bounds.peer_start = row_idx;
985: 
986: 			// find end of partition
987: 			bounds.partition_end = bounds.input_size;
988: 			if (bounds.partition_count) {
989: 				idx_t n = 1;
990: 				bounds.partition_end = FindNextStart(partition_mask, bounds.partition_start + 1, bounds.input_size, n);
991: 			}
992: 
993: 			// Find valid ordering values for the new partition
994: 			// so we can exclude NULLs from RANGE expression computations
995: 			bounds.valid_start = bounds.partition_start;
996: 			bounds.valid_end = bounds.partition_end;
997: 
998: 			if ((bounds.valid_start < bounds.valid_end) && bounds.has_preceding_range) {
999: 				// Exclude any leading NULLs
1000: 				if (range_collection.CellIsNull(bounds.valid_start)) {
1001: 					idx_t n = 1;
1002: 					bounds.valid_start = FindNextStart(order_mask, bounds.valid_start + 1, bounds.valid_end, n);
1003: 				}
1004: 			}
1005: 
1006: 			if ((bounds.valid_start < bounds.valid_end) && bounds.has_following_range) {
1007: 				// Exclude any trailing NULLs
1008: 				if (range_collection.CellIsNull(bounds.valid_end - 1)) {
1009: 					idx_t n = 1;
1010: 					bounds.valid_end = FindPrevStart(order_mask, bounds.valid_start, bounds.valid_end, n);
1011: 				}
1012: 			}
1013: 
1014: 		} else if (!bounds.is_peer) {
1015: 			bounds.peer_start = row_idx;
1016: 		}
1017: 
1018: 		if (bounds.needs_peer) {
1019: 			bounds.peer_end = bounds.partition_end;
1020: 			if (bounds.order_count) {
1021: 				idx_t n = 1;
1022: 				bounds.peer_end = FindNextStart(order_mask, bounds.peer_start + 1, bounds.partition_end, n);
1023: 			}
1024: 		}
1025: 
1026: 	} else {
1027: 		bounds.is_same_partition = false;
1028: 		bounds.is_peer = true;
1029: 		bounds.partition_end = bounds.input_size;
1030: 		bounds.peer_end = bounds.partition_end;
1031: 	}
1032: 
1033: 	// determine window boundaries depending on the type of expression
1034: 	bounds.window_start = -1;
1035: 	bounds.window_end = -1;
1036: 
1037: 	switch (bounds.start_boundary) {
1038: 	case WindowBoundary::UNBOUNDED_PRECEDING:
1039: 		bounds.window_start = bounds.partition_start;
1040: 		break;
1041: 	case WindowBoundary::CURRENT_ROW_ROWS:
1042: 		bounds.window_start = row_idx;
1043: 		break;
1044: 	case WindowBoundary::CURRENT_ROW_RANGE:
1045: 		bounds.window_start = bounds.peer_start;
1046: 		break;
1047: 	case WindowBoundary::EXPR_PRECEDING_ROWS: {
1048: 		bounds.window_start = (int64_t)row_idx - boundary_start.GetCell<int64_t>(expr_idx);
1049: 		break;
1050: 	}
1051: 	case WindowBoundary::EXPR_FOLLOWING_ROWS: {
1052: 		bounds.window_start = row_idx + boundary_start.GetCell<int64_t>(expr_idx);
1053: 		break;
1054: 	}
1055: 	case WindowBoundary::EXPR_PRECEDING_RANGE: {
1056: 		if (boundary_start.CellIsNull(expr_idx)) {
1057: 			bounds.window_start = bounds.peer_start;
1058: 		} else {
1059: 			bounds.window_start = FindOrderedRangeBound<true>(range_collection, bounds.range_sense, bounds.valid_start,
1060: 			                                                  row_idx, boundary_start, expr_idx);
1061: 		}
1062: 		break;
1063: 	}
1064: 	case WindowBoundary::EXPR_FOLLOWING_RANGE: {
1065: 		if (boundary_start.CellIsNull(expr_idx)) {
1066: 			bounds.window_start = bounds.peer_start;
1067: 		} else {
1068: 			bounds.window_start = FindOrderedRangeBound<true>(range_collection, bounds.range_sense, row_idx,
1069: 			                                                  bounds.valid_end, boundary_start, expr_idx);
1070: 		}
1071: 		break;
1072: 	}
1073: 	default:
1074: 		throw InternalException("Unsupported window start boundary");
1075: 	}
1076: 
1077: 	switch (bounds.end_boundary) {
1078: 	case WindowBoundary::CURRENT_ROW_ROWS:
1079: 		bounds.window_end = row_idx + 1;
1080: 		break;
1081: 	case WindowBoundary::CURRENT_ROW_RANGE:
1082: 		bounds.window_end = bounds.peer_end;
1083: 		break;
1084: 	case WindowBoundary::UNBOUNDED_FOLLOWING:
1085: 		bounds.window_end = bounds.partition_end;
1086: 		break;
1087: 	case WindowBoundary::EXPR_PRECEDING_ROWS:
1088: 		bounds.window_end = (int64_t)row_idx - boundary_end.GetCell<int64_t>(expr_idx) + 1;
1089: 		break;
1090: 	case WindowBoundary::EXPR_FOLLOWING_ROWS:
1091: 		bounds.window_end = row_idx + boundary_end.GetCell<int64_t>(expr_idx) + 1;
1092: 		break;
1093: 	case WindowBoundary::EXPR_PRECEDING_RANGE: {
1094: 		if (boundary_end.CellIsNull(expr_idx)) {
1095: 			bounds.window_end = bounds.peer_end;
1096: 		} else {
1097: 			bounds.window_end = FindOrderedRangeBound<false>(range_collection, bounds.range_sense, bounds.valid_start,
1098: 			                                                 row_idx, boundary_end, expr_idx);
1099: 		}
1100: 		break;
1101: 	}
1102: 	case WindowBoundary::EXPR_FOLLOWING_RANGE: {
1103: 		if (boundary_end.CellIsNull(expr_idx)) {
1104: 			bounds.window_end = bounds.peer_end;
1105: 		} else {
1106: 			bounds.window_end = FindOrderedRangeBound<false>(range_collection, bounds.range_sense, row_idx,
1107: 			                                                 bounds.valid_end, boundary_end, expr_idx);
1108: 		}
1109: 		break;
1110: 	}
1111: 	default:
1112: 		throw InternalException("Unsupported window end boundary");
1113: 	}
1114: 
1115: 	// clamp windows to partitions if they should exceed
1116: 	if (bounds.window_start < (int64_t)bounds.partition_start) {
1117: 		bounds.window_start = bounds.partition_start;
1118: 	}
1119: 	if (bounds.window_start > (int64_t)bounds.partition_end) {
1120: 		bounds.window_start = bounds.partition_end;
1121: 	}
1122: 	if (bounds.window_end < (int64_t)bounds.partition_start) {
1123: 		bounds.window_end = bounds.partition_start;
1124: 	}
1125: 	if (bounds.window_end > (int64_t)bounds.partition_end) {
1126: 		bounds.window_end = bounds.partition_end;
1127: 	}
1128: 
1129: 	if (bounds.window_start < 0 || bounds.window_end < 0) {
1130: 		throw InternalException("Failed to compute window boundaries");
1131: 	}
1132: }
1133: 
1134: struct WindowExecutor {
1135: 	WindowExecutor(BoundWindowExpression *wexpr, Allocator &allocator, const idx_t count);
1136: 
1137: 	void Sink(DataChunk &input_chunk, const idx_t input_idx, const idx_t total_count);
1138: 	void Finalize(WindowAggregationMode mode);
1139: 
1140: 	void Evaluate(idx_t row_idx, DataChunk &input_chunk, Vector &result, const ValidityMask &partition_mask,
1141: 	              const ValidityMask &order_mask);
1142: 
1143: 	// The function
1144: 	BoundWindowExpression *wexpr;
1145: 
1146: 	// Frame management
1147: 	WindowBoundariesState bounds;
1148: 	uint64_t dense_rank = 1;
1149: 	uint64_t rank_equal = 0;
1150: 	uint64_t rank = 1;
1151: 
1152: 	// Expression collections
1153: 	ChunkCollection payload_collection;
1154: 	ExpressionExecutor payload_executor;
1155: 	DataChunk payload_chunk;
1156: 
1157: 	ExpressionExecutor filter_executor;
1158: 	ValidityMask filter_mask;
1159: 	vector<validity_t> filter_bits;
1160: 	SelectionVector filter_sel;
1161: 
1162: 	// LEAD/LAG Evaluation
1163: 	WindowInputExpression leadlag_offset;
1164: 	WindowInputExpression leadlag_default;
1165: 
1166: 	// evaluate boundaries if present. Parser has checked boundary types.
1167: 	WindowInputExpression boundary_start;
1168: 	WindowInputExpression boundary_end;
1169: 
1170: 	// evaluate RANGE expressions, if needed
1171: 	WindowInputColumn range;
1172: 
1173: 	// IGNORE NULLS
1174: 	ValidityMask ignore_nulls;
1175: 
1176: 	// build a segment tree for frame-adhering aggregates
1177: 	// see http://www.vldb.org/pvldb/vol8/p1058-leis.pdf
1178: 	unique_ptr<WindowSegmentTree> segment_tree = nullptr;
1179: };
1180: 
1181: WindowExecutor::WindowExecutor(BoundWindowExpression *wexpr, Allocator &allocator, const idx_t count)
1182:     : wexpr(wexpr), bounds(wexpr, count), payload_collection(allocator), payload_executor(allocator),
1183:       filter_executor(allocator), leadlag_offset(wexpr->offset_expr.get(), allocator),
1184:       leadlag_default(wexpr->default_expr.get(), allocator), boundary_start(wexpr->start_expr.get(), allocator),
1185:       boundary_end(wexpr->end_expr.get(), allocator),
1186:       range((bounds.has_preceding_range || bounds.has_following_range) ? wexpr->orders[0].expression.get() : nullptr,
1187:             allocator, count)
1188: 
1189: {
1190: 	// TODO we could evaluate those expressions in parallel
1191: 
1192: 	// evaluate the FILTER clause and stuff it into a large mask for compactness and reuse
1193: 	if (wexpr->filter_expr) {
1194: 		// 	Start with all invalid and set the ones that pass
1195: 		filter_bits.resize(ValidityMask::ValidityMaskSize(count), 0);
1196: 		filter_mask.Initialize(filter_bits.data());
1197: 		filter_executor.AddExpression(*wexpr->filter_expr);
1198: 		filter_sel.Initialize(STANDARD_VECTOR_SIZE);
1199: 	}
1200: 
1201: 	// TODO: child may be a scalar, don't need to materialize the whole collection then
1202: 
1203: 	// evaluate inner expressions of window functions, could be more complex
1204: 	vector<Expression *> exprs;
1205: 	for (auto &child : wexpr->children) {
1206: 		exprs.push_back(child.get());
1207: 	}
1208: 	PrepareInputExpressions(exprs.data(), exprs.size(), payload_executor, payload_chunk);
1209: }
1210: 
1211: void WindowExecutor::Sink(DataChunk &input_chunk, const idx_t input_idx, const idx_t total_count) {
1212: 	// Single pass over the input to produce the global data.
1213: 	// Vectorisation for the win...
1214: 
1215: 	// Set up a validity mask for IGNORE NULLS
1216: 	bool check_nulls = false;
1217: 	if (wexpr->ignore_nulls) {
1218: 		switch (wexpr->type) {
1219: 		case ExpressionType::WINDOW_LEAD:
1220: 		case ExpressionType::WINDOW_LAG:
1221: 		case ExpressionType::WINDOW_FIRST_VALUE:
1222: 		case ExpressionType::WINDOW_LAST_VALUE:
1223: 		case ExpressionType::WINDOW_NTH_VALUE:
1224: 			check_nulls = true;
1225: 			break;
1226: 		default:
1227: 			break;
1228: 		}
1229: 	}
1230: 
1231: 	const auto count = input_chunk.size();
1232: 
1233: 	if (!wexpr->children.empty()) {
1234: 		payload_chunk.Reset();
1235: 		payload_executor.Execute(input_chunk, payload_chunk);
1236: 		payload_chunk.Verify();
1237: 		payload_collection.Append(payload_chunk);
1238: 
1239: 		// process payload chunks while they are still piping hot
1240: 		if (check_nulls) {
1241: 			UnifiedVectorFormat vdata;
1242: 			payload_chunk.data[0].ToUnifiedFormat(count, vdata);
1243: 			if (!vdata.validity.AllValid()) {
1244: 				//	Lazily materialise the contents when we find the first NULL
1245: 				if (ignore_nulls.AllValid()) {
1246: 					ignore_nulls.Initialize(total_count);
1247: 				}
1248: 				// Write to the current position
1249: 				// Chunks in a collection are full, so we don't have to worry about raggedness
1250: 				auto dst = ignore_nulls.GetData() + ignore_nulls.EntryCount(input_idx);
1251: 				auto src = vdata.validity.GetData();
1252: 				for (auto entry_count = vdata.validity.EntryCount(count); entry_count-- > 0;) {
1253: 					*dst++ = *src++;
1254: 				}
1255: 			}
1256: 		}
1257: 	}
1258: 
1259: 	if (wexpr->filter_expr) {
1260: 		const auto filtered = filter_executor.SelectExpression(input_chunk, filter_sel);
1261: 		for (idx_t f = 0; f < filtered; ++f) {
1262: 			filter_mask.SetValid(input_idx + filter_sel[f]);
1263: 		}
1264: 	}
1265: 
1266: 	range.Append(input_chunk);
1267: }
1268: 
1269: void WindowExecutor::Finalize(WindowAggregationMode mode) {
1270: 	// build a segment tree for frame-adhering aggregates
1271: 	// see http://www.vldb.org/pvldb/vol8/p1058-leis.pdf
1272: 
1273: 	if (wexpr->aggregate) {
1274: 		segment_tree = make_unique<WindowSegmentTree>(*(wexpr->aggregate), wexpr->bind_info.get(), wexpr->return_type,
1275: 		                                              &payload_collection, filter_mask, mode);
1276: 	}
1277: }
1278: 
1279: void WindowExecutor::Evaluate(idx_t row_idx, DataChunk &input_chunk, Vector &result, const ValidityMask &partition_mask,
1280:                               const ValidityMask &order_mask) {
1281: 	// Evaluate the row-level arguments
1282: 	boundary_start.Execute(input_chunk);
1283: 	boundary_end.Execute(input_chunk);
1284: 
1285: 	leadlag_offset.Execute(input_chunk);
1286: 	leadlag_default.Execute(input_chunk);
1287: 
1288: 	// this is the main loop, go through all sorted rows and compute window function result
1289: 	for (idx_t output_offset = 0; output_offset < input_chunk.size(); ++output_offset, ++row_idx) {
1290: 		// special case, OVER (), aggregate over everything
1291: 		bounds.Update(row_idx, range, output_offset, boundary_start, boundary_end, partition_mask, order_mask);
1292: 		if (WindowNeedsRank(wexpr)) {
1293: 			if (!bounds.is_same_partition || row_idx == 0) { // special case for first row, need to init
1294: 				dense_rank = 1;
1295: 				rank = 1;
1296: 				rank_equal = 0;
1297: 			} else if (!bounds.is_peer) {
1298: 				dense_rank++;
1299: 				rank += rank_equal;
1300: 				rank_equal = 0;
1301: 			}
1302: 			rank_equal++;
1303: 		}
1304: 
1305: 		// if no values are read for window, result is NULL
1306: 		if (bounds.window_start >= bounds.window_end) {
1307: 			FlatVector::SetNull(result, output_offset, true);
1308: 			continue;
1309: 		}
1310: 
1311: 		switch (wexpr->type) {
1312: 		case ExpressionType::WINDOW_AGGREGATE: {
1313: 			segment_tree->Compute(result, output_offset, bounds.window_start, bounds.window_end);
1314: 			break;
1315: 		}
1316: 		case ExpressionType::WINDOW_ROW_NUMBER: {
1317: 			auto rdata = FlatVector::GetData<int64_t>(result);
1318: 			rdata[output_offset] = row_idx - bounds.partition_start + 1;
1319: 			break;
1320: 		}
1321: 		case ExpressionType::WINDOW_RANK_DENSE: {
1322: 			auto rdata = FlatVector::GetData<int64_t>(result);
1323: 			rdata[output_offset] = dense_rank;
1324: 			break;
1325: 		}
1326: 		case ExpressionType::WINDOW_RANK: {
1327: 			auto rdata = FlatVector::GetData<int64_t>(result);
1328: 			rdata[output_offset] = rank;
1329: 			break;
1330: 		}
1331: 		case ExpressionType::WINDOW_PERCENT_RANK: {
1332: 			int64_t denom = (int64_t)bounds.partition_end - bounds.partition_start - 1;
1333: 			double percent_rank = denom > 0 ? ((double)rank - 1) / denom : 0;
1334: 			auto rdata = FlatVector::GetData<double>(result);
1335: 			rdata[output_offset] = percent_rank;
1336: 			break;
1337: 		}
1338: 		case ExpressionType::WINDOW_CUME_DIST: {
1339: 			int64_t denom = (int64_t)bounds.partition_end - bounds.partition_start;
1340: 			double cume_dist = denom > 0 ? ((double)(bounds.peer_end - bounds.partition_start)) / denom : 0;
1341: 			auto rdata = FlatVector::GetData<double>(result);
1342: 			rdata[output_offset] = cume_dist;
1343: 			break;
1344: 		}
1345: 		case ExpressionType::WINDOW_NTILE: {
1346: 			D_ASSERT(payload_collection.ColumnCount() == 1);
1347: 			if (CellIsNull(payload_collection, 0, row_idx)) {
1348: 				FlatVector::SetNull(result, output_offset, true);
1349: 			} else {
1350: 				auto n_param = GetCell<int64_t>(payload_collection, 0, row_idx);
1351: 				if (n_param < 1) {
1352: 					throw InvalidInputException("Argument for ntile must be greater than zero");
1353: 				}
1354: 				// With thanks from SQLite's ntileValueFunc()
1355: 				int64_t n_total = bounds.partition_end - bounds.partition_start;
1356: 				if (n_param > n_total) {
1357: 					// more groups allowed than we have values
1358: 					// map every entry to a unique group
1359: 					n_param = n_total;
1360: 				}
1361: 				int64_t n_size = (n_total / n_param);
1362: 				// find the row idx within the group
1363: 				D_ASSERT(row_idx >= bounds.partition_start);
1364: 				int64_t adjusted_row_idx = row_idx - bounds.partition_start;
1365: 				// now compute the ntile
1366: 				int64_t n_large = n_total - n_param * n_size;
1367: 				int64_t i_small = n_large * (n_size + 1);
1368: 				int64_t result_ntile;
1369: 
1370: 				D_ASSERT((n_large * (n_size + 1) + (n_param - n_large) * n_size) == n_total);
1371: 
1372: 				if (adjusted_row_idx < i_small) {
1373: 					result_ntile = 1 + adjusted_row_idx / (n_size + 1);
1374: 				} else {
1375: 					result_ntile = 1 + n_large + (adjusted_row_idx - i_small) / n_size;
1376: 				}
1377: 				// result has to be between [1, NTILE]
1378: 				D_ASSERT(result_ntile >= 1 && result_ntile <= n_param);
1379: 				auto rdata = FlatVector::GetData<int64_t>(result);
1380: 				rdata[output_offset] = result_ntile;
1381: 			}
1382: 			break;
1383: 		}
1384: 		case ExpressionType::WINDOW_LEAD:
1385: 		case ExpressionType::WINDOW_LAG: {
1386: 			int64_t offset = 1;
1387: 			if (wexpr->offset_expr) {
1388: 				offset = leadlag_offset.GetCell<int64_t>(output_offset);
1389: 			}
1390: 			int64_t val_idx = (int64_t)row_idx;
1391: 			if (wexpr->type == ExpressionType::WINDOW_LEAD) {
1392: 				val_idx += offset;
1393: 			} else {
1394: 				val_idx -= offset;
1395: 			}
1396: 
1397: 			idx_t delta = 0;
1398: 			if (val_idx < (int64_t)row_idx) {
1399: 				// Count backwards
1400: 				delta = idx_t(row_idx - val_idx);
1401: 				val_idx = FindPrevStart(ignore_nulls, bounds.partition_start, row_idx, delta);
1402: 			} else if (val_idx > (int64_t)row_idx) {
1403: 				delta = idx_t(val_idx - row_idx);
1404: 				val_idx = FindNextStart(ignore_nulls, row_idx + 1, bounds.partition_end, delta);
1405: 			}
1406: 			// else offset is zero, so don't move.
1407: 
1408: 			if (!delta) {
1409: 				payload_collection.CopyCell(0, val_idx, result, output_offset);
1410: 			} else if (wexpr->default_expr) {
1411: 				leadlag_default.CopyCell(result, output_offset);
1412: 			} else {
1413: 				FlatVector::SetNull(result, output_offset, true);
1414: 			}
1415: 			break;
1416: 		}
1417: 		case ExpressionType::WINDOW_FIRST_VALUE: {
1418: 			idx_t n = 1;
1419: 			const auto first_idx = FindNextStart(ignore_nulls, bounds.window_start, bounds.window_end, n);
1420: 			payload_collection.CopyCell(0, first_idx, result, output_offset);
1421: 			break;
1422: 		}
1423: 		case ExpressionType::WINDOW_LAST_VALUE: {
1424: 			idx_t n = 1;
1425: 			payload_collection.CopyCell(0, FindPrevStart(ignore_nulls, bounds.window_start, bounds.window_end, n),
1426: 			                            result, output_offset);
1427: 			break;
1428: 		}
1429: 		case ExpressionType::WINDOW_NTH_VALUE: {
1430: 			D_ASSERT(payload_collection.ColumnCount() == 2);
1431: 			// Returns value evaluated at the row that is the n'th row of the window frame (counting from 1);
1432: 			// returns NULL if there is no such row.
1433: 			if (CellIsNull(payload_collection, 1, row_idx)) {
1434: 				FlatVector::SetNull(result, output_offset, true);
1435: 			} else {
1436: 				auto n_param = GetCell<int64_t>(payload_collection, 1, row_idx);
1437: 				if (n_param < 1) {
1438: 					FlatVector::SetNull(result, output_offset, true);
1439: 				} else {
1440: 					auto n = idx_t(n_param);
1441: 					const auto nth_index = FindNextStart(ignore_nulls, bounds.window_start, bounds.window_end, n);
1442: 					if (!n) {
1443: 						payload_collection.CopyCell(0, nth_index, result, output_offset);
1444: 					} else {
1445: 						FlatVector::SetNull(result, output_offset, true);
1446: 					}
1447: 				}
1448: 			}
1449: 			break;
1450: 		}
1451: 		default:
1452: 			throw InternalException("Window aggregate type %s", ExpressionTypeToString(wexpr->type));
1453: 		}
1454: 	}
1455: 
1456: 	result.Verify(input_chunk.size());
1457: }
1458: 
1459: //===--------------------------------------------------------------------===//
1460: // Sink
1461: //===--------------------------------------------------------------------===//
1462: SinkResultType PhysicalWindow::Sink(ExecutionContext &context, GlobalSinkState &gstate_p, LocalSinkState &lstate_p,
1463:                                     DataChunk &input) const {
1464: 	auto &gstate = (WindowGlobalSinkState &)gstate_p;
1465: 	auto &lstate = (WindowLocalSinkState &)lstate_p;
1466: 
1467: 	lstate.Sink(input, gstate);
1468: 
1469: 	return SinkResultType::NEED_MORE_INPUT;
1470: }
1471: 
1472: void PhysicalWindow::Combine(ExecutionContext &context, GlobalSinkState &gstate_p, LocalSinkState &lstate_p) const {
1473: 	auto &gstate = (WindowGlobalSinkState &)gstate_p;
1474: 	auto &lstate = (WindowLocalSinkState &)lstate_p;
1475: 	lstate.Combine(gstate);
1476: }
1477: 
1478: unique_ptr<LocalSinkState> PhysicalWindow::GetLocalSinkState(ExecutionContext &context) const {
1479: 	return make_unique<WindowLocalSinkState>(Allocator::Get(context.client), *this);
1480: }
1481: 
1482: unique_ptr<GlobalSinkState> PhysicalWindow::GetGlobalSinkState(ClientContext &context) const {
1483: 	return make_unique<WindowGlobalSinkState>(*this, context);
1484: }
1485: 
1486: enum class WindowSortStage : uint8_t { INIT, PREPARE, MERGE, SORTED };
1487: 
1488: class WindowGlobalMergeState;
1489: 
1490: class WindowLocalMergeState {
1491: public:
1492: 	WindowLocalMergeState() : merge_state(nullptr), stage(WindowSortStage::INIT) {
1493: 		finished = true;
1494: 	}
1495: 
1496: 	bool TaskFinished() {
1497: 		return finished;
1498: 	}
1499: 	void ExecuteTask();
1500: 
1501: 	WindowGlobalMergeState *merge_state;
1502: 	WindowSortStage stage;
1503: 	atomic<bool> finished;
1504: };
1505: 
1506: class WindowGlobalMergeState {
1507: public:
1508: 	explicit WindowGlobalMergeState(GlobalSortState &sort_state)
1509: 	    : sort_state(sort_state), stage(WindowSortStage::INIT), total_tasks(0), tasks_assigned(0), tasks_completed(0) {
1510: 	}
1511: 
1512: 	bool IsSorted() const {
1513: 		lock_guard<mutex> guard(lock);
1514: 		return stage == WindowSortStage::SORTED;
1515: 	}
1516: 
1517: 	bool AssignTask(WindowLocalMergeState &local_state);
1518: 	bool TryPrepareNextStage();
1519: 	void CompleteTask();
1520: 
1521: 	GlobalSortState &sort_state;
1522: 
1523: private:
1524: 	mutable mutex lock;
1525: 	WindowSortStage stage;
1526: 	idx_t total_tasks;
1527: 	idx_t tasks_assigned;
1528: 	idx_t tasks_completed;
1529: };
1530: 
1531: void WindowLocalMergeState::ExecuteTask() {
1532: 	auto &global_sort = merge_state->sort_state;
1533: 	switch (stage) {
1534: 	case WindowSortStage::PREPARE:
1535: 		global_sort.PrepareMergePhase();
1536: 		break;
1537: 	case WindowSortStage::MERGE: {
1538: 		MergeSorter merge_sorter(global_sort, global_sort.buffer_manager);
1539: 		merge_sorter.PerformInMergeRound();
1540: 		break;
1541: 	}
1542: 	default:
1543: 		throw InternalException("Unexpected WindowGlobalMergeState in ExecuteTask!");
1544: 	}
1545: 
1546: 	merge_state->CompleteTask();
1547: 	finished = true;
1548: }
1549: 
1550: bool WindowGlobalMergeState::AssignTask(WindowLocalMergeState &local_state) {
1551: 	lock_guard<mutex> guard(lock);
1552: 
1553: 	if (tasks_assigned >= total_tasks) {
1554: 		return false;
1555: 	}
1556: 
1557: 	local_state.merge_state = this;
1558: 	local_state.stage = stage;
1559: 	local_state.finished = false;
1560: 	tasks_assigned++;
1561: 
1562: 	return true;
1563: }
1564: 
1565: void WindowGlobalMergeState::CompleteTask() {
1566: 	lock_guard<mutex> guard(lock);
1567: 
1568: 	++tasks_completed;
1569: }
1570: 
1571: bool WindowGlobalMergeState::TryPrepareNextStage() {
1572: 	lock_guard<mutex> guard(lock);
1573: 
1574: 	if (tasks_completed < total_tasks) {
1575: 		return false;
1576: 	}
1577: 
1578: 	tasks_assigned = tasks_completed = 0;
1579: 
1580: 	switch (stage) {
1581: 	case WindowSortStage::INIT:
1582: 		total_tasks = 1;
1583: 		stage = WindowSortStage::PREPARE;
1584: 		return true;
1585: 
1586: 	case WindowSortStage::PREPARE:
1587: 		total_tasks = sort_state.sorted_blocks.size() / 2;
1588: 		if (!total_tasks) {
1589: 			break;
1590: 		}
1591: 		stage = WindowSortStage::MERGE;
1592: 		sort_state.InitializeMergeRound();
1593: 		return true;
1594: 
1595: 	case WindowSortStage::MERGE:
1596: 		sort_state.CompleteMergeRound(true);
1597: 		total_tasks = sort_state.sorted_blocks.size() / 2;
1598: 		if (!total_tasks) {
1599: 			break;
1600: 		}
1601: 		sort_state.InitializeMergeRound();
1602: 		return true;
1603: 
1604: 	case WindowSortStage::SORTED:
1605: 		break;
1606: 	}
1607: 
1608: 	stage = WindowSortStage::SORTED;
1609: 
1610: 	return false;
1611: }
1612: 
1613: class WindowGlobalMergeStates {
1614: public:
1615: 	using WindowGlobalMergeStatePtr = unique_ptr<WindowGlobalMergeState>;
1616: 
1617: 	WindowGlobalMergeStates(WindowGlobalSinkState &sink, idx_t group) {
1618: 		// Schedule all the sorts for maximum thread utilisation
1619: 		for (; group < sink.hash_groups.size(); group = sink.GetNextSortGroup()) {
1620: 			auto &hash_group = *sink.hash_groups[group];
1621: 
1622: 			// Prepare for merge sort phase
1623: 			auto state = make_unique<WindowGlobalMergeState>(*hash_group.global_sort);
1624: 			states.emplace_back(move(state));
1625: 		}
1626: 	}
1627: 
1628: 	vector<WindowGlobalMergeStatePtr> states;
1629: };
1630: 
1631: class WindowMergeTask : public ExecutorTask {
1632: public:
1633: 	WindowMergeTask(shared_ptr<Event> event_p, ClientContext &context_p, WindowGlobalMergeStates &hash_groups_p)
1634: 	    : ExecutorTask(context_p), event(move(event_p)), hash_groups(hash_groups_p) {
1635: 	}
1636: 
1637: 	TaskExecutionResult ExecuteTask(TaskExecutionMode mode) override;
1638: 
1639: private:
1640: 	shared_ptr<Event> event;
1641: 	WindowLocalMergeState local_state;
1642: 	WindowGlobalMergeStates &hash_groups;
1643: };
1644: 
1645: TaskExecutionResult WindowMergeTask::ExecuteTask(TaskExecutionMode mode) {
1646: 	// Loop until all hash groups are done
1647: 	size_t sorted = 0;
1648: 	while (sorted < hash_groups.states.size()) {
1649: 		// First check if there is an unfinished task for this thread
1650: 		if (!local_state.TaskFinished()) {
1651: 			local_state.ExecuteTask();
1652: 			continue;
1653: 		}
1654: 
1655: 		// Thread is done with its assigned task, try to fetch new work
1656: 		for (auto group = sorted; group < hash_groups.states.size(); ++group) {
1657: 			auto &global_state = hash_groups.states[group];
1658: 			if (global_state->IsSorted()) {
1659: 				// This hash group is done
1660: 				// Update the high water mark of densely completed groups
1661: 				if (sorted == group) {
1662: 					++sorted;
1663: 				}
1664: 				continue;
1665: 			}
1666: 
1667: 			// Try to assign work for this hash group to this thread
1668: 			if (global_state->AssignTask(local_state)) {
1669: 				// We assigned a task to this thread!
1670: 				// Break out of this loop to re-enter the top-level loop and execute the task
1671: 				break;
1672: 			}
1673: 
1674: 			// Hash group global state couldn't assign a task to this thread
1675: 			// Try to prepare the next stage
1676: 			if (!global_state->TryPrepareNextStage()) {
1677: 				// This current hash group is not yet done
1678: 				// But we were not able to assign a task for it to this thread
1679: 				// See if the next hash group is better
1680: 				continue;
1681: 			}
1682: 
1683: 			// We were able to prepare the next stage for this hash group!
1684: 			// Try to assign a task once more
1685: 			if (global_state->AssignTask(local_state)) {
1686: 				// We assigned a task to this thread!
1687: 				// Break out of this loop to re-enter the top-level loop and execute the task
1688: 				break;
1689: 			}
1690: 
1691: 			// We were able to prepare the next merge round,
1692: 			// but we were not able to assign a task for it to this thread
1693: 			// The tasks were assigned to other threads while this thread waited for the lock
1694: 			// Go to the next iteration to see if another hash group has a task
1695: 		}
1696: 	}
1697: 
1698: 	event->FinishTask();
1699: 	return TaskExecutionResult::TASK_FINISHED;
1700: }
1701: 
1702: class WindowMergeEvent : public BasePipelineEvent {
1703: public:
1704: 	WindowMergeEvent(WindowGlobalSinkState &gstate_p, Pipeline &pipeline_p, idx_t group)
1705: 	    : BasePipelineEvent(pipeline_p), gstate(gstate_p), merge_states(gstate_p, group) {
1706: 	}
1707: 
1708: 	WindowGlobalSinkState &gstate;
1709: 	WindowGlobalMergeStates merge_states;
1710: 
1711: public:
1712: 	void Schedule() override {
1713: 		auto &context = pipeline->GetClientContext();
1714: 
1715: 		// Schedule tasks equal to the number of threads, which will each merge multiple partitions
1716: 		auto &ts = TaskScheduler::GetScheduler(context);
1717: 		idx_t num_threads = ts.NumberOfThreads();
1718: 
1719: 		vector<unique_ptr<Task>> merge_tasks;
1720: 		for (idx_t tnum = 0; tnum < num_threads; tnum++) {
1721: 			merge_tasks.push_back(make_unique<WindowMergeTask>(shared_from_this(), context, merge_states));
1722: 		}
1723: 		SetTasks(move(merge_tasks));
1724: 	}
1725: };
1726: 
1727: SinkFinalizeType PhysicalWindow::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
1728:                                           GlobalSinkState &gstate_p) const {
1729: 	auto &state = (WindowGlobalSinkState &)gstate_p;
1730: 
1731: 	// Do we have any sorting to schedule?
1732: 	if (state.rows) {
1733: 		D_ASSERT(state.hash_groups.empty());
1734: 		return state.rows->count ? SinkFinalizeType::READY : SinkFinalizeType::NO_OUTPUT_POSSIBLE;
1735: 	}
1736: 
1737: 	// Find the first group to sort
1738: 	state.Finalize();
1739: 	D_ASSERT(state.count == state.GroupCount());
1740: 	auto group = state.GetNextSortGroup();
1741: 	if (group >= state.hash_groups.size()) {
1742: 		// Empty input!
1743: 		return SinkFinalizeType::NO_OUTPUT_POSSIBLE;
1744: 	}
1745: 
1746: 	// Schedule all the sorts for maximum thread utilisation
1747: 	auto new_event = make_shared<WindowMergeEvent>(state, pipeline, group);
1748: 	event.InsertEvent(move(new_event));
1749: 
1750: 	return SinkFinalizeType::READY;
1751: }
1752: 
1753: //===--------------------------------------------------------------------===//
1754: // Source
1755: //===--------------------------------------------------------------------===//
1756: class WindowGlobalSourceState : public GlobalSourceState {
1757: public:
1758: 	explicit WindowGlobalSourceState(const PhysicalWindow &op) : op(op), next_bin(0) {
1759: 	}
1760: 
1761: 	const PhysicalWindow &op;
1762: 	//! The output read position.
1763: 	atomic<idx_t> next_bin;
1764: 
1765: public:
1766: 	idx_t MaxThreads() override {
1767: 		auto &state = (WindowGlobalSinkState &)*op.sink_state;
1768: 
1769: 		// If there is only one partition, we have to process it on one thread.
1770: 		if (state.hash_groups.empty()) {
1771: 			return 1;
1772: 		}
1773: 
1774: 		idx_t max_threads = 0;
1775: 		for (const auto &hash_group : state.hash_groups) {
1776: 			if (hash_group) {
1777: 				max_threads++;
1778: 			}
1779: 		}
1780: 
1781: 		return max_threads;
1782: 	}
1783: };
1784: 
1785: // Per-thread read state
1786: class WindowLocalSourceState : public LocalSourceState {
1787: public:
1788: 	using HashGroupPtr = unique_ptr<WindowGlobalHashGroup>;
1789: 	using WindowExecutorPtr = unique_ptr<WindowExecutor>;
1790: 	using WindowExecutors = vector<WindowExecutorPtr>;
1791: 
1792: 	WindowLocalSourceState(Allocator &allocator_p, const PhysicalWindow &op, ExecutionContext &context)
1793: 	    : allocator(allocator_p) {
1794: 		vector<LogicalType> output_types;
1795: 		for (idx_t expr_idx = 0; expr_idx < op.select_list.size(); ++expr_idx) {
1796: 			D_ASSERT(op.select_list[expr_idx]->GetExpressionClass() == ExpressionClass::BOUND_WINDOW);
1797: 			auto wexpr = reinterpret_cast<BoundWindowExpression *>(op.select_list[expr_idx].get());
1798: 			output_types.emplace_back(wexpr->return_type);
1799: 		}
1800: 		output_chunk.Initialize(allocator, output_types);
1801: 
1802: 		const auto &input_types = op.children[0]->types;
1803: 		layout.Initialize(input_types);
1804: 		input_chunk.Initialize(allocator, input_types);
1805: 	}
1806: 
1807: 	void MaterializeSortedData();
1808: 	void GeneratePartition(WindowGlobalSinkState &gstate, const idx_t hash_bin);
1809: 	void Scan(DataChunk &chunk);
1810: 
1811: 	HashGroupPtr hash_group;
1812: 	Allocator &allocator;
1813: 
1814: 	//! The generated input chunks
1815: 	unique_ptr<RowDataCollection> rows;
1816: 	unique_ptr<RowDataCollection> heap;
1817: 	RowLayout layout;
1818: 	//! The partition boundary mask
1819: 	vector<validity_t> partition_bits;
1820: 	ValidityMask partition_mask;
1821: 	//! The order boundary mask
1822: 	vector<validity_t> order_bits;
1823: 	ValidityMask order_mask;
1824: 	//! The current execution functions
1825: 	WindowExecutors window_execs;
1826: 
1827: 	//! The read partition
1828: 	idx_t hash_bin;
1829: 	//! The read cursor
1830: 	unique_ptr<RowDataCollectionScanner> scanner;
1831: 	//! Buffer for the inputs
1832: 	DataChunk input_chunk;
1833: 	//! Buffer for window results
1834: 	DataChunk output_chunk;
1835: };
1836: 
1837: void WindowLocalSourceState::MaterializeSortedData() {
1838: 	auto &global_sort_state = *hash_group->global_sort;
1839: 	if (global_sort_state.sorted_blocks.empty()) {
1840: 		return;
1841: 	}
1842: 
1843: 	// scan the sorted row data
1844: 	D_ASSERT(global_sort_state.sorted_blocks.size() == 1);
1845: 	auto &sb = *global_sort_state.sorted_blocks[0];
1846: 
1847: 	// Free up some memory before allocating more
1848: 	sb.radix_sorting_data.clear();
1849: 	sb.blob_sorting_data = nullptr;
1850: 
1851: 	// Move the sorting row blocks into our RDCs
1852: 	auto &buffer_manager = global_sort_state.buffer_manager;
1853: 	auto &sd = *sb.payload_data;
1854: 
1855: 	// Data blocks are required
1856: 	D_ASSERT(!sd.data_blocks.empty());
1857: 	auto &block = sd.data_blocks[0];
1858: 	rows = make_unique<RowDataCollection>(buffer_manager, block->capacity, block->entry_size);
1859: 	rows->blocks = move(sd.data_blocks);
1860: 	rows->count = std::accumulate(rows->blocks.begin(), rows->blocks.end(), idx_t(0),
1861: 	                              [&](idx_t c, const unique_ptr<RowDataBlock> &b) { return c + b->count; });
1862: 
1863: 	// Heap blocks are optional, but we want both for iteration.
1864: 	if (!sd.heap_blocks.empty()) {
1865: 		auto &block = sd.heap_blocks[0];
1866: 		heap = make_unique<RowDataCollection>(buffer_manager, block->capacity, block->entry_size);
1867: 		heap->blocks = move(sd.heap_blocks);
1868: 		hash_group.reset();
1869: 	} else {
1870: 		heap = make_unique<RowDataCollection>(buffer_manager, (idx_t)Storage::BLOCK_SIZE, 1, true);
1871: 	}
1872: 	heap->count = std::accumulate(heap->blocks.begin(), heap->blocks.end(), idx_t(0),
1873: 	                              [&](idx_t c, const unique_ptr<RowDataBlock> &b) { return c + b->count; });
1874: }
1875: 
1876: void WindowLocalSourceState::GeneratePartition(WindowGlobalSinkState &gstate, const idx_t hash_bin_p) {
1877: 	auto &op = (PhysicalWindow &)gstate.op;
1878: 
1879: 	//	Get rid of any stale data
1880: 	hash_bin = hash_bin_p;
1881: 	hash_group.reset();
1882: 
1883: 	// There are three types of partitions:
1884: 	// 1. No partition (no sorting)
1885: 	// 2. One partition (sorting, but no hashing)
1886: 	// 3. Multiple partitions (sorting and hashing)
1887: 
1888: 	//	How big is the partition?
1889: 	idx_t count = 0;
1890: 	if (hash_bin < gstate.hash_groups.size() && gstate.hash_groups[hash_bin]) {
1891: 		count = gstate.hash_groups[hash_bin]->count;
1892: 	} else if (gstate.rows && !hash_bin) {
1893: 		count = gstate.count;
1894: 	} else {
1895: 		return;
1896: 	}
1897: 
1898: 	// Create the executors for each function
1899: 	window_execs.clear();
1900: 	for (idx_t expr_idx = 0; expr_idx < op.select_list.size(); ++expr_idx) {
1901: 		D_ASSERT(op.select_list[expr_idx]->GetExpressionClass() == ExpressionClass::BOUND_WINDOW);
1902: 		auto wexpr = reinterpret_cast<BoundWindowExpression *>(op.select_list[expr_idx].get());
1903: 		auto wexec = make_unique<WindowExecutor>(wexpr, allocator, count);
1904: 		window_execs.emplace_back(move(wexec));
1905: 	}
1906: 
1907: 	//	Initialise masks to false
1908: 	const auto bit_count = ValidityMask::ValidityMaskSize(count);
1909: 	partition_bits.clear();
1910: 	partition_bits.resize(bit_count, 0);
1911: 	partition_mask.Initialize(partition_bits.data());
1912: 
1913: 	order_bits.clear();
1914: 	order_bits.resize(bit_count, 0);
1915: 	order_mask.Initialize(order_bits.data());
1916: 
1917: 	// Scan the sorted data into new Collections
1918: 	auto external = gstate.external;
1919: 	if (gstate.rows && !hash_bin) {
1920: 		// Simple mask
1921: 		partition_mask.SetValidUnsafe(0);
1922: 		order_mask.SetValidUnsafe(0);
1923: 		//	No partition - align the heap blocks with the row blocks
1924: 		rows = gstate.rows->CloneEmpty(gstate.rows->keep_pinned);
1925: 		heap = gstate.strings->CloneEmpty(gstate.strings->keep_pinned);
1926: 		RowDataCollectionScanner::AlignHeapBlocks(*rows, *heap, *gstate.rows, *gstate.strings, layout);
1927: 		external = true;
1928: 	} else if (hash_bin < gstate.hash_groups.size() && gstate.hash_groups[hash_bin]) {
1929: 		// Overwrite the collections with the sorted data
1930: 		hash_group = move(gstate.hash_groups[hash_bin]);
1931: 		hash_group->ComputeMasks(partition_mask, order_mask);
1932: 		MaterializeSortedData();
1933: 	} else {
1934: 		return;
1935: 	}
1936: 
1937: 	//	First pass over the input without flushing
1938: 	//	TODO: Factor out the constructor data as global state
1939: 	scanner = make_unique<RowDataCollectionScanner>(*rows, *heap, layout, external, false);
1940: 	idx_t input_idx = 0;
1941: 	while (true) {
1942: 		input_chunk.Reset();
1943: 		scanner->Scan(input_chunk);
1944: 		if (input_chunk.size() == 0) {
1945: 			break;
1946: 		}
1947: 
1948: 		//	TODO: Parallelization opportunity
1949: 		for (auto &wexec : window_execs) {
1950: 			wexec->Sink(input_chunk, input_idx, scanner->Count());
1951: 		}
1952: 		input_idx += input_chunk.size();
1953: 	}
1954: 
1955: 	//	TODO: Parallelization opportunity
1956: 	for (auto &wexec : window_execs) {
1957: 		wexec->Finalize(gstate.mode);
1958: 	}
1959: 
1960: 	// External scanning assumes all blocks are swizzled.
1961: 	scanner->ReSwizzle();
1962: 
1963: 	//	Second pass can flush
1964: 	scanner = make_unique<RowDataCollectionScanner>(*rows, *heap, layout, external, true);
1965: }
1966: 
1967: void WindowLocalSourceState::Scan(DataChunk &result) {
1968: 	D_ASSERT(scanner);
1969: 	if (!scanner->Remaining()) {
1970: 		scanner.reset();
1971: 		return;
1972: 	}
1973: 
1974: 	const auto position = scanner->Scanned();
1975: 	input_chunk.Reset();
1976: 	scanner->Scan(input_chunk);
1977: 
1978: 	output_chunk.Reset();
1979: 	for (idx_t expr_idx = 0; expr_idx < window_execs.size(); ++expr_idx) {
1980: 		auto &executor = *window_execs[expr_idx];
1981: 		executor.Evaluate(position, input_chunk, output_chunk.data[expr_idx], partition_mask, order_mask);
1982: 	}
1983: 	output_chunk.SetCardinality(input_chunk);
1984: 	output_chunk.Verify();
1985: 
1986: 	idx_t out_idx = 0;
1987: 	result.SetCardinality(input_chunk);
1988: 	for (idx_t col_idx = 0; col_idx < input_chunk.ColumnCount(); col_idx++) {
1989: 		result.data[out_idx++].Reference(input_chunk.data[col_idx]);
1990: 	}
1991: 	for (idx_t col_idx = 0; col_idx < output_chunk.ColumnCount(); col_idx++) {
1992: 		result.data[out_idx++].Reference(output_chunk.data[col_idx]);
1993: 	}
1994: 	result.Verify();
1995: }
1996: 
1997: unique_ptr<LocalSourceState> PhysicalWindow::GetLocalSourceState(ExecutionContext &context,
1998:                                                                  GlobalSourceState &gstate) const {
1999: 	return make_unique<WindowLocalSourceState>(Allocator::Get(context.client), *this, context);
2000: }
2001: 
2002: unique_ptr<GlobalSourceState> PhysicalWindow::GetGlobalSourceState(ClientContext &context) const {
2003: 	return make_unique<WindowGlobalSourceState>(*this);
2004: }
2005: 
2006: void PhysicalWindow::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate_p,
2007:                              LocalSourceState &lstate_p) const {
2008: 	auto &state = (WindowLocalSourceState &)lstate_p;
2009: 	auto &global_source = (WindowGlobalSourceState &)gstate_p;
2010: 	auto &gstate = (WindowGlobalSinkState &)*sink_state;
2011: 
2012: 	const auto bin_count = gstate.hash_groups.empty() ? 1 : gstate.hash_groups.size();
2013: 
2014: 	//	Move to the next bin if we are done.
2015: 	while (!state.scanner || !state.scanner->Remaining()) {
2016: 		state.scanner.reset();
2017: 		state.rows.reset();
2018: 		state.heap.reset();
2019: 		auto hash_bin = global_source.next_bin++;
2020: 		if (hash_bin >= bin_count) {
2021: 			return;
2022: 		}
2023: 
2024: 		for (; hash_bin < gstate.hash_groups.size(); hash_bin = global_source.next_bin++) {
2025: 			if (gstate.hash_groups[hash_bin]) {
2026: 				break;
2027: 			}
2028: 		}
2029: 		state.GeneratePartition(gstate, hash_bin);
2030: 	}
2031: 
2032: 	state.Scan(chunk);
2033: }
2034: 
2035: string PhysicalWindow::ParamsToString() const {
2036: 	string result;
2037: 	for (idx_t i = 0; i < select_list.size(); i++) {
2038: 		if (i > 0) {
2039: 			result += "\n";
2040: 		}
2041: 		result += select_list[i]->GetName();
2042: 	}
2043: 	return result;
2044: }
2045: 
2046: } // namespace duckdb
[end of src/execution/operator/aggregate/physical_window.cpp]
[start of src/execution/window_segment_tree.cpp]
1: #include "duckdb/execution/window_segment_tree.hpp"
2: 
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: #include "duckdb/common/algorithm.hpp"
5: #include "duckdb/common/helper.hpp"
6: 
7: namespace duckdb {
8: 
9: WindowSegmentTree::WindowSegmentTree(AggregateFunction &aggregate, FunctionData *bind_info,
10:                                      const LogicalType &result_type_p, ChunkCollection *input,
11:                                      const ValidityMask &filter_mask_p, WindowAggregationMode mode_p)
12:     : aggregate(aggregate), bind_info(bind_info), result_type(result_type_p), state(aggregate.state_size()),
13:       statep(Value::POINTER((idx_t)state.data())), frame(0, 0), active(0, 1),
14:       statev(Value::POINTER((idx_t)state.data())), internal_nodes(0), input_ref(input), filter_mask(filter_mask_p),
15:       mode(mode_p) {
16: #if STANDARD_VECTOR_SIZE < 512
17: 	throw NotImplementedException("Window functions are not supported for vector sizes < 512");
18: #endif
19: 	statep.Flatten(STANDARD_VECTOR_SIZE);
20: 	statev.SetVectorType(VectorType::FLAT_VECTOR); // Prevent conversion of results to constants
21: 
22: 	if (input_ref && input_ref->ColumnCount() > 0) {
23: 		filter_sel.Initialize(STANDARD_VECTOR_SIZE);
24: 		inputs.Initialize(Allocator::DefaultAllocator(), input_ref->Types());
25: 		// if we have a frame-by-frame method, share the single state
26: 		if (aggregate.window && UseWindowAPI()) {
27: 			AggregateInit();
28: 			inputs.Reference(input_ref->GetChunk(0));
29: 		} else if (aggregate.combine && UseCombineAPI()) {
30: 			ConstructTree();
31: 		}
32: 	}
33: }
34: 
35: WindowSegmentTree::~WindowSegmentTree() {
36: 	if (!aggregate.destructor) {
37: 		// nothing to destroy
38: 		return;
39: 	}
40: 	// call the destructor for all the intermediate states
41: 	data_ptr_t address_data[STANDARD_VECTOR_SIZE];
42: 	Vector addresses(LogicalType::POINTER, (data_ptr_t)address_data);
43: 	idx_t count = 0;
44: 	for (idx_t i = 0; i < internal_nodes; i++) {
45: 		address_data[count++] = data_ptr_t(levels_flat_native.get() + i * state.size());
46: 		if (count == STANDARD_VECTOR_SIZE) {
47: 			aggregate.destructor(addresses, count);
48: 			count = 0;
49: 		}
50: 	}
51: 	if (count > 0) {
52: 		aggregate.destructor(addresses, count);
53: 	}
54: 
55: 	if (aggregate.window && UseWindowAPI()) {
56: 		aggregate.destructor(statev, 1);
57: 	}
58: }
59: 
60: void WindowSegmentTree::AggregateInit() {
61: 	aggregate.initialize(state.data());
62: }
63: 
64: void WindowSegmentTree::AggegateFinal(Vector &result, idx_t rid) {
65: 	AggregateInputData aggr_input_data(bind_info, Allocator::DefaultAllocator());
66: 	aggregate.finalize(statev, aggr_input_data, result, 1, rid);
67: 
68: 	if (aggregate.destructor) {
69: 		aggregate.destructor(statev, 1);
70: 	}
71: }
72: 
73: void WindowSegmentTree::ExtractFrame(idx_t begin, idx_t end) {
74: 	const auto size = end - begin;
75: 	if (size >= STANDARD_VECTOR_SIZE) {
76: 		throw InternalException("Cannot compute window aggregation: bounds are too large");
77: 	}
78: 
79: 	const idx_t start_in_vector = begin % STANDARD_VECTOR_SIZE;
80: 	const auto input_count = input_ref->ColumnCount();
81: 	if (start_in_vector + size <= STANDARD_VECTOR_SIZE) {
82: 		inputs.SetCardinality(size);
83: 		auto &chunk = input_ref->GetChunkForRow(begin);
84: 		for (idx_t i = 0; i < input_count; ++i) {
85: 			auto &v = inputs.data[i];
86: 			auto &vec = chunk.data[i];
87: 			v.Slice(vec, start_in_vector);
88: 			v.Verify(size);
89: 		}
90: 	} else {
91: 		inputs.Reset();
92: 		inputs.SetCardinality(size);
93: 
94: 		// we cannot just slice the individual vector!
95: 		auto &chunk_a = input_ref->GetChunkForRow(begin);
96: 		auto &chunk_b = input_ref->GetChunkForRow(end);
97: 		idx_t chunk_a_count = chunk_a.size() - start_in_vector;
98: 		idx_t chunk_b_count = inputs.size() - chunk_a_count;
99: 		for (idx_t i = 0; i < input_count; ++i) {
100: 			auto &v = inputs.data[i];
101: 			VectorOperations::Copy(chunk_a.data[i], v, chunk_a.size(), start_in_vector, 0);
102: 			VectorOperations::Copy(chunk_b.data[i], v, chunk_b_count, 0, chunk_a_count);
103: 		}
104: 	}
105: 
106: 	// Slice to any filtered rows
107: 	if (!filter_mask.AllValid()) {
108: 		idx_t filtered = 0;
109: 		for (idx_t i = begin; i < end; ++i) {
110: 			if (filter_mask.RowIsValid(i)) {
111: 				filter_sel.set_index(filtered++, i - begin);
112: 			}
113: 		}
114: 		if (filtered != inputs.size()) {
115: 			inputs.Slice(filter_sel, filtered);
116: 		}
117: 	}
118: }
119: 
120: void WindowSegmentTree::WindowSegmentValue(idx_t l_idx, idx_t begin, idx_t end) {
121: 	D_ASSERT(begin <= end);
122: 	if (begin == end) {
123: 		return;
124: 	}
125: 
126: 	if (end - begin >= STANDARD_VECTOR_SIZE) {
127: 		throw InternalException("Cannot compute window aggregation: bounds are too large");
128: 	}
129: 
130: 	Vector s(statep, 0);
131: 	if (l_idx == 0) {
132: 		ExtractFrame(begin, end);
133: 		AggregateInputData aggr_input_data(bind_info, Allocator::DefaultAllocator());
134: 		aggregate.update(&inputs.data[0], aggr_input_data, input_ref->ColumnCount(), s, inputs.size());
135: 	} else {
136: 		inputs.Reset();
137: 		inputs.SetCardinality(end - begin);
138: 		// find out where the states begin
139: 		data_ptr_t begin_ptr = levels_flat_native.get() + state.size() * (begin + levels_flat_start[l_idx - 1]);
140: 		// set up a vector of pointers that point towards the set of states
141: 		Vector v(LogicalType::POINTER);
142: 		auto pdata = FlatVector::GetData<data_ptr_t>(v);
143: 		for (idx_t i = 0; i < inputs.size(); i++) {
144: 			pdata[i] = begin_ptr + i * state.size();
145: 		}
146: 		v.Verify(inputs.size());
147: 		AggregateInputData aggr_input_data(bind_info, Allocator::DefaultAllocator());
148: 		aggregate.combine(v, s, aggr_input_data, inputs.size());
149: 	}
150: }
151: 
152: void WindowSegmentTree::ConstructTree() {
153: 	D_ASSERT(input_ref);
154: 	D_ASSERT(inputs.ColumnCount() > 0);
155: 
156: 	// compute space required to store internal nodes of segment tree
157: 	internal_nodes = 0;
158: 	idx_t level_nodes = input_ref->Count();
159: 	do {
160: 		level_nodes = (level_nodes + (TREE_FANOUT - 1)) / TREE_FANOUT;
161: 		internal_nodes += level_nodes;
162: 	} while (level_nodes > 1);
163: 	levels_flat_native = unique_ptr<data_t[]>(new data_t[internal_nodes * state.size()]);
164: 	levels_flat_start.push_back(0);
165: 
166: 	idx_t levels_flat_offset = 0;
167: 	idx_t level_current = 0;
168: 	// level 0 is data itself
169: 	idx_t level_size;
170: 	// iterate over the levels of the segment tree
171: 	while ((level_size = (level_current == 0 ? input_ref->Count()
172: 	                                         : levels_flat_offset - levels_flat_start[level_current - 1])) > 1) {
173: 		for (idx_t pos = 0; pos < level_size; pos += TREE_FANOUT) {
174: 			// compute the aggregate for this entry in the segment tree
175: 			AggregateInit();
176: 			WindowSegmentValue(level_current, pos, MinValue(level_size, pos + TREE_FANOUT));
177: 
178: 			memcpy(levels_flat_native.get() + (levels_flat_offset * state.size()), state.data(), state.size());
179: 
180: 			levels_flat_offset++;
181: 		}
182: 
183: 		levels_flat_start.push_back(levels_flat_offset);
184: 		level_current++;
185: 	}
186: 
187: 	// Corner case: single element in the window
188: 	if (levels_flat_offset == 0) {
189: 		aggregate.initialize(levels_flat_native.get());
190: 	}
191: }
192: 
193: void WindowSegmentTree::Compute(Vector &result, idx_t rid, idx_t begin, idx_t end) {
194: 	D_ASSERT(input_ref);
195: 
196: 	// No arguments, so just count
197: 	if (inputs.ColumnCount() == 0) {
198: 		D_ASSERT(GetTypeIdSize(result_type.InternalType()) == sizeof(idx_t));
199: 		auto data = FlatVector::GetData<idx_t>(result);
200: 		// Slice to any filtered rows
201: 		if (!filter_mask.AllValid()) {
202: 			idx_t filtered = 0;
203: 			for (idx_t i = begin; i < end; ++i) {
204: 				filtered += filter_mask.RowIsValid(i);
205: 			}
206: 			data[rid] = filtered;
207: 		} else {
208: 			data[rid] = end - begin;
209: 		}
210: 		return;
211: 	}
212: 
213: 	// If we have a window function, use that
214: 	if (aggregate.window && UseWindowAPI()) {
215: 		// Frame boundaries
216: 		auto prev = frame;
217: 		frame = FrameBounds(begin, end);
218: 
219: 		// Extract the range
220: 		auto &coll = *input_ref;
221: 		const auto prev_active = active;
222: 		const FrameBounds combined(MinValue(frame.first, prev.first), MaxValue(frame.second, prev.second));
223: 
224: 		// The chunk bounds are the range that includes the begin and end - 1
225: 		const FrameBounds prev_chunks(coll.LocateChunk(prev_active.first), coll.LocateChunk(prev_active.second - 1));
226: 		const FrameBounds active_chunks(coll.LocateChunk(combined.first), coll.LocateChunk(combined.second - 1));
227: 
228: 		// Extract the range
229: 		if (active_chunks.first == active_chunks.second) {
230: 			// If all the data is in a single chunk, then just reference it
231: 			if (prev_chunks != active_chunks || (!prev.first && !prev.second)) {
232: 				inputs.Reference(coll.GetChunk(active_chunks.first));
233: 			}
234: 		} else if (active_chunks.first == prev_chunks.first && prev_chunks.first != prev_chunks.second) {
235: 			// If the start chunk did not change, and we are not just a reference, then extend if necessary
236: 			for (auto chunk_idx = prev_chunks.second + 1; chunk_idx <= active_chunks.second; ++chunk_idx) {
237: 				inputs.Append(coll.GetChunk(chunk_idx), true);
238: 			}
239: 		} else {
240: 			// If the first chunk changed, start over
241: 			inputs.Reset();
242: 			for (auto chunk_idx = active_chunks.first; chunk_idx <= active_chunks.second; ++chunk_idx) {
243: 				inputs.Append(coll.GetChunk(chunk_idx), true);
244: 			}
245: 		}
246: 
247: 		active = FrameBounds(active_chunks.first * STANDARD_VECTOR_SIZE,
248: 		                     MinValue((active_chunks.second + 1) * STANDARD_VECTOR_SIZE, coll.Count()));
249: 
250: 		AggregateInputData aggr_input_data(bind_info, Allocator::DefaultAllocator());
251: 		aggregate.window(inputs.data.data(), filter_mask, aggr_input_data, inputs.ColumnCount(), state.data(), frame,
252: 		                 prev, result, rid, active.first);
253: 		return;
254: 	}
255: 
256: 	AggregateInit();
257: 
258: 	// Aggregate everything at once if we can't combine states
259: 	if (!aggregate.combine || !UseCombineAPI()) {
260: 		WindowSegmentValue(0, begin, end);
261: 		AggegateFinal(result, rid);
262: 		return;
263: 	}
264: 
265: 	for (idx_t l_idx = 0; l_idx < levels_flat_start.size() + 1; l_idx++) {
266: 		idx_t parent_begin = begin / TREE_FANOUT;
267: 		idx_t parent_end = end / TREE_FANOUT;
268: 		if (parent_begin == parent_end) {
269: 			WindowSegmentValue(l_idx, begin, end);
270: 			break;
271: 		}
272: 		idx_t group_begin = parent_begin * TREE_FANOUT;
273: 		if (begin != group_begin) {
274: 			WindowSegmentValue(l_idx, begin, group_begin + TREE_FANOUT);
275: 			parent_begin++;
276: 		}
277: 		idx_t group_end = parent_end * TREE_FANOUT;
278: 		if (end != group_end) {
279: 			WindowSegmentValue(l_idx, group_end, end);
280: 		}
281: 		begin = parent_begin;
282: 		end = parent_end;
283: 	}
284: 
285: 	AggegateFinal(result, rid);
286: }
287: 
288: } // namespace duckdb
[end of src/execution/window_segment_tree.cpp]
[start of src/function/aggregate/distributive/approx_count.cpp]
1: #include "duckdb/common/exception.hpp"
2: #include "duckdb/common/types/hash.hpp"
3: #include "duckdb/common/types/hyperloglog.hpp"
4: #include "duckdb/common/vector_operations/vector_operations.hpp"
5: #include "duckdb/function/aggregate/distributive_functions.hpp"
6: #include "duckdb/function/function_set.hpp"
7: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
8: 
9: namespace duckdb {
10: 
11: struct ApproxDistinctCountState {
12: 	HyperLogLog *log;
13: };
14: 
15: struct ApproxCountDistinctFunction {
16: 	template <class STATE>
17: 	static void Initialize(STATE *state) {
18: 		state->log = nullptr;
19: 	}
20: 
21: 	template <class STATE, class OP>
22: 	static void Combine(const STATE &source, STATE *target, AggregateInputData &) {
23: 		if (!source.log) {
24: 			return;
25: 		}
26: 		if (!target->log) {
27: 			target->log = new HyperLogLog();
28: 		}
29: 		D_ASSERT(target->log);
30: 		D_ASSERT(source.log);
31: 		auto new_log = target->log->MergePointer(*source.log);
32: 		delete target->log;
33: 		target->log = new_log;
34: 	}
35: 
36: 	template <class T, class STATE>
37: 	static void Finalize(Vector &result, AggregateInputData &, STATE *state, T *target, ValidityMask &mask, idx_t idx) {
38: 		if (state->log) {
39: 			target[idx] = state->log->Count();
40: 		} else {
41: 			target[idx] = 0;
42: 		}
43: 	}
44: 
45: 	static bool IgnoreNull() {
46: 		return true;
47: 	}
48: 	template <class STATE>
49: 	static void Destroy(STATE *state) {
50: 		if (state->log) {
51: 			delete state->log;
52: 		}
53: 	}
54: };
55: 
56: static void ApproxCountDistinctSimpleUpdateFunction(Vector inputs[], AggregateInputData &, idx_t input_count,
57:                                                     data_ptr_t state, idx_t count) {
58: 	D_ASSERT(input_count == 1);
59: 
60: 	auto agg_state = (ApproxDistinctCountState *)state;
61: 	if (!agg_state->log) {
62: 		agg_state->log = new HyperLogLog();
63: 	}
64: 
65: 	UnifiedVectorFormat vdata;
66: 	inputs[0].ToUnifiedFormat(count, vdata);
67: 
68: 	uint64_t indices[STANDARD_VECTOR_SIZE];
69: 	uint8_t counts[STANDARD_VECTOR_SIZE];
70: 
71: 	HyperLogLog::ProcessEntries(vdata, inputs[0].GetType(), indices, counts, count);
72: 	agg_state->log->AddToLog(vdata, count, indices, counts);
73: }
74: 
75: static void ApproxCountDistinctUpdateFunction(Vector inputs[], AggregateInputData &, idx_t input_count,
76:                                               Vector &state_vector, idx_t count) {
77: 	D_ASSERT(input_count == 1);
78: 
79: 	UnifiedVectorFormat sdata;
80: 	state_vector.ToUnifiedFormat(count, sdata);
81: 	auto states = (ApproxDistinctCountState **)sdata.data;
82: 
83: 	for (idx_t i = 0; i < count; i++) {
84: 		auto agg_state = states[sdata.sel->get_index(i)];
85: 		if (!agg_state->log) {
86: 			agg_state->log = new HyperLogLog();
87: 		}
88: 	}
89: 
90: 	UnifiedVectorFormat vdata;
91: 	inputs[0].ToUnifiedFormat(count, vdata);
92: 
93: 	uint64_t indices[STANDARD_VECTOR_SIZE];
94: 	uint8_t counts[STANDARD_VECTOR_SIZE];
95: 
96: 	HyperLogLog::ProcessEntries(vdata, inputs[0].GetType(), indices, counts, count);
97: 	HyperLogLog::AddToLogs(vdata, count, indices, counts, (HyperLogLog ***)states, sdata.sel);
98: }
99: 
100: AggregateFunction GetApproxCountDistinctFunction(const LogicalType &input_type) {
101: 	auto fun = AggregateFunction(
102: 	    {input_type}, LogicalTypeId::BIGINT, AggregateFunction::StateSize<ApproxDistinctCountState>,
103: 	    AggregateFunction::StateInitialize<ApproxDistinctCountState, ApproxCountDistinctFunction>,
104: 	    ApproxCountDistinctUpdateFunction,
105: 	    AggregateFunction::StateCombine<ApproxDistinctCountState, ApproxCountDistinctFunction>,
106: 	    AggregateFunction::StateFinalize<ApproxDistinctCountState, int64_t, ApproxCountDistinctFunction>,
107: 	    ApproxCountDistinctSimpleUpdateFunction, nullptr,
108: 	    AggregateFunction::StateDestroy<ApproxDistinctCountState, ApproxCountDistinctFunction>);
109: 	fun.null_handling = FunctionNullHandling::SPECIAL_HANDLING;
110: 	return fun;
111: }
112: 
113: void ApproxCountDistinctFun::RegisterFunction(BuiltinFunctions &set) {
114: 	AggregateFunctionSet approx_count("approx_count_distinct");
115: 	approx_count.AddFunction(GetApproxCountDistinctFunction(LogicalType::UTINYINT));
116: 	approx_count.AddFunction(GetApproxCountDistinctFunction(LogicalType::USMALLINT));
117: 	approx_count.AddFunction(GetApproxCountDistinctFunction(LogicalType::UINTEGER));
118: 	approx_count.AddFunction(GetApproxCountDistinctFunction(LogicalType::UBIGINT));
119: 	approx_count.AddFunction(GetApproxCountDistinctFunction(LogicalType::TINYINT));
120: 	approx_count.AddFunction(GetApproxCountDistinctFunction(LogicalType::SMALLINT));
121: 	approx_count.AddFunction(GetApproxCountDistinctFunction(LogicalType::BIGINT));
122: 	approx_count.AddFunction(GetApproxCountDistinctFunction(LogicalType::HUGEINT));
123: 	approx_count.AddFunction(GetApproxCountDistinctFunction(LogicalType::FLOAT));
124: 	approx_count.AddFunction(GetApproxCountDistinctFunction(LogicalType::DOUBLE));
125: 	approx_count.AddFunction(GetApproxCountDistinctFunction(LogicalType::VARCHAR));
126: 	approx_count.AddFunction(GetApproxCountDistinctFunction(LogicalType::TIMESTAMP));
127: 	approx_count.AddFunction(GetApproxCountDistinctFunction(LogicalType::TIMESTAMP_TZ));
128: 	set.AddFunction(approx_count);
129: }
130: 
131: } // namespace duckdb
[end of src/function/aggregate/distributive/approx_count.cpp]
[start of src/include/duckdb/common/types/validity_mask.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/validity_mask.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/types.hpp"
13: #include "duckdb/common/vector_size.hpp"
14: #include "duckdb/common/to_string.hpp"
15: 
16: namespace duckdb {
17: struct ValidityMask;
18: 
19: template <typename V>
20: struct TemplatedValidityData {
21: 	static constexpr const int BITS_PER_VALUE = sizeof(V) * 8;
22: 	static constexpr const V MAX_ENTRY = ~V(0);
23: 
24: public:
25: 	inline explicit TemplatedValidityData(idx_t count) {
26: 		auto entry_count = EntryCount(count);
27: 		owned_data = unique_ptr<V[]>(new V[entry_count]);
28: 		for (idx_t entry_idx = 0; entry_idx < entry_count; entry_idx++) {
29: 			owned_data[entry_idx] = MAX_ENTRY;
30: 		}
31: 	}
32: 	inline TemplatedValidityData(const V *validity_mask, idx_t count) {
33: 		D_ASSERT(validity_mask);
34: 		auto entry_count = EntryCount(count);
35: 		owned_data = unique_ptr<V[]>(new V[entry_count]);
36: 		for (idx_t entry_idx = 0; entry_idx < entry_count; entry_idx++) {
37: 			owned_data[entry_idx] = validity_mask[entry_idx];
38: 		}
39: 	}
40: 
41: 	unique_ptr<V[]> owned_data;
42: 
43: public:
44: 	static inline idx_t EntryCount(idx_t count) {
45: 		return (count + (BITS_PER_VALUE - 1)) / BITS_PER_VALUE;
46: 	}
47: };
48: 
49: using validity_t = uint64_t;
50: 
51: struct ValidityData : TemplatedValidityData<validity_t> {
52: public:
53: 	DUCKDB_API explicit ValidityData(idx_t count);
54: 	DUCKDB_API ValidityData(const ValidityMask &original, idx_t count);
55: };
56: 
57: //! Type used for validity masks
58: template <typename V>
59: struct TemplatedValidityMask {
60: 	using ValidityBuffer = TemplatedValidityData<V>;
61: 
62: public:
63: 	static constexpr const int BITS_PER_VALUE = ValidityBuffer::BITS_PER_VALUE;
64: 	static constexpr const int STANDARD_ENTRY_COUNT = (STANDARD_VECTOR_SIZE + (BITS_PER_VALUE - 1)) / BITS_PER_VALUE;
65: 	static constexpr const int STANDARD_MASK_SIZE = STANDARD_ENTRY_COUNT * sizeof(validity_t);
66: 
67: public:
68: 	inline TemplatedValidityMask() : validity_mask(nullptr) {
69: 	}
70: 	inline explicit TemplatedValidityMask(idx_t max_count) {
71: 		Initialize(max_count);
72: 	}
73: 	inline explicit TemplatedValidityMask(V *ptr) : validity_mask(ptr) {
74: 	}
75: 	inline TemplatedValidityMask(const TemplatedValidityMask &original, idx_t count) {
76: 		Copy(original, count);
77: 	}
78: 
79: 	static inline idx_t ValidityMaskSize(idx_t count = STANDARD_VECTOR_SIZE) {
80: 		return ValidityBuffer::EntryCount(count) * sizeof(V);
81: 	}
82: 	inline bool AllValid() const {
83: 		return !validity_mask;
84: 	}
85: 	inline bool CheckAllValid(idx_t count) const {
86: 		if (AllValid()) {
87: 			return true;
88: 		}
89: 		idx_t entry_count = ValidityBuffer::EntryCount(count);
90: 		idx_t valid_count = 0;
91: 		for (idx_t i = 0; i < entry_count; i++) {
92: 			valid_count += validity_mask[i] == ValidityBuffer::MAX_ENTRY;
93: 		}
94: 		return valid_count == entry_count;
95: 	}
96: 
97: 	inline bool CheckAllValid(idx_t to, idx_t from) const {
98: 		if (AllValid()) {
99: 			return true;
100: 		}
101: 		for (idx_t i = from; i < to; i++) {
102: 			if (!RowIsValid(i)) {
103: 				return false;
104: 			}
105: 		}
106: 		return true;
107: 	}
108: 
109: 	idx_t CountValid(const idx_t count) const {
110: 		if (AllValid() || count == 0) {
111: 			return count;
112: 		}
113: 
114: 		idx_t valid = 0;
115: 		const auto entry_count = EntryCount(count);
116: 		for (idx_t entry_idx = 0; entry_idx < entry_count;) {
117: 			auto entry = GetValidityEntry(entry_idx++);
118: 			// Handle ragged end
119: 			if (entry_idx == entry_count) {
120: 				idx_t idx_in_entry;
121: 				GetEntryIndex(count, entry_idx, idx_in_entry);
122: 				for (idx_t i = 0; i < idx_in_entry; ++i) {
123: 					valid += idx_t(RowIsValid(entry, i));
124: 				}
125: 				break;
126: 			}
127: 
128: 			// Handle all set
129: 			if (AllValid(entry)) {
130: 				valid += BITS_PER_VALUE;
131: 				continue;
132: 			}
133: 
134: 			// Count partial entry (Kernighan's algorithm)
135: 			while (entry) {
136: 				entry &= (entry - 1);
137: 				++valid;
138: 			}
139: 		}
140: 
141: 		return valid;
142: 	}
143: 
144: 	inline V *GetData() const {
145: 		return validity_mask;
146: 	}
147: 	inline void Reset() {
148: 		validity_mask = nullptr;
149: 		validity_data.reset();
150: 	}
151: 
152: 	static inline idx_t EntryCount(idx_t count) {
153: 		return ValidityBuffer::EntryCount(count);
154: 	}
155: 	inline V GetValidityEntry(idx_t entry_idx) const {
156: 		if (!validity_mask) {
157: 			return ValidityBuffer::MAX_ENTRY;
158: 		}
159: 		return validity_mask[entry_idx];
160: 	}
161: 	static inline bool AllValid(V entry) {
162: 		return entry == ValidityBuffer::MAX_ENTRY;
163: 	}
164: 	static inline bool NoneValid(V entry) {
165: 		return entry == 0;
166: 	}
167: 	static inline bool RowIsValid(V entry, idx_t idx_in_entry) {
168: 		return entry & (V(1) << V(idx_in_entry));
169: 	}
170: 	static inline void GetEntryIndex(idx_t row_idx, idx_t &entry_idx, idx_t &idx_in_entry) {
171: 		entry_idx = row_idx / BITS_PER_VALUE;
172: 		idx_in_entry = row_idx % BITS_PER_VALUE;
173: 	}
174: 	//! Get an entry that has first-n bits set as valid and rest set as invalid
175: 	static inline V EntryWithValidBits(idx_t n) {
176: 		if (n == 0) {
177: 			return V(0);
178: 		}
179: 		return ValidityBuffer::MAX_ENTRY >> (BITS_PER_VALUE - n);
180: 	}
181: 
182: 	//! RowIsValidUnsafe should only be used if AllValid() is false: it achieves the same as RowIsValid but skips a
183: 	//! not-null check
184: 	inline bool RowIsValidUnsafe(idx_t row_idx) const {
185: 		D_ASSERT(validity_mask);
186: 		idx_t entry_idx, idx_in_entry;
187: 		GetEntryIndex(row_idx, entry_idx, idx_in_entry);
188: 		auto entry = GetValidityEntry(entry_idx);
189: 		return RowIsValid(entry, idx_in_entry);
190: 	}
191: 
192: 	//! Returns true if a row is valid (i.e. not null), false otherwise
193: 	inline bool RowIsValid(idx_t row_idx) const {
194: 		if (!validity_mask) {
195: 			return true;
196: 		}
197: 		return RowIsValidUnsafe(row_idx);
198: 	}
199: 
200: 	//! Same as SetValid, but skips a null check on validity_mask
201: 	inline void SetValidUnsafe(idx_t row_idx) {
202: 		D_ASSERT(validity_mask);
203: 		idx_t entry_idx, idx_in_entry;
204: 		GetEntryIndex(row_idx, entry_idx, idx_in_entry);
205: 		validity_mask[entry_idx] |= (V(1) << V(idx_in_entry));
206: 	}
207: 
208: 	//! Marks the entry at the specified row index as valid (i.e. not-null)
209: 	inline void SetValid(idx_t row_idx) {
210: 		if (!validity_mask) {
211: 			// if AllValid() we don't need to do anything
212: 			// the row is already valid
213: 			return;
214: 		}
215: 		SetValidUnsafe(row_idx);
216: 	}
217: 
218: 	//! Marks the bit at the specified entry as invalid (i.e. null)
219: 	inline void SetInvalidUnsafe(idx_t entry_idx, idx_t idx_in_entry) {
220: 		D_ASSERT(validity_mask);
221: 		validity_mask[entry_idx] &= ~(V(1) << V(idx_in_entry));
222: 	}
223: 
224: 	//! Marks the bit at the specified row index as invalid (i.e. null)
225: 	inline void SetInvalidUnsafe(idx_t row_idx) {
226: 		idx_t entry_idx, idx_in_entry;
227: 		GetEntryIndex(row_idx, entry_idx, idx_in_entry);
228: 		SetInvalidUnsafe(entry_idx, idx_in_entry);
229: 	}
230: 
231: 	//! Marks the entry at the specified row index as invalid (i.e. null)
232: 	inline void SetInvalid(idx_t row_idx) {
233: 		if (!validity_mask) {
234: 			D_ASSERT(row_idx <= STANDARD_VECTOR_SIZE);
235: 			Initialize(STANDARD_VECTOR_SIZE);
236: 		}
237: 		SetInvalidUnsafe(row_idx);
238: 	}
239: 
240: 	//! Mark the entry at the specified index as either valid or invalid (non-null or null)
241: 	inline void Set(idx_t row_idx, bool valid) {
242: 		if (valid) {
243: 			SetValid(row_idx);
244: 		} else {
245: 			SetInvalid(row_idx);
246: 		}
247: 	}
248: 
249: 	//! Ensure the validity mask is writable, allocating space if it is not initialized
250: 	inline void EnsureWritable() {
251: 		if (!validity_mask) {
252: 			Initialize();
253: 		}
254: 	}
255: 
256: 	//! Marks exactly "count" bits in the validity mask as invalid (null)
257: 	inline void SetAllInvalid(idx_t count) {
258: 		EnsureWritable();
259: 		if (count == 0) {
260: 			return;
261: 		}
262: 		auto last_entry_index = ValidityBuffer::EntryCount(count) - 1;
263: 		for (idx_t i = 0; i < last_entry_index; i++) {
264: 			validity_mask[i] = 0;
265: 		}
266: 		auto last_entry_bits = count % static_cast<idx_t>(BITS_PER_VALUE);
267: 		validity_mask[last_entry_index] = (last_entry_bits == 0) ? 0 : (ValidityBuffer::MAX_ENTRY << (last_entry_bits));
268: 	}
269: 
270: 	//! Marks exactly "count" bits in the validity mask as valid (not null)
271: 	inline void SetAllValid(idx_t count) {
272: 		EnsureWritable();
273: 		if (count == 0) {
274: 			return;
275: 		}
276: 		auto last_entry_index = ValidityBuffer::EntryCount(count) - 1;
277: 		for (idx_t i = 0; i < last_entry_index; i++) {
278: 			validity_mask[i] = ValidityBuffer::MAX_ENTRY;
279: 		}
280: 		auto last_entry_bits = count % static_cast<idx_t>(BITS_PER_VALUE);
281: 		validity_mask[last_entry_index] |=
282: 		    (last_entry_bits == 0) ? ValidityBuffer::MAX_ENTRY : ~(ValidityBuffer::MAX_ENTRY << (last_entry_bits));
283: 	}
284: 
285: 	inline bool IsMaskSet() const {
286: 		if (validity_mask) {
287: 			return true;
288: 		}
289: 		return false;
290: 	}
291: 
292: public:
293: 	inline void Initialize(validity_t *validity) {
294: 		validity_data.reset();
295: 		validity_mask = validity;
296: 	}
297: 	inline void Initialize(const TemplatedValidityMask &other) {
298: 		validity_mask = other.validity_mask;
299: 		validity_data = other.validity_data;
300: 	}
301: 	inline void Initialize(idx_t count = STANDARD_VECTOR_SIZE) {
302: 		validity_data = make_buffer<ValidityBuffer>(count);
303: 		validity_mask = validity_data->owned_data.get();
304: 	}
305: 	inline void Copy(const TemplatedValidityMask &other, idx_t count) {
306: 		if (other.AllValid()) {
307: 			validity_data = nullptr;
308: 			validity_mask = nullptr;
309: 		} else {
310: 			validity_data = make_buffer<ValidityBuffer>(other.validity_mask, count);
311: 			validity_mask = validity_data->owned_data.get();
312: 		}
313: 	}
314: 
315: protected:
316: 	V *validity_mask;
317: 	buffer_ptr<ValidityBuffer> validity_data;
318: };
319: 
320: struct ValidityMask : public TemplatedValidityMask<validity_t> {
321: public:
322: 	inline ValidityMask() : TemplatedValidityMask(nullptr) {
323: 	}
324: 	inline explicit ValidityMask(idx_t max_count) : TemplatedValidityMask(max_count) {
325: 	}
326: 	inline explicit ValidityMask(validity_t *ptr) : TemplatedValidityMask(ptr) {
327: 	}
328: 	inline ValidityMask(const ValidityMask &original, idx_t count) : TemplatedValidityMask(original, count) {
329: 	}
330: 
331: public:
332: 	DUCKDB_API void Resize(idx_t old_size, idx_t new_size);
333: 
334: 	DUCKDB_API void Slice(const ValidityMask &other, idx_t offset);
335: 	DUCKDB_API void Combine(const ValidityMask &other, idx_t count);
336: 	DUCKDB_API string ToString(idx_t count) const;
337: };
338: 
339: } // namespace duckdb
[end of src/include/duckdb/common/types/validity_mask.hpp]
[start of src/include/duckdb/common/types/vector.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/vector.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/bitset.hpp"
12: #include "duckdb/common/common.hpp"
13: #include "duckdb/common/enums/vector_type.hpp"
14: #include "duckdb/common/types/selection_vector.hpp"
15: #include "duckdb/common/types/validity_mask.hpp"
16: #include "duckdb/common/types/value.hpp"
17: #include "duckdb/common/types/vector_buffer.hpp"
18: #include "duckdb/common/vector_size.hpp"
19: 
20: namespace duckdb {
21: 
22: struct UnifiedVectorFormat {
23: 	const SelectionVector *sel;
24: 	data_ptr_t data;
25: 	ValidityMask validity;
26: 	SelectionVector owned_sel;
27: };
28: 
29: class VectorCache;
30: class VectorStructBuffer;
31: class VectorListBuffer;
32: 
33: struct SelCache;
34: 
35: //!  Vector of values of a specified PhysicalType.
36: class Vector {
37: 	friend struct ConstantVector;
38: 	friend struct DictionaryVector;
39: 	friend struct FlatVector;
40: 	friend struct ListVector;
41: 	friend struct StringVector;
42: 	friend struct FSSTVector;
43: 	friend struct StructVector;
44: 	friend struct SequenceVector;
45: 
46: 	friend class DataChunk;
47: 	friend class VectorCacheBuffer;
48: 
49: public:
50: 	//! Create a vector that references the other vector
51: 	DUCKDB_API explicit Vector(Vector &other);
52: 	//! Create a vector that slices another vector
53: 	DUCKDB_API explicit Vector(Vector &other, const SelectionVector &sel, idx_t count);
54: 	//! Create a vector that slices another vector starting from a specific offset
55: 	DUCKDB_API explicit Vector(Vector &other, idx_t offset);
56: 	//! Create a vector of size one holding the passed on value
57: 	DUCKDB_API explicit Vector(const Value &value);
58: 	//! Create a vector of size tuple_count (non-standard)
59: 	DUCKDB_API explicit Vector(LogicalType type, idx_t capacity = STANDARD_VECTOR_SIZE);
60: 	//! Create an empty standard vector with a type, equivalent to calling Vector(type, true, false)
61: 	DUCKDB_API explicit Vector(const VectorCache &cache);
62: 	//! Create a non-owning vector that references the specified data
63: 	DUCKDB_API Vector(LogicalType type, data_ptr_t dataptr);
64: 	//! Create an owning vector that holds at most STANDARD_VECTOR_SIZE entries.
65: 	/*!
66: 	    Create a new vector
67: 	    If create_data is true, the vector will be an owning empty vector.
68: 	    If zero_data is true, the allocated data will be zero-initialized.
69: 	*/
70: 	DUCKDB_API Vector(LogicalType type, bool create_data, bool zero_data, idx_t capacity = STANDARD_VECTOR_SIZE);
71: 	// implicit copying of Vectors is not allowed
72: 	Vector(const Vector &) = delete;
73: 	// but moving of vectors is allowed
74: 	DUCKDB_API Vector(Vector &&other) noexcept;
75: 
76: public:
77: 	//! Create a vector that references the specified value.
78: 	DUCKDB_API void Reference(const Value &value);
79: 	//! Causes this vector to reference the data held by the other vector.
80: 	//! The type of the "other" vector should match the type of this vector
81: 	DUCKDB_API void Reference(Vector &other);
82: 	//! Reinterpret the data of the other vector as the type of this vector
83: 	//! Note that this takes the data of the other vector as-is and places it in this vector
84: 	//! Without changing the type of this vector
85: 	DUCKDB_API void Reinterpret(Vector &other);
86: 
87: 	//! Causes this vector to reference the data held by the other vector, changes the type if required.
88: 	DUCKDB_API void ReferenceAndSetType(Vector &other);
89: 
90: 	//! Resets a vector from a vector cache.
91: 	//! This turns the vector back into an empty FlatVector with STANDARD_VECTOR_SIZE entries.
92: 	//! The VectorCache is used so this can be done without requiring any allocations.
93: 	DUCKDB_API void ResetFromCache(const VectorCache &cache);
94: 
95: 	//! Creates a reference to a slice of the other vector
96: 	DUCKDB_API void Slice(Vector &other, idx_t offset);
97: 	//! Creates a reference to a slice of the other vector
98: 	DUCKDB_API void Slice(Vector &other, const SelectionVector &sel, idx_t count);
99: 	//! Turns the vector into a dictionary vector with the specified dictionary
100: 	DUCKDB_API void Slice(const SelectionVector &sel, idx_t count);
101: 	//! Slice the vector, keeping the result around in a cache or potentially using the cache instead of slicing
102: 	DUCKDB_API void Slice(const SelectionVector &sel, idx_t count, SelCache &cache);
103: 
104: 	//! Creates the data of this vector with the specified type. Any data that
105: 	//! is currently in the vector is destroyed.
106: 	DUCKDB_API void Initialize(bool zero_data = false, idx_t capacity = STANDARD_VECTOR_SIZE);
107: 
108: 	//! Converts this Vector to a printable string representation
109: 	DUCKDB_API string ToString(idx_t count) const;
110: 	DUCKDB_API void Print(idx_t count);
111: 
112: 	DUCKDB_API string ToString() const;
113: 	DUCKDB_API void Print();
114: 
115: 	//! Flatten the vector, removing any compression and turning it into a FLAT_VECTOR
116: 	DUCKDB_API void Flatten(idx_t count);
117: 	DUCKDB_API void Flatten(const SelectionVector &sel, idx_t count);
118: 	//! Creates a UnifiedVectorFormat of a vector
119: 	//! The UnifiedVectorFormat allows efficient reading of vectors regardless of their vector type
120: 	//! It contains (1) a data pointer, (2) a validity mask, and (3) a selection vector
121: 	//! Access to the individual vector elements can be performed through data_pointer[sel_idx[i]]/validity[sel_idx[i]]
122: 	//! The most common vector types (flat, constant & dictionary) can be converted to the canonical format "for free"
123: 	//! ToUnifiedFormat was originally called Orrify, as a tribute to Orri Erling who came up with it
124: 	DUCKDB_API void ToUnifiedFormat(idx_t count, UnifiedVectorFormat &data);
125: 
126: 	//! Turn the vector into a sequence vector
127: 	DUCKDB_API void Sequence(int64_t start, int64_t increment, idx_t count);
128: 
129: 	//! Verify that the Vector is in a consistent, not corrupt state. DEBUG
130: 	//! FUNCTION ONLY!
131: 	DUCKDB_API void Verify(idx_t count);
132: 	//! Asserts that the CheckMapValidity returns MapInvalidReason::VALID
133: 	DUCKDB_API static void VerifyMap(Vector &map, const SelectionVector &sel, idx_t count);
134: 	DUCKDB_API static void Verify(Vector &vector, const SelectionVector &sel, idx_t count);
135: 	DUCKDB_API void UTFVerify(idx_t count);
136: 	DUCKDB_API void UTFVerify(const SelectionVector &sel, idx_t count);
137: 
138: 	//! Returns the [index] element of the Vector as a Value.
139: 	DUCKDB_API Value GetValue(idx_t index) const;
140: 	//! Sets the [index] element of the Vector to the specified Value.
141: 	DUCKDB_API void SetValue(idx_t index, const Value &val);
142: 
143: 	inline void SetAuxiliary(buffer_ptr<VectorBuffer> new_buffer) {
144: 		auxiliary = std::move(new_buffer);
145: 	};
146: 
147: 	//! This functions resizes the vector
148: 	DUCKDB_API void Resize(idx_t cur_size, idx_t new_size);
149: 
150: 	//! Serializes a Vector to a stand-alone binary blob
151: 	DUCKDB_API void Serialize(idx_t count, Serializer &serializer);
152: 	//! Deserializes a blob back into a Vector
153: 	DUCKDB_API void Deserialize(idx_t count, Deserializer &source);
154: 
155: 	// Getters
156: 	inline VectorType GetVectorType() const {
157: 		return vector_type;
158: 	}
159: 	inline const LogicalType &GetType() const {
160: 		return type;
161: 	}
162: 	inline data_ptr_t GetData() {
163: 		return data;
164: 	}
165: 
166: 	inline buffer_ptr<VectorBuffer> GetAuxiliary() {
167: 		return auxiliary;
168: 	}
169: 
170: 	inline buffer_ptr<VectorBuffer> GetBuffer() {
171: 		return buffer;
172: 	}
173: 
174: 	// Setters
175: 	DUCKDB_API void SetVectorType(VectorType vector_type);
176: 
177: private:
178: 	//! Returns the [index] element of the Vector as a Value.
179: 	static Value GetValue(const Vector &v, idx_t index);
180: 	//! Returns the [index] element of the Vector as a Value.
181: 	static Value GetValueInternal(const Vector &v, idx_t index);
182: 
183: protected:
184: 	//! The vector type specifies how the data of the vector is physically stored (i.e. if it is a single repeated
185: 	//! constant, if it is compressed)
186: 	VectorType vector_type;
187: 	//! The type of the elements stored in the vector (e.g. integer, float)
188: 	LogicalType type;
189: 	//! A pointer to the data.
190: 	data_ptr_t data;
191: 	//! The validity mask of the vector
192: 	ValidityMask validity;
193: 	//! The main buffer holding the data of the vector
194: 	buffer_ptr<VectorBuffer> buffer;
195: 	//! The buffer holding auxiliary data of the vector
196: 	//! e.g. a string vector uses this to store strings
197: 	buffer_ptr<VectorBuffer> auxiliary;
198: };
199: 
200: //! The DictionaryBuffer holds a selection vector
201: class VectorChildBuffer : public VectorBuffer {
202: public:
203: 	VectorChildBuffer(Vector vector) : VectorBuffer(VectorBufferType::VECTOR_CHILD_BUFFER), data(move(vector)) {
204: 	}
205: 
206: public:
207: 	Vector data;
208: };
209: 
210: struct ConstantVector {
211: 	static inline const_data_ptr_t GetData(const Vector &vector) {
212: 		D_ASSERT(vector.GetVectorType() == VectorType::CONSTANT_VECTOR ||
213: 		         vector.GetVectorType() == VectorType::FLAT_VECTOR);
214: 		return vector.data;
215: 	}
216: 	static inline data_ptr_t GetData(Vector &vector) {
217: 		D_ASSERT(vector.GetVectorType() == VectorType::CONSTANT_VECTOR ||
218: 		         vector.GetVectorType() == VectorType::FLAT_VECTOR);
219: 		return vector.data;
220: 	}
221: 	template <class T>
222: 	static inline const T *GetData(const Vector &vector) {
223: 		return (const T *)ConstantVector::GetData(vector);
224: 	}
225: 	template <class T>
226: 	static inline T *GetData(Vector &vector) {
227: 		return (T *)ConstantVector::GetData(vector);
228: 	}
229: 	static inline bool IsNull(const Vector &vector) {
230: 		D_ASSERT(vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
231: 		return !vector.validity.RowIsValid(0);
232: 	}
233: 	DUCKDB_API static void SetNull(Vector &vector, bool is_null);
234: 	static inline ValidityMask &Validity(Vector &vector) {
235: 		D_ASSERT(vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
236: 		return vector.validity;
237: 	}
238: 	DUCKDB_API static const SelectionVector *ZeroSelectionVector(idx_t count, SelectionVector &owned_sel);
239: 	DUCKDB_API static const SelectionVector *ZeroSelectionVector();
240: 	//! Turns "vector" into a constant vector by referencing a value within the source vector
241: 	DUCKDB_API static void Reference(Vector &vector, Vector &source, idx_t position, idx_t count);
242: 
243: 	static const sel_t ZERO_VECTOR[STANDARD_VECTOR_SIZE];
244: };
245: 
246: struct DictionaryVector {
247: 	static inline const SelectionVector &SelVector(const Vector &vector) {
248: 		D_ASSERT(vector.GetVectorType() == VectorType::DICTIONARY_VECTOR);
249: 		return ((const DictionaryBuffer &)*vector.buffer).GetSelVector();
250: 	}
251: 	static inline SelectionVector &SelVector(Vector &vector) {
252: 		D_ASSERT(vector.GetVectorType() == VectorType::DICTIONARY_VECTOR);
253: 		return ((DictionaryBuffer &)*vector.buffer).GetSelVector();
254: 	}
255: 	static inline const Vector &Child(const Vector &vector) {
256: 		D_ASSERT(vector.GetVectorType() == VectorType::DICTIONARY_VECTOR);
257: 		return ((const VectorChildBuffer &)*vector.auxiliary).data;
258: 	}
259: 	static inline Vector &Child(Vector &vector) {
260: 		D_ASSERT(vector.GetVectorType() == VectorType::DICTIONARY_VECTOR);
261: 		return ((VectorChildBuffer &)*vector.auxiliary).data;
262: 	}
263: };
264: 
265: struct FlatVector {
266: 	static inline data_ptr_t GetData(Vector &vector) {
267: 		return ConstantVector::GetData(vector);
268: 	}
269: 	template <class T>
270: 	static inline const T *GetData(const Vector &vector) {
271: 		return ConstantVector::GetData<T>(vector);
272: 	}
273: 	template <class T>
274: 	static inline T *GetData(Vector &vector) {
275: 		return ConstantVector::GetData<T>(vector);
276: 	}
277: 	static inline void SetData(Vector &vector, data_ptr_t data) {
278: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
279: 		vector.data = data;
280: 	}
281: 	template <class T>
282: 	static inline T GetValue(Vector &vector, idx_t idx) {
283: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
284: 		return FlatVector::GetData<T>(vector)[idx];
285: 	}
286: 	static inline const ValidityMask &Validity(const Vector &vector) {
287: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
288: 		return vector.validity;
289: 	}
290: 	static inline ValidityMask &Validity(Vector &vector) {
291: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
292: 		return vector.validity;
293: 	}
294: 	static inline void SetValidity(Vector &vector, ValidityMask &new_validity) {
295: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
296: 		vector.validity.Initialize(new_validity);
297: 	}
298: 	DUCKDB_API static void SetNull(Vector &vector, idx_t idx, bool is_null);
299: 	static inline bool IsNull(const Vector &vector, idx_t idx) {
300: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
301: 		return !vector.validity.RowIsValid(idx);
302: 	}
303: 	DUCKDB_API static const SelectionVector *IncrementalSelectionVector();
304: };
305: 
306: struct ListVector {
307: 	static inline list_entry_t *GetData(Vector &v) {
308: 		if (v.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
309: 			auto &child = DictionaryVector::Child(v);
310: 			return GetData(child);
311: 		}
312: 		return FlatVector::GetData<list_entry_t>(v);
313: 	}
314: 	//! Gets a reference to the underlying child-vector of a list
315: 	DUCKDB_API static const Vector &GetEntry(const Vector &vector);
316: 	//! Gets a reference to the underlying child-vector of a list
317: 	DUCKDB_API static Vector &GetEntry(Vector &vector);
318: 	//! Gets the total size of the underlying child-vector of a list
319: 	DUCKDB_API static idx_t GetListSize(const Vector &vector);
320: 	//! Sets the total size of the underlying child-vector of a list
321: 	DUCKDB_API static void SetListSize(Vector &vec, idx_t size);
322: 	DUCKDB_API static void Reserve(Vector &vec, idx_t required_capacity);
323: 	DUCKDB_API static void Append(Vector &target, const Vector &source, idx_t source_size, idx_t source_offset = 0);
324: 	DUCKDB_API static void Append(Vector &target, const Vector &source, const SelectionVector &sel, idx_t source_size,
325: 	                              idx_t source_offset = 0);
326: 	DUCKDB_API static void PushBack(Vector &target, const Value &insert);
327: 	DUCKDB_API static vector<idx_t> Search(Vector &list, const Value &key, idx_t row);
328: 	DUCKDB_API static Value GetValuesFromOffsets(Vector &list, vector<idx_t> &offsets);
329: 	//! Share the entry of the other list vector
330: 	DUCKDB_API static void ReferenceEntry(Vector &vector, Vector &other);
331: };
332: 
333: struct StringVector {
334: 	//! Add a string to the string heap of the vector (auxiliary data)
335: 	DUCKDB_API static string_t AddString(Vector &vector, const char *data, idx_t len);
336: 	//! Add a string or a blob to the string heap of the vector (auxiliary data)
337: 	//! This function is the same as ::AddString, except the added data does not need to be valid UTF8
338: 	DUCKDB_API static string_t AddStringOrBlob(Vector &vector, const char *data, idx_t len);
339: 	//! Add a string to the string heap of the vector (auxiliary data)
340: 	DUCKDB_API static string_t AddString(Vector &vector, const char *data);
341: 	//! Add a string to the string heap of the vector (auxiliary data)
342: 	DUCKDB_API static string_t AddString(Vector &vector, string_t data);
343: 	//! Add a string to the string heap of the vector (auxiliary data)
344: 	DUCKDB_API static string_t AddString(Vector &vector, const string &data);
345: 	//! Add a string or a blob to the string heap of the vector (auxiliary data)
346: 	//! This function is the same as ::AddString, except the added data does not need to be valid UTF8
347: 	DUCKDB_API static string_t AddStringOrBlob(Vector &vector, string_t data);
348: 	//! Allocates an empty string of the specified size, and returns a writable pointer that can be used to store the
349: 	//! result of an operation
350: 	DUCKDB_API static string_t EmptyString(Vector &vector, idx_t len);
351: 	//! Adds a reference to a handle that stores strings of this vector
352: 	DUCKDB_API static void AddHandle(Vector &vector, BufferHandle handle);
353: 	//! Adds a reference to an unspecified vector buffer that stores strings of this vector
354: 	DUCKDB_API static void AddBuffer(Vector &vector, buffer_ptr<VectorBuffer> buffer);
355: 	//! Add a reference from this vector to the string heap of the provided vector
356: 	DUCKDB_API static void AddHeapReference(Vector &vector, Vector &other);
357: };
358: 
359: struct FSSTVector {
360: 	static inline const ValidityMask &Validity(const Vector &vector) {
361: 		D_ASSERT(vector.GetVectorType() == VectorType::FSST_VECTOR);
362: 		return vector.validity;
363: 	}
364: 	static inline ValidityMask &Validity(Vector &vector) {
365: 		D_ASSERT(vector.GetVectorType() == VectorType::FSST_VECTOR);
366: 		return vector.validity;
367: 	}
368: 	static inline void SetValidity(Vector &vector, ValidityMask &new_validity) {
369: 		D_ASSERT(vector.GetVectorType() == VectorType::FSST_VECTOR);
370: 		vector.validity.Initialize(new_validity);
371: 	}
372: 	static inline const_data_ptr_t GetCompressedData(const Vector &vector) {
373: 		D_ASSERT(vector.GetVectorType() == VectorType::FSST_VECTOR);
374: 		return vector.data;
375: 	}
376: 	static inline data_ptr_t GetCompressedData(Vector &vector) {
377: 		D_ASSERT(vector.GetVectorType() == VectorType::FSST_VECTOR);
378: 		return vector.data;
379: 	}
380: 	template <class T>
381: 	static inline const T *GetCompressedData(const Vector &vector) {
382: 		return (const T *)FSSTVector::GetCompressedData(vector);
383: 	}
384: 	template <class T>
385: 	static inline T *GetCompressedData(Vector &vector) {
386: 		return (T *)FSSTVector::GetCompressedData(vector);
387: 	}
388: 	//! Decompresses an FSST_VECTOR into a FLAT_VECTOR. Note: validity is not copied.
389: 	static void DecompressVector(const Vector &src, Vector &dst, idx_t src_offset, idx_t dst_offset, idx_t copy_count,
390: 	                             const SelectionVector *sel);
391: 
392: 	DUCKDB_API static string_t AddCompressedString(Vector &vector, string_t data);
393: 	DUCKDB_API static string_t AddCompressedString(Vector &vector, const char *data, idx_t len);
394: 	DUCKDB_API static void RegisterDecoder(Vector &vector, buffer_ptr<void> &duckdb_fsst_decoder);
395: 	DUCKDB_API static void *GetDecoder(const Vector &vector);
396: 	//! Setting the string count is required to be able to correctly flatten the vector
397: 	DUCKDB_API static void SetCount(Vector &vector, idx_t count);
398: 	DUCKDB_API static idx_t GetCount(Vector &vector);
399: };
400: 
401: struct MapVector {
402: 	DUCKDB_API static const Vector &GetKeys(const Vector &vector);
403: 	DUCKDB_API static const Vector &GetValues(const Vector &vector);
404: 	DUCKDB_API static Vector &GetKeys(Vector &vector);
405: 	DUCKDB_API static Vector &GetValues(Vector &vector);
406: };
407: 
408: struct StructVector {
409: 	DUCKDB_API static const vector<unique_ptr<Vector>> &GetEntries(const Vector &vector);
410: 	DUCKDB_API static vector<unique_ptr<Vector>> &GetEntries(Vector &vector);
411: };
412: 
413: struct SequenceVector {
414: 	static void GetSequence(const Vector &vector, int64_t &start, int64_t &increment, int64_t &sequence_count) {
415: 		D_ASSERT(vector.GetVectorType() == VectorType::SEQUENCE_VECTOR);
416: 		auto data = (int64_t *)vector.buffer->GetData();
417: 		start = data[0];
418: 		increment = data[1];
419: 		sequence_count = data[2];
420: 	}
421: 	static void GetSequence(const Vector &vector, int64_t &start, int64_t &increment) {
422: 		int64_t sequence_count;
423: 		GetSequence(vector, start, increment, sequence_count);
424: 	}
425: };
426: 
427: } // namespace duckdb
[end of src/include/duckdb/common/types/vector.hpp]
[start of src/include/duckdb/execution/window_segment_tree.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/window_segment_tree.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/chunk_collection.hpp"
12: #include "duckdb/execution/physical_operator.hpp"
13: #include "duckdb/function/aggregate_function.hpp"
14: #include "duckdb/common/enums/window_aggregation_mode.hpp"
15: 
16: namespace duckdb {
17: 
18: class WindowSegmentTree {
19: public:
20: 	using FrameBounds = std::pair<idx_t, idx_t>;
21: 
22: 	WindowSegmentTree(AggregateFunction &aggregate, FunctionData *bind_info, const LogicalType &result_type,
23: 	                  ChunkCollection *input, const ValidityMask &filter_mask, WindowAggregationMode mode);
24: 	~WindowSegmentTree();
25: 
26: 	//! First row contains the result.
27: 	void Compute(Vector &result, idx_t rid, idx_t start, idx_t end);
28: 
29: private:
30: 	void ConstructTree();
31: 	void ExtractFrame(idx_t begin, idx_t end);
32: 	void WindowSegmentValue(idx_t l_idx, idx_t begin, idx_t end);
33: 	void AggregateInit();
34: 	void AggegateFinal(Vector &result, idx_t rid);
35: 
36: 	//! Use the window API, if available
37: 	inline bool UseWindowAPI() const {
38: 		return mode < WindowAggregationMode::COMBINE;
39: 	}
40: 	//! Use the combine API, if available
41: 	inline bool UseCombineAPI() const {
42: 		return mode < WindowAggregationMode::SEPARATE;
43: 	}
44: 
45: 	//! The aggregate that the window function is computed over
46: 	AggregateFunction aggregate;
47: 	//! The bind info of the aggregate
48: 	FunctionData *bind_info;
49: 	//! The result type of the window function
50: 	LogicalType result_type;
51: 
52: 	//! Data pointer that contains a single state, used for intermediate window segment aggregation
53: 	vector<data_t> state;
54: 	//! Input data chunk, used for intermediate window segment aggregation
55: 	DataChunk inputs;
56: 	//! The filtered rows in inputs.
57: 	SelectionVector filter_sel;
58: 	//! A vector of pointers to "state", used for intermediate window segment aggregation
59: 	Vector statep;
60: 	//! The frame boundaries, used for the window functions
61: 	FrameBounds frame;
62: 	//! The active data in the inputs. Used for the window functions
63: 	FrameBounds active;
64: 	//! Reused result state container for the window functions
65: 	Vector statev;
66: 
67: 	//! The actual window segment tree: an array of aggregate states that represent all the intermediate nodes
68: 	unique_ptr<data_t[]> levels_flat_native;
69: 	//! For each level, the starting location in the levels_flat_native array
70: 	vector<idx_t> levels_flat_start;
71: 
72: 	//! The total number of internal nodes of the tree, stored in levels_flat_native
73: 	idx_t internal_nodes;
74: 
75: 	//! The (sorted) input chunk collection on which the tree is built
76: 	ChunkCollection *input_ref;
77: 
78: 	//! The filtered rows in input_ref.
79: 	const ValidityMask &filter_mask;
80: 
81: 	//! Use the window API, if available
82: 	WindowAggregationMode mode;
83: 
84: 	// TREE_FANOUT needs to cleanly divide STANDARD_VECTOR_SIZE
85: 	static constexpr idx_t TREE_FANOUT = 64;
86: };
87: 
88: } // namespace duckdb
[end of src/include/duckdb/execution/window_segment_tree.hpp]
[start of src/storage/table/row_group.cpp]
1: #include "duckdb/storage/table/row_group.hpp"
2: #include "duckdb/common/types/vector.hpp"
3: #include "duckdb/transaction/transaction.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/common/field_writer.hpp"
6: #include "duckdb/storage/table/column_data.hpp"
7: #include "duckdb/storage/table/standard_column_data.hpp"
8: #include "duckdb/storage/table/update_segment.hpp"
9: #include "duckdb/common/chrono.hpp"
10: #include "duckdb/planner/table_filter.hpp"
11: #include "duckdb/execution/expression_executor.hpp"
12: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
13: #include "duckdb/storage/meta_block_reader.hpp"
14: #include "duckdb/transaction/transaction_manager.hpp"
15: #include "duckdb/main/database.hpp"
16: 
17: namespace duckdb {
18: 
19: constexpr const idx_t RowGroup::ROW_GROUP_VECTOR_COUNT;
20: constexpr const idx_t RowGroup::ROW_GROUP_SIZE;
21: 
22: RowGroup::RowGroup(DatabaseInstance &db, BlockManager &block_manager, DataTableInfo &table_info, idx_t start,
23:                    idx_t count)
24:     : SegmentBase(start, count), db(db), block_manager(block_manager), table_info(table_info) {
25: 
26: 	Verify();
27: }
28: 
29: RowGroup::RowGroup(DatabaseInstance &db, BlockManager &block_manager, DataTableInfo &table_info,
30:                    const vector<LogicalType> &types, RowGroupPointer &&pointer)
31:     : SegmentBase(pointer.row_start, pointer.tuple_count), db(db), block_manager(block_manager),
32:       table_info(table_info) {
33: 	// deserialize the columns
34: 	if (pointer.data_pointers.size() != types.size()) {
35: 		throw IOException("Row group column count is unaligned with table column count. Corrupt file?");
36: 	}
37: 	for (idx_t i = 0; i < pointer.data_pointers.size(); i++) {
38: 		auto &block_pointer = pointer.data_pointers[i];
39: 		MetaBlockReader column_data_reader(block_manager, block_pointer.block_id);
40: 		column_data_reader.offset = block_pointer.offset;
41: 		this->columns.push_back(
42: 		    ColumnData::Deserialize(block_manager, table_info, i, start, column_data_reader, types[i], nullptr));
43: 	}
44: 
45: 	// set up the statistics
46: 	for (auto &stats : pointer.statistics) {
47: 		auto stats_type = stats->type;
48: 		this->stats.push_back(make_shared<SegmentStatistics>(stats_type, move(stats)));
49: 	}
50: 	this->version_info = move(pointer.versions);
51: 
52: 	Verify();
53: }
54: 
55: RowGroup::RowGroup(RowGroup &row_group, idx_t start)
56:     : SegmentBase(start, row_group.count), db(row_group.db), block_manager(row_group.block_manager),
57:       table_info(row_group.table_info), version_info(move(row_group.version_info)), stats(move(row_group.stats)) {
58: 	for (auto &column : row_group.columns) {
59: 		this->columns.push_back(ColumnData::CreateColumn(*column, start));
60: 	}
61: 	if (version_info) {
62: 		version_info->SetStart(start);
63: 	}
64: 	Verify();
65: }
66: 
67: void VersionNode::SetStart(idx_t start) {
68: 	idx_t current_start = start;
69: 	for (idx_t i = 0; i < RowGroup::ROW_GROUP_VECTOR_COUNT; i++) {
70: 		if (info[i]) {
71: 			info[i]->start = current_start;
72: 		}
73: 		current_start += STANDARD_VECTOR_SIZE;
74: 	}
75: }
76: 
77: RowGroup::~RowGroup() {
78: }
79: 
80: void RowGroup::InitializeEmpty(const vector<LogicalType> &types) {
81: 	// set up the segment trees for the column segments
82: 	for (idx_t i = 0; i < types.size(); i++) {
83: 		auto column_data = ColumnData::CreateColumn(block_manager, GetTableInfo(), i, start, types[i]);
84: 		stats.push_back(make_shared<SegmentStatistics>(types[i]));
85: 		columns.push_back(move(column_data));
86: 	}
87: }
88: 
89: bool RowGroup::InitializeScanWithOffset(RowGroupScanState &state, idx_t vector_offset) {
90: 	auto &column_ids = state.GetColumnIds();
91: 	auto filters = state.GetFilters();
92: 	auto parent_max_row = state.GetParentMaxRow();
93: 	if (filters) {
94: 		if (!CheckZonemap(*filters, column_ids)) {
95: 			return false;
96: 		}
97: 	}
98: 
99: 	state.row_group = this;
100: 	state.vector_index = vector_offset;
101: 	state.max_row = this->start > parent_max_row ? 0 : MinValue<idx_t>(this->count, parent_max_row - this->start);
102: 	state.column_scans = unique_ptr<ColumnScanState[]>(new ColumnScanState[column_ids.size()]);
103: 	for (idx_t i = 0; i < column_ids.size(); i++) {
104: 		auto column = column_ids[i];
105: 		if (column != COLUMN_IDENTIFIER_ROW_ID) {
106: 			columns[column]->InitializeScanWithOffset(state.column_scans[i],
107: 			                                          start + vector_offset * STANDARD_VECTOR_SIZE);
108: 		} else {
109: 			state.column_scans[i].current = nullptr;
110: 		}
111: 	}
112: 	return true;
113: }
114: 
115: bool RowGroup::InitializeScan(RowGroupScanState &state) {
116: 	auto &column_ids = state.GetColumnIds();
117: 	auto filters = state.GetFilters();
118: 	auto parent_max_row = state.GetParentMaxRow();
119: 	if (filters) {
120: 		if (!CheckZonemap(*filters, column_ids)) {
121: 			return false;
122: 		}
123: 	}
124: 	state.row_group = this;
125: 	state.vector_index = 0;
126: 	state.max_row = this->start > parent_max_row ? 0 : MinValue<idx_t>(this->count, parent_max_row - this->start);
127: 	state.column_scans = unique_ptr<ColumnScanState[]>(new ColumnScanState[column_ids.size()]);
128: 	for (idx_t i = 0; i < column_ids.size(); i++) {
129: 		auto column = column_ids[i];
130: 		if (column != COLUMN_IDENTIFIER_ROW_ID) {
131: 			columns[column]->InitializeScan(state.column_scans[i]);
132: 		} else {
133: 			state.column_scans[i].current = nullptr;
134: 		}
135: 	}
136: 	return true;
137: }
138: 
139: unique_ptr<RowGroup> RowGroup::AlterType(const LogicalType &target_type, idx_t changed_idx,
140:                                          ExpressionExecutor &executor, RowGroupScanState &scan_state,
141:                                          DataChunk &scan_chunk) {
142: 	Verify();
143: 
144: 	// construct a new column data for this type
145: 	auto column_data = ColumnData::CreateColumn(block_manager, GetTableInfo(), changed_idx, start, target_type);
146: 
147: 	ColumnAppendState append_state;
148: 	column_data->InitializeAppend(append_state);
149: 
150: 	// scan the original table, and fill the new column with the transformed value
151: 	InitializeScan(scan_state);
152: 
153: 	Vector append_vector(target_type);
154: 	auto altered_col_stats = make_shared<SegmentStatistics>(target_type);
155: 	while (true) {
156: 		// scan the table
157: 		scan_chunk.Reset();
158: 		ScanCommitted(scan_state, scan_chunk, TableScanType::TABLE_SCAN_COMMITTED_ROWS);
159: 		if (scan_chunk.size() == 0) {
160: 			break;
161: 		}
162: 		// execute the expression
163: 		executor.ExecuteExpression(scan_chunk, append_vector);
164: 		column_data->Append(*altered_col_stats->statistics, append_state, append_vector, scan_chunk.size());
165: 	}
166: 
167: 	// set up the row_group based on this row_group
168: 	auto row_group = make_unique<RowGroup>(db, block_manager, table_info, this->start, this->count);
169: 	row_group->version_info = version_info;
170: 	for (idx_t i = 0; i < columns.size(); i++) {
171: 		if (i == changed_idx) {
172: 			// this is the altered column: use the new column
173: 			row_group->columns.push_back(move(column_data));
174: 			row_group->stats.push_back(move(altered_col_stats));
175: 		} else {
176: 			// this column was not altered: use the data directly
177: 			row_group->columns.push_back(columns[i]);
178: 			row_group->stats.push_back(stats[i]);
179: 		}
180: 	}
181: 	row_group->Verify();
182: 	return row_group;
183: }
184: 
185: unique_ptr<RowGroup> RowGroup::AddColumn(ColumnDefinition &new_column, ExpressionExecutor &executor,
186:                                          Expression *default_value, Vector &result) {
187: 	Verify();
188: 
189: 	// construct a new column data for the new column
190: 	auto added_column =
191: 	    ColumnData::CreateColumn(block_manager, GetTableInfo(), columns.size(), start, new_column.Type());
192: 	auto added_col_stats = make_shared<SegmentStatistics>(
193: 	    new_column.Type(), BaseStatistics::CreateEmpty(new_column.Type(), StatisticsType::LOCAL_STATS));
194: 
195: 	idx_t rows_to_write = this->count;
196: 	if (rows_to_write > 0) {
197: 		DataChunk dummy_chunk;
198: 
199: 		ColumnAppendState state;
200: 		added_column->InitializeAppend(state);
201: 		for (idx_t i = 0; i < rows_to_write; i += STANDARD_VECTOR_SIZE) {
202: 			idx_t rows_in_this_vector = MinValue<idx_t>(rows_to_write - i, STANDARD_VECTOR_SIZE);
203: 			if (default_value) {
204: 				dummy_chunk.SetCardinality(rows_in_this_vector);
205: 				executor.ExecuteExpression(dummy_chunk, result);
206: 			}
207: 			added_column->Append(*added_col_stats->statistics, state, result, rows_in_this_vector);
208: 		}
209: 	}
210: 
211: 	// set up the row_group based on this row_group
212: 	auto row_group = make_unique<RowGroup>(db, block_manager, table_info, this->start, this->count);
213: 	row_group->version_info = version_info;
214: 	row_group->columns = columns;
215: 	row_group->stats = stats;
216: 	// now add the new column
217: 	row_group->columns.push_back(move(added_column));
218: 	row_group->stats.push_back(move(added_col_stats));
219: 
220: 	row_group->Verify();
221: 	return row_group;
222: }
223: 
224: unique_ptr<RowGroup> RowGroup::RemoveColumn(idx_t removed_column) {
225: 	Verify();
226: 
227: 	D_ASSERT(removed_column < columns.size());
228: 
229: 	auto row_group = make_unique<RowGroup>(db, block_manager, table_info, this->start, this->count);
230: 	row_group->version_info = version_info;
231: 	row_group->columns = columns;
232: 	row_group->stats = stats;
233: 	// now remove the column
234: 	row_group->columns.erase(row_group->columns.begin() + removed_column);
235: 	row_group->stats.erase(row_group->stats.begin() + removed_column);
236: 
237: 	row_group->Verify();
238: 	return row_group;
239: }
240: 
241: void RowGroup::CommitDrop() {
242: 	for (idx_t column_idx = 0; column_idx < columns.size(); column_idx++) {
243: 		CommitDropColumn(column_idx);
244: 	}
245: }
246: 
247: void RowGroup::CommitDropColumn(idx_t column_idx) {
248: 	D_ASSERT(column_idx < columns.size());
249: 	columns[column_idx]->CommitDropColumn();
250: }
251: 
252: void RowGroup::NextVector(RowGroupScanState &state) {
253: 	state.vector_index++;
254: 	auto &column_ids = state.GetColumnIds();
255: 	for (idx_t i = 0; i < column_ids.size(); i++) {
256: 		auto column = column_ids[i];
257: 		if (column == COLUMN_IDENTIFIER_ROW_ID) {
258: 			continue;
259: 		}
260: 		D_ASSERT(column < columns.size());
261: 		columns[column]->Skip(state.column_scans[i]);
262: 	}
263: }
264: 
265: bool RowGroup::CheckZonemap(TableFilterSet &filters, const vector<column_t> &column_ids) {
266: 	for (auto &entry : filters.filters) {
267: 		auto column_index = entry.first;
268: 		auto &filter = entry.second;
269: 		auto base_column_index = column_ids[column_index];
270: 
271: 		auto propagate_result = filter->CheckStatistics(*stats[base_column_index]->statistics);
272: 		if (propagate_result == FilterPropagateResult::FILTER_ALWAYS_FALSE ||
273: 		    propagate_result == FilterPropagateResult::FILTER_FALSE_OR_NULL) {
274: 			return false;
275: 		}
276: 	}
277: 	return true;
278: }
279: 
280: bool RowGroup::CheckZonemapSegments(RowGroupScanState &state) {
281: 	auto &column_ids = state.GetColumnIds();
282: 	auto filters = state.GetFilters();
283: 	if (!filters) {
284: 		return true;
285: 	}
286: 	for (auto &entry : filters->filters) {
287: 		D_ASSERT(entry.first < column_ids.size());
288: 		auto column_idx = entry.first;
289: 		auto base_column_idx = column_ids[column_idx];
290: 		bool read_segment = columns[base_column_idx]->CheckZonemap(state.column_scans[column_idx], *entry.second);
291: 		if (!read_segment) {
292: 			idx_t target_row =
293: 			    state.column_scans[column_idx].current->start + state.column_scans[column_idx].current->count;
294: 			D_ASSERT(target_row >= this->start);
295: 			D_ASSERT(target_row <= this->start + this->count);
296: 			idx_t target_vector_index = (target_row - this->start) / STANDARD_VECTOR_SIZE;
297: 			if (state.vector_index == target_vector_index) {
298: 				// we can't skip any full vectors because this segment contains less than a full vector
299: 				// for now we just bail-out
300: 				// FIXME: we could check if we can ALSO skip the next segments, in which case skipping a full vector
301: 				// might be possible
302: 				// we don't care that much though, since a single segment that fits less than a full vector is
303: 				// exceedingly rare
304: 				return true;
305: 			}
306: 			while (state.vector_index < target_vector_index) {
307: 				NextVector(state);
308: 			}
309: 			return false;
310: 		}
311: 	}
312: 
313: 	return true;
314: }
315: 
316: template <TableScanType TYPE>
317: void RowGroup::TemplatedScan(TransactionData transaction, RowGroupScanState &state, DataChunk &result) {
318: 	const bool ALLOW_UPDATES = TYPE != TableScanType::TABLE_SCAN_COMMITTED_ROWS_DISALLOW_UPDATES &&
319: 	                           TYPE != TableScanType::TABLE_SCAN_COMMITTED_ROWS_OMIT_PERMANENTLY_DELETED;
320: 	auto table_filters = state.GetFilters();
321: 	auto &column_ids = state.GetColumnIds();
322: 	auto adaptive_filter = state.GetAdaptiveFilter();
323: 	while (true) {
324: 		if (state.vector_index * STANDARD_VECTOR_SIZE >= state.max_row) {
325: 			// exceeded the amount of rows to scan
326: 			return;
327: 		}
328: 		idx_t current_row = state.vector_index * STANDARD_VECTOR_SIZE;
329: 		auto max_count = MinValue<idx_t>(STANDARD_VECTOR_SIZE, state.max_row - current_row);
330: 
331: 		//! first check the zonemap if we have to scan this partition
332: 		if (!CheckZonemapSegments(state)) {
333: 			continue;
334: 		}
335: 		// second, scan the version chunk manager to figure out which tuples to load for this transaction
336: 		idx_t count;
337: 		SelectionVector valid_sel(STANDARD_VECTOR_SIZE);
338: 		if (TYPE == TableScanType::TABLE_SCAN_REGULAR) {
339: 			count = state.row_group->GetSelVector(transaction, state.vector_index, valid_sel, max_count);
340: 			if (count == 0) {
341: 				// nothing to scan for this vector, skip the entire vector
342: 				NextVector(state);
343: 				continue;
344: 			}
345: 		} else if (TYPE == TableScanType::TABLE_SCAN_COMMITTED_ROWS_OMIT_PERMANENTLY_DELETED) {
346: 			count = state.row_group->GetCommittedSelVector(transaction.start_time, transaction.transaction_id,
347: 			                                               state.vector_index, valid_sel, max_count);
348: 			if (count == 0) {
349: 				// nothing to scan for this vector, skip the entire vector
350: 				NextVector(state);
351: 				continue;
352: 			}
353: 		} else {
354: 			count = max_count;
355: 		}
356: 		if (count == max_count && !table_filters) {
357: 			// scan all vectors completely: full scan without deletions or table filters
358: 			for (idx_t i = 0; i < column_ids.size(); i++) {
359: 				auto column = column_ids[i];
360: 				if (column == COLUMN_IDENTIFIER_ROW_ID) {
361: 					// scan row id
362: 					D_ASSERT(result.data[i].GetType().InternalType() == ROW_TYPE);
363: 					result.data[i].Sequence(this->start + current_row, 1, count);
364: 				} else {
365: 					if (TYPE != TableScanType::TABLE_SCAN_REGULAR) {
366: 						columns[column]->ScanCommitted(state.vector_index, state.column_scans[i], result.data[i],
367: 						                               ALLOW_UPDATES);
368: 					} else {
369: 						columns[column]->Scan(transaction, state.vector_index, state.column_scans[i], result.data[i]);
370: 					}
371: 				}
372: 			}
373: 		} else {
374: 			// partial scan: we have deletions or table filters
375: 			idx_t approved_tuple_count = count;
376: 			SelectionVector sel;
377: 			if (count != max_count) {
378: 				sel.Initialize(valid_sel);
379: 			} else {
380: 				sel.Initialize(nullptr);
381: 			}
382: 			//! first, we scan the columns with filters, fetch their data and generate a selection vector.
383: 			//! get runtime statistics
384: 			auto start_time = high_resolution_clock::now();
385: 			if (table_filters) {
386: 				D_ASSERT(adaptive_filter);
387: 				D_ASSERT(ALLOW_UPDATES);
388: 				for (idx_t i = 0; i < table_filters->filters.size(); i++) {
389: 					auto tf_idx = adaptive_filter->permutation[i];
390: 					auto col_idx = column_ids[tf_idx];
391: 					columns[col_idx]->Select(transaction, state.vector_index, state.column_scans[tf_idx],
392: 					                         result.data[tf_idx], sel, approved_tuple_count,
393: 					                         *table_filters->filters[tf_idx]);
394: 				}
395: 				for (auto &table_filter : table_filters->filters) {
396: 					result.data[table_filter.first].Slice(sel, approved_tuple_count);
397: 				}
398: 			}
399: 			if (approved_tuple_count == 0) {
400: 				// all rows were filtered out by the table filters
401: 				// skip this vector in all the scans that were not scanned yet
402: 				D_ASSERT(table_filters);
403: 				result.Reset();
404: 				for (idx_t i = 0; i < column_ids.size(); i++) {
405: 					auto col_idx = column_ids[i];
406: 					if (col_idx == COLUMN_IDENTIFIER_ROW_ID) {
407: 						continue;
408: 					}
409: 					if (table_filters->filters.find(i) == table_filters->filters.end()) {
410: 						columns[col_idx]->Skip(state.column_scans[i]);
411: 					}
412: 				}
413: 				state.vector_index++;
414: 				continue;
415: 			}
416: 			//! Now we use the selection vector to fetch data for the other columns.
417: 			for (idx_t i = 0; i < column_ids.size(); i++) {
418: 				if (!table_filters || table_filters->filters.find(i) == table_filters->filters.end()) {
419: 					auto column = column_ids[i];
420: 					if (column == COLUMN_IDENTIFIER_ROW_ID) {
421: 						D_ASSERT(result.data[i].GetType().InternalType() == PhysicalType::INT64);
422: 						result.data[i].SetVectorType(VectorType::FLAT_VECTOR);
423: 						auto result_data = (int64_t *)FlatVector::GetData(result.data[i]);
424: 						for (size_t sel_idx = 0; sel_idx < approved_tuple_count; sel_idx++) {
425: 							result_data[sel_idx] = this->start + current_row + sel.get_index(sel_idx);
426: 						}
427: 					} else {
428: 						if (TYPE == TableScanType::TABLE_SCAN_REGULAR) {
429: 							columns[column]->FilterScan(transaction, state.vector_index, state.column_scans[i],
430: 							                            result.data[i], sel, approved_tuple_count);
431: 						} else {
432: 							columns[column]->FilterScanCommitted(state.vector_index, state.column_scans[i],
433: 							                                     result.data[i], sel, approved_tuple_count,
434: 							                                     ALLOW_UPDATES);
435: 						}
436: 					}
437: 				}
438: 			}
439: 			auto end_time = high_resolution_clock::now();
440: 			if (adaptive_filter && table_filters->filters.size() > 1) {
441: 				adaptive_filter->AdaptRuntimeStatistics(duration_cast<duration<double>>(end_time - start_time).count());
442: 			}
443: 			D_ASSERT(approved_tuple_count > 0);
444: 			count = approved_tuple_count;
445: 		}
446: 		result.SetCardinality(count);
447: 		state.vector_index++;
448: 		break;
449: 	}
450: }
451: 
452: void RowGroup::Scan(TransactionData transaction, RowGroupScanState &state, DataChunk &result) {
453: 	TemplatedScan<TableScanType::TABLE_SCAN_REGULAR>(transaction, state, result);
454: }
455: 
456: void RowGroup::ScanCommitted(RowGroupScanState &state, DataChunk &result, TableScanType type) {
457: 	auto &transaction_manager = TransactionManager::Get(db);
458: 	auto lowest_active_start = transaction_manager.LowestActiveStart();
459: 	auto lowest_active_id = transaction_manager.LowestActiveId();
460: 	TransactionData data(lowest_active_id, lowest_active_start);
461: 	switch (type) {
462: 	case TableScanType::TABLE_SCAN_COMMITTED_ROWS:
463: 		TemplatedScan<TableScanType::TABLE_SCAN_COMMITTED_ROWS>(data, state, result);
464: 		break;
465: 	case TableScanType::TABLE_SCAN_COMMITTED_ROWS_DISALLOW_UPDATES:
466: 		TemplatedScan<TableScanType::TABLE_SCAN_COMMITTED_ROWS_DISALLOW_UPDATES>(data, state, result);
467: 		break;
468: 	case TableScanType::TABLE_SCAN_COMMITTED_ROWS_OMIT_PERMANENTLY_DELETED:
469: 		TemplatedScan<TableScanType::TABLE_SCAN_COMMITTED_ROWS_OMIT_PERMANENTLY_DELETED>(data, state, result);
470: 		break;
471: 	default:
472: 		throw InternalException("Unrecognized table scan type");
473: 	}
474: }
475: 
476: ChunkInfo *RowGroup::GetChunkInfo(idx_t vector_idx) {
477: 	if (!version_info) {
478: 		return nullptr;
479: 	}
480: 	return version_info->info[vector_idx].get();
481: }
482: 
483: idx_t RowGroup::GetSelVector(TransactionData transaction, idx_t vector_idx, SelectionVector &sel_vector,
484:                              idx_t max_count) {
485: 	lock_guard<mutex> lock(row_group_lock);
486: 
487: 	auto info = GetChunkInfo(vector_idx);
488: 	if (!info) {
489: 		return max_count;
490: 	}
491: 	return info->GetSelVector(transaction, sel_vector, max_count);
492: }
493: 
494: idx_t RowGroup::GetCommittedSelVector(transaction_t start_time, transaction_t transaction_id, idx_t vector_idx,
495:                                       SelectionVector &sel_vector, idx_t max_count) {
496: 	lock_guard<mutex> lock(row_group_lock);
497: 
498: 	auto info = GetChunkInfo(vector_idx);
499: 	if (!info) {
500: 		return max_count;
501: 	}
502: 	return info->GetCommittedSelVector(start_time, transaction_id, sel_vector, max_count);
503: }
504: 
505: bool RowGroup::Fetch(TransactionData transaction, idx_t row) {
506: 	D_ASSERT(row < this->count);
507: 	lock_guard<mutex> lock(row_group_lock);
508: 
509: 	idx_t vector_index = row / STANDARD_VECTOR_SIZE;
510: 	auto info = GetChunkInfo(vector_index);
511: 	if (!info) {
512: 		return true;
513: 	}
514: 	return info->Fetch(transaction, row - vector_index * STANDARD_VECTOR_SIZE);
515: }
516: 
517: void RowGroup::FetchRow(TransactionData transaction, ColumnFetchState &state, const vector<column_t> &column_ids,
518:                         row_t row_id, DataChunk &result, idx_t result_idx) {
519: 	for (idx_t col_idx = 0; col_idx < column_ids.size(); col_idx++) {
520: 		auto column = column_ids[col_idx];
521: 		if (column == COLUMN_IDENTIFIER_ROW_ID) {
522: 			// row id column: fill in the row ids
523: 			D_ASSERT(result.data[col_idx].GetType().InternalType() == PhysicalType::INT64);
524: 			result.data[col_idx].SetVectorType(VectorType::FLAT_VECTOR);
525: 			auto data = FlatVector::GetData<row_t>(result.data[col_idx]);
526: 			data[result_idx] = row_id;
527: 		} else {
528: 			// regular column: fetch data from the base column
529: 			columns[column]->FetchRow(transaction, state, row_id, result.data[col_idx], result_idx);
530: 		}
531: 	}
532: }
533: 
534: void RowGroup::AppendVersionInfo(TransactionData transaction, idx_t count) {
535: 	idx_t row_group_start = this->count.load();
536: 	idx_t row_group_end = row_group_start + count;
537: 	if (row_group_end > RowGroup::ROW_GROUP_SIZE) {
538: 		row_group_end = RowGroup::ROW_GROUP_SIZE;
539: 	}
540: 	lock_guard<mutex> lock(row_group_lock);
541: 
542: 	// create the version_info if it doesn't exist yet
543: 	if (!version_info) {
544: 		version_info = make_unique<VersionNode>();
545: 	}
546: 	idx_t start_vector_idx = row_group_start / STANDARD_VECTOR_SIZE;
547: 	idx_t end_vector_idx = (row_group_end - 1) / STANDARD_VECTOR_SIZE;
548: 	for (idx_t vector_idx = start_vector_idx; vector_idx <= end_vector_idx; vector_idx++) {
549: 		idx_t start = vector_idx == start_vector_idx ? row_group_start - start_vector_idx * STANDARD_VECTOR_SIZE : 0;
550: 		idx_t end =
551: 		    vector_idx == end_vector_idx ? row_group_end - end_vector_idx * STANDARD_VECTOR_SIZE : STANDARD_VECTOR_SIZE;
552: 		if (start == 0 && end == STANDARD_VECTOR_SIZE) {
553: 			// entire vector is encapsulated by append: append a single constant
554: 			auto constant_info = make_unique<ChunkConstantInfo>(this->start + vector_idx * STANDARD_VECTOR_SIZE);
555: 			constant_info->insert_id = transaction.transaction_id;
556: 			constant_info->delete_id = NOT_DELETED_ID;
557: 			version_info->info[vector_idx] = move(constant_info);
558: 		} else {
559: 			// part of a vector is encapsulated: append to that part
560: 			ChunkVectorInfo *info;
561: 			if (!version_info->info[vector_idx]) {
562: 				// first time appending to this vector: create new info
563: 				auto insert_info = make_unique<ChunkVectorInfo>(this->start + vector_idx * STANDARD_VECTOR_SIZE);
564: 				info = insert_info.get();
565: 				version_info->info[vector_idx] = move(insert_info);
566: 			} else {
567: 				D_ASSERT(version_info->info[vector_idx]->type == ChunkInfoType::VECTOR_INFO);
568: 				// use existing vector
569: 				info = (ChunkVectorInfo *)version_info->info[vector_idx].get();
570: 			}
571: 			info->Append(start, end, transaction.transaction_id);
572: 		}
573: 	}
574: 	this->count = row_group_end;
575: }
576: 
577: void RowGroup::CommitAppend(transaction_t commit_id, idx_t row_group_start, idx_t count) {
578: 	D_ASSERT(version_info.get());
579: 	idx_t row_group_end = row_group_start + count;
580: 	lock_guard<mutex> lock(row_group_lock);
581: 
582: 	idx_t start_vector_idx = row_group_start / STANDARD_VECTOR_SIZE;
583: 	idx_t end_vector_idx = (row_group_end - 1) / STANDARD_VECTOR_SIZE;
584: 	for (idx_t vector_idx = start_vector_idx; vector_idx <= end_vector_idx; vector_idx++) {
585: 		idx_t start = vector_idx == start_vector_idx ? row_group_start - start_vector_idx * STANDARD_VECTOR_SIZE : 0;
586: 		idx_t end =
587: 		    vector_idx == end_vector_idx ? row_group_end - end_vector_idx * STANDARD_VECTOR_SIZE : STANDARD_VECTOR_SIZE;
588: 
589: 		auto info = version_info->info[vector_idx].get();
590: 		info->CommitAppend(commit_id, start, end);
591: 	}
592: }
593: 
594: void RowGroup::RevertAppend(idx_t row_group_start) {
595: 	if (!version_info) {
596: 		return;
597: 	}
598: 	idx_t start_row = row_group_start - this->start;
599: 	idx_t start_vector_idx = (start_row + (STANDARD_VECTOR_SIZE - 1)) / STANDARD_VECTOR_SIZE;
600: 	for (idx_t vector_idx = start_vector_idx; vector_idx < RowGroup::ROW_GROUP_VECTOR_COUNT; vector_idx++) {
601: 		version_info->info[vector_idx].reset();
602: 	}
603: 	for (auto &column : columns) {
604: 		column->RevertAppend(row_group_start);
605: 	}
606: 	this->count = MinValue<idx_t>(row_group_start - this->start, this->count);
607: 	Verify();
608: }
609: 
610: void RowGroup::InitializeAppend(RowGroupAppendState &append_state) {
611: 	append_state.row_group = this;
612: 	append_state.offset_in_row_group = this->count;
613: 	// for each column, initialize the append state
614: 	append_state.states = unique_ptr<ColumnAppendState[]>(new ColumnAppendState[columns.size()]);
615: 	for (idx_t i = 0; i < columns.size(); i++) {
616: 		columns[i]->InitializeAppend(append_state.states[i]);
617: 	}
618: }
619: 
620: void RowGroup::Append(RowGroupAppendState &state, DataChunk &chunk, idx_t append_count) {
621: 	// append to the current row_group
622: 	for (idx_t i = 0; i < columns.size(); i++) {
623: 		columns[i]->Append(*stats[i]->statistics, state.states[i], chunk.data[i], append_count);
624: 	}
625: 	state.offset_in_row_group += append_count;
626: }
627: 
628: void RowGroup::Update(TransactionData transaction, DataChunk &update_chunk, row_t *ids, idx_t offset, idx_t count,
629:                       const vector<column_t> &column_ids) {
630: #ifdef DEBUG
631: 	for (size_t i = offset; i < offset + count; i++) {
632: 		D_ASSERT(ids[i] >= row_t(this->start) && ids[i] < row_t(this->start + this->count));
633: 	}
634: #endif
635: 	for (idx_t i = 0; i < column_ids.size(); i++) {
636: 		auto column = column_ids[i];
637: 		D_ASSERT(column != COLUMN_IDENTIFIER_ROW_ID);
638: 		D_ASSERT(columns[column]->type.id() == update_chunk.data[i].GetType().id());
639: 		if (offset > 0) {
640: 			Vector sliced_vector(update_chunk.data[i], offset);
641: 			sliced_vector.Flatten(count);
642: 			columns[column]->Update(transaction, column, sliced_vector, ids + offset, count);
643: 		} else {
644: 			columns[column]->Update(transaction, column, update_chunk.data[i], ids, count);
645: 		}
646: 		MergeStatistics(column, *columns[column]->GetUpdateStatistics());
647: 	}
648: }
649: 
650: void RowGroup::UpdateColumn(TransactionData transaction, DataChunk &updates, Vector &row_ids,
651:                             const vector<column_t> &column_path) {
652: 	D_ASSERT(updates.ColumnCount() == 1);
653: 	auto ids = FlatVector::GetData<row_t>(row_ids);
654: 
655: 	auto primary_column_idx = column_path[0];
656: 	D_ASSERT(primary_column_idx != COLUMN_IDENTIFIER_ROW_ID);
657: 	D_ASSERT(primary_column_idx < columns.size());
658: 	columns[primary_column_idx]->UpdateColumn(transaction, column_path, updates.data[0], ids, updates.size(), 1);
659: 	MergeStatistics(primary_column_idx, *columns[primary_column_idx]->GetUpdateStatistics());
660: }
661: 
662: unique_ptr<BaseStatistics> RowGroup::GetStatistics(idx_t column_idx) {
663: 	D_ASSERT(column_idx < stats.size());
664: 
665: 	lock_guard<mutex> slock(stats_lock);
666: 	return stats[column_idx]->statistics->Copy();
667: }
668: 
669: void RowGroup::MergeStatistics(idx_t column_idx, const BaseStatistics &other) {
670: 	D_ASSERT(column_idx < stats.size());
671: 
672: 	lock_guard<mutex> slock(stats_lock);
673: 	stats[column_idx]->statistics->Merge(other);
674: }
675: 
676: void RowGroup::MergeIntoStatistics(idx_t column_idx, BaseStatistics &other) {
677: 	D_ASSERT(column_idx < stats.size());
678: 
679: 	lock_guard<mutex> slock(stats_lock);
680: 	other.Merge(*stats[column_idx]->statistics);
681: }
682: 
683: RowGroupWriteData RowGroup::WriteToDisk(PartialBlockManager &manager,
684:                                         const vector<CompressionType> &compression_types) {
685: 	RowGroupWriteData result;
686: 	result.states.reserve(columns.size());
687: 	result.statistics.reserve(columns.size());
688: 
689: 	// Checkpoint the individual columns of the row group
690: 	// Here we're iterating over columns. Each column can have multiple segments.
691: 	// (Some columns will be wider than others, and require different numbers
692: 	// of blocks to encode.) Segments cannot span blocks.
693: 	//
694: 	// Some of these columns are composite (list, struct). The data is written
695: 	// first sequentially, and the pointers are written later, so that the
696: 	// pointers all end up densely packed, and thus more cache-friendly.
697: 	for (idx_t column_idx = 0; column_idx < columns.size(); column_idx++) {
698: 		auto &column = columns[column_idx];
699: 		ColumnCheckpointInfo checkpoint_info {compression_types[column_idx]};
700: 		auto checkpoint_state = column->Checkpoint(*this, manager, checkpoint_info);
701: 		D_ASSERT(checkpoint_state);
702: 
703: 		auto stats = checkpoint_state->GetStatistics();
704: 		D_ASSERT(stats);
705: 
706: 		result.statistics.push_back(move(stats));
707: 		result.states.push_back(move(checkpoint_state));
708: 	}
709: 	D_ASSERT(result.states.size() == result.statistics.size());
710: 	return result;
711: }
712: 
713: RowGroupPointer RowGroup::Checkpoint(RowGroupWriter &writer, vector<unique_ptr<BaseStatistics>> &global_stats) {
714: 	RowGroupPointer row_group_pointer;
715: 
716: 	vector<CompressionType> compression_types;
717: 	compression_types.reserve(columns.size());
718: 	for (idx_t column_idx = 0; column_idx < columns.size(); column_idx++) {
719: 		compression_types.push_back(writer.GetColumnCompressionType(column_idx));
720: 	}
721: 	auto result = WriteToDisk(writer.GetPartialBlockManager(), compression_types);
722: 	for (idx_t column_idx = 0; column_idx < columns.size(); column_idx++) {
723: 		global_stats[column_idx]->Merge(*result.statistics[column_idx]);
724: 	}
725: 	row_group_pointer.statistics = move(result.statistics);
726: 
727: 	// construct the row group pointer and write the column meta data to disk
728: 	D_ASSERT(result.states.size() == columns.size());
729: 	row_group_pointer.row_start = start;
730: 	row_group_pointer.tuple_count = count;
731: 	for (auto &state : result.states) {
732: 		// get the current position of the table data writer
733: 		auto &data_writer = writer.GetPayloadWriter();
734: 		auto pointer = data_writer.GetBlockPointer();
735: 
736: 		// store the stats and the data pointers in the row group pointers
737: 		row_group_pointer.data_pointers.push_back(pointer);
738: 
739: 		// Write pointers to the column segments.
740: 		//
741: 		// Just as above, the state can refer to many other states, so this
742: 		// can cascade recursively into more pointer writes.
743: 		state->WriteDataPointers(writer);
744: 	}
745: 	row_group_pointer.versions = version_info;
746: 	Verify();
747: 	return row_group_pointer;
748: }
749: 
750: void RowGroup::CheckpointDeletes(VersionNode *versions, Serializer &serializer) {
751: 	if (!versions) {
752: 		// no version information: write nothing
753: 		serializer.Write<idx_t>(0);
754: 		return;
755: 	}
756: 	// first count how many ChunkInfo's we need to deserialize
757: 	idx_t chunk_info_count = 0;
758: 	for (idx_t vector_idx = 0; vector_idx < RowGroup::ROW_GROUP_VECTOR_COUNT; vector_idx++) {
759: 		auto chunk_info = versions->info[vector_idx].get();
760: 		if (!chunk_info) {
761: 			continue;
762: 		}
763: 		chunk_info_count++;
764: 	}
765: 	// now serialize the actual version information
766: 	serializer.Write<idx_t>(chunk_info_count);
767: 	for (idx_t vector_idx = 0; vector_idx < RowGroup::ROW_GROUP_VECTOR_COUNT; vector_idx++) {
768: 		auto chunk_info = versions->info[vector_idx].get();
769: 		if (!chunk_info) {
770: 			continue;
771: 		}
772: 		serializer.Write<idx_t>(vector_idx);
773: 		chunk_info->Serialize(serializer);
774: 	}
775: }
776: 
777: shared_ptr<VersionNode> RowGroup::DeserializeDeletes(Deserializer &source) {
778: 	auto chunk_count = source.Read<idx_t>();
779: 	if (chunk_count == 0) {
780: 		// no deletes
781: 		return nullptr;
782: 	}
783: 	auto version_info = make_shared<VersionNode>();
784: 	for (idx_t i = 0; i < chunk_count; i++) {
785: 		idx_t vector_index = source.Read<idx_t>();
786: 		if (vector_index >= RowGroup::ROW_GROUP_VECTOR_COUNT) {
787: 			throw Exception("In DeserializeDeletes, vector_index is out of range for the row group. Corrupted file?");
788: 		}
789: 		version_info->info[vector_index] = ChunkInfo::Deserialize(source);
790: 	}
791: 	return version_info;
792: }
793: 
794: void RowGroup::Serialize(RowGroupPointer &pointer, Serializer &main_serializer) {
795: 	FieldWriter writer(main_serializer);
796: 	writer.WriteField<uint64_t>(pointer.row_start);
797: 	writer.WriteField<uint64_t>(pointer.tuple_count);
798: 	auto &serializer = writer.GetSerializer();
799: 	for (auto &stats : pointer.statistics) {
800: 		stats->Serialize(serializer);
801: 	}
802: 	for (auto &data_pointer : pointer.data_pointers) {
803: 		serializer.Write<block_id_t>(data_pointer.block_id);
804: 		serializer.Write<uint64_t>(data_pointer.offset);
805: 	}
806: 	CheckpointDeletes(pointer.versions.get(), serializer);
807: 	writer.Finalize();
808: }
809: 
810: RowGroupPointer RowGroup::Deserialize(Deserializer &main_source, const vector<ColumnDefinition> &columns) {
811: 	RowGroupPointer result;
812: 
813: 	FieldReader reader(main_source);
814: 	result.row_start = reader.ReadRequired<uint64_t>();
815: 	result.tuple_count = reader.ReadRequired<uint64_t>();
816: 
817: 	result.data_pointers.reserve(columns.size());
818: 	result.statistics.reserve(columns.size());
819: 
820: 	auto &source = reader.GetSource();
821: 	for (idx_t i = 0; i < columns.size(); i++) {
822: 		auto &col = columns[i];
823: 		if (col.Generated()) {
824: 			continue;
825: 		}
826: 		auto stats = BaseStatistics::Deserialize(source, columns[i].Type());
827: 		result.statistics.push_back(move(stats));
828: 	}
829: 	for (idx_t i = 0; i < columns.size(); i++) {
830: 		auto &col = columns[i];
831: 		if (col.Generated()) {
832: 			continue;
833: 		}
834: 		BlockPointer pointer;
835: 		pointer.block_id = source.Read<block_id_t>();
836: 		pointer.offset = source.Read<uint64_t>();
837: 		result.data_pointers.push_back(pointer);
838: 	}
839: 	result.versions = DeserializeDeletes(source);
840: 
841: 	reader.Finalize();
842: 	return result;
843: }
844: 
845: //===--------------------------------------------------------------------===//
846: // GetStorageInfo
847: //===--------------------------------------------------------------------===//
848: void RowGroup::GetStorageInfo(idx_t row_group_index, vector<vector<Value>> &result) {
849: 	for (idx_t col_idx = 0; col_idx < columns.size(); col_idx++) {
850: 		columns[col_idx]->GetStorageInfo(row_group_index, {col_idx}, result);
851: 	}
852: }
853: 
854: //===--------------------------------------------------------------------===//
855: // Version Delete Information
856: //===--------------------------------------------------------------------===//
857: class VersionDeleteState {
858: public:
859: 	VersionDeleteState(RowGroup &info, TransactionData transaction, DataTable *table, idx_t base_row)
860: 	    : info(info), transaction(transaction), table(table), current_info(nullptr),
861: 	      current_chunk(DConstants::INVALID_INDEX), count(0), base_row(base_row), delete_count(0) {
862: 	}
863: 
864: 	RowGroup &info;
865: 	TransactionData transaction;
866: 	DataTable *table;
867: 	ChunkVectorInfo *current_info;
868: 	idx_t current_chunk;
869: 	row_t rows[STANDARD_VECTOR_SIZE];
870: 	idx_t count;
871: 	idx_t base_row;
872: 	idx_t chunk_row;
873: 	idx_t delete_count;
874: 
875: public:
876: 	void Delete(row_t row_id);
877: 	void Flush();
878: };
879: 
880: idx_t RowGroup::Delete(TransactionData transaction, DataTable *table, row_t *ids, idx_t count) {
881: 	lock_guard<mutex> lock(row_group_lock);
882: 	VersionDeleteState del_state(*this, transaction, table, this->start);
883: 
884: 	// obtain a write lock
885: 	for (idx_t i = 0; i < count; i++) {
886: 		D_ASSERT(ids[i] >= 0);
887: 		D_ASSERT(idx_t(ids[i]) >= this->start && idx_t(ids[i]) < this->start + this->count);
888: 		del_state.Delete(ids[i] - this->start);
889: 	}
890: 	del_state.Flush();
891: 	return del_state.delete_count;
892: }
893: 
894: void RowGroup::Verify() {
895: #ifdef DEBUG
896: 	for (auto &column : columns) {
897: 		column->Verify(*this);
898: 	}
899: #endif
900: }
901: 
902: void VersionDeleteState::Delete(row_t row_id) {
903: 	D_ASSERT(row_id >= 0);
904: 	idx_t vector_idx = row_id / STANDARD_VECTOR_SIZE;
905: 	idx_t idx_in_vector = row_id - vector_idx * STANDARD_VECTOR_SIZE;
906: 	if (current_chunk != vector_idx) {
907: 		Flush();
908: 
909: 		if (!info.version_info) {
910: 			info.version_info = make_unique<VersionNode>();
911: 		}
912: 
913: 		if (!info.version_info->info[vector_idx]) {
914: 			// no info yet: create it
915: 			info.version_info->info[vector_idx] =
916: 			    make_unique<ChunkVectorInfo>(info.start + vector_idx * STANDARD_VECTOR_SIZE);
917: 		} else if (info.version_info->info[vector_idx]->type == ChunkInfoType::CONSTANT_INFO) {
918: 			auto &constant = (ChunkConstantInfo &)*info.version_info->info[vector_idx];
919: 			// info exists but it's a constant info: convert to a vector info
920: 			auto new_info = make_unique<ChunkVectorInfo>(info.start + vector_idx * STANDARD_VECTOR_SIZE);
921: 			new_info->insert_id = constant.insert_id.load();
922: 			for (idx_t i = 0; i < STANDARD_VECTOR_SIZE; i++) {
923: 				new_info->inserted[i] = constant.insert_id.load();
924: 			}
925: 			info.version_info->info[vector_idx] = move(new_info);
926: 		}
927: 		D_ASSERT(info.version_info->info[vector_idx]->type == ChunkInfoType::VECTOR_INFO);
928: 		current_info = (ChunkVectorInfo *)info.version_info->info[vector_idx].get();
929: 		current_chunk = vector_idx;
930: 		chunk_row = vector_idx * STANDARD_VECTOR_SIZE;
931: 	}
932: 	rows[count++] = idx_in_vector;
933: }
934: 
935: void VersionDeleteState::Flush() {
936: 	if (count == 0) {
937: 		return;
938: 	}
939: 	// it is possible for delete statements to delete the same tuple multiple times when combined with a USING clause
940: 	// in the current_info->Delete, we check which tuples are actually deleted (excluding duplicate deletions)
941: 	// this is returned in the actual_delete_count
942: 	auto actual_delete_count = current_info->Delete(transaction.transaction_id, rows, count);
943: 	delete_count += actual_delete_count;
944: 	if (transaction.transaction && actual_delete_count > 0) {
945: 		// now push the delete into the undo buffer, but only if any deletes were actually performed
946: 		transaction.transaction->PushDelete(table, current_info, rows, actual_delete_count, base_row + chunk_row);
947: 	}
948: 	count = 0;
949: }
950: 
951: } // namespace duckdb
[end of src/storage/table/row_group.cpp]
[start of tools/rpkg/src/statement.cpp]
1: #include "rapi.hpp"
2: #include "typesr.hpp"
3: #include "altrepstring.hpp"
4: 
5: #include <R_ext/Utils.h>
6: 
7: #include "duckdb/common/arrow/arrow.hpp"
8: #include "duckdb/common/arrow/arrow_converter.hpp"
9: #include "duckdb/common/types/timestamp.hpp"
10: #include "duckdb/common/arrow/arrow_wrapper.hpp"
11: #include "duckdb/common/arrow/result_arrow_wrapper.hpp"
12: #include "duckdb/main/stream_query_result.hpp"
13: #include "duckdb/common/types/uuid.hpp"
14: 
15: #include "duckdb/parser/statement/relation_statement.hpp"
16: 
17: using namespace duckdb;
18: using namespace cpp11::literals;
19: 
20: // converter for primitive types
21: template <class SRC, class DEST>
22: static void VectorToR(Vector &src_vec, size_t count, void *dest, uint64_t dest_offset, DEST na_val) {
23: 	auto src_ptr = FlatVector::GetData<SRC>(src_vec);
24: 	auto &mask = FlatVector::Validity(src_vec);
25: 	auto dest_ptr = ((DEST *)dest) + dest_offset;
26: 	for (size_t row_idx = 0; row_idx < count; row_idx++) {
27: 		dest_ptr[row_idx] = !mask.RowIsValid(row_idx) ? na_val : src_ptr[row_idx];
28: 	}
29: }
30: 
31: [[cpp11::register]] void rapi_release(duckdb::stmt_eptr_t stmt) {
32: 	auto stmt_ptr = stmt.release();
33: 	if (stmt_ptr) {
34: 		delete stmt_ptr;
35: 	}
36: }
37: 
38: [[cpp11::register]] SEXP rapi_get_substrait(duckdb::conn_eptr_t conn, std::string query) {
39: 	if (!conn || !conn.get() || !conn->conn) {
40: 		cpp11::stop("rapi_get_substrait: Invalid connection");
41: 	}
42: 
43: 	auto rel = conn->conn->TableFunction("get_substrait", {Value(query)});
44: 	auto res = rel->Execute();
45: 	auto chunk = res->Fetch();
46: 	auto blob_string = StringValue::Get(chunk->GetValue(0, 0));
47: 
48: 	SEXP rawval = NEW_RAW(blob_string.size());
49: 	if (!rawval) {
50: 		throw std::bad_alloc();
51: 	}
52: 	memcpy(RAW_POINTER(rawval), blob_string.data(), blob_string.size());
53: 
54: 	return rawval;
55: }
56: 
57: [[cpp11::register]] SEXP rapi_get_substrait_json(duckdb::conn_eptr_t conn, std::string query) {
58: 	if (!conn || !conn.get() || !conn->conn) {
59: 		cpp11::stop("rapi_get_substrait_json: Invalid connection");
60: 	}
61: 
62: 	auto rel = conn->conn->TableFunction("get_substrait_json", {Value(query)});
63: 	auto res = rel->Execute();
64: 	auto chunk = res->Fetch();
65: 	auto json = StringValue::Get(chunk->GetValue(0, 0));
66: 
67: 	return StringsToSexp({json});
68: }
69: 
70: static cpp11::list construct_retlist(unique_ptr<PreparedStatement> stmt, const string &query, idx_t n_param) {
71: 	cpp11::writable::list retlist;
72: 	retlist.reserve(6);
73: 	retlist.push_back({"str"_nm = query});
74: 
75: 	auto stmtholder = new RStatement();
76: 	stmtholder->stmt = move(stmt);
77: 
78: 	retlist.push_back({"ref"_nm = stmt_eptr_t(stmtholder)});
79: 	retlist.push_back({"type"_nm = StatementTypeToString(stmtholder->stmt->GetStatementType())});
80: 	retlist.push_back({"names"_nm = cpp11::as_sexp(stmtholder->stmt->GetNames())});
81: 
82: 	cpp11::writable::strings rtypes;
83: 	rtypes.reserve(stmtholder->stmt->GetTypes().size());
84: 
85: 	for (auto &stype : stmtholder->stmt->GetTypes()) {
86: 		string rtype = RApiTypes::DetectLogicalType(stype, "rapi_prepare");
87: 		rtypes.push_back(rtype);
88: 	}
89: 
90: 	retlist.push_back({"rtypes"_nm = rtypes});
91: 	retlist.push_back({"n_param"_nm = n_param});
92: 	retlist.push_back(
93: 	    {"return_type"_nm = StatementReturnTypeToString(stmtholder->stmt->GetStatementProperties().return_type)});
94: 
95: 	return retlist;
96: }
97: 
98: [[cpp11::register]] cpp11::list rapi_prepare_substrait(duckdb::conn_eptr_t conn, cpp11::sexp query) {
99: 	if (!conn || !conn.get() || !conn->conn) {
100: 		cpp11::stop("rapi_prepare_substrait: Invalid connection");
101: 	}
102: 
103: 	if (!IS_RAW(query)) {
104: 		cpp11::stop("rapi_prepare_substrait: Query is not a raw()/BLOB");
105: 	}
106: 
107: 	auto rel = conn->conn->TableFunction("from_substrait", {Value::BLOB(RAW_POINTER(query), LENGTH(query))});
108: 	auto relation_stmt = make_unique<RelationStatement>(rel);
109: 	relation_stmt->n_param = 0;
110: 	relation_stmt->query = "";
111: 	auto stmt = conn->conn->Prepare(move(relation_stmt));
112: 	if (stmt->HasError()) {
113: 		cpp11::stop("rapi_prepare_substrait: Failed to prepare query %s\nError: %s", stmt->error.Message().c_str());
114: 	}
115: 
116: 	return construct_retlist(move(stmt), "", 0);
117: }
118: 
119: [[cpp11::register]] cpp11::list rapi_prepare(duckdb::conn_eptr_t conn, std::string query) {
120: 	if (!conn || !conn.get() || !conn->conn) {
121: 		cpp11::stop("rapi_prepare: Invalid connection");
122: 	}
123: 
124: 	auto statements = conn->conn->ExtractStatements(query.c_str());
125: 	if (statements.empty()) {
126: 		// no statements to execute
127: 		cpp11::stop("rapi_prepare: No statements to execute");
128: 	}
129: 	// if there are multiple statements, we directly execute the statements besides the last one
130: 	// we only return the result of the last statement to the user, unless one of the previous statements fails
131: 	for (idx_t i = 0; i + 1 < statements.size(); i++) {
132: 		auto res = conn->conn->Query(move(statements[i]));
133: 		if (res->HasError()) {
134: 			cpp11::stop("rapi_prepare: Failed to execute statement %s\nError: %s", query.c_str(),
135: 			            res->GetError().c_str());
136: 		}
137: 	}
138: 	auto stmt = conn->conn->Prepare(move(statements.back()));
139: 	if (stmt->HasError()) {
140: 		cpp11::stop("rapi_prepare: Failed to prepare query %s\nError: %s", query.c_str(),
141: 		            stmt->error.Message().c_str());
142: 	}
143: 	auto n_param = stmt->n_param;
144: 	return construct_retlist(move(stmt), query, n_param);
145: }
146: 
147: [[cpp11::register]] cpp11::list rapi_bind(duckdb::stmt_eptr_t stmt, cpp11::list params, bool arrow, bool integer64) {
148: 	if (!stmt || !stmt.get() || !stmt->stmt) {
149: 		cpp11::stop("rapi_bind: Invalid statement");
150: 	}
151: 
152: 	stmt->parameters.clear();
153: 	stmt->parameters.resize(stmt->stmt->n_param);
154: 
155: 	if (stmt->stmt->n_param == 0) {
156: 		cpp11::stop("rapi_bind: dbBind called but query takes no parameters");
157: 	}
158: 
159: 	if (params.size() != R_xlen_t(stmt->stmt->n_param)) {
160: 		cpp11::stop("rapi_bind: Bind parameters need to be a list of length %i", stmt->stmt->n_param);
161: 	}
162: 
163: 	R_len_t n_rows = Rf_length(params[0]);
164: 
165: 	for (auto param = std::next(params.begin()); param != params.end(); ++param) {
166: 		if (Rf_length(*param) != n_rows) {
167: 			cpp11::stop("rapi_bind: Bind parameter values need to have the same length");
168: 		}
169: 	}
170: 
171: 	if (n_rows != 1 && arrow) {
172: 		cpp11::stop("rapi_bind: Bind parameter values need to have length one for arrow queries");
173: 	}
174: 
175: 	cpp11::writable::list out;
176: 	out.reserve(n_rows);
177: 
178: 	for (idx_t row_idx = 0; row_idx < (size_t)n_rows; ++row_idx) {
179: 		for (idx_t param_idx = 0; param_idx < (idx_t)params.size(); param_idx++) {
180: 			SEXP valsexp = params[(size_t)param_idx];
181: 			auto val = RApiTypes::SexpToValue(valsexp, row_idx);
182: 			stmt->parameters[param_idx] = val;
183: 		}
184: 
185: 		// No protection, assigned immediately
186: 		out.push_back(rapi_execute(stmt, arrow, integer64));
187: 	}
188: 
189: 	return out;
190: }
191: 
192: static SEXP allocate(const LogicalType &type, RProtector &r_varvalue, idx_t nrows) {
193: 	SEXP varvalue = NULL;
194: 	switch (type.id()) {
195: 	case LogicalTypeId::BOOLEAN:
196: 		varvalue = r_varvalue.Protect(NEW_LOGICAL(nrows));
197: 		break;
198: 	case LogicalTypeId::UTINYINT:
199: 	case LogicalTypeId::TINYINT:
200: 	case LogicalTypeId::SMALLINT:
201: 	case LogicalTypeId::USMALLINT:
202: 	case LogicalTypeId::INTEGER:
203: 		varvalue = r_varvalue.Protect(NEW_INTEGER(nrows));
204: 		break;
205: 	case LogicalTypeId::UINTEGER:
206: 	case LogicalTypeId::UBIGINT:
207: 	case LogicalTypeId::BIGINT:
208: 	case LogicalTypeId::HUGEINT:
209: 	case LogicalTypeId::FLOAT:
210: 	case LogicalTypeId::DOUBLE:
211: 	case LogicalTypeId::DECIMAL:
212: 	case LogicalTypeId::TIMESTAMP_SEC:
213: 	case LogicalTypeId::TIMESTAMP_MS:
214: 	case LogicalTypeId::TIMESTAMP:
215: 	case LogicalTypeId::TIMESTAMP_TZ:
216: 	case LogicalTypeId::TIMESTAMP_NS:
217: 	case LogicalTypeId::DATE:
218: 	case LogicalTypeId::TIME:
219: 	case LogicalTypeId::INTERVAL:
220: 		varvalue = r_varvalue.Protect(NEW_NUMERIC(nrows));
221: 		break;
222: 	case LogicalTypeId::LIST:
223: 		varvalue = r_varvalue.Protect(NEW_LIST(nrows));
224: 		break;
225: 	case LogicalTypeId::STRUCT: {
226: 		cpp11::writable::list dest_list;
227: 
228: 		for (const auto &child : StructType::GetChildTypes(type)) {
229: 			const auto &name = child.first;
230: 			const auto &child_type = child.second;
231: 
232: 			RProtector child_protector;
233: 			auto dest_child = allocate(child_type, child_protector, nrows);
234: 			dest_list.push_back(cpp11::named_arg(name.c_str()) = std::move(dest_child));
235: 		}
236: 
237: 		// Note we cannot use cpp11's data frame here as it tries to calculate the number of rows itself,
238: 		// but gives the wrong answer if the first column is another data frame or the struct is empty.
239: 		dest_list.attr(R_ClassSymbol) = RStrings::get().dataframe_str;
240: 		dest_list.attr(R_RowNamesSymbol) = {NA_INTEGER, -static_cast<int>(nrows)};
241: 
242: 		varvalue = r_varvalue.Protect(cpp11::as_sexp(dest_list));
243: 		break;
244: 	}
245: 	case LogicalTypeId::JSON:
246: 	case LogicalTypeId::VARCHAR: {
247: 		auto wrapper = new DuckDBAltrepStringWrapper();
248: 		wrapper->length = nrows;
249: 		wrapper->string_data = std::unique_ptr<string_t[]>(new string_t[nrows]);
250: 		wrapper->mask_data = std::unique_ptr<bool[]>(new bool[nrows]);
251: 
252: 		cpp11::external_pointer<DuckDBAltrepStringWrapper> ptr(wrapper);
253: 		varvalue = r_varvalue.Protect(R_new_altrep(AltrepString::rclass, ptr, R_NilValue));
254: 		break;
255: 	}
256: 	case LogicalTypeId::UUID:
257: 		varvalue = r_varvalue.Protect(NEW_STRING(nrows));
258: 		break;
259: 	case LogicalTypeId::BLOB:
260: 		varvalue = r_varvalue.Protect(NEW_LIST(nrows));
261: 		break;
262: 	case LogicalTypeId::ENUM: {
263: 		auto physical_type = type.InternalType();
264: 		if (physical_type == PhysicalType::UINT64) { // DEDUP_POINTER_ENUM
265: 			varvalue = r_varvalue.Protect(NEW_STRING(nrows));
266: 		} else {
267: 			varvalue = r_varvalue.Protect(NEW_INTEGER(nrows));
268: 		}
269: 		break;
270: 	}
271: 	default:
272: 		cpp11::stop("rapi_execute: Unknown column type for execute: %s", type.ToString().c_str());
273: 	}
274: 	if (!varvalue) {
275: 		throw std::bad_alloc();
276: 	}
277: 	return varvalue;
278: }
279: 
280: // Convert DuckDB's timestamp to R's timestamp (POSIXct). This is a represented as the number of seconds since the
281: // epoch, stored as a double.
282: template <LogicalTypeId>
283: double ConvertTimestampValue(int64_t timestamp);
284: 
285: template <>
286: double ConvertTimestampValue<LogicalTypeId::TIMESTAMP_SEC>(int64_t timestamp) {
287: 	return static_cast<double>(timestamp);
288: }
289: 
290: template <>
291: double ConvertTimestampValue<LogicalTypeId::TIMESTAMP_MS>(int64_t timestamp) {
292: 	return static_cast<double>(timestamp) / Interval::MSECS_PER_SEC;
293: }
294: 
295: template <>
296: double ConvertTimestampValue<LogicalTypeId::TIMESTAMP>(int64_t timestamp) {
297: 	return static_cast<double>(timestamp) / Interval::MICROS_PER_SEC;
298: }
299: 
300: template <>
301: double ConvertTimestampValue<LogicalTypeId::TIMESTAMP_TZ>(int64_t timestamp) {
302: 	return ConvertTimestampValue<LogicalTypeId::TIMESTAMP>(timestamp);
303: }
304: 
305: template <>
306: double ConvertTimestampValue<LogicalTypeId::TIMESTAMP_NS>(int64_t timestamp) {
307: 	return static_cast<double>(timestamp) / Interval::NANOS_PER_SEC;
308: }
309: 
310: template <LogicalTypeId LT>
311: void ConvertTimestampVector(Vector &src_vec, size_t count, SEXP &dest, uint64_t dest_offset) {
312: 	auto src_data = FlatVector::GetData<int64_t>(src_vec);
313: 	auto &mask = FlatVector::Validity(src_vec);
314: 	double *dest_ptr = ((double *)NUMERIC_POINTER(dest)) + dest_offset;
315: 	for (size_t row_idx = 0; row_idx < count; row_idx++) {
316: 		dest_ptr[row_idx] = !mask.RowIsValid(row_idx) ? NA_REAL : ConvertTimestampValue<LT>(src_data[row_idx]);
317: 	}
318: 
319: 	// some dresssup for R
320: 	SET_CLASS(dest, RStrings::get().POSIXct_POSIXt_str);
321: 	Rf_setAttrib(dest, RStrings::get().tzone_sym, RStrings::get().UTC_str);
322: }
323: 
324: std::once_flag nanosecond_coercion_warning;
325: 
326: static void transform(Vector &src_vec, SEXP &dest, idx_t dest_offset, idx_t n, bool integer64) {
327: 	switch (src_vec.GetType().id()) {
328: 	case LogicalTypeId::BOOLEAN:
329: 		VectorToR<int8_t, uint32_t>(src_vec, n, LOGICAL_POINTER(dest), dest_offset, NA_LOGICAL);
330: 		break;
331: 	case LogicalTypeId::UTINYINT:
332: 		VectorToR<uint8_t, uint32_t>(src_vec, n, INTEGER_POINTER(dest), dest_offset, NA_INTEGER);
333: 		break;
334: 	case LogicalTypeId::TINYINT:
335: 		VectorToR<int8_t, uint32_t>(src_vec, n, INTEGER_POINTER(dest), dest_offset, NA_INTEGER);
336: 		break;
337: 	case LogicalTypeId::USMALLINT:
338: 		VectorToR<uint16_t, uint32_t>(src_vec, n, INTEGER_POINTER(dest), dest_offset, NA_INTEGER);
339: 		break;
340: 	case LogicalTypeId::SMALLINT:
341: 		VectorToR<int16_t, uint32_t>(src_vec, n, INTEGER_POINTER(dest), dest_offset, NA_INTEGER);
342: 		break;
343: 	case LogicalTypeId::INTEGER:
344: 		VectorToR<int32_t, uint32_t>(src_vec, n, INTEGER_POINTER(dest), dest_offset, NA_INTEGER);
345: 		break;
346: 	case LogicalTypeId::TIMESTAMP_SEC:
347: 		ConvertTimestampVector<LogicalTypeId::TIMESTAMP_SEC>(src_vec, n, dest, dest_offset);
348: 		break;
349: 	case LogicalTypeId::TIMESTAMP_MS:
350: 		ConvertTimestampVector<LogicalTypeId::TIMESTAMP_MS>(src_vec, n, dest, dest_offset);
351: 		break;
352: 	case LogicalTypeId::TIMESTAMP:
353: 		ConvertTimestampVector<LogicalTypeId::TIMESTAMP>(src_vec, n, dest, dest_offset);
354: 		break;
355: 	case LogicalTypeId::TIMESTAMP_TZ:
356: 		ConvertTimestampVector<LogicalTypeId::TIMESTAMP_TZ>(src_vec, n, dest, dest_offset);
357: 		break;
358: 	case LogicalTypeId::TIMESTAMP_NS:
359: 		ConvertTimestampVector<LogicalTypeId::TIMESTAMP_NS>(src_vec, n, dest, dest_offset);
360: 		std::call_once(nanosecond_coercion_warning, Rf_warning,
361: 		               "Coercing nanoseconds to a lower resolution may result in a loss of data.");
362: 		break;
363: 	case LogicalTypeId::DATE: {
364: 		auto src_data = FlatVector::GetData<date_t>(src_vec);
365: 		auto &mask = FlatVector::Validity(src_vec);
366: 		double *dest_ptr = ((double *)NUMERIC_POINTER(dest)) + dest_offset;
367: 		for (size_t row_idx = 0; row_idx < n; row_idx++) {
368: 			dest_ptr[row_idx] = !mask.RowIsValid(row_idx) ? NA_REAL : (double)int32_t(src_data[row_idx]);
369: 		}
370: 
371: 		// some dresssup for R
372: 		SET_CLASS(dest, RStrings::get().Date_str);
373: 		break;
374: 	}
375: 	case LogicalTypeId::TIME: {
376: 		auto src_data = FlatVector::GetData<dtime_t>(src_vec);
377: 		auto &mask = FlatVector::Validity(src_vec);
378: 		double *dest_ptr = ((double *)NUMERIC_POINTER(dest)) + dest_offset;
379: 		for (size_t row_idx = 0; row_idx < n; row_idx++) {
380: 			if (!mask.RowIsValid(row_idx)) {
381: 				dest_ptr[row_idx] = NA_REAL;
382: 			} else {
383: 				dest_ptr[row_idx] = src_data[row_idx].micros / Interval::MICROS_PER_SEC;
384: 			}
385: 		}
386: 		SET_CLASS(dest, RStrings::get().difftime_str);
387: 		Rf_setAttrib(dest, RStrings::get().units_sym, RStrings::get().secs_str);
388: 		break;
389: 	}
390: 	case LogicalTypeId::INTERVAL: {
391: 		auto src_data = FlatVector::GetData<interval_t>(src_vec);
392: 		auto &mask = FlatVector::Validity(src_vec);
393: 		double *dest_ptr = ((double *)NUMERIC_POINTER(dest)) + dest_offset;
394: 		for (size_t row_idx = 0; row_idx < n; row_idx++) {
395: 			if (!mask.RowIsValid(row_idx)) {
396: 				dest_ptr[row_idx] = NA_REAL;
397: 			} else {
398: 				dest_ptr[row_idx] = Interval::GetMicro(src_data[row_idx]) / Interval::MICROS_PER_SEC;
399: 			}
400: 		}
401: 		SET_CLASS(dest, RStrings::get().difftime_str);
402: 		Rf_setAttrib(dest, RStrings::get().units_sym, RStrings::get().secs_str);
403: 		break;
404: 	}
405: 	case LogicalTypeId::UINTEGER:
406: 		VectorToR<uint32_t, double>(src_vec, n, NUMERIC_POINTER(dest), dest_offset, NA_REAL);
407: 		break;
408: 	case LogicalTypeId::UBIGINT:
409: 		if (integer64) {
410: 			// this silently loses the high bit
411: 			VectorToR<uint64_t, int64_t>(src_vec, n, NUMERIC_POINTER(dest), dest_offset,
412: 			                             NumericLimits<int64_t>::Minimum());
413: 			Rf_setAttrib(dest, R_ClassSymbol, RStrings::get().integer64_str);
414: 		} else {
415: 			VectorToR<uint64_t, double>(src_vec, n, NUMERIC_POINTER(dest), dest_offset, NA_REAL);
416: 		}
417: 		break;
418: 	case LogicalTypeId::BIGINT:
419: 		if (integer64) {
420: 			VectorToR<int64_t, int64_t>(src_vec, n, NUMERIC_POINTER(dest), dest_offset,
421: 			                            NumericLimits<int64_t>::Minimum());
422: 			Rf_setAttrib(dest, R_ClassSymbol, RStrings::get().integer64_str);
423: 		} else {
424: 			VectorToR<int64_t, double>(src_vec, n, NUMERIC_POINTER(dest), dest_offset, NA_REAL);
425: 		}
426: 		break;
427: 	case LogicalTypeId::HUGEINT: {
428: 		auto src_data = FlatVector::GetData<hugeint_t>(src_vec);
429: 		auto &mask = FlatVector::Validity(src_vec);
430: 		double *dest_ptr = ((double *)NUMERIC_POINTER(dest)) + dest_offset;
431: 		for (size_t row_idx = 0; row_idx < n; row_idx++) {
432: 			if (!mask.RowIsValid(row_idx)) {
433: 				dest_ptr[row_idx] = NA_REAL;
434: 			} else {
435: 				Hugeint::TryCast(src_data[row_idx], dest_ptr[row_idx]);
436: 			}
437: 		}
438: 		break;
439: 	}
440: 	case LogicalTypeId::DECIMAL: {
441: 		auto &decimal_type = src_vec.GetType();
442: 		double *dest_ptr = ((double *)NUMERIC_POINTER(dest)) + dest_offset;
443: 		auto dec_scale = DecimalType::GetScale(decimal_type);
444: 		switch (decimal_type.InternalType()) {
445: 		case PhysicalType::INT16:
446: 			RDecimalCastLoop<int16_t>(src_vec, n, dest_ptr, dec_scale);
447: 			break;
448: 		case PhysicalType::INT32:
449: 			RDecimalCastLoop<int32_t>(src_vec, n, dest_ptr, dec_scale);
450: 			break;
451: 		case PhysicalType::INT64:
452: 			RDecimalCastLoop<int64_t>(src_vec, n, dest_ptr, dec_scale);
453: 			break;
454: 		case PhysicalType::INT128:
455: 			RDecimalCastLoop<hugeint_t>(src_vec, n, dest_ptr, dec_scale);
456: 			break;
457: 		default:
458: 			throw NotImplementedException("Unimplemented internal type for DECIMAL");
459: 		}
460: 		break;
461: 	}
462: 	case LogicalTypeId::FLOAT:
463: 		VectorToR<float, double>(src_vec, n, NUMERIC_POINTER(dest), dest_offset, NA_REAL);
464: 		break;
465: 
466: 	case LogicalTypeId::DOUBLE:
467: 		VectorToR<double, double>(src_vec, n, NUMERIC_POINTER(dest), dest_offset, NA_REAL);
468: 		break;
469: 	case LogicalTypeId::JSON:
470: 	case LogicalTypeId::VARCHAR: {
471: 		auto wrapper = (DuckDBAltrepStringWrapper *)R_ExternalPtrAddr(R_altrep_data1(dest));
472: 		auto src_data = FlatVector::GetData<string_t>(src_vec);
473: 		auto &mask = FlatVector::Validity(src_vec);
474: 		for (size_t row_idx = 0; row_idx < n; row_idx++) {
475: 			auto valid = mask.RowIsValid(row_idx);
476: 			auto dest_idx = dest_offset + row_idx;
477: 			wrapper->mask_data[dest_idx] = valid;
478: 			if (valid) {
479: 				wrapper->string_data[dest_idx] =
480: 				    src_data[row_idx].IsInlined() ? src_data[row_idx] : wrapper->heap.AddString(src_data[row_idx]);
481: 			}
482: 		}
483: 		break;
484: 	}
485: 	case LogicalTypeId::LIST: {
486: 		// figure out the total and max element length of the list vector child
487: 		auto src_data = ListVector::GetData(src_vec);
488: 		auto &child_type = ListType::GetChildType(src_vec.GetType());
489: 		Vector child_vector(child_type, nullptr);
490: 
491: 		// actual loop over rows
492: 		for (size_t row_idx = 0; row_idx < n; row_idx++) {
493: 			if (!FlatVector::Validity(src_vec).RowIsValid(row_idx)) {
494: 				SET_ELEMENT(dest, dest_offset + row_idx, R_NilValue);
495: 			} else {
496: 				child_vector.Slice(ListVector::GetEntry(src_vec), src_data[row_idx].offset);
497: 
498: 				RProtector ele_prot;
499: 				// transform the list child vector to a single R SEXP
500: 				auto list_element =
501: 				    allocate(ListType::GetChildType(src_vec.GetType()), ele_prot, src_data[row_idx].length);
502: 				transform(child_vector, list_element, 0, src_data[row_idx].length, integer64);
503: 
504: 				// call R's own extract subset method
505: 				SET_ELEMENT(dest, dest_offset + row_idx, list_element);
506: 			}
507: 		}
508: 		break;
509: 	}
510: 	case LogicalTypeId::STRUCT: {
511: 		const auto &children = StructVector::GetEntries(src_vec);
512: 
513: 		for (size_t i = 0; i < children.size(); i++) {
514: 			const auto &struct_child = children[i];
515: 			SEXP child_dest = VECTOR_ELT(dest, i);
516: 			transform(*struct_child, child_dest, dest_offset, n, integer64);
517: 		}
518: 
519: 		break;
520: 	}
521: 	case LogicalTypeId::BLOB: {
522: 		auto src_ptr = FlatVector::GetData<string_t>(src_vec);
523: 		auto &mask = FlatVector::Validity(src_vec);
524: 		for (size_t row_idx = 0; row_idx < n; row_idx++) {
525: 			if (!mask.RowIsValid(row_idx)) {
526: 				SET_VECTOR_ELT(dest, dest_offset + row_idx, R_NilValue);
527: 			} else {
528: 				SEXP rawval = NEW_RAW(src_ptr[row_idx].GetSize());
529: 				if (!rawval) {
530: 					throw std::bad_alloc();
531: 				}
532: 				memcpy(RAW_POINTER(rawval), src_ptr[row_idx].GetDataUnsafe(), src_ptr[row_idx].GetSize());
533: 				SET_VECTOR_ELT(dest, dest_offset + row_idx, rawval);
534: 			}
535: 		}
536: 		break;
537: 	}
538: 	case LogicalTypeId::ENUM: {
539: 		auto physical_type = src_vec.GetType().InternalType();
540: 		auto dummy = NEW_STRING(1);
541: 		ptrdiff_t sexp_header_size = (data_ptr_t)DATAPTR(dummy) - (data_ptr_t)dummy; // don't tell anyone
542: 		if (physical_type == PhysicalType::UINT64) {                                 // DEDUP_POINTER_ENUM
543: 			auto src_ptr = FlatVector::GetData<uint64_t>(src_vec);
544: 			auto &mask = FlatVector::Validity(src_vec);
545: 			/* we have to use SET_STRING_ELT here because otherwise those SEXPs dont get referenced */
546: 			for (size_t row_idx = 0; row_idx < n; row_idx++) {
547: 				if (!mask.RowIsValid(row_idx)) {
548: 					SET_STRING_ELT(dest, dest_offset + row_idx, NA_STRING);
549: 				} else {
550: 					SET_STRING_ELT(dest, dest_offset + row_idx,
551: 					               (SEXP)((data_ptr_t)src_ptr[row_idx] - sexp_header_size));
552: 				}
553: 			}
554: 			break;
555: 		}
556: 
557: 		switch (physical_type) {
558: 		case PhysicalType::UINT8:
559: 			VectorToR<uint8_t, uint32_t>(src_vec, n, INTEGER_POINTER(dest), dest_offset, NA_INTEGER);
560: 			break;
561: 
562: 		case PhysicalType::UINT16:
563: 			VectorToR<uint16_t, uint32_t>(src_vec, n, INTEGER_POINTER(dest), dest_offset, NA_INTEGER);
564: 			break;
565: 
566: 		case PhysicalType::UINT32:
567: 			VectorToR<uint8_t, uint32_t>(src_vec, n, INTEGER_POINTER(dest), dest_offset, NA_INTEGER);
568: 			break;
569: 
570: 		default:
571: 			cpp11::stop("rapi_execute: Unknown enum type for convert: %s", TypeIdToString(physical_type).c_str());
572: 		}
573: 		// increment by one cause R factor offsets start at 1
574: 		auto dest_ptr = ((int32_t *)INTEGER_POINTER(dest)) + dest_offset;
575: 		for (idx_t i = 0; i < n; i++) {
576: 			if (dest_ptr[i] == NA_INTEGER) {
577: 				continue;
578: 			}
579: 			dest_ptr[i]++;
580: 		}
581: 
582: 		auto &str_vec = EnumType::GetValuesInsertOrder(src_vec.GetType());
583: 		auto size = EnumType::GetSize(src_vec.GetType());
584: 		vector<string> str_c_vec(size);
585: 		for (idx_t i = 0; i < size; i++) {
586: 			str_c_vec[i] = str_vec.GetValue(i).ToString();
587: 		}
588: 
589: 		SET_LEVELS(dest, StringsToSexp(str_c_vec));
590: 		SET_CLASS(dest, RStrings::get().factor_str);
591: 		break;
592: 	}
593: 	case LogicalTypeId::UUID: {
594: 		auto src_ptr = FlatVector::GetData<hugeint_t>(src_vec);
595: 		auto &mask = FlatVector::Validity(src_vec);
596: 		for (size_t row_idx = 0; row_idx < n; row_idx++) {
597: 			if (!mask.RowIsValid(row_idx)) {
598: 				SET_STRING_ELT(dest, dest_offset + row_idx, NA_STRING);
599: 			} else {
600: 				char uuid_buf[UUID::STRING_SIZE];
601: 				UUID::ToString(src_ptr[row_idx], uuid_buf);
602: 				SET_STRING_ELT(dest, dest_offset + row_idx, Rf_mkCharLen(uuid_buf, UUID::STRING_SIZE));
603: 			}
604: 		}
605: 		break;
606: 	}
607: 	default:
608: 		cpp11::stop("rapi_execute: Unknown column type for convert: %s", src_vec.GetType().ToString().c_str());
609: 		break;
610: 	}
611: }
612: 
613: SEXP duckdb::duckdb_execute_R_impl(MaterializedQueryResult *result, bool integer64) {
614: 	// step 2: create result data frame and allocate columns
615: 	uint32_t ncols = result->types.size();
616: 	if (ncols == 0) {
617: 		return Rf_ScalarReal(0); // no need for protection because no allocation can happen afterwards
618: 	}
619: 
620: 	uint64_t nrows = result->RowCount();
621: 
622: 	// Note we cannot use cpp11's data frame here as it tries to calculate the number of rows itself,
623: 	// but gives the wrong answer if the first column is another data frame. So we set the necessary
624: 	// attributes manually.
625: 	cpp11::writable::list data_frame(NEW_LIST(ncols));
626: 	data_frame.attr(R_ClassSymbol) = RStrings::get().dataframe_str;
627: 	data_frame.attr(R_RowNamesSymbol) = {NA_INTEGER, -static_cast<int>(nrows)};
628: 	SET_NAMES(data_frame, StringsToSexp(result->names));
629: 
630: 	for (size_t col_idx = 0; col_idx < ncols; col_idx++) {
631: 		// TODO move the protector to allocate?
632: 		RProtector r_varvalue;
633: 		auto varvalue = allocate(result->types[col_idx], r_varvalue, nrows);
634: 		SET_VECTOR_ELT(data_frame, col_idx, varvalue);
635: 	}
636: 
637: 	// at this point data_frame is fully allocated and the only protected SEXP
638: 
639: 	// step 3: set values from chunks
640: 	uint64_t dest_offset = 0;
641: 	for (auto &chunk : result->Collection().Chunks()) {
642: 		D_ASSERT(chunk.ColumnCount() == ncols);
643: 		D_ASSERT(chunk.ColumnCount() == (idx_t)Rf_length(data_frame));
644: 		for (size_t col_idx = 0; col_idx < chunk.ColumnCount(); col_idx++) {
645: 			SEXP dest = VECTOR_ELT(data_frame, col_idx);
646: 			transform(chunk.data[col_idx], dest, dest_offset, chunk.size(), integer64);
647: 		}
648: 		dest_offset += chunk.size();
649: 	}
650: 
651: 	D_ASSERT(dest_offset == nrows);
652: 	return data_frame;
653: }
654: 
655: struct AppendableRList {
656: 	AppendableRList() {
657: 		the_list = r.Protect(NEW_LIST(capacity));
658: 	}
659: 	void PrepAppend() {
660: 		if (size >= capacity) {
661: 			capacity = capacity * 2;
662: 			SEXP new_list = r.Protect(NEW_LIST(capacity));
663: 			D_ASSERT(new_list);
664: 			for (idx_t i = 0; i < size; i++) {
665: 				SET_VECTOR_ELT(new_list, i, VECTOR_ELT(the_list, i));
666: 			}
667: 			the_list = new_list;
668: 		}
669: 	}
670: 
671: 	void Append(SEXP val) {
672: 		D_ASSERT(size < capacity);
673: 		D_ASSERT(the_list != R_NilValue);
674: 		SET_VECTOR_ELT(the_list, size++, val);
675: 	}
676: 	SEXP the_list;
677: 	idx_t capacity = 1000;
678: 	idx_t size = 0;
679: 	RProtector r;
680: };
681: 
682: bool FetchArrowChunk(QueryResult *result, AppendableRList &batches_list, ArrowArray &arrow_data,
683:                      ArrowSchema &arrow_schema, SEXP batch_import_from_c, SEXP arrow_namespace, idx_t chunk_size) {
684: 	auto count = ArrowUtil::FetchChunk(result, chunk_size, &arrow_data);
685: 	if (count == 0) {
686: 		return false;
687: 	}
688: 	string timezone_config = QueryResult::GetConfigTimezone(*result);
689: 	ArrowConverter::ToArrowSchema(&arrow_schema, result->types, result->names, timezone_config);
690: 	batches_list.PrepAppend();
691: 	batches_list.Append(cpp11::safe[Rf_eval](batch_import_from_c, arrow_namespace));
692: 	return true;
693: }
694: 
695: // Turn a DuckDB result set into an Arrow Table
696: [[cpp11::register]] SEXP rapi_execute_arrow(duckdb::rqry_eptr_t qry_res, int chunk_size) {
697: 	auto result = qry_res->result.get();
698: 	// somewhat dark magic below
699: 	cpp11::function getNamespace = RStrings::get().getNamespace_sym;
700: 	cpp11::sexp arrow_namespace(getNamespace(RStrings::get().arrow_str));
701: 
702: 	// export schema setup
703: 	ArrowSchema arrow_schema;
704: 	cpp11::doubles schema_ptr_sexp(Rf_ScalarReal(static_cast<double>(reinterpret_cast<uintptr_t>(&arrow_schema))));
705: 	cpp11::sexp schema_import_from_c(Rf_lang2(RStrings::get().ImportSchema_sym, schema_ptr_sexp));
706: 
707: 	// export data setup
708: 	ArrowArray arrow_data;
709: 	cpp11::doubles data_ptr_sexp(Rf_ScalarReal(static_cast<double>(reinterpret_cast<uintptr_t>(&arrow_data))));
710: 	cpp11::sexp batch_import_from_c(Rf_lang3(RStrings::get().ImportRecordBatch_sym, data_ptr_sexp, schema_ptr_sexp));
711: 	// create data batches
712: 	AppendableRList batches_list;
713: 
714: 	while (FetchArrowChunk(result, batches_list, arrow_data, arrow_schema, batch_import_from_c, arrow_namespace,
715: 	                       chunk_size)) {
716: 	}
717: 
718: 	SET_LENGTH(batches_list.the_list, batches_list.size);
719: 	string timezone_config = QueryResult::GetConfigTimezone(*result);
720: 	ArrowConverter::ToArrowSchema(&arrow_schema, result->types, result->names, timezone_config);
721: 	cpp11::sexp schema_arrow_obj(cpp11::safe[Rf_eval](schema_import_from_c, arrow_namespace));
722: 
723: 	// create arrow::Table
724: 	cpp11::sexp from_record_batches(
725: 	    Rf_lang3(RStrings::get().Table__from_record_batches_sym, batches_list.the_list, schema_arrow_obj));
726: 	return cpp11::safe[Rf_eval](from_record_batches, arrow_namespace);
727: }
728: 
729: // Turn a DuckDB result set into an RecordBatchReader
730: [[cpp11::register]] SEXP rapi_record_batch(duckdb::rqry_eptr_t qry_res, int chunk_size) {
731: 	// somewhat dark magic below
732: 	cpp11::function getNamespace = RStrings::get().getNamespace_sym;
733: 	cpp11::sexp arrow_namespace(getNamespace(RStrings::get().arrow_str));
734: 
735: 	ResultArrowArrayStreamWrapper *result_stream = new ResultArrowArrayStreamWrapper(move(qry_res->result), chunk_size);
736: 	cpp11::sexp stream_ptr_sexp(
737: 	    Rf_ScalarReal(static_cast<double>(reinterpret_cast<uintptr_t>(&result_stream->stream))));
738: 	cpp11::sexp record_batch_reader(Rf_lang2(RStrings::get().ImportRecordBatchReader_sym, stream_ptr_sexp));
739: 	return cpp11::safe[Rf_eval](record_batch_reader, arrow_namespace);
740: }
741: 
742: [[cpp11::register]] SEXP rapi_execute(duckdb::stmt_eptr_t stmt, bool arrow, bool integer64) {
743: 	if (!stmt || !stmt.get() || !stmt->stmt) {
744: 		cpp11::stop("rapi_execute: Invalid statement");
745: 	}
746: 
747: 	auto pending_query = stmt->stmt->PendingQuery(stmt->parameters, arrow);
748: 	duckdb::PendingExecutionResult execution_result;
749: 	do {
750: 		execution_result = pending_query->ExecuteTask();
751: 		R_CheckUserInterrupt();
752: 	} while (execution_result == PendingExecutionResult::RESULT_NOT_READY);
753: 	if (execution_result == PendingExecutionResult::EXECUTION_ERROR) {
754: 		cpp11::stop("rapi_execute: Failed to run query\nError: %s", pending_query->GetError().c_str());
755: 	}
756: 	auto generic_result = pending_query->Execute();
757: 	if (generic_result->HasError()) {
758: 		cpp11::stop("rapi_execute: Failed to run query\nError: %s", generic_result->GetError().c_str());
759: 	}
760: 
761: 	if (arrow) {
762: 		auto query_result = new RQueryResult();
763: 		query_result->result = move(generic_result);
764: 		rqry_eptr_t query_resultsexp(query_result);
765: 		return query_resultsexp;
766: 	} else {
767: 		D_ASSERT(generic_result->type == QueryResultType::MATERIALIZED_RESULT);
768: 		MaterializedQueryResult *result = (MaterializedQueryResult *)generic_result.get();
769: 		return duckdb_execute_R_impl(result, integer64);
770: 	}
771: }
[end of tools/rpkg/src/statement.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: