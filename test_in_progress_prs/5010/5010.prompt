You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Crash When Creating Index
### What happens?

Crash.

### To Reproduce

```sql
CREATE TABLE t0(c0 DOUBLE, c1 TIMESTAMP DEFAULT(TIMESTAMP '1970-01-04 12:58:32'));
INSERT INTO t0(c1, c0) VALUES (TIMESTAMP '1969-12-28 23:02:08', 1);
INSERT INTO t0(c0) VALUES (DEFAULT);
CREATE INDEX i2 ON t0(c1, c0); -- segment fault
```

### OS:

Ubuntu 20:04

### DuckDB Version:

v0.5.2-dev737 9913e0e80

### DuckDB Client:

CLI

### Full Name:

Jinsheng Ba

### Affiliation:

National University of Singapore

### Have you tried this on the latest `master` branch?

- [X] I agree

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] I agree

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
[end of README.md]
[start of src/execution/index/art/art.cpp]
1: #include "duckdb/execution/index/art/art.hpp"
2: 
3: #include "duckdb/common/radix.hpp"
4: #include "duckdb/common/vector_operations/vector_operations.hpp"
5: #include "duckdb/execution/expression_executor.hpp"
6: #include "duckdb/storage/arena_allocator.hpp"
7: #include "duckdb/execution/index/art/art_key.hpp"
8: 
9: #include <algorithm>
10: #include <cstring>
11: #include <ctgmath>
12: 
13: namespace duckdb {
14: 
15: ART::ART(const vector<column_t> &column_ids, TableIOManager &table_io_manager,
16:          const vector<unique_ptr<Expression>> &unbound_expressions, IndexConstraintType constraint_type,
17:          DatabaseInstance &db, idx_t block_id, idx_t block_offset)
18:     : Index(IndexType::ART, table_io_manager, column_ids, unbound_expressions, constraint_type), db(db) {
19: 	if (block_id != DConstants::INVALID_INDEX) {
20: 		tree = Node::Deserialize(*this, block_id, block_offset);
21: 	} else {
22: 		tree = nullptr;
23: 	}
24: 	serialized_data_pointer = BlockPointer(block_id, block_offset);
25: 	for (idx_t i = 0; i < types.size(); i++) {
26: 		switch (types[i]) {
27: 		case PhysicalType::BOOL:
28: 		case PhysicalType::INT8:
29: 		case PhysicalType::INT16:
30: 		case PhysicalType::INT32:
31: 		case PhysicalType::INT64:
32: 		case PhysicalType::INT128:
33: 		case PhysicalType::UINT8:
34: 		case PhysicalType::UINT16:
35: 		case PhysicalType::UINT32:
36: 		case PhysicalType::UINT64:
37: 		case PhysicalType::FLOAT:
38: 		case PhysicalType::DOUBLE:
39: 		case PhysicalType::VARCHAR:
40: 			break;
41: 		default:
42: 			throw InvalidTypeException(logical_types[i], "Invalid type for index");
43: 		}
44: 	}
45: }
46: 
47: ART::~ART() {
48: 	if (tree) {
49: 		delete tree;
50: 		tree = nullptr;
51: 	}
52: }
53: 
54: unique_ptr<IndexScanState> ART::InitializeScanSinglePredicate(Transaction &transaction, Value value,
55:                                                               ExpressionType expression_type) {
56: 	auto result = make_unique<ARTIndexScanState>();
57: 	result->values[0] = value;
58: 	result->expressions[0] = expression_type;
59: 	return move(result);
60: }
61: 
62: unique_ptr<IndexScanState> ART::InitializeScanTwoPredicates(Transaction &transaction, Value low_value,
63:                                                             ExpressionType low_expression_type, Value high_value,
64:                                                             ExpressionType high_expression_type) {
65: 	auto result = make_unique<ARTIndexScanState>();
66: 	result->values[0] = low_value;
67: 	result->expressions[0] = low_expression_type;
68: 	result->values[1] = high_value;
69: 	result->expressions[1] = high_expression_type;
70: 	return move(result);
71: }
72: 
73: //===--------------------------------------------------------------------===//
74: // Keys
75: //===--------------------------------------------------------------------===//
76: 
77: template <class T>
78: static void TemplatedGenerateKeys(ArenaAllocator &allocator, Vector &input, idx_t count, vector<Key> &keys) {
79: 	UnifiedVectorFormat idata;
80: 	input.ToUnifiedFormat(count, idata);
81: 
82: 	D_ASSERT(keys.size() >= count);
83: 	auto input_data = (T *)idata.data;
84: 	for (idx_t i = 0; i < count; i++) {
85: 		auto idx = idata.sel->get_index(i);
86: 		if (idata.validity.RowIsValid(idx)) {
87: 			Key::CreateKey<T>(allocator, keys[i], input_data[idx]);
88: 		}
89: 	}
90: }
91: 
92: template <class T>
93: static void ConcatenateKeys(ArenaAllocator &allocator, Vector &input, idx_t count, vector<Key> &keys) {
94: 	UnifiedVectorFormat idata;
95: 	input.ToUnifiedFormat(count, idata);
96: 
97: 	auto input_data = (T *)idata.data;
98: 	for (idx_t i = 0; i < count; i++) {
99: 		auto idx = idata.sel->get_index(i);
100: 
101: 		// key is not NULL (no previous column entry was NULL)
102: 		if (!keys[i].Empty()) {
103: 			if (!idata.validity.RowIsValid(idx)) {
104: 				// this column entry is NULL, set whole key to NULL
105: 				keys[i] = Key();
106: 			} else {
107: 				auto other_key = Key::CreateKey<T>(allocator, input_data[idx]);
108: 				keys[i].ConcatenateKey(allocator, other_key);
109: 			}
110: 		}
111: 	}
112: }
113: 
114: void ART::GenerateKeys(ArenaAllocator &allocator, DataChunk &input, vector<Key> &keys) {
115: 	// generate keys for the first input column
116: 	switch (input.data[0].GetType().InternalType()) {
117: 	case PhysicalType::BOOL:
118: 		TemplatedGenerateKeys<bool>(allocator, input.data[0], input.size(), keys);
119: 		break;
120: 	case PhysicalType::INT8:
121: 		TemplatedGenerateKeys<int8_t>(allocator, input.data[0], input.size(), keys);
122: 		break;
123: 	case PhysicalType::INT16:
124: 		TemplatedGenerateKeys<int16_t>(allocator, input.data[0], input.size(), keys);
125: 		break;
126: 	case PhysicalType::INT32:
127: 		TemplatedGenerateKeys<int32_t>(allocator, input.data[0], input.size(), keys);
128: 		break;
129: 	case PhysicalType::INT64:
130: 		TemplatedGenerateKeys<int64_t>(allocator, input.data[0], input.size(), keys);
131: 		break;
132: 	case PhysicalType::INT128:
133: 		TemplatedGenerateKeys<hugeint_t>(allocator, input.data[0], input.size(), keys);
134: 		break;
135: 	case PhysicalType::UINT8:
136: 		TemplatedGenerateKeys<uint8_t>(allocator, input.data[0], input.size(), keys);
137: 		break;
138: 	case PhysicalType::UINT16:
139: 		TemplatedGenerateKeys<uint16_t>(allocator, input.data[0], input.size(), keys);
140: 		break;
141: 	case PhysicalType::UINT32:
142: 		TemplatedGenerateKeys<uint32_t>(allocator, input.data[0], input.size(), keys);
143: 		break;
144: 	case PhysicalType::UINT64:
145: 		TemplatedGenerateKeys<uint64_t>(allocator, input.data[0], input.size(), keys);
146: 		break;
147: 	case PhysicalType::FLOAT:
148: 		TemplatedGenerateKeys<float>(allocator, input.data[0], input.size(), keys);
149: 		break;
150: 	case PhysicalType::DOUBLE:
151: 		TemplatedGenerateKeys<double>(allocator, input.data[0], input.size(), keys);
152: 		break;
153: 	case PhysicalType::VARCHAR:
154: 		TemplatedGenerateKeys<string_t>(allocator, input.data[0], input.size(), keys);
155: 		break;
156: 	default:
157: 		throw InternalException("Invalid type for index");
158: 	}
159: 
160: 	for (idx_t i = 1; i < input.ColumnCount(); i++) {
161: 		// for each of the remaining columns, concatenate
162: 		switch (input.data[i].GetType().InternalType()) {
163: 		case PhysicalType::BOOL:
164: 			ConcatenateKeys<bool>(allocator, input.data[i], input.size(), keys);
165: 			break;
166: 		case PhysicalType::INT8:
167: 			ConcatenateKeys<int8_t>(allocator, input.data[i], input.size(), keys);
168: 			break;
169: 		case PhysicalType::INT16:
170: 			ConcatenateKeys<int16_t>(allocator, input.data[i], input.size(), keys);
171: 			break;
172: 		case PhysicalType::INT32:
173: 			ConcatenateKeys<int32_t>(allocator, input.data[i], input.size(), keys);
174: 			break;
175: 		case PhysicalType::INT64:
176: 			ConcatenateKeys<int64_t>(allocator, input.data[i], input.size(), keys);
177: 			break;
178: 		case PhysicalType::INT128:
179: 			ConcatenateKeys<hugeint_t>(allocator, input.data[i], input.size(), keys);
180: 			break;
181: 		case PhysicalType::UINT8:
182: 			ConcatenateKeys<uint8_t>(allocator, input.data[i], input.size(), keys);
183: 			break;
184: 		case PhysicalType::UINT16:
185: 			ConcatenateKeys<uint16_t>(allocator, input.data[i], input.size(), keys);
186: 			break;
187: 		case PhysicalType::UINT32:
188: 			ConcatenateKeys<uint32_t>(allocator, input.data[i], input.size(), keys);
189: 			break;
190: 		case PhysicalType::UINT64:
191: 			ConcatenateKeys<uint64_t>(allocator, input.data[i], input.size(), keys);
192: 			break;
193: 		case PhysicalType::FLOAT:
194: 			ConcatenateKeys<float>(allocator, input.data[i], input.size(), keys);
195: 			break;
196: 		case PhysicalType::DOUBLE:
197: 			ConcatenateKeys<double>(allocator, input.data[i], input.size(), keys);
198: 			break;
199: 		case PhysicalType::VARCHAR:
200: 			ConcatenateKeys<string_t>(allocator, input.data[i], input.size(), keys);
201: 			break;
202: 		default:
203: 			throw InternalException("Invalid type for index");
204: 		}
205: 	}
206: }
207: 
208: //===--------------------------------------------------------------------===//
209: // Insert
210: //===--------------------------------------------------------------------===//
211: 
212: struct KeySection {
213: 	KeySection(idx_t start_p, idx_t end_p, idx_t depth_p, data_t key_byte_p)
214: 	    : start(start_p), end(end_p), depth(depth_p), key_byte(key_byte_p) {};
215: 	KeySection(idx_t start_p, idx_t end_p, vector<Key> &keys, KeySection &key_section)
216: 	    : start(start_p), end(end_p), depth(key_section.depth + 1), key_byte(keys[end_p].data[key_section.depth]) {};
217: 	idx_t start;
218: 	idx_t end;
219: 	idx_t depth;
220: 	data_t key_byte;
221: };
222: 
223: void GetChildSections(vector<KeySection> &child_sections, vector<Key> &keys, KeySection &key_section) {
224: 
225: 	idx_t child_start_idx = key_section.start;
226: 	for (idx_t i = key_section.start + 1; i <= key_section.end; i++) {
227: 		if (keys[i - 1].data[key_section.depth] != keys[i].data[key_section.depth]) {
228: 			child_sections.emplace_back(child_start_idx, i - 1, keys, key_section);
229: 			child_start_idx = i;
230: 		}
231: 	}
232: 	child_sections.emplace_back(child_start_idx, key_section.end, keys, key_section);
233: }
234: 
235: void Construct(vector<Key> &keys, row_t *row_ids, Node *&node, KeySection &key_section, bool &has_constraint) {
236: 
237: 	D_ASSERT(key_section.start < keys.size());
238: 	D_ASSERT(key_section.end < keys.size());
239: 	D_ASSERT(key_section.start <= key_section.end);
240: 
241: 	auto &start_key = keys[key_section.start];
242: 	auto &end_key = keys[key_section.end];
243: 
244: 	// increment the depth until we reach a leaf or find a mismatching byte
245: 	auto prefix_start = key_section.depth;
246: 	while (start_key.len != key_section.depth && start_key.ByteMatches(end_key, key_section.depth)) {
247: 		key_section.depth++;
248: 	}
249: 
250: 	// we reached a leaf, i.e. all the bytes of start_key and end_key match
251: 	if (start_key.len == key_section.depth) {
252: 
253: 		// end_idx is inclusive
254: 		auto num_row_ids = key_section.end - key_section.start + 1;
255: 
256: 		// check for possible constraint violation
257: 		if (has_constraint && num_row_ids != 1) {
258: 			throw ConstraintException("New data contains duplicates on indexed column(s)");
259: 		}
260: 
261: 		// new row ids of this leaf
262: 		auto new_row_ids = unique_ptr<row_t[]>(new row_t[num_row_ids]);
263: 		for (idx_t i = 0; i < num_row_ids; i++) {
264: 			new_row_ids[i] = row_ids[key_section.start + i];
265: 		}
266: 
267: 		node = new Leaf(start_key, prefix_start, move(new_row_ids), num_row_ids);
268: 
269: 	} else { // create a new node and recurse
270: 
271: 		// we will find at least two child entries of this node, otherwise we'd have reached a leaf
272: 		vector<KeySection> child_sections;
273: 		GetChildSections(child_sections, keys, key_section);
274: 
275: 		auto node_type = Node::GetTypeBySize(child_sections.size());
276: 		Node::New(node_type, node);
277: 
278: 		auto prefix_length = key_section.depth - prefix_start;
279: 		node->prefix = Prefix(start_key, prefix_start, prefix_length);
280: 
281: 		// recurse on each child section
282: 		for (auto &child_section : child_sections) {
283: 			Node *new_child = nullptr;
284: 			Construct(keys, row_ids, new_child, child_section, has_constraint);
285: 			Node::InsertChild(node, child_section.key_byte, new_child);
286: 		}
287: 	}
288: }
289: 
290: void FindFirstNotNullKey(vector<Key> &keys, bool &skipped_all_nulls, idx_t &start_idx) {
291: 
292: 	if (!skipped_all_nulls) {
293: 		for (idx_t i = 0; i < keys.size(); i++) {
294: 			if (!keys[i].Empty()) {
295: 				start_idx = i;
296: 				skipped_all_nulls = true;
297: 				return;
298: 			}
299: 		}
300: 	}
301: }
302: 
303: void ART::ConstructAndMerge(IndexLock &lock, PayloadScanner &scanner, Allocator &allocator) {
304: 
305: 	auto payload_types = logical_types;
306: 	payload_types.emplace_back(LogicalType::ROW_TYPE);
307: 
308: 	ArenaAllocator arena_allocator(allocator);
309: 	vector<Key> keys(STANDARD_VECTOR_SIZE);
310: 
311: 	auto skipped_all_nulls = false;
312: 	auto temp_art = make_unique<ART>(this->column_ids, this->table_io_manager, this->unbound_expressions,
313: 	                                 this->constraint_type, this->db);
314: 
315: 	for (;;) {
316: 		DataChunk ordered_chunk;
317: 		ordered_chunk.Initialize(allocator, payload_types);
318: 		ordered_chunk.SetCardinality(0);
319: 		scanner.Scan(ordered_chunk);
320: 		if (ordered_chunk.size() == 0) {
321: 			break;
322: 		}
323: 
324: 		// get the key chunk and the row_identifiers vector
325: 		DataChunk row_id_chunk;
326: 		ordered_chunk.Split(row_id_chunk, ordered_chunk.ColumnCount() - 1);
327: 		auto &row_identifiers = row_id_chunk.data[0];
328: 
329: 		D_ASSERT(row_identifiers.GetType().InternalType() == ROW_TYPE);
330: 		D_ASSERT(logical_types[0] == ordered_chunk.data[0].GetType());
331: 
332: 		// generate the keys for the given input
333: 		arena_allocator.Reset();
334: 		GenerateKeys(arena_allocator, ordered_chunk, keys);
335: 
336: 		// we order NULLS FIRST, so we might have to skip nulls at the start of our sorted data
337: 		idx_t start_idx = 0;
338: 		FindFirstNotNullKey(keys, skipped_all_nulls, start_idx);
339: 
340: 		if (start_idx != 0 && IsPrimary()) {
341: 			throw ConstraintException("NULLs in new data violate the primary key constraint of the index");
342: 		}
343: 
344: 		if (!skipped_all_nulls) {
345: 			if (IsPrimary()) {
346: 				// chunk consists only of NULLs
347: 				throw ConstraintException("NULLs in new data violate the primary key constraint of the index");
348: 			}
349: 			continue;
350: 		}
351: 
352: 		// prepare the row_identifiers
353: 		row_identifiers.Flatten(ordered_chunk.size());
354: 		auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
355: 
356: 		// construct the ART of this chunk
357: 		auto art = make_unique<ART>(this->column_ids, this->table_io_manager, this->unbound_expressions,
358: 		                            this->constraint_type, this->db);
359: 		auto key_section = KeySection(start_idx, ordered_chunk.size() - 1, 0, 0);
360: 		auto has_constraint = IsUnique();
361: 		Construct(keys, row_ids, art->tree, key_section, has_constraint);
362: 
363: 		// merge art into temp_art
364: 		if (!temp_art->MergeIndexes(lock, art.get())) {
365: 			throw ConstraintException("Data contains duplicates on indexed column(s)");
366: 		}
367: 	}
368: 
369: 	// NOTE: currently this code is only used for index creation, so we can assume that there are no
370: 	// duplicate violations between the existing index and the new data,
371: 	// so we do not need to revert any changes
372: 	if (!this->MergeIndexes(lock, temp_art.get())) {
373: 		throw ConstraintException("Data contains duplicates on indexed column(s)");
374: 	}
375: }
376: 
377: bool ART::Insert(IndexLock &lock, DataChunk &input, Vector &row_ids) {
378: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
379: 	D_ASSERT(logical_types[0] == input.data[0].GetType());
380: 
381: 	// generate the keys for the given input
382: 	ArenaAllocator arena_allocator(Allocator::DefaultAllocator());
383: 	vector<Key> keys(input.size());
384: 	GenerateKeys(arena_allocator, input, keys);
385: 
386: 	// now insert the elements into the index
387: 	row_ids.Flatten(input.size());
388: 	auto row_identifiers = FlatVector::GetData<row_t>(row_ids);
389: 	idx_t failed_index = DConstants::INVALID_INDEX;
390: 	for (idx_t i = 0; i < input.size(); i++) {
391: 		if (keys[i].Empty()) {
392: 			continue;
393: 		}
394: 
395: 		row_t row_id = row_identifiers[i];
396: 		if (!Insert(tree, keys[i], 0, row_id)) {
397: 			// failed to insert because of constraint violation
398: 			failed_index = i;
399: 			break;
400: 		}
401: 	}
402: 	if (failed_index != DConstants::INVALID_INDEX) {
403: 
404: 		// failed to insert because of constraint violation: remove previously inserted entries
405: 		for (idx_t i = 0; i < failed_index; i++) {
406: 			if (keys[i].Empty()) {
407: 				continue;
408: 			}
409: 			row_t row_id = row_identifiers[i];
410: 			Erase(tree, keys[i], 0, row_id);
411: 		}
412: 		return false;
413: 	}
414: 	return true;
415: }
416: 
417: bool ART::Append(IndexLock &lock, DataChunk &appended_data, Vector &row_identifiers) {
418: 	DataChunk expression_result;
419: 	expression_result.Initialize(Allocator::DefaultAllocator(), logical_types);
420: 
421: 	// first resolve the expressions for the index
422: 	ExecuteExpressions(appended_data, expression_result);
423: 
424: 	// now insert into the index
425: 	return Insert(lock, expression_result, row_identifiers);
426: }
427: 
428: void ART::VerifyAppend(DataChunk &chunk) {
429: 	VerifyExistence(chunk, VerifyExistenceType::APPEND);
430: }
431: 
432: void ART::VerifyAppendForeignKey(DataChunk &chunk, string *err_msg_ptr) {
433: 	VerifyExistence(chunk, VerifyExistenceType::APPEND_FK, err_msg_ptr);
434: }
435: 
436: void ART::VerifyDeleteForeignKey(DataChunk &chunk, string *err_msg_ptr) {
437: 	VerifyExistence(chunk, VerifyExistenceType::DELETE_FK, err_msg_ptr);
438: }
439: 
440: bool ART::InsertToLeaf(Leaf &leaf, row_t row_id) {
441: #ifdef DEBUG
442: 	for (idx_t k = 0; k < leaf.count; k++) {
443: 		D_ASSERT(leaf.GetRowId(k) != row_id);
444: 	}
445: #endif
446: 	if (IsUnique() && leaf.count != 0) {
447: 		return false;
448: 	}
449: 	leaf.Insert(row_id);
450: 	return true;
451: }
452: 
453: bool ART::Insert(Node *&node, Key &key, idx_t depth, row_t row_id) {
454: 
455: 	if (!node) {
456: 		// node is currently empty, create a leaf here with the key
457: 		node = new Leaf(key, depth, row_id);
458: 		return true;
459: 	}
460: 
461: 	if (node->type == NodeType::NLeaf) {
462: 		// Replace leaf with Node4 and store both leaves in it
463: 		auto leaf = (Leaf *)node;
464: 
465: 		auto &leaf_prefix = leaf->prefix;
466: 		uint32_t new_prefix_length = 0;
467: 
468: 		// Leaf node is already there (its key matches the current key), update row_id vector
469: 		if (new_prefix_length == leaf->prefix.Size() && depth + leaf->prefix.Size() == key.len) {
470: 			return InsertToLeaf(*leaf, row_id);
471: 		}
472: 		while (leaf_prefix[new_prefix_length] == key[depth + new_prefix_length]) {
473: 			new_prefix_length++;
474: 			// Leaf node is already there (its key matches the current key), update row_id vector
475: 			if (new_prefix_length == leaf->prefix.Size() && depth + leaf->prefix.Size() == key.len) {
476: 				return InsertToLeaf(*leaf, row_id);
477: 			}
478: 		}
479: 
480: 		Node *new_node = new Node4();
481: 		new_node->prefix = Prefix(key, depth, new_prefix_length);
482: 		auto key_byte = node->prefix.Reduce(new_prefix_length);
483: 		Node4::InsertChild(new_node, key_byte, node);
484: 		Node *leaf_node = new Leaf(key, depth + new_prefix_length + 1, row_id);
485: 		Node4::InsertChild(new_node, key[depth + new_prefix_length], leaf_node);
486: 		node = new_node;
487: 		return true;
488: 	}
489: 
490: 	// Handle prefix of inner node
491: 	if (node->prefix.Size()) {
492: 		uint32_t mismatch_pos = node->prefix.KeyMismatchPosition(key, depth);
493: 		if (mismatch_pos != node->prefix.Size()) {
494: 			// Prefix differs, create new node
495: 			Node *new_node = new Node4();
496: 			new_node->prefix = Prefix(key, depth, mismatch_pos);
497: 			// Break up prefix
498: 			auto key_byte = node->prefix.Reduce(mismatch_pos);
499: 			Node4::InsertChild(new_node, key_byte, node);
500: 
501: 			Node *leaf_node = new Leaf(key, depth + mismatch_pos + 1, row_id);
502: 			Node4::InsertChild(new_node, key[depth + mismatch_pos], leaf_node);
503: 			node = new_node;
504: 			return true;
505: 		}
506: 		depth += node->prefix.Size();
507: 	}
508: 
509: 	// Recurse
510: 	D_ASSERT(depth < key.len);
511: 	idx_t pos = node->GetChildPos(key[depth]);
512: 	if (pos != DConstants::INVALID_INDEX) {
513: 		auto child = node->GetChild(*this, pos);
514: 		bool insertion_result = Insert(child, key, depth + 1, row_id);
515: 		node->ReplaceChildPointer(pos, child);
516: 		return insertion_result;
517: 	}
518: 	Node *new_node = new Leaf(key, depth + 1, row_id);
519: 	Node::InsertChild(node, key[depth], new_node);
520: 	return true;
521: }
522: 
523: //===--------------------------------------------------------------------===//
524: // Delete
525: //===--------------------------------------------------------------------===//
526: void ART::Delete(IndexLock &state, DataChunk &input, Vector &row_ids) {
527: 	DataChunk expression;
528: 	expression.Initialize(Allocator::DefaultAllocator(), logical_types);
529: 
530: 	// first resolve the expressions
531: 	ExecuteExpressions(input, expression);
532: 
533: 	// then generate the keys for the given input
534: 	ArenaAllocator arena_allocator(Allocator::DefaultAllocator());
535: 	vector<Key> keys(expression.size());
536: 	GenerateKeys(arena_allocator, expression, keys);
537: 
538: 	// now erase the elements from the database
539: 	row_ids.Flatten(input.size());
540: 	auto row_identifiers = FlatVector::GetData<row_t>(row_ids);
541: 
542: 	for (idx_t i = 0; i < input.size(); i++) {
543: 		if (keys[i].Empty()) {
544: 			continue;
545: 		}
546: 		Erase(tree, keys[i], 0, row_identifiers[i]);
547: #ifdef DEBUG
548: 		auto node = Lookup(tree, keys[i], 0);
549: 		if (node) {
550: 			auto leaf = static_cast<Leaf *>(node);
551: 			for (idx_t k = 0; k < leaf->count; k++) {
552: 				D_ASSERT(leaf->GetRowId(k) != row_identifiers[i]);
553: 			}
554: 		}
555: #endif
556: 	}
557: }
558: 
559: void ART::Erase(Node *&node, Key &key, idx_t depth, row_t row_id) {
560: 	if (!node) {
561: 		return;
562: 	}
563: 	// Delete a leaf from a tree
564: 	if (node->type == NodeType::NLeaf) {
565: 		// Make sure we have the right leaf
566: 		auto leaf = static_cast<Leaf *>(node);
567: 		leaf->Remove(row_id);
568: 		if (leaf->count == 0) {
569: 			delete node;
570: 			node = nullptr;
571: 		}
572: 
573: 		return;
574: 	}
575: 
576: 	// Handle prefix
577: 	if (node->prefix.Size()) {
578: 		if (node->prefix.KeyMismatchPosition(key, depth) != node->prefix.Size()) {
579: 			return;
580: 		}
581: 		depth += node->prefix.Size();
582: 	}
583: 	idx_t pos = node->GetChildPos(key[depth]);
584: 	if (pos != DConstants::INVALID_INDEX) {
585: 		auto child = node->GetChild(*this, pos);
586: 		D_ASSERT(child);
587: 
588: 		if (child->type == NodeType::NLeaf) {
589: 			// Leaf found, remove entry
590: 			auto leaf = (Leaf *)child;
591: 			leaf->Remove(row_id);
592: 			if (leaf->count == 0) {
593: 				// Leaf is empty, delete leaf, decrement node counter and maybe shrink node
594: 				Node::EraseChild(node, pos, *this);
595: 			}
596: 		} else {
597: 			// Recurse
598: 			Erase(child, key, depth + 1, row_id);
599: 			node->ReplaceChildPointer(pos, child);
600: 		}
601: 	}
602: }
603: 
604: //===--------------------------------------------------------------------===//
605: // Point Query
606: //===--------------------------------------------------------------------===//
607: static Key CreateKey(ArenaAllocator &allocator, PhysicalType type, Value &value) {
608: 	D_ASSERT(type == value.type().InternalType());
609: 	switch (type) {
610: 	case PhysicalType::BOOL:
611: 		return Key::CreateKey<bool>(allocator, value);
612: 	case PhysicalType::INT8:
613: 		return Key::CreateKey<int8_t>(allocator, value);
614: 	case PhysicalType::INT16:
615: 		return Key::CreateKey<int16_t>(allocator, value);
616: 	case PhysicalType::INT32:
617: 		return Key::CreateKey<int32_t>(allocator, value);
618: 	case PhysicalType::INT64:
619: 		return Key::CreateKey<int64_t>(allocator, value);
620: 	case PhysicalType::UINT8:
621: 		return Key::CreateKey<uint8_t>(allocator, value);
622: 	case PhysicalType::UINT16:
623: 		return Key::CreateKey<uint16_t>(allocator, value);
624: 	case PhysicalType::UINT32:
625: 		return Key::CreateKey<uint32_t>(allocator, value);
626: 	case PhysicalType::UINT64:
627: 		return Key::CreateKey<uint64_t>(allocator, value);
628: 	case PhysicalType::INT128:
629: 		return Key::CreateKey<hugeint_t>(allocator, value);
630: 	case PhysicalType::FLOAT:
631: 		return Key::CreateKey<float>(allocator, value);
632: 	case PhysicalType::DOUBLE:
633: 		return Key::CreateKey<double>(allocator, value);
634: 	case PhysicalType::VARCHAR:
635: 		return Key::CreateKey<string_t>(allocator, value);
636: 	default:
637: 		throw InternalException("Invalid type for index");
638: 	}
639: }
640: 
641: bool ART::SearchEqual(Key &key, idx_t max_count, vector<row_t> &result_ids) {
642: 
643: 	auto leaf = static_cast<Leaf *>(Lookup(tree, key, 0));
644: 	if (!leaf) {
645: 		return true;
646: 	}
647: 	if (leaf->count > max_count) {
648: 		return false;
649: 	}
650: 	for (idx_t i = 0; i < leaf->count; i++) {
651: 		row_t row_id = leaf->GetRowId(i);
652: 		result_ids.push_back(row_id);
653: 	}
654: 	return true;
655: }
656: 
657: void ART::SearchEqualJoinNoFetch(Key &key, idx_t &result_size) {
658: 
659: 	// we need to look for a leaf
660: 	auto leaf = Lookup(tree, key, 0);
661: 	if (!leaf) {
662: 		return;
663: 	}
664: 	result_size = leaf->count;
665: }
666: 
667: Leaf *ART::Lookup(Node *node, Key &key, idx_t depth) {
668: 	while (node) {
669: 		if (node->type == NodeType::NLeaf) {
670: 			auto leaf = (Leaf *)node;
671: 			auto &leaf_prefix = leaf->prefix;
672: 			//! Check leaf
673: 			for (idx_t i = 0; i < leaf->prefix.Size(); i++) {
674: 				if (leaf_prefix[i] != key[i + depth]) {
675: 					return nullptr;
676: 				}
677: 			}
678: 			return (Leaf *)node;
679: 		}
680: 		if (node->prefix.Size()) {
681: 			for (idx_t pos = 0; pos < node->prefix.Size(); pos++) {
682: 				if (key[depth + pos] != node->prefix[pos]) {
683: 					return nullptr;
684: 				}
685: 			}
686: 			depth += node->prefix.Size();
687: 		}
688: 		idx_t pos = node->GetChildPos(key[depth]);
689: 		if (pos == DConstants::INVALID_INDEX) {
690: 			return nullptr;
691: 		}
692: 		node = node->GetChild(*this, pos);
693: 		D_ASSERT(node);
694: 		depth++;
695: 	}
696: 	return nullptr;
697: }
698: 
699: //===--------------------------------------------------------------------===//
700: // Greater Than
701: // Returns: True (If found leaf >= key)
702: //          False (Otherwise)
703: //===--------------------------------------------------------------------===//
704: bool ART::SearchGreater(ARTIndexScanState *state, Key &key, bool inclusive, idx_t max_count,
705:                         vector<row_t> &result_ids) {
706: 
707: 	Iterator *it = &state->iterator;
708: 
709: 	// greater than scan: first set the iterator to the node at which we will start our scan by finding the lowest node
710: 	// that satisfies our requirement
711: 	if (!it->art) {
712: 		it->art = this;
713: 		bool found = it->LowerBound(tree, key, inclusive);
714: 		if (!found) {
715: 			return true;
716: 		}
717: 	}
718: 	// after that we continue the scan; we don't need to check the bounds as any value following this value is
719: 	// automatically bigger and hence satisfies our predicate
720: 	Key empty_key = Key();
721: 	return it->Scan(empty_key, max_count, result_ids, false);
722: }
723: 
724: //===--------------------------------------------------------------------===//
725: // Less Than
726: //===--------------------------------------------------------------------===//
727: bool ART::SearchLess(ARTIndexScanState *state, Key &upper_bound, bool inclusive, idx_t max_count,
728:                      vector<row_t> &result_ids) {
729: 
730: 	if (!tree) {
731: 		return true;
732: 	}
733: 
734: 	Iterator *it = &state->iterator;
735: 
736: 	if (!it->art) {
737: 		it->art = this;
738: 		// first find the minimum value in the ART: we start scanning from this value
739: 		it->FindMinimum(*tree);
740: 		// early out min value higher than upper bound query
741: 		if (it->cur_key > upper_bound) {
742: 			return true;
743: 		}
744: 	}
745: 	// now continue the scan until we reach the upper bound
746: 	return it->Scan(upper_bound, max_count, result_ids, inclusive);
747: }
748: 
749: //===--------------------------------------------------------------------===//
750: // Closed Range Query
751: //===--------------------------------------------------------------------===//
752: bool ART::SearchCloseRange(ARTIndexScanState *state, Key &lower_bound, Key &upper_bound, bool left_inclusive,
753:                            bool right_inclusive, idx_t max_count, vector<row_t> &result_ids) {
754: 
755: 	Iterator *it = &state->iterator;
756: 
757: 	// first find the first node that satisfies the left predicate
758: 	if (!it->art) {
759: 		it->art = this;
760: 		bool found = it->LowerBound(tree, lower_bound, left_inclusive);
761: 		if (!found) {
762: 			return true;
763: 		}
764: 	}
765: 	// now continue the scan until we reach the upper bound
766: 	return it->Scan(upper_bound, max_count, result_ids, right_inclusive);
767: }
768: 
769: bool ART::Scan(Transaction &transaction, DataTable &table, IndexScanState &table_state, idx_t max_count,
770:                vector<row_t> &result_ids) {
771: 
772: 	auto state = (ARTIndexScanState *)&table_state;
773: 	vector<row_t> row_ids;
774: 	bool success;
775: 
776: 	// FIXME: the key directly owning the data for a single key might be more efficient
777: 	D_ASSERT(state->values[0].type().InternalType() == types[0]);
778: 	ArenaAllocator arena_allocator(Allocator::DefaultAllocator());
779: 	auto key = CreateKey(arena_allocator, types[0], state->values[0]);
780: 
781: 	if (state->values[1].IsNull()) {
782: 
783: 		// single predicate
784: 		lock_guard<mutex> l(lock);
785: 		switch (state->expressions[0]) {
786: 		case ExpressionType::COMPARE_EQUAL:
787: 			success = SearchEqual(key, max_count, row_ids);
788: 			break;
789: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
790: 			success = SearchGreater(state, key, true, max_count, row_ids);
791: 			break;
792: 		case ExpressionType::COMPARE_GREATERTHAN:
793: 			success = SearchGreater(state, key, false, max_count, row_ids);
794: 			break;
795: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
796: 			success = SearchLess(state, key, true, max_count, row_ids);
797: 			break;
798: 		case ExpressionType::COMPARE_LESSTHAN:
799: 			success = SearchLess(state, key, false, max_count, row_ids);
800: 			break;
801: 		default:
802: 			throw InternalException("Operation not implemented");
803: 		}
804: 
805: 	} else {
806: 
807: 		// two predicates
808: 		lock_guard<mutex> l(lock);
809: 
810: 		D_ASSERT(state->values[1].type().InternalType() == types[0]);
811: 		auto upper_bound = CreateKey(arena_allocator, types[0], state->values[1]);
812: 
813: 		bool left_inclusive = state->expressions[0] == ExpressionType ::COMPARE_GREATERTHANOREQUALTO;
814: 		bool right_inclusive = state->expressions[1] == ExpressionType ::COMPARE_LESSTHANOREQUALTO;
815: 		success = SearchCloseRange(state, key, upper_bound, left_inclusive, right_inclusive, max_count, row_ids);
816: 	}
817: 
818: 	if (!success) {
819: 		return false;
820: 	}
821: 	if (row_ids.empty()) {
822: 		return true;
823: 	}
824: 
825: 	// sort the row ids
826: 	sort(row_ids.begin(), row_ids.end());
827: 	// duplicate eliminate the row ids and append them to the row ids of the state
828: 	result_ids.reserve(row_ids.size());
829: 
830: 	result_ids.push_back(row_ids[0]);
831: 	for (idx_t i = 1; i < row_ids.size(); i++) {
832: 		if (row_ids[i] != row_ids[i - 1]) {
833: 			result_ids.push_back(row_ids[i]);
834: 		}
835: 	}
836: 	return true;
837: }
838: 
839: void ART::VerifyExistence(DataChunk &chunk, VerifyExistenceType verify_type, string *err_msg_ptr) {
840: 	if (verify_type != VerifyExistenceType::DELETE_FK && !IsUnique()) {
841: 		return;
842: 	}
843: 
844: 	DataChunk expression_chunk;
845: 	expression_chunk.Initialize(Allocator::DefaultAllocator(), logical_types);
846: 
847: 	// unique index, check
848: 	lock_guard<mutex> l(lock);
849: 	// first resolve the expressions for the index
850: 	ExecuteExpressions(chunk, expression_chunk);
851: 
852: 	// generate the keys for the given input
853: 	ArenaAllocator arena_allocator(Allocator::DefaultAllocator());
854: 	vector<Key> keys(expression_chunk.size());
855: 	GenerateKeys(arena_allocator, expression_chunk, keys);
856: 
857: 	for (idx_t i = 0; i < chunk.size(); i++) {
858: 		if (keys[i].Empty()) {
859: 			continue;
860: 		}
861: 		Node *node_ptr = Lookup(tree, keys[i], 0);
862: 		bool throw_exception =
863: 		    verify_type == VerifyExistenceType::APPEND_FK ? node_ptr == nullptr : node_ptr != nullptr;
864: 		if (!throw_exception) {
865: 			continue;
866: 		}
867: 		string key_name;
868: 		for (idx_t k = 0; k < expression_chunk.ColumnCount(); k++) {
869: 			if (k > 0) {
870: 				key_name += ", ";
871: 			}
872: 			key_name += unbound_expressions[k]->GetName() + ": " + expression_chunk.data[k].GetValue(i).ToString();
873: 		}
874: 		string exception_msg;
875: 		switch (verify_type) {
876: 		case VerifyExistenceType::APPEND: {
877: 			// node already exists in tree
878: 			string type = IsPrimary() ? "primary key" : "unique";
879: 			exception_msg = "duplicate key \"" + key_name + "\" violates ";
880: 			exception_msg += type + " constraint";
881: 			break;
882: 		}
883: 		case VerifyExistenceType::APPEND_FK: {
884: 			// found node no exists in tree
885: 			exception_msg =
886: 			    "violates foreign key constraint because key \"" + key_name + "\" does not exist in referenced table";
887: 			break;
888: 		}
889: 		case VerifyExistenceType::DELETE_FK: {
890: 			// found node exists in tree
891: 			exception_msg =
892: 			    "violates foreign key constraint because key \"" + key_name + "\" exist in table has foreign key";
893: 			break;
894: 		}
895: 		}
896: 		if (err_msg_ptr) {
897: 			err_msg_ptr[i] = exception_msg;
898: 		} else {
899: 			throw ConstraintException(exception_msg);
900: 		}
901: 	}
902: }
903: 
904: //===--------------------------------------------------------------------===//
905: // Serialization
906: //===--------------------------------------------------------------------===//
907: BlockPointer ART::Serialize(duckdb::MetaBlockWriter &writer) {
908: 	lock_guard<mutex> l(lock);
909: 	if (tree) {
910: 		serialized_data_pointer = tree->Serialize(*this, writer);
911: 	} else {
912: 		serialized_data_pointer = {(block_id_t)DConstants::INVALID_INDEX, (uint32_t)DConstants::INVALID_INDEX};
913: 	}
914: 	return serialized_data_pointer;
915: }
916: 
917: //===--------------------------------------------------------------------===//
918: // Merge ARTs
919: //===--------------------------------------------------------------------===//
920: bool ART::MergeIndexes(IndexLock &state, Index *other_index) {
921: 
922: 	auto other_art = (ART *)other_index;
923: 
924: 	if (!this->tree) {
925: 		this->tree = other_art->tree;
926: 		other_art->tree = nullptr;
927: 		return true;
928: 	}
929: 
930: 	return Node::MergeARTs(this, other_art);
931: }
932: 
933: string ART::ToString() {
934: 	if (tree) {
935: 		return tree->ToString(*this);
936: 	}
937: 	return "[empty]";
938: }
939: 
940: } // namespace duckdb
[end of src/execution/index/art/art.cpp]
[start of src/execution/physical_plan/plan_create_index.cpp]
1: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
2: #include "duckdb/execution/operator/schema/physical_create_index.hpp"
3: #include "duckdb/execution/physical_plan_generator.hpp"
4: #include "duckdb/planner/operator/logical_create_index.hpp"
5: #include "duckdb/execution/operator/scan/physical_table_scan.hpp"
6: #include "duckdb/function/table/table_scan.hpp"
7: 
8: namespace duckdb {
9: 
10: unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalCreateIndex &op) {
11: 
12: 	D_ASSERT(op.children.empty());
13: 
14: 	unique_ptr<TableFilterSet> table_filters;
15: 	op.info->column_ids.emplace_back(COLUMN_IDENTIFIER_ROW_ID);
16: 
17: 	auto &bind_data = (TableScanBindData &)*op.bind_data;
18: 	bind_data.is_create_index = true;
19: 	auto table_scan =
20: 	    make_unique<PhysicalTableScan>(op.info->scan_types, op.function, move(op.bind_data), op.info->column_ids,
21: 	                                   op.info->names, move(table_filters), op.estimated_cardinality);
22: 
23: 	dependencies.insert(&op.table);
24: 	op.info->column_ids.pop_back();
25: 
26: 	auto physical_create_index =
27: 	    make_unique<PhysicalCreateIndex>(op, op.table, op.info->column_ids, move(op.expressions), move(op.info),
28: 	                                     move(op.unbound_expressions), op.estimated_cardinality);
29: 	physical_create_index->children.push_back(move(table_scan));
30: 	return move(physical_create_index);
31: }
32: 
33: } // namespace duckdb
[end of src/execution/physical_plan/plan_create_index.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: