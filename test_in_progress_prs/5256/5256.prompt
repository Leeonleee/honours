You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Creating table from pandas dataframe produces incorrect results
### What happens?

I create a table from a pandas DataFrame. When I select the data back out, the data is different from the input.

### To Reproduce

I've attached the `example.pkl` file inside a zip. 

[example.pkl.zip](https://github.com/duckdb/duckdb/files/9962969/example.pkl.zip)

```
import duckdb
import pyarrow as pa
import pandas as pd
import pickle

con = duckdb.connect()
with open('example.pkl', 'rb') as f:
    in_df = pickle.load(f)
# Replacing the load from pandas with a load from pyarrow produces correct results.
# in_tbl = pa.Table.from_pandas(in_df)
# con.execute('create table tiles as select * from in_tbl')
con.execute('create table tiles as select * from in_df')
out = con.execute('select * from tiles limit 10000').fetch_df()
print(in_df['alpha0'])
print(out['alpha0'])
```

Output:
```
0    0.011102
1    0.011101
2    0.011111
3    0.011123
4    0.011102
5    0.011101
6    0.011111
7    0.011123
8    0.011139
9    0.011137
Name: alpha0, dtype: float32
0    0.011102
1    0.102584
2    0.114941
3    0.104812
4    0.099318
5    0.104231
6    0.099771
7    0.106665
8    0.103630
9    0.100998
Name: alpha0, dtype: float32
```

Also note that saving the dataframe to CSV and then reloading it also fixes the issue. So, I'm guessing this is an edge case where the pandas data is stored 

Thank you!!

### OS:

Mac OS Ventura, M1

### DuckDB Version:

0.5.1

### DuckDB Client:

Python

### Full Name:

Ben Thompson

### Affiliation:

Confirm Solutions

### Have you tried this on the latest `master` branch?

- [X] I agree

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] I agree

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
[end of README.md]
[start of tools/pythonpkg/src/vector_conversion.cpp]
1: #include "duckdb_python/pyrelation.hpp"
2: #include "duckdb_python/pyconnection.hpp"
3: #include "duckdb_python/pyresult.hpp"
4: #include "duckdb_python/vector_conversion.hpp"
5: #include "duckdb_python/python_conversion.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/common/types/timestamp.hpp"
8: #include "utf8proc_wrapper.hpp"
9: #include "duckdb/common/case_insensitive_map.hpp"
10: #include "duckdb_python/pandas_type.hpp"
11: #include "duckdb_python/pandas_analyzer.hpp"
12: #include "duckdb_python/pandas_type.hpp"
13: #include "duckdb/function/scalar/nested_functions.hpp"
14: 
15: namespace duckdb {
16: 
17: template <class T>
18: void ScanPandasColumn(py::array &numpy_col, idx_t stride, idx_t offset, Vector &out, idx_t count) {
19: 	auto src_ptr = (T *)numpy_col.data();
20: 	if (stride == sizeof(T)) {
21: 		FlatVector::SetData(out, (data_ptr_t)(src_ptr + offset));
22: 	} else {
23: 		auto tgt_ptr = (T *)FlatVector::GetData(out);
24: 		for (idx_t i = 0; i < count; i++) {
25: 			tgt_ptr[i] = src_ptr[stride / sizeof(T) * (i + offset)];
26: 		}
27: 	}
28: }
29: 
30: template <class T, class V>
31: void ScanPandasCategoryTemplated(py::array &column, idx_t offset, Vector &out, idx_t count) {
32: 	auto src_ptr = (T *)column.data();
33: 	auto tgt_ptr = (V *)FlatVector::GetData(out);
34: 	auto &tgt_mask = FlatVector::Validity(out);
35: 	for (idx_t i = 0; i < count; i++) {
36: 		if (src_ptr[i + offset] == -1) {
37: 			// Null value
38: 			tgt_mask.SetInvalid(i);
39: 		} else {
40: 			tgt_ptr[i] = src_ptr[i + offset];
41: 		}
42: 	}
43: }
44: 
45: template <class T>
46: void ScanPandasCategory(py::array &column, idx_t count, idx_t offset, Vector &out, string &src_type) {
47: 	if (src_type == "int8") {
48: 		ScanPandasCategoryTemplated<int8_t, T>(column, offset, out, count);
49: 	} else if (src_type == "int16") {
50: 		ScanPandasCategoryTemplated<int16_t, T>(column, offset, out, count);
51: 	} else if (src_type == "int32") {
52: 		ScanPandasCategoryTemplated<int32_t, T>(column, offset, out, count);
53: 	} else {
54: 		throw NotImplementedException("The Pandas type " + src_type + " for categorical types is not implemented yet");
55: 	}
56: }
57: 
58: template <class T>
59: void ScanPandasMasked(PandasColumnBindData &bind_data, idx_t count, idx_t offset, Vector &out) {
60: 	ScanPandasColumn<T>(bind_data.numpy_col, bind_data.numpy_stride, offset, out, count);
61: 	auto &result_mask = FlatVector::Validity(out);
62: 	if (bind_data.mask) {
63: 		auto mask = (bool *)bind_data.mask->numpy_array.data();
64: 		for (idx_t i = 0; i < count; i++) {
65: 			auto is_null = mask[offset + i];
66: 			if (is_null) {
67: 				result_mask.SetInvalid(i);
68: 			}
69: 		}
70: 	}
71: }
72: 
73: template <class T>
74: bool ValueIsNull(T value) {
75: 	throw InvalidInputException("Unsupported type for ValueIsNull");
76: }
77: 
78: template <>
79: bool ValueIsNull(float value) {
80: 	return !Value::FloatIsFinite(value);
81: }
82: 
83: template <>
84: bool ValueIsNull(double value) {
85: 	return !Value::DoubleIsFinite(value);
86: }
87: 
88: template <class T>
89: void ScanPandasFpColumn(T *src_ptr, idx_t count, idx_t offset, Vector &out) {
90: 	FlatVector::SetData(out, (data_ptr_t)(src_ptr + offset));
91: 	auto tgt_ptr = FlatVector::GetData<T>(out);
92: 	auto &mask = FlatVector::Validity(out);
93: 	for (idx_t i = 0; i < count; i++) {
94: 		if (Value::IsNan<T>(tgt_ptr[i])) {
95: 			mask.SetInvalid(i);
96: 		}
97: 	}
98: }
99: 
100: template <class T>
101: static string_t DecodePythonUnicode(T *codepoints, idx_t codepoint_count, Vector &out) {
102: 	// first figure out how many bytes to allocate
103: 	idx_t utf8_length = 0;
104: 	for (idx_t i = 0; i < codepoint_count; i++) {
105: 		int len = Utf8Proc::CodepointLength(int(codepoints[i]));
106: 		D_ASSERT(len >= 1);
107: 		utf8_length += len;
108: 	}
109: 	int sz;
110: 	auto result = StringVector::EmptyString(out, utf8_length);
111: 	auto target = result.GetDataWriteable();
112: 	for (idx_t i = 0; i < codepoint_count; i++) {
113: 		Utf8Proc::CodepointToUtf8(int(codepoints[i]), sz, target);
114: 		D_ASSERT(sz >= 1);
115: 		target += sz;
116: 	}
117: 	result.Finalize();
118: 	return result;
119: }
120: 
121: template <typename T>
122: bool TryCast(const py::object stuf, T &value) {
123: 	try {
124: 		value = stuf.cast<T>();
125: 		return true;
126: 	} catch (py::cast_error &e) {
127: 		return false;
128: 	}
129: }
130: 
131: template <typename T>
132: T Cast(const py::object obj) {
133: 	return obj.cast<T>();
134: }
135: 
136: static void SetInvalidRecursive(Vector &out, idx_t index) {
137: 	auto &validity = FlatVector::Validity(out);
138: 	validity.SetInvalid(index);
139: 	if (out.GetType().InternalType() == PhysicalType::STRUCT) {
140: 		auto &children = StructVector::GetEntries(out);
141: 		for (idx_t i = 0; i < children.size(); i++) {
142: 			SetInvalidRecursive(*children[i], index);
143: 		}
144: 	}
145: }
146: 
147: //! 'count' is the amount of rows in the 'out' vector
148: //! 'offset' is the current row number within this vector
149: void ScanPandasObject(PandasColumnBindData &bind_data, PyObject *object, idx_t offset, Vector &out) {
150: 
151: 	// handle None
152: 	if (object == Py_None) {
153: 		SetInvalidRecursive(out, offset);
154: 		return;
155: 	}
156: 
157: 	auto val = TransformPythonValue(object, out.GetType());
158: 	// Check if the Value type is accepted for the LogicalType of Vector
159: 	out.SetValue(offset, val);
160: }
161: 
162: static void VerifyMapConstraints(Vector &vec, idx_t count) {
163: 	auto invalid_reason = CheckMapValidity(vec, count);
164: 	switch (invalid_reason) {
165: 	case MapInvalidReason::VALID:
166: 		return;
167: 	case MapInvalidReason::DUPLICATE_KEY:
168: 		throw InvalidInputException("Dict->Map conversion failed because 'key' list contains duplicates");
169: 	case MapInvalidReason::NULL_KEY_LIST:
170: 		throw InvalidInputException("Dict->Map conversion failed because 'key' list is None");
171: 	case MapInvalidReason::NULL_KEY:
172: 		throw InvalidInputException("Dict->Map conversion failed because 'key' list contains None");
173: 	default:
174: 		throw InvalidInputException("Option not implemented for MapInvalidReason");
175: 	}
176: }
177: 
178: void VerifyTypeConstraints(Vector &vec, idx_t count) {
179: 	switch (vec.GetType().id()) {
180: 	case LogicalTypeId::MAP: {
181: 		VerifyMapConstraints(vec, count);
182: 		break;
183: 	}
184: 	default:
185: 		return;
186: 	}
187: }
188: 
189: void ScanPandasObjectColumn(PandasColumnBindData &bind_data, PyObject **col, idx_t count, idx_t offset, Vector &out) {
190: 	// numpy_col is a sequential list of objects, that make up one "column" (Vector)
191: 	out.SetVectorType(VectorType::FLAT_VECTOR);
192: 	auto gil = make_unique<PythonGILWrapper>(); // We're creating python objects here, so we need the GIL
193: 
194: 	for (idx_t i = 0; i < count; i++) {
195: 		idx_t source_idx = offset + i;
196: 		ScanPandasObject(bind_data, col[source_idx], i, out);
197: 	}
198: 	gil.reset();
199: 	VerifyTypeConstraints(out, count);
200: }
201: 
202: //! 'offset' is the offset within the column
203: //! 'count' is the amount of values we will convert in this batch
204: void VectorConversion::NumpyToDuckDB(PandasColumnBindData &bind_data, py::array &numpy_col, idx_t count, idx_t offset,
205:                                      Vector &out) {
206: 	switch (bind_data.pandas_type) {
207: 	case PandasType::BOOL:
208: 		ScanPandasMasked<bool>(bind_data, count, offset, out);
209: 		break;
210: 	case PandasType::UINT_8:
211: 		ScanPandasMasked<uint8_t>(bind_data, count, offset, out);
212: 		break;
213: 	case PandasType::UINT_16:
214: 		ScanPandasMasked<uint16_t>(bind_data, count, offset, out);
215: 		break;
216: 	case PandasType::UINT_32:
217: 		ScanPandasMasked<uint32_t>(bind_data, count, offset, out);
218: 		break;
219: 	case PandasType::UINT_64:
220: 		ScanPandasMasked<uint64_t>(bind_data, count, offset, out);
221: 		break;
222: 	case PandasType::INT_8:
223: 		ScanPandasMasked<int8_t>(bind_data, count, offset, out);
224: 		break;
225: 	case PandasType::INT_16:
226: 		ScanPandasMasked<int16_t>(bind_data, count, offset, out);
227: 		break;
228: 	case PandasType::INT_32:
229: 		ScanPandasMasked<int32_t>(bind_data, count, offset, out);
230: 		break;
231: 	case PandasType::INT_64:
232: 		ScanPandasMasked<int64_t>(bind_data, count, offset, out);
233: 		break;
234: 	case PandasType::FLOAT_32:
235: 		ScanPandasFpColumn<float>((float *)numpy_col.data(), count, offset, out);
236: 		break;
237: 	case PandasType::FLOAT_64:
238: 		ScanPandasFpColumn<double>((double *)numpy_col.data(), count, offset, out);
239: 		break;
240: 	case PandasType::DATETIME:
241: 	case PandasType::DATETIME_TZ: {
242: 		auto src_ptr = (int64_t *)numpy_col.data();
243: 		auto tgt_ptr = FlatVector::GetData<timestamp_t>(out);
244: 		auto &mask = FlatVector::Validity(out);
245: 
246: 		for (idx_t row = 0; row < count; row++) {
247: 			auto source_idx = offset + row;
248: 			if (src_ptr[source_idx] <= NumericLimits<int64_t>::Minimum()) {
249: 				// pandas Not a Time (NaT)
250: 				mask.SetInvalid(row);
251: 				continue;
252: 			}
253: 			tgt_ptr[row] = Timestamp::FromEpochNanoSeconds(src_ptr[source_idx]);
254: 		}
255: 		break;
256: 	}
257: 	case PandasType::TIMEDELTA: {
258: 		auto src_ptr = (int64_t *)numpy_col.data();
259: 		auto tgt_ptr = FlatVector::GetData<interval_t>(out);
260: 		auto &mask = FlatVector::Validity(out);
261: 
262: 		for (idx_t row = 0; row < count; row++) {
263: 			auto source_idx = offset + row;
264: 			if (src_ptr[source_idx] <= NumericLimits<int64_t>::Minimum()) {
265: 				// pandas Not a Time (NaT)
266: 				mask.SetInvalid(row);
267: 				continue;
268: 			}
269: 			int64_t micro = src_ptr[source_idx] / 1000;
270: 			int64_t days = micro / Interval::MICROS_PER_DAY;
271: 			micro = micro % Interval::MICROS_PER_DAY;
272: 			int64_t months = days / Interval::DAYS_PER_MONTH;
273: 			days = days % Interval::DAYS_PER_MONTH;
274: 			interval_t interval;
275: 			interval.months = months;
276: 			interval.days = days;
277: 			interval.micros = micro;
278: 			tgt_ptr[row] = interval;
279: 		}
280: 		break;
281: 	}
282: 	case PandasType::OBJECT: {
283: 		//! We have determined the underlying logical type of this object column
284: 		// Get the source pointer of the numpy array
285: 		auto src_ptr = (PyObject **)numpy_col.data();
286: 		if (out.GetType().id() != LogicalTypeId::VARCHAR) {
287: 			return ScanPandasObjectColumn(bind_data, src_ptr, count, offset, out);
288: 		}
289: 
290: 		// Get the data pointer and the validity mask of the result vector
291: 		auto tgt_ptr = FlatVector::GetData<string_t>(out);
292: 		auto &out_mask = FlatVector::Validity(out);
293: 		unique_ptr<PythonGILWrapper> gil;
294: 
295: 		// Loop over every row of the arrays contents
296: 		for (idx_t row = 0; row < count; row++) {
297: 			auto source_idx = offset + row;
298: 
299: 			// Get the pointer to the object
300: 			PyObject *val = src_ptr[source_idx];
301: 			if (bind_data.pandas_type == PandasType::OBJECT && !PyUnicode_CheckExact(val)) {
302: 				if (val == Py_None) {
303: 					out_mask.SetInvalid(row);
304: 					continue;
305: 				}
306: 				if (py::isinstance<py::float_>(val) && std::isnan(PyFloat_AsDouble(val))) {
307: 					out_mask.SetInvalid(row);
308: 					continue;
309: 				}
310: 				if (!py::isinstance<py::str>(val)) {
311: 					if (!gil) {
312: 						gil = bind_data.object_str_val.GetLock();
313: 					}
314: 					bind_data.object_str_val.AssignInternal<PyObject>(
315: 					    [](py::str &obj, PyObject &new_val) {
316: 						    py::handle object_handle = &new_val;
317: 						    obj = py::str(object_handle);
318: 					    },
319: 					    *val, *gil);
320: 					val = (PyObject *)bind_data.object_str_val.GetPointerTop()->ptr();
321: 				}
322: 			}
323: 			// Python 3 string representation:
324: 			// https://github.com/python/cpython/blob/3a8fdb28794b2f19f6c8464378fb8b46bce1f5f4/Include/cpython/unicodeobject.h#L79
325: 			if (!PyUnicode_CheckExact(val)) {
326: 				out_mask.SetInvalid(row);
327: 				continue;
328: 			}
329: 			if (PyUnicode_IS_COMPACT_ASCII(val)) {
330: 				// ascii string: we can zero copy
331: 				tgt_ptr[row] = string_t((const char *)PyUnicode_DATA(val), PyUnicode_GET_LENGTH(val));
332: 			} else {
333: 				// unicode gunk
334: 				auto ascii_obj = (PyASCIIObject *)val;
335: 				auto unicode_obj = (PyCompactUnicodeObject *)val;
336: 				// compact unicode string: is there utf8 data available?
337: 				if (unicode_obj->utf8) {
338: 					// there is! zero copy
339: 					tgt_ptr[row] = string_t((const char *)unicode_obj->utf8, unicode_obj->utf8_length);
340: 				} else if (PyUnicode_IS_COMPACT(unicode_obj) && !PyUnicode_IS_ASCII(unicode_obj)) {
341: 					auto kind = PyUnicode_KIND(val);
342: 					switch (kind) {
343: 					case PyUnicode_1BYTE_KIND:
344: 						tgt_ptr[row] =
345: 						    DecodePythonUnicode<Py_UCS1>(PyUnicode_1BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);
346: 						break;
347: 					case PyUnicode_2BYTE_KIND:
348: 						tgt_ptr[row] =
349: 						    DecodePythonUnicode<Py_UCS2>(PyUnicode_2BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);
350: 						break;
351: 					case PyUnicode_4BYTE_KIND:
352: 						tgt_ptr[row] =
353: 						    DecodePythonUnicode<Py_UCS4>(PyUnicode_4BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);
354: 						break;
355: 					default:
356: 						throw NotImplementedException(
357: 						    "Unsupported typekind constant %d for Python Unicode Compact decode", kind);
358: 					}
359: 				} else if (ascii_obj->state.kind == PyUnicode_WCHAR_KIND) {
360: 					throw InvalidInputException("Unsupported: decode not ready legacy string");
361: 				} else if (!PyUnicode_IS_COMPACT(unicode_obj) && ascii_obj->state.kind != PyUnicode_WCHAR_KIND) {
362: 					throw InvalidInputException("Unsupported: decode ready legacy string");
363: 				} else {
364: 					throw InvalidInputException("Unsupported string type: no clue what this string is");
365: 				}
366: 			}
367: 		}
368: 		break;
369: 	}
370: 	case PandasType::CATEGORY: {
371: 		switch (out.GetType().InternalType()) {
372: 		case PhysicalType::UINT8:
373: 			ScanPandasCategory<uint8_t>(numpy_col, count, offset, out, bind_data.internal_categorical_type);
374: 			break;
375: 		case PhysicalType::UINT16:
376: 			ScanPandasCategory<uint16_t>(numpy_col, count, offset, out, bind_data.internal_categorical_type);
377: 			break;
378: 		case PhysicalType::UINT32:
379: 			ScanPandasCategory<uint32_t>(numpy_col, count, offset, out, bind_data.internal_categorical_type);
380: 			break;
381: 		default:
382: 			throw InternalException("Invalid Physical Type for ENUMs");
383: 		}
384: 		break;
385: 	}
386: 
387: 	default:
388: 		throw NotImplementedException("Unsupported pandas type");
389: 	}
390: }
391: 
392: void VectorConversion::BindPandas(const DBConfig &config, py::handle df, vector<PandasColumnBindData> &bind_columns,
393:                                   vector<LogicalType> &return_types, vector<string> &names) {
394: 	// This performs a shallow copy that allows us to rename the dataframe
395: 	auto df_columns = py::list(df.attr("columns"));
396: 	auto df_types = py::list(df.attr("dtypes"));
397: 	auto get_fun = df.attr("__getitem__");
398: 	// TODO support masked arrays as well
399: 	// TODO support dicts of numpy arrays as well
400: 	if (py::len(df_columns) == 0 || py::len(df_types) == 0 || py::len(df_columns) != py::len(df_types)) {
401: 		throw InvalidInputException("Need a DataFrame with at least one column");
402: 	}
403: 	py::array column_attributes = df.attr("columns").attr("values");
404: 
405: 	// loop over every column
406: 	for (idx_t col_idx = 0; col_idx < py::len(df_columns); col_idx++) {
407: 		LogicalType duckdb_col_type;
408: 		PandasColumnBindData bind_data;
409: 
410: 		names.emplace_back(py::str(df_columns[col_idx]));
411: 		bind_data.pandas_type = ConvertPandasType(df_types[col_idx]);
412: 		bool column_has_mask = py::hasattr(get_fun(df_columns[col_idx]).attr("array"), "_mask");
413: 
414: 		if (column_has_mask) {
415: 			// masked object, fetch the internal data and mask array
416: 			bind_data.mask = make_unique<NumPyArrayWrapper>(get_fun(df_columns[col_idx]).attr("array").attr("_mask"));
417: 		}
418: 
419: 		auto column = get_fun(df_columns[col_idx]);
420: 		if (bind_data.pandas_type == PandasType::CATEGORY) {
421: 			// for category types, we create an ENUM type for string or use the converted numpy type for the rest
422: 			D_ASSERT(py::hasattr(column, "cat"));
423: 			D_ASSERT(py::hasattr(column.attr("cat"), "categories"));
424: 			auto categories = py::array(column.attr("cat").attr("categories"));
425: 			auto categories_pd_type = ConvertPandasType(categories.attr("dtype"));
426: 			if (categories_pd_type == PandasType::OBJECT) {
427: 				// Let's hope the object type is a string.
428: 				bind_data.pandas_type = PandasType::CATEGORY;
429: 				auto enum_name = string(py::str(df_columns[col_idx]));
430: 				vector<string> enum_entries = py::cast<vector<string>>(categories);
431: 				idx_t size = enum_entries.size();
432: 				Vector enum_entries_vec(LogicalType::VARCHAR, size);
433: 				auto enum_entries_ptr = FlatVector::GetData<string_t>(enum_entries_vec);
434: 				for (idx_t i = 0; i < size; i++) {
435: 					enum_entries_ptr[i] = StringVector::AddStringOrBlob(enum_entries_vec, enum_entries[i]);
436: 				}
437: 				D_ASSERT(py::hasattr(column.attr("cat"), "codes"));
438: 				duckdb_col_type = LogicalType::ENUM(enum_name, enum_entries_vec, size);
439: 				bind_data.numpy_col = py::array(column.attr("cat").attr("codes"));
440: 				D_ASSERT(py::hasattr(bind_data.numpy_col, "dtype"));
441: 				bind_data.internal_categorical_type = string(py::str(bind_data.numpy_col.attr("dtype")));
442: 			} else {
443: 				bind_data.numpy_col = py::array(column.attr("to_numpy")());
444: 				auto numpy_type = bind_data.numpy_col.attr("dtype");
445: 				// for category types (non-strings), we use the converted numpy type
446: 				bind_data.pandas_type = ConvertPandasType(numpy_type);
447: 				duckdb_col_type = PandasToLogicalType(bind_data.pandas_type);
448: 			}
449: 		} else {
450: 			auto pandas_array = get_fun(df_columns[col_idx]).attr("array");
451: 			if (py::hasattr(pandas_array, "_data")) {
452: 				// This means we can access the numpy array directly
453: 				bind_data.numpy_col = get_fun(df_columns[col_idx]).attr("array").attr("_data");
454: 			} else {
455: 				// Otherwise we have to get it through 'to_numpy()'
456: 				bind_data.numpy_col = py::array(column.attr("to_numpy")());
457: 			}
458: 			duckdb_col_type = PandasToLogicalType(bind_data.pandas_type);
459: 		}
460: 		// Analyze the inner data type of the 'object' column
461: 		if (bind_data.pandas_type == PandasType::OBJECT) {
462: 			PandasAnalyzer analyzer(config);
463: 			if (analyzer.Analyze(get_fun(df_columns[col_idx]))) {
464: 				duckdb_col_type = analyzer.AnalyzedType();
465: 			}
466: 		}
467: 
468: 		D_ASSERT(py::hasattr(bind_data.numpy_col, "strides"));
469: 		bind_data.numpy_stride = bind_data.numpy_col.attr("strides").attr("__getitem__")(0).cast<idx_t>();
470: 		return_types.push_back(duckdb_col_type);
471: 		bind_columns.push_back(move(bind_data));
472: 	}
473: }
474: 
475: } // namespace duckdb
[end of tools/pythonpkg/src/vector_conversion.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: